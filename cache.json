{"2024-12-16T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.12096v1","updated":"2024-12-16T18:59:45Z","published":"2024-12-16T18:59:45Z","title":"PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting","summary":"  With the advent of portable 360{\\deg} cameras, panorama has gained\nsignificant attention in applications like virtual reality (VR), virtual tours,\nrobotics, and autonomous driving. As a result, wide-baseline panorama view\nsynthesis has emerged as a vital task, where high resolution, fast inference,\nand memory efficiency are essential. Nevertheless, existing methods are\ntypically constrained to lower resolutions (512 $\\times$ 1024) due to demanding\nmemory and computational requirements. In this paper, we present PanSplat, a\ngeneralizable, feed-forward approach that efficiently supports resolution up to\n4K (2048 $\\times$ 4096). Our approach features a tailored spherical 3D Gaussian\npyramid with a Fibonacci lattice arrangement, enhancing image quality while\nreducing information redundancy. To accommodate the demands of high resolution,\nwe propose a pipeline that integrates a hierarchical spherical cost volume and\nGaussian heads with local operations, enabling two-step deferred\nbackpropagation for memory-efficient training on a single A100 GPU. Experiments\ndemonstrate that PanSplat achieves state-of-the-art results with superior\nefficiency and image quality across both synthetic and real-world datasets.\nCode will be available at \\url{https://github.com/chengzhag/PanSplat}.\n","authors":["Cheng Zhang","Haofei Xu","Qianyi Wu","Camilo Cruz Gambardella","Dinh Phung","Jianfei Cai"],"pdf_url":"https://arxiv.org/pdf/2412.12096v1.pdf","comment":"Project Page: https://chengzhag.github.io/publication/pansplat/ Code:\n  https://github.com/chengzhag/PanSplat"},{"id":"http://arxiv.org/abs/2412.12095v1","updated":"2024-12-16T18:59:29Z","published":"2024-12-16T18:59:29Z","title":"Causal Diffusion Transformers for Generative Modeling","summary":"  We introduce Causal Diffusion as the autoregressive (AR) counterpart of\nDiffusion models. It is a next-token(s) forecasting framework that is friendly\nto both discrete and continuous modalities and compatible with existing\nnext-token prediction models like LLaMA and GPT. While recent works attempt to\ncombine diffusion with AR models, we show that introducing sequential\nfactorization to a diffusion model can substantially improve its performance\nand enables a smooth transition between AR and diffusion generation modes.\nHence, we propose CausalFusion - a decoder-only transformer that\ndual-factorizes data across sequential tokens and diffusion noise levels,\nleading to state-of-the-art results on the ImageNet generation benchmark while\nalso enjoying the AR advantage of generating an arbitrary number of tokens for\nin-context reasoning. We further demonstrate CausalFusion's multimodal\ncapabilities through a joint image generation and captioning model, and\nshowcase CausalFusion's ability for zero-shot in-context image manipulations.\nWe hope that this work could provide the community with a fresh perspective on\ntraining multimodal models over discrete and continuous data.\n","authors":["Chaorui Deng","Deyao Zh","Kunchang Li","Shi Guan","Haoqi Fan"],"pdf_url":"https://arxiv.org/pdf/2412.12095v1.pdf","comment":"21 pages, 22 figures"},{"id":"http://arxiv.org/abs/2412.12093v1","updated":"2024-12-16T18:58:51Z","published":"2024-12-16T18:58:51Z","title":"CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View\n  Diffusion Models","summary":"  Reconstructing photorealistic and dynamic portrait avatars from images is\nessential to many applications including advertising, visual effects, and\nvirtual reality. Depending on the application, avatar reconstruction involves\ndifferent capture setups and constraints $-$ for example, visual effects\nstudios use camera arrays to capture hundreds of reference images, while\ncontent creators may seek to animate a single portrait image downloaded from\nthe internet. As such, there is a large and heterogeneous ecosystem of methods\nfor avatar reconstruction. Techniques based on multi-view stereo or neural\nrendering achieve the highest quality results, but require hundreds of\nreference images. Recent generative models produce convincing avatars from a\nsingle reference image, but visual fidelity yet lags behind multi-view\ntechniques. Here, we present CAP4D: an approach that uses a morphable\nmulti-view diffusion model to reconstruct photoreal 4D (dynamic 3D) portrait\navatars from any number of reference images (i.e., one to 100) and animate and\nrender them in real time. Our approach demonstrates state-of-the-art\nperformance for single-, few-, and multi-image 4D portrait avatar\nreconstruction, and takes steps to bridge the gap in visual fidelity between\nsingle-image and multi-view reconstruction techniques.\n","authors":["Felix Taubner","Ruihang Zhang","Mathieu Tuli","David B. Lindell"],"pdf_url":"https://arxiv.org/pdf/2412.12093v1.pdf","comment":"23 pages, 15 figures"},{"id":"http://arxiv.org/abs/2412.12091v1","updated":"2024-12-16T18:58:17Z","published":"2024-12-16T18:58:17Z","title":"Wonderland: Navigating 3D Scenes from a Single Image","summary":"  This paper addresses a challenging question: How can we efficiently create\nhigh-quality, wide-scope 3D scenes from a single arbitrary image? Existing\nmethods face several constraints, such as requiring multi-view data,\ntime-consuming per-scene optimization, low visual quality in backgrounds, and\ndistorted reconstructions in unseen areas. We propose a novel pipeline to\novercome these limitations. Specifically, we introduce a large-scale\nreconstruction model that uses latents from a video diffusion model to predict\n3D Gaussian Splattings for the scenes in a feed-forward manner. The video\ndiffusion model is designed to create videos precisely following specified\ncamera trajectories, allowing it to generate compressed video latents that\ncontain multi-view information while maintaining 3D consistency. We train the\n3D reconstruction model to operate on the video latent space with a progressive\ntraining strategy, enabling the efficient generation of high-quality,\nwide-scope, and generic 3D scenes. Extensive evaluations across various\ndatasets demonstrate that our model significantly outperforms existing methods\nfor single-view 3D scene generation, particularly with out-of-domain images.\nFor the first time, we demonstrate that a 3D reconstruction model can be\neffectively built upon the latent space of a diffusion model to realize\nefficient 3D scene generation.\n","authors":["Hanwen Liang","Junli Cao","Vidit Goel","Guocheng Qian","Sergei Korolev","Demetri Terzopoulos","Konstantinos N. Plataniotis","Sergey Tulyakov","Jian Ren"],"pdf_url":"https://arxiv.org/pdf/2412.12091v1.pdf","comment":"Project page: https://snap-research.github.io/wonderland/"},{"id":"http://arxiv.org/abs/2412.12089v1","updated":"2024-12-16T18:56:24Z","published":"2024-12-16T18:56:24Z","title":"Stabilizing Reinforcement Learning in Differentiable Multiphysics\n  Simulation","summary":"  Recent advances in GPU-based parallel simulation have enabled practitioners\nto collect large amounts of data and train complex control policies using deep\nreinforcement learning (RL), on commodity GPUs. However, such successes for RL\nin robotics have been limited to tasks sufficiently simulated by fast\nrigid-body dynamics. Simulation techniques for soft bodies are comparatively\nseveral orders of magnitude slower, thereby limiting the use of RL due to\nsample complexity requirements. To address this challenge, this paper presents\nboth a novel RL algorithm and a simulation platform to enable scaling RL on\ntasks involving rigid bodies and deformables. We introduce Soft Analytic Policy\nOptimization (SAPO), a maximum entropy first-order model-based actor-critic RL\nalgorithm, which uses first-order analytic gradients from differentiable\nsimulation to train a stochastic actor to maximize expected return and entropy.\nAlongside our approach, we develop Rewarped, a parallel differentiable\nmultiphysics simulation platform that supports simulating various materials\nbeyond rigid bodies. We re-implement challenging manipulation and locomotion\ntasks in Rewarped, and show that SAPO outperforms baselines over a range of\ntasks that involve interaction between rigid bodies, articulations, and\ndeformables.\n","authors":["Eliot Xing","Vernon Luk","Jean Oh"],"pdf_url":"https://arxiv.org/pdf/2412.12089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12087v1","updated":"2024-12-16T18:56:17Z","published":"2024-12-16T18:56:17Z","title":"Instruction-based Image Manipulation by Watching How Things Move","summary":"  This paper introduces a novel dataset construction pipeline that samples\npairs of frames from videos and uses multimodal large language models (MLLMs)\nto generate editing instructions for training instruction-based image\nmanipulation models. Video frames inherently preserve the identity of subjects\nand scenes, ensuring consistent content preservation during editing.\nAdditionally, video data captures diverse, natural dynamics-such as non-rigid\nsubject motion and complex camera movements-that are difficult to model\notherwise, making it an ideal source for scalable dataset construction. Using\nthis approach, we create a new dataset to train InstructMove, a model capable\nof instruction-based complex manipulations that are difficult to achieve with\nsynthetically generated datasets. Our model demonstrates state-of-the-art\nperformance in tasks such as adjusting subject poses, rearranging elements, and\naltering camera perspectives.\n","authors":["Mingdeng Cao","Xuaner Zhang","Yinqiang Zheng","Zhihao Xia"],"pdf_url":"https://arxiv.org/pdf/2412.12087v1.pdf","comment":"Project page: https://ljzycmd.github.io/projects/InstructMove/"},{"id":"http://arxiv.org/abs/2411.17474v2","updated":"2024-12-16T18:55:09Z","published":"2024-11-25T18:59:50Z","title":"Probing the Mid-level Vision Capabilities of Self-Supervised Learning","summary":"  Mid-level vision capabilities - such as generic object localization and 3D\ngeometric understanding - are not only fundamental to human vision but are also\ncrucial for many real-world applications of computer vision. These abilities\nemerge with minimal supervision during the early stages of human visual\ndevelopment. Despite their significance, current self-supervised learning (SSL)\napproaches are primarily designed and evaluated for high-level recognition\ntasks, leaving their mid-level vision capabilities largely unexamined.\n  In this study, we introduce a suite of benchmark protocols to systematically\nassess mid-level vision capabilities and present a comprehensive, controlled\nevaluation of 22 prominent SSL models across 8 mid-level vision tasks. Our\nexperiments reveal a weak correlation between mid-level and high-level task\nperformance. We also identify several SSL methods with highly imbalanced\nperformance across mid-level and high-level capabilities, as well as some that\nexcel in both. Additionally, we investigate key factors contributing to\nmid-level vision performance, such as pretraining objectives and network\narchitectures. Our study provides a holistic and timely view of what SSL models\nhave learned, complementing existing research that primarily focuses on\nhigh-level vision tasks. We hope our findings guide future SSL research to\nbenchmark models not only on high-level vision tasks but on mid-level as well.\n","authors":["Xuweiyi Chen","Markus Marks","Zezhou Cheng"],"pdf_url":"https://arxiv.org/pdf/2411.17474v2.pdf","comment":"Project Page: https://midvision-probe.cs.virginia.edu/"},{"id":"http://arxiv.org/abs/2412.12083v1","updated":"2024-12-16T18:52:56Z","published":"2024-12-16T18:52:56Z","title":"IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and\n  Illuminations","summary":"  Capturing geometric and material information from images remains a\nfundamental challenge in computer vision and graphics. Traditional\noptimization-based methods often require hours of computational time to\nreconstruct geometry, material properties, and environmental lighting from\ndense multi-view inputs, while still struggling with inherent ambiguities\nbetween lighting and material. On the other hand, learning-based approaches\nleverage rich material priors from existing 3D object datasets but face\nchallenges with maintaining multi-view consistency. In this paper, we introduce\nIDArb, a diffusion-based model designed to perform intrinsic decomposition on\nan arbitrary number of images under varying illuminations. Our method achieves\naccurate and multi-view consistent estimation on surface normals and material\nproperties. This is made possible through a novel cross-view, cross-domain\nattention module and an illumination-augmented, view-adaptive training\nstrategy. Additionally, we introduce ARB-Objaverse, a new dataset that provides\nlarge-scale multi-view intrinsic data and renderings under diverse lighting\nconditions, supporting robust training. Extensive experiments demonstrate that\nIDArb outperforms state-of-the-art methods both qualitatively and\nquantitatively. Moreover, our approach facilitates a range of downstream tasks,\nincluding single-image relighting, photometric stereo, and 3D reconstruction,\nhighlighting its broad applications in realistic 3D content creation.\n","authors":["Zhibing Li","Tong Wu","Jing Tan","Mengchen Zhang","Jiaqi Wang","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2412.12083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12079v1","updated":"2024-12-16T18:48:58Z","published":"2024-12-16T18:48:58Z","title":"UniLoc: Towards Universal Place Recognition Using Any Single Modality","summary":"  To date, most place recognition methods focus on single-modality retrieval.\nWhile they perform well in specific environments, cross-modal methods offer\ngreater flexibility by allowing seamless switching between map and query\nsources. It also promises to reduce computation requirements by having a\nunified model, and achieving greater sample efficiency by sharing parameters.\nIn this work, we develop a universal solution to place recognition, UniLoc,\nthat works with any single query modality (natural language, image, or point\ncloud). UniLoc leverages recent advances in large-scale contrastive learning,\nand learns by matching hierarchically at two levels: instance-level matching\nand scene-level matching. Specifically, we propose a novel Self-Attention based\nPooling (SAP) module to evaluate the importance of instance descriptors when\naggregated into a place-level descriptor. Experiments on the KITTI-360 dataset\ndemonstrate the benefits of cross-modality for place recognition, achieving\nsuperior performance in cross-modal settings and competitive results also for\nuni-modal scenarios. Our project page is publicly available at\nhttps://yan-xia.github.io/projects/UniLoc/.\n","authors":["Yan Xia","Zhendong Li","Yun-Jin Li","Letian Shi","Hu Cao","João F. Henriques","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2412.12079v1.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.12077v1","updated":"2024-12-16T18:46:58Z","published":"2024-12-16T18:46:58Z","title":"CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole\n  Slide Image Analysis in Computational Pathology","summary":"  The emergence of large multimodal models (LMMs) has brought significant\nadvancements to pathology. Previous research has primarily focused on\nseparately training patch-level and whole-slide image (WSI)-level models,\nlimiting the integration of learned knowledge across patches and WSIs, and\nresulting in redundant models. In this work, we introduce CPath-Omni, the first\n15-billion-parameter LMM designed to unify both patch and WSI level image\nanalysis, consolidating a variety of tasks at both levels, including\nclassification, visual question answering, captioning, and visual referring\nprompting. Extensive experiments demonstrate that CPath-Omni achieves\nstate-of-the-art (SOTA) performance across seven diverse tasks on 39 out of 42\ndatasets, outperforming or matching task-specific models trained for individual\ntasks. Additionally, we develop a specialized pathology CLIP-based visual\nprocessor for CPath-Omni, CPath-CLIP, which, for the first time, integrates\ndifferent vision models and incorporates a large language model as a text\nencoder to build a more powerful CLIP model, which achieves SOTA performance on\nnine zero-shot and four few-shot datasets. Our findings highlight CPath-Omni's\nability to unify diverse pathology tasks, demonstrating its potential to\nstreamline and advance the field of foundation model in pathology.\n","authors":["Yuxuan Sun","Yixuan Si","Chenglu Zhu","Xuan Gong","Kai Zhang","Pingyi Chen","Ye Zhang","Zhongyi Shui","Tao Lin","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2412.12077v1.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2412.12075v1","updated":"2024-12-16T18:46:45Z","published":"2024-12-16T18:46:45Z","title":"CG-Bench: Clue-grounded Question Answering Benchmark for Long Video\n  Understanding","summary":"  Most existing video understanding benchmarks for multimodal large language\nmodels (MLLMs) focus only on short videos. The limited number of benchmarks for\nlong video understanding often rely solely on multiple-choice questions (MCQs).\nHowever, because of the inherent limitation of MCQ-based evaluation and the\nincreasing reasoning ability of MLLMs, models can give the current answer\npurely by combining short video understanding with elimination, without\ngenuinely understanding the video content. To address this gap, we introduce\nCG-Bench, a novel benchmark designed for clue-grounded question answering in\nlong videos. CG-Bench emphasizes the model's ability to retrieve relevant clues\nfor questions, enhancing evaluation credibility. It features 1,219 manually\ncurated videos categorized by a granular system with 14 primary categories, 171\nsecondary categories, and 638 tertiary categories, making it the largest\nbenchmark for long video analysis. The benchmark includes 12,129 QA pairs in\nthree major question types: perception, reasoning, and hallucination.\nCompensating the drawbacks of pure MCQ-based evaluation, we design two novel\nclue-based evaluation methods: clue-grounded white box and black box\nevaluations, to assess whether the model generates answers based on the correct\nunderstanding of the video. We evaluate multiple closed-source and open-source\nMLLMs on CG-Bench. Results indicate that current models significantly\nunderperform in understanding long videos compared to short ones, and a\nsignificant gap exists between open-source and commercial models. We hope\nCG-Bench can advance the development of more trustworthy and capable MLLMs for\nlong video understanding. All annotations and video data are released at\nhttps://cg-bench.github.io/leaderboard/.\n","authors":["Guo Chen","Yicheng Liu","Yifei Huang","Yuping He","Baoqi Pei","Jilan Xu","Yali Wang","Tong Lu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12075v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.12068v1","updated":"2024-12-16T18:42:05Z","published":"2024-12-16T18:42:05Z","title":"SPADE: Spectroscopic Photoacoustic Denoising using an Analytical and\n  Data-free Enhancement Framework","summary":"  Spectroscopic photoacoustic (sPA) imaging uses multiple wavelengths to\ndifferentiate chromophores based on their unique optical absorption spectra.\nThis technique has been widely applied in areas such as vascular mapping, tumor\ndetection, and therapeutic monitoring. However, sPA imaging is highly\nsusceptible to noise, leading to poor signal-to-noise ratio (SNR) and\ncompromised image quality. Traditional denoising techniques like frame\naveraging, though effective in improving SNR, can be impractical for dynamic\nimaging scenarios due to reduced frame rates. Advanced methods, including\nlearning-based approaches and analytical algorithms, have demonstrated promise\nbut often require extensive training data and parameter tuning, limiting their\nadaptability for real-time clinical use. In this work, we propose a sPA\ndenoising using a tuning-free analytical and data-free enhancement (SPADE)\nframework for denoising sPA images. This framework integrates a data-free\nlearning-based method with an efficient BM3D-based analytical approach while\npreserves spectral linearity, providing noise reduction and ensuring that\nfunctional information is maintained. The SPADE framework was validated through\nsimulation, phantom, ex vivo, and in vivo experiments. Results demonstrated\nthat SPADE improved SNR and preserved spectral information, outperforming\nconventional methods, especially in challenging imaging conditions. SPADE\npresents a promising solution for enhancing sPA imaging quality in clinical\napplications where noise reduction and spectral preservation are critical.\n","authors":["Fangzhou Lin","Shang Gao","Yichuan Tang","Xihan Ma","Ryo Murakami","Ziming Zhang","John D. Obayemic","Winston W. Soboyejo","Haichong K. Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12068v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.12050v1","updated":"2024-12-16T18:20:06Z","published":"2024-12-16T18:20:06Z","title":"Exploring Semantic Consistency and Style Diversity for Domain\n  Generalized Semantic Segmentation","summary":"  Domain Generalized Semantic Segmentation (DGSS) seeks to utilize source\ndomain data exclusively to enhance the generalization of semantic segmentation\nacross unknown target domains. Prevailing studies predominantly concentrate on\nfeature normalization and domain randomization, these approaches exhibit\nsignificant limitations. Feature normalization-based methods tend to confuse\nsemantic features in the process of constraining the feature space\ndistribution, resulting in classification misjudgment. Domain\nrandomization-based methods frequently incorporate domain-irrelevant noise due\nto the uncontrollability of style transformations, resulting in segmentation\nambiguity. To address these challenges, we introduce a novel framework, named\nSCSD for Semantic Consistency prediction and Style Diversity generalization. It\ncomprises three pivotal components: Firstly, a Semantic Query Booster is\ndesigned to enhance the semantic awareness and discrimination capabilities of\nobject queries in the mask decoder, enabling cross-domain semantic consistency\nprediction. Secondly, we develop a Text-Driven Style Transform module that\nutilizes domain difference text embeddings to controllably guide the style\ntransformation of image features, thereby increasing inter-domain style\ndiversity. Lastly, to prevent the collapse of similar domain feature spaces, we\nintroduce a Style Synergy Optimization mechanism that fortifies the separation\nof inter-domain features and the aggregation of intra-domain features by\nsynergistically weighting style contrastive loss and style aggregation loss.\nExtensive experiments demonstrate that the proposed SCSD significantly\noutperforms existing state-of-theart methods. Notably, SCSD trained on GTAV\nachieved an average of 49.11 mIoU on the four unseen domain datasets,\nsurpassing the previous state-of-the-art method by +4.08 mIoU. Code is\navailable at https://github.com/nhw649/SCSD.\n","authors":["Hongwei Niu","Linhuang Xie","Jianghang Lin","Shengchuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12050v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12048v1","updated":"2024-12-16T18:18:17Z","published":"2024-12-16T18:18:17Z","title":"A LoRA is Worth a Thousand Pictures","summary":"  Recent advances in diffusion models and parameter-efficient fine-tuning\n(PEFT) have made text-to-image generation and customization widely accessible,\nwith Low Rank Adaptation (LoRA) able to replicate an artist's style or subject\nusing minimal data and computation. In this paper, we examine the relationship\nbetween LoRA weights and artistic styles, demonstrating that LoRA weights alone\ncan serve as an effective descriptor of style, without the need for additional\nimage generation or knowledge of the original training set. Our findings show\nthat LoRA weights yield better performance in clustering of artistic styles\ncompared to traditional pre-trained features, such as CLIP and DINO, with\nstrong structural similarities between LoRA-based and conventional image-based\nembeddings observed both qualitatively and quantitatively. We identify various\nretrieval scenarios for the growing collection of customized models and show\nthat our approach enables more accurate retrieval in real-world settings where\nknowledge of the training images is unavailable and additional generation is\nrequired. We conclude with a discussion on potential future applications, such\nas zero-shot LoRA fine-tuning and model attribution.\n","authors":["Chenxi Liu","Towaki Takikawa","Alec Jacobson"],"pdf_url":"https://arxiv.org/pdf/2412.12048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08628v2","updated":"2024-12-16T18:16:14Z","published":"2024-12-11T18:48:20Z","title":"EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation","summary":"  Open-vocabulary panoptic segmentation aims to segment and classify everything\nin diverse scenes across an unbounded vocabulary. Existing methods typically\nemploy two-stage or single-stage framework. The two-stage framework involves\ncropping the image multiple times using masks generated by a mask generator,\nfollowed by feature extraction, while the single-stage framework relies on a\nheavyweight mask decoder to make up for the lack of spatial position\ninformation through self-attention and cross-attention in multiple stacked\nTransformer blocks. Both methods incur substantial computational overhead,\nthereby hindering the efficiency of model inference. To fill the gap in\nefficiency, we propose EOV-Seg, a novel single-stage, shared, efficient, and\nspatialaware framework designed for open-vocabulary panoptic segmentation.\nSpecifically, EOV-Seg innovates in two aspects. First, a Vocabulary-Aware\nSelection (VAS) module is proposed to improve the semantic comprehension of\nvisual aggregated features and alleviate the feature interaction burden on the\nmask decoder. Second, we introduce a Two-way Dynamic Embedding Experts (TDEE),\nwhich efficiently utilizes the spatial awareness capabilities of ViT-based CLIP\nbackbone. To the best of our knowledge, EOV-Seg is the first open-vocabulary\npanoptic segmentation framework towards efficiency, which runs faster and\nachieves competitive performance compared with state-of-the-art methods.\nSpecifically, with COCO training only, EOV-Seg achieves 24.5 PQ, 32.1 mIoU, and\n11.6 FPS on the ADE20K dataset and the inference time of EOV-Seg is 4-19 times\nfaster than state-of-theart methods. Especially, equipped with ResNet50\nbackbone, EOV-Seg runs 23.8 FPS with only 71M parameters on a single RTX 3090\nGPU. Code is available at https://github.com/nhw649/EOV-Seg.\n","authors":["Hongwei Niu","Jie Hu","Jianghang Lin","Guannan Jiang","Shengchuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.08628v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12032v1","updated":"2024-12-16T17:58:45Z","published":"2024-12-16T17:58:45Z","title":"FSFM: A Generalizable Face Security Foundation Model via Self-Supervised\n  Facial Representation Learning","summary":"  This work asks: with abundant, unlabeled real faces, how to learn a robust\nand transferable facial representation that boosts various face security tasks\nwith respect to generalization performance? We make the first attempt and\npropose a self-supervised pretraining framework to learn fundamental\nrepresentations of real face images, FSFM, that leverages the synergy between\nmasked image modeling (MIM) and instance discrimination (ID). We explore\nvarious facial masking strategies for MIM and present a simple yet powerful\nCRFR-P masking, which explicitly forces the model to capture meaningful\nintra-region consistency and challenging inter-region coherency. Furthermore,\nwe devise the ID network that naturally couples with MIM to establish\nunderlying local-to-global correspondence via tailored self-distillation. These\nthree learning objectives, namely 3C, empower encoding both local features and\nglobal semantics of real faces. After pretraining, a vanilla ViT serves as a\nuniversal vision foundation model for downstream face security tasks:\ncross-dataset deepfake detection, cross-domain face anti-spoofing, and unseen\ndiffusion facial forgery detection. Extensive experiments on 10 public datasets\ndemonstrate that our model transfers better than supervised pretraining, visual\nand facial self-supervised learning arts, and even outperforms task-specialized\nSOTA methods.\n","authors":["Gaojian Wang","Feng Lin","Tong Wu","Zhenguang Liu","Zhongjie Ba","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2412.12032v1.pdf","comment":"21 pages, 11 figures, project page: https://fsfm-3c.github.io"},{"id":"http://arxiv.org/abs/2412.12031v1","updated":"2024-12-16T17:57:33Z","published":"2024-12-16T17:57:33Z","title":"RepFace: Refining Closed-Set Noise with Progressive Label Correction for\n  Face Recognition","summary":"  Face recognition has made remarkable strides, driven by the expanding scale\nof datasets, advancements in various backbone and discriminative losses.\nHowever, face recognition performance is heavily affected by the label noise,\nespecially closed-set noise. While numerous studies have focused on handling\nlabel noise, addressing closed-set noise still poses challenges. This paper\nidentifies this challenge as training isn't robust to noise at the early-stage\ntraining, and necessitating an appropriate learning strategy for samples with\nlow confidence, which are often misclassified as closed-set noise in later\ntraining phases. To address these issues, we propose a new framework to\nstabilize the training at early stages and split the samples into clean,\nambiguous and noisy groups which are devised with separate training strategies.\nInitially, we employ generated auxiliary closed-set noisy samples to enable the\nmodel to identify noisy data at the early stages of training. Subsequently, we\nintroduce how samples are split into clean, ambiguous and noisy groups by their\nsimilarity to the positive and nearest negative centers. Then we perform label\nfusion for ambiguous samples by incorporating accumulated model predictions.\nFinally, we apply label smoothing within the closed set, adjusting the label to\na point between the nearest negative class and the initially assigned label.\nExtensive experiments validate the effectiveness of our method on mainstream\nface datasets, achieving state-of-the-art results. The code will be released\nupon acceptance.\n","authors":["Jie Zhang","Xun Gong","Zhonglin Sun"],"pdf_url":"https://arxiv.org/pdf/2412.12031v1.pdf","comment":"11 pages, 5 figures, AAAI2025"},{"id":"http://arxiv.org/abs/2412.10316v2","updated":"2024-12-16T17:54:44Z","published":"2024-12-13T17:58:06Z","title":"BrushEdit: All-In-One Image Inpainting and Editing","summary":"  Image editing has advanced significantly with the development of diffusion\nmodels using both inversion-based and instruction-based methods. However,\ncurrent inversion-based approaches struggle with big modifications (e.g.,\nadding or removing objects) due to the structured nature of inversion noise,\nwhich hinders substantial changes. Meanwhile, instruction-based methods often\nconstrain users to black-box operations, limiting direct interaction for\nspecifying editing regions and intensity. To address these limitations, we\npropose BrushEdit, a novel inpainting-based instruction-guided image editing\nparadigm, which leverages multimodal large language models (MLLMs) and image\ninpainting models to enable autonomous, user-friendly, and interactive\nfree-form instruction editing. Specifically, we devise a system enabling\nfree-form instruction editing by integrating MLLMs and a dual-branch image\ninpainting model in an agent-cooperative framework to perform editing category\nclassification, main object identification, mask acquisition, and editing area\ninpainting. Extensive experiments show that our framework effectively combines\nMLLMs and inpainting models, achieving superior performance across seven\nmetrics including mask region preservation and editing effect coherence.\n","authors":["Yaowei Li","Yuxuan Bian","Xuan Ju","Zhaoyang Zhang","Ying Shan","Yuexian Zou","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2412.10316v2.pdf","comment":"WebPage available at\n  https://liyaowei-stu.github.io/project/BrushEdit/"},{"id":"http://arxiv.org/abs/2409.00397v2","updated":"2024-12-16T17:43:04Z","published":"2024-08-31T09:14:54Z","title":"COSMo: CLIP Talks on Open-Set Multi-Target Domain Adaptation","summary":"  Multi-Target Domain Adaptation (MTDA) entails learning domain-invariant\ninformation from a single source domain and applying it to multiple unlabeled\ntarget domains. Yet, existing MTDA methods predominantly focus on addressing\ndomain shifts within visual features, often overlooking semantic features and\nstruggling to handle unknown classes, resulting in what is known as Open-Set\n(OS) MTDA. While large-scale vision-language foundation models like CLIP show\npromise, their potential for MTDA remains largely unexplored. This paper\nintroduces COSMo, a novel method that learns domain-agnostic prompts through\nsource domain-guided prompt learning to tackle the MTDA problem in the prompt\nspace. By leveraging a domain-specific bias network and separate prompts for\nknown and unknown classes, COSMo effectively adapts across domain and class\nshifts. To the best of our knowledge, COSMo is the first method to address\nOpen-Set Multi-Target DA (OSMTDA), offering a more realistic representation of\nreal-world scenarios and addressing the challenges of both open-set and\nmulti-target DA. COSMo demonstrates an average improvement of $5.1\\%$ across\nthree challenging datasets: Mini-DomainNet, Office-31, and Office-Home,\ncompared to other related DA methods adapted to operate within the OSMTDA\nsetting. Code is available at: https://github.com/munish30monga/COSMo\n","authors":["Munish Monga","Sachin Kumar Giroh","Ankit Jha","Mainak Singha","Biplab Banerjee","Jocelyn Chanussot"],"pdf_url":"https://arxiv.org/pdf/2409.00397v2.pdf","comment":"Accepted in BMVC 2024"},{"id":"http://arxiv.org/abs/2412.12001v1","updated":"2024-12-16T17:29:51Z","published":"2024-12-16T17:29:51Z","title":"LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse\n  Input Contexts","summary":"  Drafting radiology reports is a complex task requiring flexibility, where\nradiologists tail content to available information and particular clinical\ndemands. However, most current radiology report generation (RRG) models are\nconstrained to a fixed task paradigm, such as predicting the full ``finding''\nsection from a single image, inherently involving a mismatch between inputs and\noutputs. The trained models lack the flexibility for diverse inputs and could\ngenerate harmful, input-agnostic hallucinations. To bridge the gap between\ncurrent RRG models and the clinical demands in practice, we first develop a\ndata generation pipeline to create a new MIMIC-RG4 dataset, which considers\nfour common radiology report drafting scenarios and has perfectly corresponded\ninput and output. Secondly, we propose a novel large language model (LLM) based\nRRG framework, namely LLM-RG4, which utilizes LLM's flexible\ninstruction-following capabilities and extensive general knowledge. We further\ndevelop an adaptive token fusion module that offers flexibility to handle\ndiverse scenarios with different input combinations, while minimizing the\nadditional computational burden associated with increased input volumes.\nBesides, we propose a token-level loss weighting strategy to direct the model's\nattention towards positive and uncertain descriptions. Experimental results\ndemonstrate that LLM-RG4 achieves state-of-the-art performance in both clinical\nefficiency and natural language generation on the MIMIC-RG4 and MIMIC-CXR\ndatasets. We quantitatively demonstrate that our model has minimal\ninput-agnostic hallucinations, whereas current open-source models commonly\nsuffer from this problem.\n","authors":["Zhuhao Wang","Yihua Sun","Zihan Li","Xuan Yang","Fang Chen","Hongen Liao"],"pdf_url":"https://arxiv.org/pdf/2412.12001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11998v1","updated":"2024-12-16T17:26:06Z","published":"2024-12-16T17:26:06Z","title":"SAMIC: Segment Anything with In-Context Spatial Prompt Engineering","summary":"  Few-shot segmentation is the problem of learning to identify specific types\nof objects (e.g., airplanes) in images from a small set of labeled reference\nimages. The current state of the art is driven by resource-intensive\nconstruction of models for every new domain-specific application. Such models\nmust be trained on enormous labeled datasets of unrelated objects (e.g., cars,\ntrains, animals) so that their ``knowledge'' can be transferred to new types of\nobjects. In this paper, we show how to leverage existing vision foundation\nmodels (VFMs) to reduce the incremental cost of creating few-shot segmentation\nmodels for new domains. Specifically, we introduce SAMIC, a small network that\nlearns how to prompt VFMs in order to segment new types of objects in\ndomain-specific applications. SAMIC enables any task to be approached as a\nfew-shot learning problem. At 2.6 million parameters, it is 94% smaller than\nthe leading models (e.g., having ResNet 101 backbone with 45+ million\nparameters). Even using 1/5th of the training data provided by one-shot\nbenchmarks, SAMIC is competitive with, or sets the state of the art, on a\nvariety of few-shot and semantic segmentation datasets including COCO-$20^i$,\nPascal-$5^i$, PerSeg, FSS-1000, and NWPU VHR-10.\n","authors":["Savinay Nagendra","Kashif Rashid","Chaopeng Shen","Daniel Kifer"],"pdf_url":"https://arxiv.org/pdf/2412.11998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16726v2","updated":"2024-12-16T17:11:49Z","published":"2024-11-23T04:38:51Z","title":"EmotiveTalk: Expressive Talking Head Generation through Audio\n  Information Decoupling and Emotional Video Diffusion","summary":"  Diffusion models have revolutionized the field of talking head generation,\nyet still face challenges in expressiveness, controllability, and stability in\nlong-time generation. In this research, we propose an EmotiveTalk framework to\naddress these issues. Firstly, to realize better control over the generation of\nlip movement and facial expression, a Vision-guided Audio Information\nDecoupling (V-AID) approach is designed to generate audio-based decoupled\nrepresentations aligned with lip movements and expression. Specifically, to\nachieve alignment between audio and facial expression representation spaces, we\npresent a Diffusion-based Co-speech Temporal Expansion (Di-CTE) module within\nV-AID to generate expression-related representations under multi-source emotion\ncondition constraints. Then we propose a well-designed Emotional Talking Head\nDiffusion (ETHD) backbone to efficiently generate highly expressive talking\nhead videos, which contains an Expression Decoupling Injection (EDI) module to\nautomatically decouple the expressions from reference portraits while\nintegrating the target expression information, achieving more expressive\ngeneration performance. Experimental results show that EmotiveTalk can generate\nexpressive talking head videos, ensuring the promised controllability of\nemotions and stability during long-time generation, yielding state-of-the-art\nperformance compared to existing methods.\n","authors":["Haotian Wang","Yuzhe Weng","Yueyan Li","Zilu Guo","Jun Du","Shutong Niu","Jiefeng Ma","Shan He","Xiaoyan Wu","Qiming Hu","Bing Yin","Cong Liu","Qingfeng Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16726v2.pdf","comment":"https://emotivetalk.github.io/"},{"id":"http://arxiv.org/abs/2412.11974v1","updated":"2024-12-16T16:58:28Z","published":"2024-12-16T16:58:28Z","title":"Emma-X: An Embodied Multimodal Action Model with Grounded Chain of\n  Thought and Look-ahead Spatial Reasoning","summary":"  Traditional reinforcement learning-based robotic control methods are often\ntask-specific and fail to generalize across diverse environments or unseen\nobjects and instructions. Visual Language Models (VLMs) demonstrate strong\nscene understanding and planning capabilities but lack the ability to generate\nactionable policies tailored to specific robotic embodiments. To address this,\nVisual-Language-Action (VLA) models have emerged, yet they face challenges in\nlong-horizon spatial reasoning and grounded task planning. In this work, we\npropose the Embodied Multimodal Action Model with Grounded Chain of Thought and\nLook-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed\nhierarchical embodiment dataset based on BridgeV2, containing 60,000 robot\nmanipulation trajectories auto-annotated with grounded task reasoning and\nspatial guidance. Additionally, we introduce a trajectory segmentation strategy\nbased on gripper states and motion trajectories, which can help mitigate\nhallucination in grounding subtask reasoning generation. Experimental results\ndemonstrate that Emma-X achieves superior performance over competitive\nbaselines, particularly in real-world robotic tasks requiring spatial\nreasoning.\n","authors":["Qi Sun","Pengfei Hong","Tej Deep Pala","Vernon Toh","U-Xuan Tan","Deepanway Ghosal","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2412.11974v1.pdf","comment":"https://github.com/declare-lab/Emma-X,\n  https://huggingface.co/declare-lab/Emma-X"},{"id":"http://arxiv.org/abs/2412.11972v1","updated":"2024-12-16T16:55:22Z","published":"2024-12-16T16:55:22Z","title":"Controllable Shadow Generation with Single-Step Diffusion Models from\n  Synthetic Data","summary":"  Realistic shadow generation is a critical component for high-quality image\ncompositing and visual effects, yet existing methods suffer from certain\nlimitations: Physics-based approaches require a 3D scene geometry, which is\noften unavailable, while learning-based techniques struggle with control and\nvisual artifacts. We introduce a novel method for fast, controllable, and\nbackground-free shadow generation for 2D object images. We create a large\nsynthetic dataset using a 3D rendering engine to train a diffusion model for\ncontrollable shadow generation, generating shadow maps for diverse light source\nparameters. Through extensive ablation studies, we find that rectified flow\nobjective achieves high-quality results with just a single sampling step\nenabling real-time applications. Furthermore, our experiments demonstrate that\nthe model generalizes well to real-world images. To facilitate further research\nin evaluating quality and controllability in shadow generation, we release a\nnew public benchmark containing a diverse set of object images and shadow maps\nin various settings. The project page is available at\nhttps://gojasper.github.io/controllable-shadow-generation-project/\n","authors":["Onur Tasar","Clément Chadebec","Benjamin Aubin"],"pdf_url":"https://arxiv.org/pdf/2412.11972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16258v2","updated":"2024-12-16T16:46:03Z","published":"2024-08-29T04:40:31Z","title":"GSDiff: Synthesizing Vector Floorplans via Geometry-enhanced Structural\n  Graph Generation","summary":"  Automating architectural floorplan design is vital for housing and interior\ndesign, offering a faster, cost-effective alternative to manual sketches by\narchitects. However, existing methods, including rule-based and learning-based\napproaches, face challenges in design complexity and constrained generation\nwith extensive post-processing, and tend to obvious geometric inconsistencies\nsuch as misalignment, overlap, and gaps. In this work, we propose a novel\ngenerative framework for vector floorplan design via structural graph\ngeneration, called GSDiff, focusing on wall junction generation and wall\nsegment prediction to capture both geometric and semantic aspects of structural\ngraphs. To improve the geometric rationality of generated structural graphs, we\npropose two innovative geometry enhancement methods. In wall junction\ngeneration, we propose a novel alignment loss function to improve geometric\nconsistency. In wall segment prediction, we propose a random self-supervision\nmethod to enhance the model's perception of the overall geometric structure,\nthereby promoting the generation of reasonable geometric structures. Employing\nthe diffusion model and the Transformer model, as well as the geometry\nenhancement strategies, our framework can generate wall junctions, wall\nsegments and room polygons with structural and semantic information, resulting\nin structural graphs that accurately represent floorplans. Extensive\nexperiments show that the proposed method surpasses existing techniques,\nenabling free generation and constrained generation, marking a shift towards\nstructure generation in architectural design. Code and data are available at\nhttps://github.com/SizheHu/GSDiff.\n","authors":["Sizhe Hu","Wenming Wu","Yuntao Wang","Benzhu Xu","Liping Zheng"],"pdf_url":"https://arxiv.org/pdf/2408.16258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11959v1","updated":"2024-12-16T16:41:51Z","published":"2024-12-16T16:41:51Z","title":"Gramian Multimodal Representation Learning and Alignment","summary":"  Human perception integrates multiple modalities, such as vision, hearing, and\nlanguage, into a unified understanding of the surrounding reality. While recent\nmultimodal models have achieved significant progress by aligning pairs of\nmodalities via contrastive learning, their solutions are unsuitable when\nscaling to multiple modalities. These models typically align each modality to a\ndesignated anchor without ensuring the alignment of all modalities with each\nother, leading to suboptimal performance in tasks requiring a joint\nunderstanding of multiple modalities. In this paper, we structurally rethink\nthe pairwise conventional approach to multimodal learning and we present the\nnovel Gramian Representation Alignment Measure (GRAM), which overcomes the\nabove-mentioned limitations. GRAM learns and then aligns $n$ modalities\ndirectly in the higher-dimensional space in which modality embeddings lie by\nminimizing the Gramian volume of the $k$-dimensional parallelotope spanned by\nthe modality vectors, ensuring the geometric alignment of all modalities\nsimultaneously. GRAM can replace cosine similarity in any downstream method,\nholding for 2 to $n$ modality and providing more meaningful alignment with\nrespect to previous similarity measures. The novel GRAM-based contrastive loss\nfunction enhances the alignment of multimodal models in the higher-dimensional\nembedding space, leading to new state-of-the-art performance in downstream\ntasks such as video-audio-text retrieval and audio-video classification. The\nproject page, the code, and the pretrained models are available at\nhttps://ispamm.github.io/GRAM/.\n","authors":["Giordano Cicchetti","Eleonora Grassucci","Luigi Sigillo","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.11959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11953v1","updated":"2024-12-16T16:37:03Z","published":"2024-12-16T16:37:03Z","title":"Reliable Breast Cancer Molecular Subtype Prediction based on\n  uncertainty-aware Bayesian Deep Learning by Mammography","summary":"  Breast cancer is a heterogeneous disease with different molecular subtypes,\nclinical behavior, treatment responses as well as survival outcomes. The\ndevelopment of a reliable, accurate, available and inexpensive method to\npredict the molecular subtypes using medical images plays an important role in\nthe diagnosis and prognosis of breast cancer. Recently, deep learning methods\nhave shown good performance in the breast cancer classification tasks using\nvarious medical images. Despite all that success, classical deep learning\ncannot deliver the predictive uncertainty. The uncertainty represents the\nvalidity of the predictions.Therefore, the high predicted uncertainty might\ncause a negative effect in the accurate diagnosis of breast cancer molecular\nsubtypes. To overcome this, uncertainty quantification methods are used to\ndetermine the predictive uncertainty. Accordingly, in this study, we proposed\nan uncertainty-aware Bayesian deep learning model using the full mammogram\nimages. In addition, to increase the performance of the multi-class molecular\nsubtype classification task, we proposed a novel hierarchical classification\nstrategy, named the two-stage classification strategy. The separate AUC of the\nproposed model for each subtype was 0.71, 0.75 and 0.86 for HER2-enriched,\nluminal and triple-negative classes, respectively. The proposed model not only\nhas a comparable performance to other studies in the field of breast cancer\nmolecular subtypes prediction, even using full mammography images, but it is\nalso more reliable, due to quantify the predictive uncertainty.\n","authors":["Mohaddeseh Chegini","Ali Mahloojifar"],"pdf_url":"https://arxiv.org/pdf/2412.11953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11952v1","updated":"2024-12-16T16:35:35Z","published":"2024-12-16T16:35:35Z","title":"Advancing Comprehensive Aesthetic Insight with Multi-Scale Text-Guided\n  Self-Supervised Learning","summary":"  Image Aesthetic Assessment (IAA) is a vital and intricate task that entails\nanalyzing and assessing an image's aesthetic values, and identifying its\nhighlights and areas for improvement. Traditional methods of IAA often\nconcentrate on a single aesthetic task and suffer from inadequate labeled\ndatasets, thus impairing in-depth aesthetic comprehension. Despite efforts to\novercome this challenge through the application of Multi-modal Large Language\nModels (MLLMs), such models remain underdeveloped for IAA purposes. To address\nthis, we propose a comprehensive aesthetic MLLM capable of nuanced aesthetic\ninsight. Central to our approach is an innovative multi-scale text-guided\nself-supervised learning technique. This technique features a multi-scale\nfeature alignment module and capitalizes on a wealth of unlabeled data in a\nself-supervised manner to structurally and functionally enhance aesthetic\nability. The empirical evidence indicates that accompanied with extensive\ninstruct-tuning, our model sets new state-of-the-art benchmarks across multiple\ntasks, including aesthetic scoring, aesthetic commenting, and personalized\nimage aesthetic assessment. Remarkably, it also demonstrates zero-shot learning\ncapabilities in the emerging task of aesthetic suggesting. Furthermore, for\npersonalized image aesthetic assessment, we harness the potential of in-context\nlearning and showcase its inherent advantages.\n","authors":["Yuti Liu","Shice Liu","Junyuan Gao","Pengtao Jiang","Hao Zhang","Jinwei Chen","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2412.11952v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11949v1","updated":"2024-12-16T16:33:28Z","published":"2024-12-16T16:33:28Z","title":"Coconut Palm Tree Counting on Drone Images with Deep Object Detection\n  and Synthetic Training Data","summary":"  Drones have revolutionized various domains, including agriculture. Recent\nadvances in deep learning have propelled among other things object detection in\ncomputer vision. This study utilized YOLO, a real-time object detector, to\nidentify and count coconut palm trees in Ghanaian farm drone footage. The farm\npresented has lost track of its trees due to different planting phases. While\nmanual counting would be very tedious and error-prone, accurately determining\nthe number of trees is crucial for efficient planning and management of\nagricultural processes, especially for optimizing yields and predicting\nproduction. We assessed YOLO for palm detection within a semi-automated\nframework, evaluated accuracy augmentations, and pondered its potential for\nfarmers. Data was captured in September 2022 via drones. To optimize YOLO with\nscarce data, synthetic images were created for model training and validation.\nThe YOLOv7 model, pretrained on the COCO dataset (excluding coconut palms), was\nadapted using tailored data. Trees from footage were repositioned on synthetic\nimages, with testing on distinct authentic images. In our experiments, we\nadjusted hyperparameters, improving YOLO's mean average precision (mAP). We\nalso tested various altitudes to determine the best drone height. From an\ninitial mAP@.5 of $0.65$, we achieved 0.88, highlighting the value of synthetic\nimages in agricultural scenarios.\n","authors":["Tobias Rohe","Barbara Böhm","Michael Kölle","Jonas Stein","Robert Müller","Claudia Linnhoff-Popien"],"pdf_url":"https://arxiv.org/pdf/2412.11949v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2412.11938v1","updated":"2024-12-16T16:23:05Z","published":"2024-12-16T16:23:05Z","title":"Are the Latent Representations of Foundation Models for Pathology\n  Invariant to Rotation?","summary":"  Self-supervised foundation models for digital pathology encode small patches\nfrom H\\&E whole slide images into latent representations used for downstream\ntasks. However, the invariance of these representations to patch rotation\nremains unexplored. This study investigates the rotational invariance of latent\nrepresentations across twelve foundation models by quantifying the alignment\nbetween non-rotated and rotated patches using mutual $k$-nearest neighbours and\ncosine distance. Models that incorporated rotation augmentation during\nself-supervised training exhibited significantly greater invariance to\nrotations. We hypothesise that the absence of rotational inductive bias in the\ntransformer architecture necessitates rotation augmentation during training to\nachieve learned invariance. Code:\nhttps://github.com/MatousE/rot-invariance-analysis.\n","authors":["Matouš Elphick","Samra Turajlic","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11938v1.pdf","comment":"Samra Turajlic and Guang Yang are joint last authors"},{"id":"http://arxiv.org/abs/2402.12121v2","updated":"2024-12-16T16:09:47Z","published":"2024-02-19T13:16:10Z","title":"IRR: Image Review Ranking Framework for Evaluating Vision-Language\n  Models","summary":"  Large-scale Vision-Language Models (LVLMs) process both images and text,\nexcelling in multimodal tasks such as image captioning and description\ngeneration. However, while these models excel at generating factual content,\ntheir ability to generate and evaluate texts reflecting perspectives on the\nsame image, depending on the context, has not been sufficiently explored. To\naddress this, we propose IRR: Image Review Rank, a novel evaluation framework\ndesigned to assess critic review texts from multiple perspectives. IRR\nevaluates LVLMs by measuring how closely their judgments align with human\ninterpretations. We validate it using a dataset of images from 15 categories,\neach with five critic review texts and annotated rankings in both English and\nJapanese, totaling over 2,000 data instances. The datasets are available at\nhttps://hf.co/datasets/naist-nlp/Wiki-ImageReview1.0. Our results indicate\nthat, although LVLMs exhibited consistent performance across languages, their\ncorrelation with human annotations was insufficient, highlighting the need for\nfurther advancements. These findings highlight the limitations of current\nevaluation methods and the need for approaches that better capture human\nreasoning in Vision & Language tasks.\n","authors":["Kazuki Hayashi","Kazuma Onishi","Toma Suzuki","Yusuke Ide","Seiji Gobara","Shigeki Saito","Yusuke Sakai","Hidetaka Kamigaito","Katsuhiko Hayashi","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2402.12121v2.pdf","comment":"18pages, Accepted at COLING25"},{"id":"http://arxiv.org/abs/2412.11917v1","updated":"2024-12-16T16:01:18Z","published":"2024-12-16T16:01:18Z","title":"Does VLM Classification Benefit from LLM Description Semantics?","summary":"  Accurately describing images via text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect. Considering this, we ask how to distinguish the actual discriminative\npower of descriptions from performance boosts that potentially rely on an\nensembling effect. To study this, we propose an alternative evaluation scenario\nthat shows a characteristic behavior if the used descriptions have\ndiscriminative power. Furthermore, we propose a training-free method to select\ndiscriminative descriptions that work independently of classname ensembling\neffects. The training-free method works in the following way: A test image has\na local CLIP label neighborhood, i.e., its top-$k$ label predictions. Then,\nw.r.t. to a small selection set, we extract descriptions that distinguish each\nclass well in the local neighborhood. Using the selected descriptions, we\ndemonstrate improved classification accuracy across seven datasets and provide\nin-depth analysis and insights into the explainability of description-based\nimage classification by VLMs.\n","authors":["Pingchuan Ma","Lennart Rietdorf","Dmytro Kotovenko","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2412.11917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11906v1","updated":"2024-12-16T15:52:59Z","published":"2024-12-16T15:52:59Z","title":"PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension","summary":"  Multimodal punchlines, which involve humor or sarcasm conveyed in\nimage-caption pairs, are a popular way of communication on online multimedia\nplatforms. With the rapid development of multimodal large language models\n(MLLMs), it is essential to assess their ability to effectively comprehend\nthese punchlines. However, existing benchmarks on punchline comprehension\nsuffer from three major limitations: 1) language shortcuts that allow models to\nsolely rely on text, 2) lack of question diversity, and 3) narrow focus on a\nspecific domain of multimodal content (e.g., cartoon). To address these\nlimitations, we introduce a multimodal \\textbf{Punch}line comprehension\n\\textbf{Bench}mark, named \\textbf{PunchBench}, which is tailored for accurate\nand comprehensive evaluation of punchline comprehension. To enhance the\nevaluation accuracy, we generate synonymous and antonymous captions by\nmodifying original captions, which mitigates the impact of shortcuts in the\ncaptions. To provide a comprehensive evaluation, PunchBench incorporates\ndiverse question formats and image-captions from various domains. On this\nbasis, we conduct extensive evaluations and reveal a significant gap between\nstate-of-the-art MLLMs and humans in punchline comprehension. To improve\npunchline comprehension, we propose Simple-to-Complex Chain-of-Question\n(SC-CoQ) strategy, enabling the models to incrementally address complicated\nquestions by first mastering simple ones. SC-CoQ effectively enhances the\nperformance of various MLLMs on PunchBench, surpassing in-context learning and\nchain-of-thought.\n","authors":["Kun Ouyang","Yuanxin Liu","Shicheng Li","Yi Liu","Hao Zhou","Fandong Meng","Jie Zhou","Xu Sun"],"pdf_url":"https://arxiv.org/pdf/2412.11906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02329v3","updated":"2024-12-16T15:42:53Z","published":"2024-01-04T16:06:31Z","title":"Exploring Vacant Classes in Label-Skewed Federated Learning","summary":"  Label skews, characterized by disparities in local label distribution across\nclients, pose a significant challenge in federated learning. As minority\nclasses suffer from worse accuracy due to overfitting on local imbalanced data,\nprior methods often incorporate class-balanced learning techniques during local\ntraining. Although these methods improve the mean accuracy across all classes,\nwe observe that vacant classes-referring to categories absent from a client's\ndata distribution-remain poorly recognized. Besides, there is still a gap in\nthe accuracy of local models on minority classes compared to the global model.\nThis paper introduces FedVLS, a novel approach to label-skewed federated\nlearning that integrates both vacant-class distillation and logit suppression\nsimultaneously. Specifically, vacant-class distillation leverages knowledge\ndistillation during local training on each client to retain essential\ninformation related to vacant classes from the global model. Moreover, logit\nsuppression directly penalizes network logits for non-label classes,\neffectively addressing misclassifications in minority classes that may be\nbiased toward majority classes. Extensive experiments validate the efficacy of\nFedVLS, demonstrating superior performance compared to previous\nstate-of-the-art (SOTA) methods across diverse datasets with varying degrees of\nlabel skews. Our code is available at https://github.com/krumpguo/FedVLS.\n","authors":["Kuangpu Guo","Yuhe Ding","Jian Liang","Ran He","Zilei Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2401.02329v3.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.11892v1","updated":"2024-12-16T15:41:14Z","published":"2024-12-16T15:41:14Z","title":"From 2D CAD Drawings to 3D Parametric Models: A Vision-Language Approach","summary":"  In this paper, we present CAD2Program, a new method for reconstructing 3D\nparametric models from 2D CAD drawings. Our proposed method is inspired by\nrecent successes in vision-language models (VLMs), and departs from traditional\nmethods which rely on task-specific data representations and/or algorithms.\nSpecifically, on the input side, we simply treat the 2D CAD drawing as a raster\nimage, regardless of its original format, and encode the image with a standard\nViT model. We show that such an encoding scheme achieves competitive\nperformance against existing methods that operate on vector-graphics inputs,\nwhile imposing substantially fewer restrictions on the 2D drawings. On the\noutput side, our method auto-regressively predicts a general-purpose language\ndescribing 3D parametric models in text form. Compared to other sequence\nmodeling methods for CAD which use domain-specific sequence representations\nwith fixed-size slots, our text-based representation is more flexible, and can\nbe easily extended to arbitrary geometric entities and semantic or functional\nproperties. Experimental results on a large-scale dataset of cabinet models\ndemonstrate the effectiveness of our method.\n","authors":["Xilin Wang","Jia Zheng","Yuanchao Hu","Hao Zhu","Qian Yu","Zihan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.11892v1.pdf","comment":"To Appear in AAAI 2025. The project page is at\n  https://manycore-research.github.io/CAD2Program"},{"id":"http://arxiv.org/abs/2412.11890v1","updated":"2024-12-16T15:38:25Z","published":"2024-12-16T15:38:25Z","title":"SegMAN: Omni-scale Context Modeling with State Space Models and Local\n  Attention for Semantic Segmentation","summary":"  High-quality semantic segmentation relies on three key capabilities: global\ncontext modeling, local detail encoding, and multi-scale feature extraction.\nHowever, recent methods struggle to possess all these capabilities\nsimultaneously. Hence, we aim to empower segmentation networks to\nsimultaneously carry out efficient global context modeling, high-quality local\ndetail encoding, and rich multi-scale feature representation for varying input\nresolutions. In this paper, we introduce SegMAN, a novel linear-time model\ncomprising a hybrid feature encoder dubbed SegMAN Encoder, and a decoder based\non state space models. Specifically, the SegMAN Encoder synergistically\nintegrates sliding local attention with dynamic state space models, enabling\nhighly efficient global context modeling while preserving fine-grained local\ndetails. Meanwhile, the MMSCopE module in our decoder enhances multi-scale\ncontext feature extraction and adaptively scales with the input resolution. We\ncomprehensively evaluate SegMAN on three challenging datasets: ADE20K,\nCityscapes, and COCO-Stuff. For instance, SegMAN-B achieves 52.6% mIoU on\nADE20K, outperforming SegNeXt-L by 1.6% mIoU while reducing computational\ncomplexity by over 15% GFLOPs. On Cityscapes, SegMAN-B attains 83.8% mIoU,\nsurpassing SegFormer-B3 by 2.1% mIoU with approximately half the GFLOPs.\nSimilarly, SegMAN-B improves upon VWFormer-B3 by 1.6% mIoU with lower GFLOPs on\nthe COCO-Stuff dataset. Our code is available at\nhttps://github.com/yunxiangfu2001/SegMAN.\n","authors":["Yunxiang Fu","Meng Lou","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2412.11890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11883v1","updated":"2024-12-16T15:32:05Z","published":"2024-12-16T15:32:05Z","title":"Towards Physically-Based Sky-Modeling","summary":"  Accurate environment maps are a key component in rendering photorealistic\noutdoor scenes with coherent illumination. They enable captivating visual arts,\nimmersive virtual reality and a wide range of engineering and scientific\napplications. Recent works have extended sky-models to be more comprehensive\nand inclusive of cloud formations but existing approaches fall short in\nfaithfully recreating key-characteristics in physically captured HDRI. As we\ndemonstrate, environment maps produced by sky-models do not relight scenes with\nthe same tones, shadows, and illumination coherence as physically captured HDR\nimagery. Though the visual quality of DNN-generated LDR and HDR imagery has\ngreatly progressed in recent years, we demonstrate this progress to be\ntangential to sky-modelling. Due to the Extended Dynamic Range (EDR) of 14EV\nrequired for outdoor environment maps inclusive of the sun, sky-modelling\nextends beyond the conventional paradigm of High Dynamic Range Imagery (HDRI).\nIn this work, we propose an all-weather sky-model, learning weathered-skies\ndirectly from physically captured HDR imagery. Per user-controlled positioning\nof the sun and cloud formations, our model (AllSky) allows for emulation of\nphysically captured environment maps with improved retention of the Extended\nDynamic Range (EDR) of the sky.\n","authors":["Ian J. Maquignaz"],"pdf_url":"https://arxiv.org/pdf/2412.11883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11866v1","updated":"2024-12-16T15:20:54Z","published":"2024-12-16T15:20:54Z","title":"Event-based Motion Deblurring via Multi-Temporal Granularity Fusion","summary":"  Conventional frame-based cameras inevitably produce blurry effects due to\nmotion occurring during the exposure time. Event camera, a bio-inspired sensor\noffering continuous visual information could enhance the deblurring\nperformance. Effectively utilizing the high-temporal-resolution event data is\ncrucial for extracting precise motion information and enhancing deblurring\nperformance. However, existing event-based image deblurring methods usually\nutilize voxel-based event representations, losing the fine-grained temporal\ndetails that are mathematically essential for fast motion deblurring. In this\npaper, we first introduce point cloud-based event representation into the image\ndeblurring task and propose a Multi-Temporal Granularity Network (MTGNet). It\ncombines the spatially dense but temporally coarse-grained voxel-based event\nrepresentation and the temporally fine-grained but spatially sparse point\ncloud-based event. To seamlessly integrate such complementary representations,\nwe design a Fine-grained Point Branch. An Aggregation and Mapping Module (AMM)\nis proposed to align the low-level point-based features with frame-based\nfeatures and an Adaptive Feature Diffusion Module (AFDM) is designed to manage\nthe resolution discrepancies between event data and image data by enriching the\nsparse point feature. Extensive subjective and objective evaluations\ndemonstrate that our method outperforms current state-of-the-art approaches on\nboth synthetic and real-world datasets.\n","authors":["Xiaopeng Lin","Hongwei Ren","Yulong Huang","Zunchang Liu","Yue Zhou","Haotian Fu","Biao Pan","Bojun Cheng"],"pdf_url":"https://arxiv.org/pdf/2412.11866v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.11863v1","updated":"2024-12-16T15:20:03Z","published":"2024-12-16T15:20:03Z","title":"GeoX: Geometric Problem Solving Through Unified Formalized\n  Vision-Language Pre-training","summary":"  Despite their proficiency in general tasks, Multi-modal Large Language Models\n(MLLMs) struggle with automatic Geometry Problem Solving (GPS), which demands\nunderstanding diagrams, interpreting symbols, and performing complex reasoning.\nThis limitation arises from their pre-training on natural images and texts,\nalong with the lack of automated verification in the problem-solving process.\nBesides, current geometric specialists are limited by their task-specific\ndesigns, making them less effective for broader geometric problems. To this\nend, we present GeoX, a multi-modal large model focusing on geometric\nunderstanding and reasoning tasks. Given the significant differences between\ngeometric diagram-symbol and natural image-text, we introduce unimodal\npre-training to develop a diagram encoder and symbol decoder, enhancing the\nunderstanding of geometric images and corpora. Furthermore, we introduce\ngeometry-language alignment, an effective pre-training paradigm that bridges\nthe modality gap between unimodal geometric experts. We propose a\nGenerator-And-Sampler Transformer (GS-Former) to generate discriminative\nqueries and eliminate uninformative representations from unevenly distributed\ngeometric signals. Finally, GeoX benefits from visual instruction tuning,\nempowering it to take geometric images and questions as input and generate\nverifiable solutions. Experiments show that GeoX outperforms both generalists\nand geometric specialists on publicly recognized benchmarks, such as GeoQA,\nUniGeo, Geometry3K, and PGPS9k.\n","authors":["Renqiu Xia","Mingsheng Li","Hancheng Ye","Wenjie Wu","Hongbin Zhou","Jiakang Yuan","Tianshuo Peng","Xinyu Cai","Xiangchao Yan","Bin Wang","Conghui He","Botian Shi","Tao Chen","Junchi Yan","Bo Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11863v1.pdf","comment":"Our code is available at https://github.com/UniModal4Reasoning/GeoX"},{"id":"http://arxiv.org/abs/2409.12784v5","updated":"2024-12-16T15:13:24Z","published":"2024-09-19T13:51:21Z","title":"Evaluating Image Hallucination in Text-to-Image Generation with\n  Question-Answering","summary":"  Despite the impressive success of text-to-image (TTI) generation models,\nexisting studies overlook the issue of whether these models accurately convey\nfactual information. In this paper, we focus on the problem of image\nhallucination, where images created by generation models fail to faithfully\ndepict factual content. To address this, we introduce I-HallA (Image\nHallucination evaluation with Question Answering), a novel automated evaluation\nmetric that measures the factuality of generated images through visual question\nanswering (VQA). We also introduce I-HallA v1.0, a curated benchmark dataset\nfor this purpose. As part of this process, we develop a pipeline that generates\nhigh-quality question-answer pairs using multiple GPT-4 Omni-based agents, with\nhuman judgments to ensure accuracy. Our evaluation protocols measure image\nhallucination by testing if images from existing text-to-image models can\ncorrectly respond to these questions. The I-HallA v1.0 dataset comprises 1.2K\ndiverse image-text pairs across nine categories with 1,000 rigorously curated\nquestions covering various compositional challenges. We evaluate five\ntext-to-image models using I-HallA and reveal that these state-of-the-art\nmodels often fail to accurately convey factual information. Moreover, we\nvalidate the reliability of our metric by demonstrating a strong Spearman\ncorrelation (rho=0.95) with human judgments. We believe our benchmark dataset\nand metric can serve as a foundation for developing factually accurate\ntext-to-image generation models.\n","authors":["Youngsun Lim","Hojun Choi","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2409.12784v5.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2412.11849v1","updated":"2024-12-16T15:10:53Z","published":"2024-12-16T15:10:53Z","title":"Ensemble Learning and 3D Pix2Pix for Comprehensive Brain Tumor Analysis\n  in Multimodal MRI","summary":"  Motivated by the need for advanced solutions in the segmentation and\ninpainting of glioma-affected brain regions in multi-modal magnetic resonance\nimaging (MRI), this study presents an integrated approach leveraging the\nstrengths of ensemble learning with hybrid transformer models and convolutional\nneural networks (CNNs), alongside the innovative application of 3D Pix2Pix\nGenerative Adversarial Network (GAN). Our methodology combines robust tumor\nsegmentation capabilities, utilizing axial attention and transformer encoders\nfor enhanced spatial relationship modeling, with the ability to synthesize\nbiologically plausible brain tissue through 3D Pix2Pix GAN. This integrated\napproach addresses the BraTS 2023 cluster challenges by offering precise\nsegmentation and realistic inpainting, tailored for diverse tumor types and\nsub-regions. The results demonstrate outstanding performance, evidenced by\nquantitative evaluations such as the Dice Similarity Coefficient (DSC),\nHausdorff Distance (HD95) for segmentation, and Structural Similarity Index\nMeasure (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Mean-Square Error (MSE)\nfor inpainting. Qualitative assessments further validate the high-quality,\nclinically relevant outputs. In conclusion, this study underscores the\npotential of combining advanced machine learning techniques for comprehensive\nbrain tumor analysis, promising significant advancements in clinical\ndecision-making and patient care within the realm of medical imaging.\n","authors":["Ramy A. Zeineldin","Franziska Mathis-Ullrich"],"pdf_url":"https://arxiv.org/pdf/2412.11849v1.pdf","comment":"Accepted at the MICCAI BraTS Challenge 2023"},{"id":"http://arxiv.org/abs/2412.11840v1","updated":"2024-12-16T15:03:08Z","published":"2024-12-16T15:03:08Z","title":"Sonar-based Deep Learning in Underwater Robotics: Overview, Robustness\n  and Challenges","summary":"  With the growing interest in underwater exploration and monitoring,\nAutonomous Underwater Vehicles (AUVs) have become essential. The recent\ninterest in onboard Deep Learning (DL) has advanced real-time environmental\ninteraction capabilities relying on efficient and accurate vision-based DL\nmodels. However, the predominant use of sonar in underwater environments,\ncharacterized by limited training data and inherent noise, poses challenges to\nmodel robustness. This autonomy improvement raises safety concerns for\ndeploying such models during underwater operations, potentially leading to\nhazardous situations. This paper aims to provide the first comprehensive\noverview of sonar-based DL under the scope of robustness. It studies\nsonar-based DL perception task models, such as classification, object\ndetection, segmentation, and SLAM. Furthermore, the paper systematizes\nsonar-based state-of-the-art datasets, simulators, and robustness methods such\nas neural network verification, out-of-distribution, and adversarial attacks.\nThis paper highlights the lack of robustness in sonar-based DL research and\nsuggests future research pathways, notably establishing a baseline sonar-based\ndataset and bridging the simulation-to-reality gap.\n","authors":["Martin Aubard","Ana Madureira","Luís Teixeira","José Pinto"],"pdf_url":"https://arxiv.org/pdf/2412.11840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11836v1","updated":"2024-12-16T14:57:40Z","published":"2024-12-16T14:57:40Z","title":"UnMA-CapSumT: Unified and Multi-Head Attention-driven Caption\n  Summarization Transformer","summary":"  Image captioning is the generation of natural language descriptions of images\nwhich have increased immense popularity in the recent past. With this different\ndeep-learning techniques are devised for the development of factual and\nstylized image captioning models. Previous models focused more on the\ngeneration of factual and stylized captions separately providing more than one\ncaption for a single image. The descriptions generated from these suffer from\nout-of-vocabulary and repetition issues. To the best of our knowledge, no such\nwork exists that provided a description that integrates different captioning\nmethods to describe the contents of an image with factual and stylized\n(romantic and humorous) elements. To overcome these limitations, this paper\npresents a novel Unified Attention and Multi-Head Attention-driven Caption\nSummarization Transformer (UnMA-CapSumT) based Captioning Framework. It\nutilizes both factual captions and stylized captions generated by the Modified\nAdaptive Attention-based factual image captioning model (MAA-FIC) and Style\nFactored Bi-LSTM with attention (SF-Bi-ALSTM) driven stylized image captioning\nmodel respectively. SF-Bi-ALSTM-based stylized IC model generates two prominent\nstyles of expression- {romance, and humor}. The proposed summarizer UnMHA-ST\ncombines both factual and stylized descriptions of an input image to generate\nstyled rich coherent summarized captions. The proposed UnMHA-ST transformer\nlearns and summarizes different linguistic styles efficiently by incorporating\nproposed word embedding fastText with Attention Word Embedding (fTA-WE) and\npointer-generator network with coverage mechanism concept to solve the\nout-of-vocabulary issues and repetition problem. Extensive experiments are\nconducted on Flickr8K and a subset of FlickrStyle10K with supporting ablation\nstudies to prove the efficiency and efficacy of the proposed framework.\n","authors":["Dhruv Sharma","Chhavi Dhiman","Dinesh Kumar"],"pdf_url":"https://arxiv.org/pdf/2412.11836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17629v4","updated":"2024-12-16T14:56:52Z","published":"2023-11-29T13:43:17Z","title":"RQFormer: Rotated Query Transformer for End-to-End Oriented Object\n  Detection","summary":"  Oriented object detection presents a challenging task due to the presence of\nobject instances with multiple orientations, varying scales, and dense\ndistributions. Recently, end-to-end detectors have made significant strides by\nemploying attention mechanisms and refining a fixed number of queries through\nconsecutive decoder layers. However, existing end-to-end oriented object\ndetectors still face two primary challenges: 1) misalignment between positional\nqueries and keys, leading to inconsistency between classification and\nlocalization; and 2) the presence of a large number of similar queries, which\ncomplicates one-to-one label assignments and optimization. To address these\nlimitations, we propose an end-to-end oriented detector called the Rotated\nQuery Transformer, which integrates two key technologies: Rotated RoI Attention\n(RRoI Attention) and Selective Distinct Queries (SDQ). First, RRoI Attention\naligns positional queries and keys from oriented regions of interest through\ncross-attention. Second, SDQ collects queries from intermediate decoder layers\nand filters out similar ones to generate distinct queries, thereby facilitating\nthe optimization of one-to-one label assignments. Finally, extensive\nexperiments conducted on four remote sensing datasets and one scene text\ndataset demonstrate the effectiveness of our method. To further validate its\ngeneralization capability, we also extend our approach to horizontal object\ndetection The code is available at\n\\url{https://github.com/wokaikaixinxin/RQFormer}.\n","authors":["Jiaqi Zhao","Zeyu Ding","Yong Zhou","Hancheng Zhu","Wenliang Du","Rui Yao","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2311.17629v4.pdf","comment":"This article is accepted by Expert Systems With Applications (ESWA)\n  2024"},{"id":"http://arxiv.org/abs/2403.13804v2","updated":"2024-12-16T14:53:21Z","published":"2024-03-20T17:59:43Z","title":"Learning from Synthetic Data for Visual Grounding","summary":"  This paper extensively investigates the effectiveness of synthetic training\ndata to improve the capabilities of vision-and-language models for grounding\ntextual descriptions to image regions. We explore various strategies to best\ngenerate image-text pairs and image-text-box triplets using a series of\npretrained models under different settings and varying degrees of reliance on\nreal data. Through comparative analyses with synthetic, real, and web-crawled\ndata, we identify factors that contribute to performance differences, and\npropose SynGround, an effective pipeline for generating useful synthetic data\nfor visual grounding. Our findings show that SynGround can improve the\nlocalization capabilities of off-the-shelf vision-and-language models and\noffers the potential for arbitrarily large scale data generation. Particularly,\ndata generated with SynGround improves the pointing game accuracy of a\npretrained ALBEF and BLIP models by 4.81% and 17.11% absolute percentage\npoints, respectively, across the RefCOCO+ and the Flickr30k benchmarks.\n","authors":["Ruozhen He","Ziyan Yang","Paola Cascante-Bonilla","Alexander C. Berg","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2403.13804v2.pdf","comment":"Project Page: https://catherine-r-he.github.io/SynGround/"},{"id":"http://arxiv.org/abs/2412.07527v2","updated":"2024-12-16T14:43:29Z","published":"2024-12-10T14:03:41Z","title":"Deep Joint Unrolling for Deblurring and Low-Light Image Enhancement\n  (JUDE)","summary":"  Low-light and blurring issues are prevalent when capturing photos at night,\noften due to the use of long exposure to address dim environments. Addressing\nthese joint problems can be challenging and error-prone if an end-to-end model\nis trained without incorporating an appropriate physical model. In this paper,\nwe introduce JUDE, a Deep Joint Unrolling for Deblurring and Low-Light Image\nEnhancement, inspired by the image physical model. Based on Retinex theory and\nthe blurring model, the low-light blurry input is iteratively deblurred and\ndecomposed, producing sharp low-light reflectance and illuminance through an\nunrolling mechanism. Additionally, we incorporate various modules to estimate\nthe initial blur kernel, enhance brightness, and eliminate noise in the final\nimage. Comprehensive experiments on LOL-Blur and Real-LOL-Blur demonstrate that\nour method outperforms existing techniques both quantitatively and\nqualitatively.\n","authors":["Tu Vo","Chan Y. Park"],"pdf_url":"https://arxiv.org/pdf/2412.07527v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2412.11820v1","updated":"2024-12-16T14:37:16Z","published":"2024-12-16T14:37:16Z","title":"Spatiotemporal Blind-Spot Network with Calibrated Flow Alignment for\n  Self-Supervised Video Denoising","summary":"  Self-supervised video denoising aims to remove noise from videos without\nrelying on ground truth data, leveraging the video itself to recover clean\nframes. Existing methods often rely on simplistic feature stacking or apply\noptical flow without thorough analysis. This results in suboptimal utilization\nof both inter-frame and intra-frame information, and it also neglects the\npotential of optical flow alignment under self-supervised conditions, leading\nto biased and insufficient denoising outcomes. To this end, we first explore\nthe practicality of optical flow in the self-supervised setting and introduce a\nSpatioTemporal Blind-spot Network (STBN) for global frame feature utilization.\nIn the temporal domain, we utilize bidirectional blind-spot feature propagation\nthrough the proposed blind-spot alignment block to ensure accurate temporal\nalignment and effectively capture long-range dependencies. In the spatial\ndomain, we introduce the spatial receptive field expansion module, which\nenhances the receptive field and improves global perception capabilities.\nAdditionally, to reduce the sensitivity of optical flow estimation to noise, we\npropose an unsupervised optical flow distillation mechanism that refines\nfine-grained inter-frame interactions during optical flow alignment. Our method\ndemonstrates superior performance across both synthetic and real-world video\ndenoising datasets. The source code is publicly available at\nhttps://github.com/ZKCCZ/STBN.\n","authors":["Zikang Chen","Tao Jiang","Xiaowan Hu","Wang Zhang","Huaqiu Li","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11819v1","updated":"2024-12-16T14:35:52Z","published":"2024-12-16T14:35:52Z","title":"HiGDA: Hierarchical Graph of Nodes to Learn Local-to-Global Topology for\n  Semi-Supervised Domain Adaptation","summary":"  The enhanced representational power and broad applicability of deep learning\nmodels have attracted significant interest from the research community in\nrecent years. However, these models often struggle to perform effectively under\ndomain shift conditions, where the training data (the source domain) is related\nto but exhibits different distributions from the testing data (the target\ndomain). To address this challenge, previous studies have attempted to reduce\nthe domain gap between source and target data by incorporating a few labeled\ntarget samples during training - a technique known as semi-supervised domain\nadaptation (SSDA). While this strategy has demonstrated notable improvements in\nclassification performance, the network architectures used in these approaches\nprimarily focus on exploiting the features of individual images, leaving room\nfor improvement in capturing rich representations. In this study, we introduce\na Hierarchical Graph of Nodes designed to simultaneously present\nrepresentations at both feature and category levels. At the feature level, we\nintroduce a local graph to identify the most relevant patches within an image,\nfacilitating adaptability to defined main object representations. At the\ncategory level, we employ a global graph to aggregate the features from samples\nwithin the same category, thereby enriching overall representations. Extensive\nexperiments on widely used SSDA benchmark datasets, including Office-Home,\nDomainNet, and VisDA2017, demonstrate that both quantitative and qualitative\nresults substantiate the effectiveness of HiGDA, establishing it as a new\nstate-of-the-art method.\n","authors":["Ba Hung Ngo","Doanh C. Bui","Nhat-Tuong Do-Tran","Tae Jong Choi"],"pdf_url":"https://arxiv.org/pdf/2412.11819v1.pdf","comment":"Accepted for presentation at AAAI2025"},{"id":"http://arxiv.org/abs/2404.13282v2","updated":"2024-12-16T14:33:03Z","published":"2024-04-20T06:01:09Z","title":"Wills Aligner: Multi-Subject Collaborative Brain Visual Decoding","summary":"  Decoding visual information from human brain activity has seen remarkable\nadvancements in recent research. However, the diversity in cortical\nparcellation and fMRI patterns across individuals has prompted the development\nof deep learning models tailored to each subject. The personalization limits\nthe broader applicability of brain visual decoding in real-world scenarios. To\naddress this issue, we introduce Wills Aligner, a novel approach designed to\nachieve multi-subject collaborative brain visual decoding. Wills Aligner begins\nby aligning the fMRI data from different subjects at the anatomical level. It\nthen employs delicate mixture-of-brain-expert adapters and a meta-learning\nstrategy to account for individual fMRI pattern differences. Additionally,\nWills Aligner leverages the semantic relation of visual stimuli to guide the\nlearning of inter-subject commonality, enabling visual decoding for each\nsubject to draw insights from other subjects' data. We rigorously evaluate our\nWills Aligner across various visual decoding tasks, including classification,\ncross-modal retrieval, and image reconstruction. The experimental results\ndemonstrate that Wills Aligner achieves promising performance.\n","authors":["Guangyin Bao","Qi Zhang","Zixuan Gong","Jialei Zhou","Wei Fan","Kun Yi","Usman Naseem","Liang Hu","Duoqian Miao"],"pdf_url":"https://arxiv.org/pdf/2404.13282v2.pdf","comment":"AAAI 2025, 16 pages"},{"id":"http://arxiv.org/abs/2412.11815v1","updated":"2024-12-16T14:32:49Z","published":"2024-12-16T14:32:49Z","title":"ColorFlow: Retrieval-Augmented Image Sequence Colorization","summary":"  Automatic black-and-white image sequence colorization while preserving\ncharacter and object identity (ID) is a complex task with significant market\ndemand, such as in cartoon or comic series colorization. Despite advancements\nin visual colorization using large-scale generative models like diffusion\nmodels, challenges with controllability and identity consistency persist,\nmaking current solutions unsuitable for industrial application.To address this,\nwe propose ColorFlow, a three-stage diffusion-based framework tailored for\nimage sequence colorization in industrial applications. Unlike existing methods\nthat require per-ID finetuning or explicit ID embedding extraction, we propose\na novel robust and generalizable Retrieval Augmented Colorization pipeline for\ncolorizing images with relevant color references. Our pipeline also features a\ndual-branch design: one branch for color identity extraction and the other for\ncolorization, leveraging the strengths of diffusion models. We utilize the\nself-attention mechanism in diffusion models for strong in-context learning and\ncolor identity matching. To evaluate our model, we introduce ColorFlow-Bench, a\ncomprehensive benchmark for reference-based colorization. Results show that\nColorFlow outperforms existing models across multiple metrics, setting a new\nstandard in sequential image colorization and potentially benefiting the art\nindustry. We release our codes and models on our project page:\nhttps://zhuang2002.github.io/ColorFlow/.\n","authors":["Junhao Zhuang","Xuan Ju","Zhaoyang Zhang","Yong Liu","Shiyi Zhang","Chun Yuan","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2412.11815v1.pdf","comment":"Project Page: https://zhuang2002.github.io/ColorFlow/"},{"id":"http://arxiv.org/abs/2412.11813v1","updated":"2024-12-16T14:29:31Z","published":"2024-12-16T14:29:31Z","title":"Designing Semi-Structured Pruning of Graph Convolutional Networks for\n  Skeleton-based Recognition","summary":"  Deep neural networks (DNNs) are nowadays witnessing a major success in\nsolving many pattern recognition tasks including skeleton-based classification.\nThe deployment of DNNs on edge-devices, endowed with limited time and memory\nresources, requires designing lightweight and efficient variants of these\nnetworks. Pruning is one of the lightweight network design techniques that\noperate by removing unnecessary network parts, in a structured or an\nunstructured manner, including individual weights, neurons or even entire\nchannels. Nonetheless, structured and unstructured pruning methods, when\napplied separately, may either be inefficient or ineffective. In this paper, we\ndevise a novel semi-structured method that discards the downsides of structured\nand unstructured pruning while gathering their upsides to some extent. The\nproposed solution is based on a differentiable cascaded parametrization which\ncombines (i) a band-stop mechanism that prunes weights depending on their\nmagnitudes, (ii) a weight-sharing parametrization that prunes connections\neither individually or group-wise, and (iii) a gating mechanism which\narbitrates between different group-wise and entry-wise pruning. All these\ncascaded parametrizations are built upon a common latent tensor which is\ntrained end-to-end by minimizing a classification loss and a surrogate tensor\nrank regularizer. Extensive experiments, conducted on the challenging tasks of\naction and hand-gesture recognition, show the clear advantage of our proposed\nsemi-structured pruning approach against both structured and unstructured\npruning, when taken separately, as well as the related work.\n","authors":["Hichem Sahbi"],"pdf_url":"https://arxiv.org/pdf/2412.11813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11812v1","updated":"2024-12-16T14:25:52Z","published":"2024-12-16T14:25:52Z","title":"CLDA-YOLO: Visual Contrastive Learning Based Domain Adaptive YOLO\n  Detector","summary":"  Unsupervised domain adaptive (UDA) algorithms can markedly enhance the\nperformance of object detectors under conditions of domain shifts, thereby\nreducing the necessity for extensive labeling and retraining. Current domain\nadaptive object detection algorithms primarily cater to two-stage detectors,\nwhich tend to offer minimal improvements when directly applied to single-stage\ndetectors such as YOLO. Intending to benefit the YOLO detector from UDA, we\nbuild a comprehensive domain adaptive architecture using a teacher-student\ncooperative system for the YOLO detector. In this process, we propose\nuncertainty learning to cope with pseudo-labeling generated by the teacher\nmodel with extreme uncertainty and leverage dynamic data augmentation to\nasymptotically adapt the teacher-student system to the environment. To address\nthe inability of single-stage object detectors to align at multiple stages, we\nutilize a unified visual contrastive learning paradigm that aligns instance at\nbackbone and head respectively, which steadily improves the robustness of the\ndetectors in cross-domain tasks. In summary, we present an unsupervised domain\nadaptive YOLO detector based on visual contrastive learning (CLDA-YOLO), which\nachieves highly competitive results across multiple domain adaptive datasets\nwithout any reduction in inference speed.\n","authors":["Tianheng Qiu","Ka Lung Law","Guanghua Pan","Jufei Wang","Xin Gao","Xuan Huang","Hu Wei"],"pdf_url":"https://arxiv.org/pdf/2412.11812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11807v1","updated":"2024-12-16T14:18:01Z","published":"2024-12-16T14:18:01Z","title":"PhysAug: A Physical-guided and Frequency-based Data Augmentation for\n  Single-Domain Generalized Object Detection","summary":"  Single-Domain Generalized Object Detection~(S-DGOD) aims to train on a single\nsource domain for robust performance across a variety of unseen target domains\nby taking advantage of an object detector. Existing S-DGOD approaches often\nrely on data augmentation strategies, including a composition of visual\ntransformations, to enhance the detector's generalization ability. However, the\nabsence of real-world prior knowledge hinders data augmentation from\ncontributing to the diversity of training data distributions. To address this\nissue, we propose PhysAug, a novel physical model-based non-ideal imaging\ncondition data augmentation method, to enhance the adaptability of the S-DGOD\ntasks. Drawing upon the principles of atmospheric optics, we develop a\nuniversal perturbation model that serves as the foundation for our proposed\nPhysAug. Given that visual perturbations typically arise from the interaction\nof light with atmospheric particles, the image frequency spectrum is harnessed\nto simulate real-world variations during training. This approach fosters the\ndetector to learn domain-invariant representations, thereby enhancing its\nability to generalize across various settings. Without altering the network\narchitecture or loss function, our approach significantly outperforms the\nstate-of-the-art across various S-DGOD datasets. In particular, it achieves a\nsubstantial improvement of $7.3\\%$ and $7.2\\%$ over the baseline on DWD and\nCityscape-C, highlighting its enhanced generalizability in real-world settings.\n","authors":["Xiaoran Xu","Jiangang Yang","Wenhui Shi","Siyuan Ding","Luqing Luo","Jian Liu"],"pdf_url":"https://arxiv.org/pdf/2412.11807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11802v1","updated":"2024-12-16T14:12:06Z","published":"2024-12-16T14:12:06Z","title":"AMI-Net: Adaptive Mask Inpainting Network for Industrial Anomaly\n  Detection and Localization","summary":"  Unsupervised visual anomaly detection is crucial for enhancing industrial\nproduction quality and efficiency. Among unsupervised methods, reconstruction\napproaches are popular due to their simplicity and effectiveness. The key\naspect of reconstruction methods lies in the restoration of anomalous regions,\nwhich current methods have not satisfactorily achieved. To tackle this issue,\nwe introduce a novel \\uline{A}daptive \\uline{M}ask \\uline{I}npainting\n\\uline{Net}work (AMI-Net) from the perspective of adaptive mask-inpainting. In\ncontrast to traditional reconstruction methods that treat non-semantic image\npixels as targets, our method uses a pre-trained network to extract multi-scale\nsemantic features as reconstruction targets. Given the multiscale nature of\nindustrial defects, we incorporate a training strategy involving random\npositional and quantitative masking. Moreover, we propose an innovative\nadaptive mask generator capable of generating adaptive masks that effectively\nmask anomalous regions while preserving normal regions. In this manner, the\nmodel can leverage the visible normal global contextual information to restore\nthe masked anomalous regions, thereby effectively suppressing the\nreconstruction of defects. Extensive experimental results on the MVTec AD and\nBTAD industrial datasets validate the effectiveness of the proposed method.\nAdditionally, AMI-Net exhibits exceptional real-time performance, striking a\nfavorable balance between detection accuracy and speed, rendering it highly\nsuitable for industrial applications. Code is available at:\nhttps://github.com/luow23/AMI-Net\n","authors":["Wei Luo","Haiming Yao","Wenyong Yu","Zhengyong Li"],"pdf_url":"https://arxiv.org/pdf/2412.11802v1.pdf","comment":"Accepted by IEEE Transactions on Automation Science and\n  Engineering.Code is available at: https://github.com/luow23/AMI-Net"},{"id":"http://arxiv.org/abs/2409.09424v2","updated":"2024-12-16T14:05:25Z","published":"2024-09-14T12:25:14Z","title":"NBBOX: Noisy Bounding Box Improves Remote Sensing Object Detection","summary":"  Data augmentation has shown significant advancements in computer vision to\nimprove model performance over the years, particularly in scenarios with\nlimited and insufficient data. Currently, most studies focus on adjusting the\nimage or its features to expand the size, quality, and variety of samples\nduring training in various tasks including object detection. However, we argue\nthat it is necessary to investigate bounding box transformations as a data\naugmentation technique rather than image-level transformations, especially in\naerial imagery due to potentially inconsistent bounding box annotations. Hence,\nthis letter presents a thorough investigation of bounding box transformation in\nterms of scaling, rotation, and translation for remote sensing object\ndetection. We call this augmentation strategy NBBOX (Noise Injection into\nBounding Box). We conduct extensive experiments on DOTA and DIOR-R, both\nwell-known datasets that include a variety of rotated generic objects in aerial\nimages. Experimental results show that our approach significantly improves\nremote sensing object detection without whistles and bells and it is more\ntime-efficient than other state-of-the-art augmentation strategies.\n","authors":["Yechan Kim","SooYeon Kim","Moongu Jeon"],"pdf_url":"https://arxiv.org/pdf/2409.09424v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11788v1","updated":"2024-12-16T14:00:30Z","published":"2024-12-16T14:00:30Z","title":"Neural Collapse Inspired Knowledge Distillation","summary":"  Existing knowledge distillation (KD) methods have demonstrated their ability\nin achieving student network performance on par with their teachers. However,\nthe knowledge gap between the teacher and student remains significant and may\nhinder the effectiveness of the distillation process. In this work, we\nintroduce the structure of Neural Collapse (NC) into the KD framework. NC\ntypically occurs in the final phase of training, resulting in a graceful\ngeometric structure where the last-layer features form a simplex equiangular\ntight frame. Such phenomenon has improved the generalization of deep network\ntraining. We hypothesize that NC can also alleviate the knowledge gap in\ndistillation, thereby enhancing student performance. This paper begins with an\nempirical analysis to bridge the connection between knowledge distillation and\nneural collapse. Through this analysis, we establish that transferring the\nteacher's NC structure to the student benefits the distillation process.\nTherefore, instead of merely transferring instance-level logits or features, as\ndone by existing distillation methods, we encourage students to learn the\nteacher's NC structure. Thereby, we propose a new distillation paradigm termed\nNeural Collapse-inspired Knowledge Distillation (NCKD). Comprehensive\nexperiments demonstrate that NCKD is simple yet effective, improving the\ngeneralization of all distilled student models and achieving state-of-the-art\naccuracy performance.\n","authors":["Shuoxi Zhang","Zijian Song","Kun He"],"pdf_url":"https://arxiv.org/pdf/2412.11788v1.pdf","comment":"13 pages, 7 figures. Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2404.12630v2","updated":"2024-12-16T13:59:51Z","published":"2024-04-19T05:12:04Z","title":"MindTuner: Cross-Subject Visual Decoding with Visual Fingerprint and\n  Semantic Correction","summary":"  Decoding natural visual scenes from brain activity has flourished, with\nextensive research in single-subject tasks and, however, less in cross-subject\ntasks. Reconstructing high-quality images in cross-subject tasks is a\nchallenging problem due to profound individual differences between subjects and\nthe scarcity of data annotation. In this work, we proposed MindTuner for\ncross-subject visual decoding, which achieves high-quality and rich semantic\nreconstructions using only 1 hour of fMRI training data benefiting from the\nphenomena of visual fingerprint in the human visual system and a novel\nfMRI-to-text alignment paradigm. Firstly, we pre-train a multi-subject model\namong 7 subjects and fine-tune it with scarce data on new subjects, where LoRAs\nwith Skip-LoRAs are utilized to learn the visual fingerprint. Then, we take the\nimage modality as the intermediate pivot modality to achieve fMRI-to-text\nalignment, which achieves impressive fMRI-to-text retrieval performance and\ncorrects fMRI-to-image reconstruction with fine-tuned semantics. The results of\nboth qualitative and quantitative analyses demonstrate that MindTuner surpasses\nstate-of-the-art cross-subject visual decoding models on the Natural Scenes\nDataset (NSD), whether using training data of 1 hour or 40 hours.\n","authors":["Zixuan Gong","Qi Zhang","Guangyin Bao","Lei Zhu","Ke Liu","Liang Hu","Duoqian Miao"],"pdf_url":"https://arxiv.org/pdf/2404.12630v2.pdf","comment":"AAAI 2025, 14 pages"},{"id":"http://arxiv.org/abs/2412.11785v1","updated":"2024-12-16T13:57:02Z","published":"2024-12-16T13:57:02Z","title":"InterDyn: Controllable Interactive Dynamics with Video Diffusion Models","summary":"  Predicting the dynamics of interacting objects is essential for both humans\nand intelligent systems. However, existing approaches are limited to\nsimplified, toy settings and lack generalizability to complex, real-world\nenvironments. Recent advances in generative models have enabled the prediction\nof state transitions based on interventions, but focus on generating a single\nfuture state which neglects the continuous motion and subsequent dynamics\nresulting from the interaction. To address this gap, we propose InterDyn, a\nnovel framework that generates videos of interactive dynamics given an initial\nframe and a control signal encoding the motion of a driving object or actor.\nOur key insight is that large video foundation models can act as both neural\nrenderers and implicit physics simulators by learning interactive dynamics from\nlarge-scale video data. To effectively harness this capability, we introduce an\ninteractive control mechanism that conditions the video generation process on\nthe motion of the driving entity. Qualitative results demonstrate that InterDyn\ngenerates plausible, temporally consistent videos of complex object\ninteractions while generalizing to unseen objects. Quantitative evaluations\nshow that InterDyn outperforms baselines that focus on static state\ntransitions. This work highlights the potential of leveraging video generative\nmodels as implicit physics engines.\n","authors":["Rick Akkerman","Haiwen Feng","Michael J. Black","Dimitrios Tzionas","Victoria Fernández Abrevaya"],"pdf_url":"https://arxiv.org/pdf/2412.11785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19746v2","updated":"2024-12-16T13:53:50Z","published":"2024-05-30T06:49:59Z","title":"DenseSeg: Joint Learning for Semantic Segmentation and Landmark\n  Detection Using Dense Image-to-Shape Representation","summary":"  Purpose: Semantic segmentation and landmark detection are fundamental tasks\nof medical image processing, facilitating further analysis of anatomical\nobjects. Although deep learning-based pixel-wise classification has set a\nnew-state-of-the-art for segmentation, it falls short in landmark detection, a\nstrength of shape-based approaches. Methods: In this work, we propose a dense\nimage-to-shape representation that enables the joint learning of landmarks and\nsemantic segmentation by employing a fully convolutional architecture. Our\nmethod intuitively allows the extraction of arbitrary landmarks due to its\nrepresentation of anatomical correspondences. We benchmark our method against\nthe state-of-the-art for semantic segmentation (nnUNet), a shape-based approach\nemploying geometric deep learning and a convolutional neural network-based\nmethod for landmark detection. Results: We evaluate our method on two medical\ndataset: one common benchmark featuring the lungs, heart, and clavicle from\nthorax X-rays, and another with 17 different bones in the paediatric wrist.\nWhile our method is on pair with the landmark detection baseline in the thorax\nsetting (error in mm of $2.6\\pm0.9$ vs $2.7\\pm0.9$), it substantially surpassed\nit in the more complex wrist setting ($1.1\\pm0.6$ vs $1.9\\pm0.5$). Conclusion:\nWe demonstrate that dense geometric shape representation is beneficial for\nchallenging landmark detection tasks and outperforms previous state-of-the-art\nusing heatmap regression. While it does not require explicit training on the\nlandmarks themselves, allowing for the addition of new landmarks without\nnecessitating retraining.}\n","authors":["Ron Keuth","Lasse Hansen","Maren Balks","Ronja Jäger","Anne-Nele Schröder","Ludger Tüshaus","Mattias Heinrich"],"pdf_url":"https://arxiv.org/pdf/2405.19746v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11779v1","updated":"2024-12-16T13:49:57Z","published":"2024-12-16T13:49:57Z","title":"Impact of Face Alignment on Face Image Quality","summary":"  Face alignment is a crucial step in preparing face images for feature\nextraction in facial analysis tasks. For applications such as face recognition,\nfacial expression recognition, and facial attribute classification, alignment\nis widely utilized during both training and inference to standardize the\npositions of key landmarks in the face. It is well known that the application\nand method of face alignment significantly affect the performance of facial\nanalysis models. However, the impact of alignment on face image quality has not\nbeen thoroughly investigated. Current FIQA studies often assume alignment as a\nprerequisite but do not explicitly evaluate how alignment affects quality\nmetrics, especially with the advent of modern deep learning-based detectors\nthat integrate detection and landmark localization. To address this need, our\nstudy examines the impact of face alignment on face image quality scores. We\nconducted experiments on the LFW, IJB-B, and SCFace datasets, employing MTCNN\nand RetinaFace models for face detection and alignment. To evaluate face image\nquality, we utilized several assessment methods, including SER-FIQ, FaceQAN,\nDifFIQA, and SDD-FIQA. Our analysis included examining quality score\ndistributions for the LFW and IJB-B datasets and analyzing average quality\nscores at varying distances in the SCFace dataset. Our findings reveal that\nface image quality assessment methods are sensitive to alignment. Moreover,\nthis sensitivity increases under challenging real-life conditions, highlighting\nthe importance of evaluating alignment's role in quality assessment.\n","authors":["Eren Onaran","Erdi Sarıtaş","Hazım Kemal Ekenel"],"pdf_url":"https://arxiv.org/pdf/2412.11779v1.pdf","comment":"Accepted at EAI ROSENET 2024 - 8th EAI International Conference on\n  Robotic Sensor Networks"},{"id":"http://arxiv.org/abs/2412.04903v2","updated":"2024-12-16T13:47:29Z","published":"2024-12-06T09:59:47Z","title":"EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation","summary":"  Multimodal large language models (MLLMs) have achieved remarkable progress on\nvarious visual question answering and reasoning tasks leveraging instruction\nfine-tuning specific datasets. They can also learn from preference data\nannotated by human to enhance their reasoning ability and mitigate\nhallucinations. Most of preference data is generated from the model itself.\nHowever, existing methods require high-quality critical labels, which are\ncostly and rely on human or proprietary models like GPT-4V. In this work, we\npropose Enhancing Alignment in MLLMs via Critical Observation (EACO), which\naligns MLLMs by self-generated preference data using only 5k images\neconomically. Our approach begins with collecting and refining a Scoring\nEvaluation Instruction-tuning dataset to train a critical evaluation model,\ntermed the Critic. This Critic observes model responses across multiple\ndimensions, selecting preferred and non-preferred outputs for refined Direct\nPreference Optimization (DPO) tuning. To further enhance model performance, we\nemploy an additional supervised fine-tuning stage after preference tuning. EACO\nreduces the overall hallucinations by 65.6% on HallusionBench and improves the\nreasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement\nover LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also\nshows the potential critical ability in open-source MLLMs, demonstrating that\nEACO is a viable path to boost the competence of MLLMs.\n","authors":["Yongxin Wang","Meng Cao","Haokun Lin","Mingfei Han","Liang Ma","Jin Jiang","Yuhao Cheng","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2412.04903v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2412.11771v1","updated":"2024-12-16T13:44:26Z","published":"2024-12-16T13:44:26Z","title":"Point Cloud-Assisted Neural Image Compression","summary":"  High-efficient image compression is a critical requirement. In several\nscenarios where multiple modalities of data are captured by different sensors,\nthe auxiliary information from other modalities are not fully leveraged by\nexisting image-only codecs, leading to suboptimal compression efficiency. In\nthis paper, we increase image compression performance with the assistance of\npoint cloud, which is widely adopted in the area of autonomous driving. We\nfirst unify the data representation for both modalities to facilitate data\nprocessing. Then, we propose the point cloud-assisted neural image codec\n(PCA-NIC) to enhance the preservation of image texture and structure by\nutilizing the high-dimensional point cloud information. We further introduce a\nmulti-modal feature fusion transform module (MMFFT) to capture more\nrepresentative image features, remove redundant information between channels\nand modalities that are not relevant to the image content. Our work is the\nfirst to improve image compression performance using point cloud and achieves\nstate-of-the-art performance.\n","authors":["Ziqun Li","Qi Zhang","Xiaofeng Huang","Zhao Wang","Siwei Ma","Wei Yan"],"pdf_url":"https://arxiv.org/pdf/2412.11771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11767v1","updated":"2024-12-16T13:39:32Z","published":"2024-12-16T13:39:32Z","title":"IDEA-Bench: How Far are Generative Models from Professional Designing?","summary":"  Real-world design tasks - such as picture book creation, film storyboard\ndevelopment using character sets, photo retouching, visual effects, and font\ntransfer - are highly diverse and complex, requiring deep interpretation and\nextraction of various elements from instructions, descriptions, and reference\nimages. The resulting images often implicitly capture key features from\nreferences or user inputs, making it challenging to develop models that can\neffectively address such varied tasks. While existing visual generative models\ncan produce high-quality images based on prompts, they face significant\nlimitations in professional design scenarios that involve varied forms and\nmultiple inputs and outputs, even when enhanced with adapters like ControlNets\nand LoRAs. To address this, we introduce IDEA-Bench, a comprehensive benchmark\nencompassing 100 real-world design tasks, including rendering, visual effects,\nstoryboarding, picture books, fonts, style-based, and identity-preserving\ngeneration, with 275 test cases to thoroughly evaluate a model's\ngeneral-purpose generation capabilities. Notably, even the best-performing\nmodel only achieves 22.48 on IDEA-Bench, while the best general-purpose model\nonly achieves 6.81. We provide a detailed analysis of these results,\nhighlighting the inherent challenges and providing actionable directions for\nimprovement. Additionally, we provide a subset of 18 representative tasks\nequipped with multimodal large language model (MLLM)-based auto-evaluation\ntechniques to facilitate rapid model development and comparison. We releases\nthe benchmark data, evaluation toolkits, and an online leaderboard at\nhttps://github.com/ali-vilab/IDEA-Bench, aiming to drive the advancement of\ngenerative models toward more versatile and applicable intelligent design\nsystems.\n","authors":["Chen Liang","Lianghua Huang","Jingwu Fang","Huanzhang Dou","Wei Wang","Zhi-Fan Wu","Yupeng Shi","Junge Zhang","Xin Zhao","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2412.11767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11762v1","updated":"2024-12-16T13:26:52Z","published":"2024-12-16T13:26:52Z","title":"GS-ProCams: Gaussian Splatting-based Projector-Camera Systems","summary":"  We present GS-ProCams, the first Gaussian Splatting-based framework for\nprojector-camera systems (ProCams). GS-ProCams significantly enhances the\nefficiency of projection mapping (PM) that requires establishing geometric and\nradiometric mappings between the projector and the camera. Previous CNN-based\nProCams are constrained to a specific viewpoint, limiting their applicability\nto novel perspectives. In contrast, NeRF-based ProCams support view-agnostic\nprojection mapping, however, they require an additional colocated light source\nand demand significant computational and memory resources. To address this\nissue, we propose GS-ProCams that employs 2D Gaussian for scene\nrepresentations, and enables efficient view-agnostic ProCams applications. In\nparticular, we explicitly model the complex geometric and photometric mappings\nof ProCams using projector responses, the target surface's geometry and\nmaterials represented by Gaussians, and global illumination component. Then, we\nemploy differentiable physically-based rendering to jointly estimate them from\ncaptured multi-view projections. Compared to state-of-the-art NeRF-based\nmethods, our GS-ProCams eliminates the need for additional devices, achieving\nsuperior ProCams simulation quality. It is also 600 times faster and uses only\n1/10 of the GPU memory.\n","authors":["Qingyue Deng","Jijiang Li","Haibin Ling","Bingyao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11755v1","updated":"2024-12-16T13:19:41Z","published":"2024-12-16T13:19:41Z","title":"Generative Inbetweening through Frame-wise Conditions-Driven Video\n  Generation","summary":"  Generative inbetweening aims to generate intermediate frame sequences by\nutilizing two key frames as input. Although remarkable progress has been made\nin video generation models, generative inbetweening still faces challenges in\nmaintaining temporal stability due to the ambiguous interpolation path between\ntwo key frames. This issue becomes particularly severe when there is a large\nmotion gap between input frames. In this paper, we propose a straightforward\nyet highly effective Frame-wise Conditions-driven Video Generation (FCVG)\nmethod that significantly enhances the temporal stability of interpolated video\nframes. Specifically, our FCVG provides an explicit condition for each frame,\nmaking it much easier to identify the interpolation path between two input\nframes and thus ensuring temporally stable production of visually plausible\nvideo frames. To achieve this, we suggest extracting matched lines from two\ninput frames that can then be easily interpolated frame by frame, serving as\nframe-wise conditions seamlessly integrated into existing video generation\nmodels. In extensive evaluations covering diverse scenarios such as natural\nlandscapes, complex human poses, camera movements and animations, existing\nmethods often exhibit incoherent transitions across frames. In contrast, our\nFCVG demonstrates the capability to generate temporally stable videos using\nboth linear and non-linear interpolation curves. Our project page and code are\navailable at \\url{https://fcvg-inbetween.github.io/}.\n","authors":["Tianyi Zhu","Dongwei Ren","Qilong Wang","Xiaohe Wu","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2412.11755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11610v3","updated":"2024-12-16T13:16:45Z","published":"2024-10-15T13:46:19Z","title":"Enhanced Encoder-Decoder Architecture for Accurate Monocular Depth\n  Estimation","summary":"  Estimating depth from a single 2D image is a challenging task due to the lack\nof stereo or multi-view data, which are typically required for depth\nperception. This paper introduces a novel deep learning-based approach using an\nenhanced encoder-decoder architecture, where the Inception-ResNet-v2 model\nserves as the encoder. This is the first instance of utilizing\nInception-ResNet-v2 as an encoder for monocular depth estimation, demonstrating\nimproved performance over previous models. Our model effectively captures\ncomplex objects and fine-grained details, which are generally difficult to\npredict. Additionally, it incorporates multi-scale feature extraction to\nenhance depth prediction accuracy across various object sizes and distances. We\npropose a composite loss function comprising depth loss, gradient edge loss,\nand Structural Similarity Index Measure (SSIM) loss, with fine-tuned weights to\noptimize the weighted sum, ensuring a balance across different aspects of depth\nestimation. Experimental results on the NYU Depth V2 dataset show that our\nmodel achieves state-of-the-art performance, with an Absolute Relative Error\n(ARE) of 0.064, Root Mean Square Error (RMSE) of 0.228, and accuracy ($\\delta$\n< 1.25) of 89.3%. These metrics demonstrate that our model can accurately\npredict depth even in challenging scenarios, providing a scalable solution for\nreal-world applications in robotics, 3D reconstruction, and augmented reality.\n","authors":["Dabbrata Das","Argho Deb Das","Farhan Sadaf"],"pdf_url":"https://arxiv.org/pdf/2410.11610v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11251v2","updated":"2024-12-16T13:15:13Z","published":"2023-06-20T03:05:28Z","title":"Lipschitz Singularities in Diffusion Models","summary":"  Diffusion models, which employ stochastic differential equations to sample\nimages through integrals, have emerged as a dominant class of generative\nmodels. However, the rationality of the diffusion process itself receives\nlimited attention, leaving the question of whether the problem is well-posed\nand well-conditioned. In this paper, we explore a perplexing tendency of\ndiffusion models: they often display the infinite Lipschitz property of the\nnetwork with respect to time variable near the zero point. We provide\ntheoretical proofs to illustrate the presence of infinite Lipschitz constants\nand empirical results to confirm it. The Lipschitz singularities pose a threat\nto the stability and accuracy during both the training and inference processes\nof diffusion models. Therefore, the mitigation of Lipschitz singularities holds\ngreat potential for enhancing the performance of diffusion models. To address\nthis challenge, we propose a novel approach, dubbed E-TSDM, which alleviates\nthe Lipschitz singularities of the diffusion model near the zero point of\ntimesteps. Remarkably, our technique yields a substantial improvement in\nperformance. Moreover, as a byproduct of our method, we achieve a dramatic\nreduction in the Fr\\'echet Inception Distance of acceleration methods relying\non network Lipschitz, including DDIM and DPM-Solver, by over 33%. Extensive\nexperiments on diverse datasets validate our theory and method. Our work may\nadvance the understanding of the general diffusion process, and also provide\ninsights for the design of diffusion models.\n","authors":["Zhantao Yang","Ruili Feng","Han Zhang","Yujun Shen","Kai Zhu","Lianghua Huang","Yifei Zhang","Yu Liu","Deli Zhao","Jingren Zhou","Fan Cheng"],"pdf_url":"https://arxiv.org/pdf/2306.11251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11753v1","updated":"2024-12-16T13:12:11Z","published":"2024-12-16T13:12:11Z","title":"DriveGazen: Event-Based Driving Status Recognition using Conventional\n  Camera","summary":"  We introduce a wearable driving status recognition device and our open-source\ndataset, along with a new real-time method robust to changes in lighting\nconditions for identifying driving status from eye observations of drivers. The\ncore of our method is generating event frames from conventional intensity\nframes, and the other is a newly designed Attention Driving State Network\n(ADSN). Compared to event cameras, conventional cameras offer complete\ninformation and lower hardware costs, enabling captured frames to encode rich\nspatial information. However, these textures lack temporal information, posing\nchallenges in effectively identifying driving status. DriveGazen addresses this\nissue from three perspectives. First, we utilize video frames to generate\nrealistic synthetic dynamic vision sensor (DVS) events. Second, we adopt a\nspiking neural network to decode pertinent temporal information. Lastly, ADSN\nextracts crucial spatial cues from corresponding intensity frames and conveys\nspatial attention to convolutional spiking layers during both training and\ninference through a novel guide attention module to guide the feature learning\nand feature enhancement of the event frame. We specifically collected the\nDriving Status (DriveGaze) dataset to demonstrate the effectiveness of our\napproach. Additionally, we validate the superiority of the DriveGazen on the\nSingle-eye Event-based Emotion (SEE) dataset. To the best of our knowledge, our\nmethod is the first to utilize guide attention spiking neural networks and\neye-based event frames generated from conventional cameras for driving status\nrecognition. Please refer to our project page for more details:\nhttps://github.com/TooyoungALEX/AAAI25-DriveGazen.\n","authors":["Xiaoyin Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11753v1.pdf","comment":"9 pages, 4 figures, (AAAI25)The 39th Annual AAAI Conference on\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2412.11752v1","updated":"2024-12-16T13:11:02Z","published":"2024-12-16T13:11:02Z","title":"Deformable Radial Kernel Splatting","summary":"  Recently, Gaussian splatting has emerged as a robust technique for\nrepresenting 3D scenes, enabling real-time rasterization and high-fidelity\nrendering. However, Gaussians' inherent radial symmetry and smoothness\nconstraints limit their ability to represent complex shapes, often requiring\nthousands of primitives to approximate detailed geometry. We introduce\nDeformable Radial Kernel (DRK), which extends Gaussian splatting into a more\ngeneral and flexible framework. Through learnable radial bases with adjustable\nangles and scales, DRK efficiently models diverse shape primitives while\nenabling precise control over edge sharpness and boundary curvature. iven DRK's\nplanar nature, we further develop accurate ray-primitive intersection\ncomputation for depth sorting and introduce efficient kernel culling strategies\nfor improved rasterization efficiency. Extensive experiments demonstrate that\nDRK outperforms existing methods in both representation efficiency and\nrendering quality, achieving state-of-the-art performance while dramatically\nreducing primitive count.\n","authors":["Yi-Hua Huang","Ming-Xian Lin","Yang-Tian Sun","Ziyi Yang","Xiaoyang Lyu","Yan-Pei Cao","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2412.11752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11735v1","updated":"2024-12-16T12:56:57Z","published":"2024-12-16T12:56:57Z","title":"Transferable Adversarial Face Attack with Text Controlled Attribute","summary":"  Traditional adversarial attacks typically produce adversarial examples under\nnorm-constrained conditions, whereas unrestricted adversarial examples are\nfree-form with semantically meaningful perturbations. Current unrestricted\nadversarial impersonation attacks exhibit limited control over adversarial face\nattributes and often suffer from low transferability. In this paper, we propose\na novel Text Controlled Attribute Attack (TCA$^2$) to generate photorealistic\nadversarial impersonation faces guided by natural language. Specifically, the\ncategory-level personal softmax vector is employed to precisely guide the\nimpersonation attacks. Additionally, we propose both data and model\naugmentation strategies to achieve transferable attacks on unknown target\nmodels. Finally, a generative model, \\textit{i.e}, Style-GAN, is utilized to\nsynthesize impersonated faces with desired attributes. Extensive experiments on\ntwo high-resolution face recognition datasets validate that our TCA$^2$ method\ncan generate natural text-guided adversarial impersonation faces with high\ntransferability. We also evaluate our method on real-world face recognition\nsystems, \\textit{i.e}, Face++ and Aliyun, further demonstrating the practical\npotential of our approach.\n","authors":["Wenyun Li","Zheng Zhang","Xiangyuan Lan","Dongmei Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.11735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01756v2","updated":"2024-12-16T12:53:25Z","published":"2024-11-04T02:43:55Z","title":"ChatTracker: Enhancing Visual Tracking Performance via Chatting with\n  Multimodal Large Language Model","summary":"  Visual object tracking aims to locate a targeted object in a video sequence\nbased on an initial bounding box. Recently, Vision-Language~(VL) trackers have\nproposed to utilize additional natural language descriptions to enhance\nversatility in various applications. However, VL trackers are still inferior to\nState-of-The-Art (SoTA) visual trackers in terms of tracking performance. We\nfound that this inferiority primarily results from their heavy reliance on\nmanual textual annotations, which include the frequent provision of ambiguous\nlanguage descriptions. In this paper, we propose ChatTracker to leverage the\nwealth of world knowledge in the Multimodal Large Language Model (MLLM) to\ngenerate high-quality language descriptions and enhance tracking performance.\nTo this end, we propose a novel reflection-based prompt optimization module to\niteratively refine the ambiguous and inaccurate descriptions of the target with\ntracking feedback. To further utilize semantic information produced by MLLM, a\nsimple yet effective VL tracking framework is proposed and can be easily\nintegrated as a plug-and-play module to boost the performance of both VL and\nvisual trackers. Experimental results show that our proposed ChatTracker\nachieves a performance comparable to existing methods.\n","authors":["Yiming Sun","Fan Yu","Shaoxiang Chen","Yu Zhang","Junwei Huang","Chenhui Li","Yang Li","Changbo Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11715v1","updated":"2024-12-16T12:35:56Z","published":"2024-12-16T12:35:56Z","title":"Discrepancy-Aware Attention Network for Enhanced Audio-Visual Zero-Shot\n  Learning","summary":"  Audio-visual Zero-Shot Learning (ZSL) has attracted significant attention for\nits ability to identify unseen classes and perform well in video classification\ntasks. However, modal imbalance in (G)ZSL leads to over-reliance on the optimal\nmodality, reducing discriminative capabilities for unseen classes. Some studies\nhave attempted to address this issue by modifying parameter gradients, but two\nchallenges still remain: (a) Quality discrepancies, where modalities offer\ndiffering quantities and qualities of information for the same concept. (b)\nContent discrepancies, where sample contributions within a modality vary\nsignificantly. To address these challenges, we propose a Discrepancy-Aware\nAttention Network (DAAN) for Enhanced Audio-Visual ZSL. Our approach introduces\na Quality-Discrepancy Mitigation Attention (QDMA) unit to minimize redundant\ninformation in the high-quality modality and a Contrastive Sample-level\nGradient Modulation (CSGM) block to adjust gradient magnitudes and balance\ncontent discrepancies. We quantify modality contributions by integrating\noptimization and convergence rate for more precise gradient modulation in CSGM.\nExperiments demonstrates DAAN achieves state-of-the-art performance on\nbenchmark datasets, with ablation studies validating the effectiveness of\nindividual modules.\n","authors":["RunLin Yu","Yipu Gong","Wenrui Li","Aiwen Sun","Mengren Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.11715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11710v1","updated":"2024-12-16T12:32:21Z","published":"2024-12-16T12:32:21Z","title":"Re-Attentional Controllable Video Diffusion Editing","summary":"  Editing videos with textual guidance has garnered popularity due to its\nstreamlined process which mandates users to solely edit the text prompt\ncorresponding to the source video. Recent studies have explored and exploited\nlarge-scale text-to-image diffusion models for text-guided video editing,\nresulting in remarkable video editing capabilities. However, they may still\nsuffer from some limitations such as mislocated objects, incorrect number of\nobjects. Therefore, the controllability of video editing remains a formidable\nchallenge. In this paper, we aim to challenge the above limitations by\nproposing a Re-Attentional Controllable Video Diffusion Editing (ReAtCo)\nmethod. Specially, to align the spatial placement of the target objects with\nthe edited text prompt in a training-free manner, we propose a Re-Attentional\nDiffusion (RAD) to refocus the cross-attention activation responses between the\nedited text prompt and the target video during the denoising stage, resulting\nin a spatially location-aligned and semantically high-fidelity manipulated\nvideo. In particular, to faithfully preserve the invariant region content with\nless border artifacts, we propose an Invariant Region-guided Joint Sampling\n(IRJS) strategy to mitigate the intrinsic sampling errors w.r.t the invariant\nregions at each denoising timestep and constrain the generated content to be\nharmonized with the invariant region content. Experimental results verify that\nReAtCo consistently improves the controllability of video diffusion editing and\nachieves superior video editing performance.\n","authors":["Yuanzhi Wang","Yong Li","Mengyi Liu","Xiaoya Zhang","Xin Liu","Zhen Cui","Antoni B. Chan"],"pdf_url":"https://arxiv.org/pdf/2412.11710v1.pdf","comment":"Accepted by AAAI 2025. Codes are released at:\n  https://github.com/mdswyz/ReAtCo"},{"id":"http://arxiv.org/abs/2412.11706v1","updated":"2024-12-16T12:28:22Z","published":"2024-12-16T12:28:22Z","title":"AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration","summary":"  Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.\n","authors":["Wenhao Sun","Rong-Cheng Tu","Jingyi Liao","Zhao Jin","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2412.11706v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.11702v1","updated":"2024-12-16T12:25:57Z","published":"2024-12-16T12:25:57Z","title":"Flex-PE: Flexible and SIMD Multi-Precision Processing Element for AI\n  Workloads","summary":"  The rapid adaptation of data driven AI models, such as deep learning\ninference, training, Vision Transformers (ViTs), and other HPC applications,\ndrives a strong need for runtime precision configurable different non linear\nactivation functions (AF) hardware support. Existing solutions support diverse\nprecision or runtime AF reconfigurability but fail to address both\nsimultaneously. This work proposes a flexible and SIMD multiprecision\nprocessing element (FlexPE), which supports diverse runtime configurable AFs,\nincluding sigmoid, tanh, ReLU and softmax, and MAC operation. The proposed\ndesign achieves an improved throughput of up to 16X FxP4, 8X FxP8, 4X FxP16 and\n1X FxP32 in pipeline mode with 100% time multiplexed hardware. This work\nproposes an area efficient multiprecision iterative mode in the SIMD systolic\narrays for edge AI use cases. The design delivers superior performance with up\nto 62X and 371X reductions in DMA reads for input feature maps and weight\nfilters in VGG16, with an energy efficiency of 8.42 GOPS / W within the\naccuracy loss of 2%. The proposed architecture supports emerging 4-bit\ncomputations for DL inference while enhancing throughput in FxP8/16 modes for\ntransformers and other HPC applications. The proposed approach enables future\nenergy-efficient AI accelerators in edge and cloud environments.\n","authors":["Mukul Lokhande","Gopal Raut","Santosh Kumar Vishvakarma"],"pdf_url":"https://arxiv.org/pdf/2412.11702v1.pdf","comment":"10 pages, 5 figures, Preprint, Submitted to TVLSI Regular papers"},{"id":"http://arxiv.org/abs/2311.02778v3","updated":"2024-12-16T12:22:12Z","published":"2023-11-05T21:46:12Z","title":"MuSHRoom: Multi-Sensor Hybrid Room Dataset for Joint 3D Reconstruction\n  and Novel View Synthesis","summary":"  Metaverse technologies demand accurate, real-time, and immersive modeling on\nconsumer-grade hardware for both non-human perception (e.g.,\ndrone/robot/autonomous car navigation) and immersive technologies like AR/VR,\nrequiring both structural accuracy and photorealism. However, there exists a\nknowledge gap in how to apply geometric reconstruction and photorealism\nmodeling (novel view synthesis) in a unified framework. To address this gap and\npromote the development of robust and immersive modeling and rendering with\nconsumer-grade devices, we propose a real-world Multi-Sensor Hybrid Room\nDataset (MuSHRoom). Our dataset presents exciting challenges and requires\nstate-of-the-art methods to be cost-effective, robust to noisy data and\ndevices, and can jointly learn 3D reconstruction and novel view synthesis\ninstead of treating them as separate tasks, making them ideal for real-world\napplications. We benchmark several famous pipelines on our dataset for joint 3D\nmesh reconstruction and novel view synthesis. Our dataset and benchmark show\ngreat potential in promoting the improvements for fusing 3D reconstruction and\nhigh-quality rendering in a robust and computationally efficient end-to-end\nfashion. The dataset and code are available at the project website:\nhttps://xuqianren.github.io/publications/MuSHRoom/.\n","authors":["Xuqian Ren","Wenjia Wang","Dingding Cai","Tuuli Tuominen","Juho Kannala","Esa Rahtu"],"pdf_url":"https://arxiv.org/pdf/2311.02778v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08509v2","updated":"2024-12-16T12:19:58Z","published":"2024-07-11T13:46:47Z","title":"Haar Nuclear Norms with Applications to Remote Sensing Imagery\n  Restoration","summary":"  Remote sensing image restoration aims to reconstruct missing or corrupted\nareas within images. To date, low-rank based models have garnered significant\ninterest in this field. This paper proposes a novel low-rank regularization\nterm, named the Haar nuclear norm (HNN), for efficient and effective remote\nsensing image restoration. It leverages the low-rank properties of wavelet\ncoefficients derived from the 2-D frontal slice-wise Haar discrete wavelet\ntransform, effectively modeling the low-rank prior for separated coarse-grained\nstructure and fine-grained textures in the image. Experimental evaluations\nconducted on hyperspectral image inpainting, multi-temporal image cloud\nremoval, and hyperspectral image denoising have revealed the HNN's potential.\nTypically, HNN achieves a performance improvement of 1-4 dB and a speedup of\n10-28x compared to some state-of-the-art methods (e.g., tensor correlated total\nvariation, and fully-connected tensor network) for inpainting tasks.\n","authors":["Shuang Xu","Chang Yu","Jiangjun Peng","Xiangyong Cao","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2407.08509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19271v2","updated":"2024-12-16T12:14:57Z","published":"2024-11-28T17:04:32Z","title":"AGS-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors\n  for Indoor Room Reconstruction Using Smartphones","summary":"  Geometric priors are often used to enhance 3D reconstruction. With many\nsmartphones featuring low-resolution depth sensors and the prevalence of\noff-the-shelf monocular geometry estimators, incorporating geometric priors as\nregularization signals has become common in 3D vision tasks. However, the\naccuracy of depth estimates from mobile devices is typically poor for highly\ndetailed geometry, and monocular estimators often suffer from poor multi-view\nconsistency and precision. In this work, we propose an approach for joint\nsurface depth and normal refinement of Gaussian Splatting methods for accurate\n3D reconstruction of indoor scenes. We develop supervision strategies that\nadaptively filters low-quality depth and normal estimates by comparing the\nconsistency of the priors during optimization. We mitigate regularization in\nregions where prior estimates have high uncertainty or ambiguities. Our\nfiltering strategy and optimization design demonstrate significant improvements\nin both mesh estimation and novel-view synthesis for both 3D and 2D Gaussian\nSplatting-based methods on challenging indoor room datasets. Furthermore, we\nexplore the use of alternative meshing strategies for finer geometry\nextraction. We develop a scale-aware meshing strategy inspired by TSDF and\noctree-based isosurface extraction, which recovers finer details from Gaussian\nmodels compared to other commonly used open-source meshing tools. Our code is\nreleased in https://xuqianren.github.io/ags_mesh_website/.\n","authors":["Xuqian Ren","Matias Turkulainen","Jiepeng Wang","Otto Seiskari","Iaroslav Melekhov","Juho Kannala","Esa Rahtu"],"pdf_url":"https://arxiv.org/pdf/2411.19271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11685v1","updated":"2024-12-16T11:55:26Z","published":"2024-12-16T11:55:26Z","title":"Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning","summary":"  With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.\n","authors":["Xingchi Chen","Zhuoran Zheng","Xuerui Li","Yuying Chen","Shu Wang","Wenqi Ren"],"pdf_url":"https://arxiv.org/pdf/2412.11685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11681v1","updated":"2024-12-16T11:47:07Z","published":"2024-12-16T11:47:07Z","title":"Fast-staged CNN Model for Accurate pulmonary diseases and Lung cancer\n  detection","summary":"  Pulmonary pathologies are a significant global health concern, often leading\nto fatal outcomes if not diagnosed and treated promptly. Chest radiography\nserves as a primary diagnostic tool, but the availability of experienced\nradiologists remains limited. Advances in Artificial Intelligence (AI) and\nmachine learning, particularly in computer vision, offer promising solutions to\naddress this challenge.\n  This research evaluates a deep learning model designed to detect lung cancer,\nspecifically pulmonary nodules, along with eight other lung pathologies, using\nchest radiographs. The study leverages diverse datasets comprising over 135,120\nfrontal chest radiographs to train a Convolutional Neural Network (CNN). A\ntwo-stage classification system, utilizing ensemble methods and transfer\nlearning, is employed to first triage images into Normal or Abnormal categories\nand then identify specific pathologies, including lung nodules.\n  The deep learning model achieves notable results in nodule classification,\nwith a top-performing accuracy of 77%, a sensitivity of 0.713, a specificity of\n0.776 during external validation, and an AUC score of 0.888. Despite these\nsuccesses, some misclassifications were observed, primarily false negatives.\n  In conclusion, the model demonstrates robust potential for generalization\nacross diverse patient populations, attributed to the geographic diversity of\nthe training dataset. Future work could focus on integrating ETL data\ndistribution strategies and expanding the dataset with additional nodule-type\nsamples to further enhance diagnostic accuracy.\n","authors":["Abdelbaki Souid","Mohamed Hamroun","Soufiene Ben Othman","Hedi Sakli","Naceur Abdelkarim"],"pdf_url":"https://arxiv.org/pdf/2412.11681v1.pdf","comment":"IEEE International Workshop on Mechatronic Systems Supervision 2023"},{"id":"http://arxiv.org/abs/2403.05435v5","updated":"2024-12-16T11:43:39Z","published":"2024-03-08T16:38:11Z","title":"OmniCount: Multi-label Object Counting with Semantic-Geometric Priors","summary":"  Object counting is pivotal for understanding the composition of scenes.\nPreviously, this task was dominated by class-specific methods, which have\ngradually evolved into more adaptable class-agnostic strategies. However, these\nstrategies come with their own set of limitations, such as the need for manual\nexemplar input and multiple passes for multiple categories, resulting in\nsignificant inefficiencies. This paper introduces a more practical approach\nenabling simultaneous counting of multiple object categories using an\nopen-vocabulary framework. Our solution, OmniCount, stands out by using\nsemantic and geometric insights (priors) from pre-trained models to count\nmultiple categories of objects as specified by users, all without additional\ntraining. OmniCount distinguishes itself by generating precise object masks and\nleveraging varied interactive prompts via the Segment Anything Model for\nefficient counting. To evaluate OmniCount, we created the OmniCount-191\nbenchmark, a first-of-its-kind dataset with multi-label object counts,\nincluding points, bounding boxes, and VQA annotations. Our comprehensive\nevaluation in OmniCount-191, alongside other leading benchmarks, demonstrates\nOmniCount's exceptional performance, significantly outpacing existing\nsolutions. The project webpage is available at\nhttps://mondalanindya.github.io/OmniCount.\n","authors":["Anindya Mondal","Sauradip Nag","Xiatian Zhu","Anjan Dutta"],"pdf_url":"https://arxiv.org/pdf/2403.05435v5.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11680v1","updated":"2024-12-16T11:42:25Z","published":"2024-12-16T11:42:25Z","title":"EGP3D: Edge-guided Geometric Preserving 3D Point Cloud Super-resolution\n  for RGB-D camera","summary":"  Point clouds or depth images captured by current RGB-D cameras often suffer\nfrom low resolution, rendering them insufficient for applications such as 3D\nreconstruction and robots. Existing point cloud super-resolution (PCSR) methods\nare either constrained by geometric artifacts or lack attention to edge\ndetails. To address these issues, we propose an edge-guided\ngeometric-preserving 3D point cloud super-resolution (EGP3D) method tailored\nfor RGB-D cameras. Our approach innovatively optimizes the point cloud with an\nedge constraint on a projected 2D space, thereby ensuring high-quality edge\npreservation in the 3D PCSR task. To tackle geometric optimization challenges\nin super-resolution point clouds, particularly preserving edge shapes and\nsmoothness, we introduce a multi-faceted loss function that simultaneously\noptimizes the Chamfer distance, Hausdorff distance, and gradient smoothness.\nExisting datasets used for point cloud upsampling are predominantly synthetic\nand inadequately represent real-world scenarios, neglecting noise and stray\nlight effects. To address the scarcity of realistic RGB-D data for PCSR tasks,\nwe built a dataset that captures real-world noise and stray-light effects,\noffering a more accurate representation of authentic environments. Validated\nthrough simulations and real-world experiments, the proposed method exhibited\nsuperior performance in preserving edge clarity and geometric details.\n","authors":["Zheng Fang","Ke Ye","Yaofang Liu","Gongzhe Li","Xianhong Zhao","Jialong Li","Ruxin Wang","Yuchen Zhang","Xiangyang Ji","Qilin Sun"],"pdf_url":"https://arxiv.org/pdf/2412.11680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06727v2","updated":"2024-12-16T11:28:49Z","published":"2024-12-09T18:16:50Z","title":"Take Fake as Real: Realistic-like Robust Black-box Adversarial Attack to\n  Evade AIGC Detection","summary":"  The security of AI-generated content (AIGC) detection is crucial for ensuring\nmultimedia content credibility. To enhance detector security, research on\nadversarial attacks has become essential. However, most existing adversarial\nattacks focus only on GAN-generated facial images detection, struggle to be\neffective on multi-class natural images and diffusion-based detectors, and\nexhibit poor invisibility. To fill this gap, we first conduct an in-depth\nanalysis of the vulnerability of AIGC detectors and discover the feature that\ndetectors vary in vulnerability to different post-processing. Then, considering\nthat the detector is agnostic in real-world scenarios and given this discovery,\nwe propose a Realistic-like Robust Black-box Adversarial attack (R$^2$BA) with\npost-processing fusion optimization. Unlike typical perturbations, R$^2$BA uses\nreal-world post-processing, i.e., Gaussian blur, JPEG compression, Gaussian\nnoise and light spot to generate adversarial examples. Specifically, we use a\nstochastic particle swarm algorithm with inertia decay to optimize\npost-processing fusion intensity and explore the detector's decision boundary.\nGuided by the detector's fake probability, R$^2$BA enhances/weakens the\ndetector-vulnerable/detector-robust post-processing intensity to strike a\nbalance between adversariality and invisibility. Extensive experiments on\npopular/commercial AIGC detectors and datasets demonstrate that R$^2$BA\nexhibits impressive anti-detection performance, excellent invisibility, and\nstrong robustness in GAN-based and diffusion-based cases. Compared to\nstate-of-the-art white-box and black-box attacks, R$^2$BA shows significant\nimprovements of 15\\%--72\\% and 21\\%--47\\% in anti-detection performance under\nthe original and robust scenario respectively, offering valuable insights for\nthe security of AIGC detection in real-world applications.\n","authors":["Caiyun Xie","Dengpan Ye","Yunming Zhang","Long Tang","Yunna Lv","Jiacheng Deng","Jiawei Song"],"pdf_url":"https://arxiv.org/pdf/2412.06727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11673v1","updated":"2024-12-16T11:26:46Z","published":"2024-12-16T11:26:46Z","title":"$\\texttt{DINO-Foresight}$: Looking into the Future with DINO","summary":"  Predicting future dynamics is crucial for applications like autonomous\ndriving and robotics, where understanding the environment is key. Existing\npixel-level methods are computationally expensive and often focus on irrelevant\ndetails. To address these challenges, we introduce $\\texttt{DINO-Foresight}$, a\nnovel framework that operates in the semantic feature space of pretrained\nVision Foundation Models (VFMs). Our approach trains a masked feature\ntransformer in a self-supervised manner to predict the evolution of VFM\nfeatures over time. By forecasting these features, we can apply off-the-shelf,\ntask-specific heads for various scene understanding tasks. In this framework,\nVFM features are treated as a latent space, to which different heads attach to\nperform specific tasks for future-frame analysis. Extensive experiments show\nthat our framework outperforms existing methods, demonstrating its robustness\nand scalability. Additionally, we highlight how intermediate transformer\nrepresentations in $\\texttt{DINO-Foresight}$ improve downstream task\nperformance, offering a promising path for the self-supervised enhancement of\nVFM features. We provide the implementation code at\nhttps://github.com/Sta8is/DINO-Foresight .\n","authors":["Efstathios Karypidis","Ioannis Kakogeorgiou","Spyros Gidaris","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2412.11673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09441v2","updated":"2024-12-16T11:26:26Z","published":"2024-08-18T11:23:21Z","title":"CLIP-CID: Efficient CLIP Distillation via Cluster-Instance\n  Discrimination","summary":"  Contrastive Language-Image Pre-training (CLIP) has achieved excellent\nperformance over a wide range of tasks. However, the effectiveness of CLIP\nheavily relies on a substantial corpus of pre-training data, resulting in\nnotable consumption of computational resources. Although knowledge distillation\nhas been widely applied in single modality models, how to efficiently expand\nknowledge distillation to vision-language foundation models with extensive data\nremains relatively unexplored. In this paper, we introduce CLIP-CID, a novel\ndistillation mechanism that effectively transfers knowledge from a large\nvision-language foundation model to a smaller model. We initially propose a\nsimple but efficient image semantic balance method to reduce transfer learning\nbias and improve distillation efficiency. This method filters out 43.7% of\nimage-text pairs from the LAION400M while maintaining superior performance.\nAfter that, we leverage cluster-instance discrimination to facilitate knowledge\ntransfer from the teacher model to the student model, thereby empowering the\nstudent model to acquire a holistic semantic comprehension of the pre-training\ndata. Experimental results demonstrate that CLIP-CID achieves state-of-the-art\nperformance on various downstream tasks including linear probe and zero-shot\nclassification.\n","authors":["Kaicheng Yang","Tiancheng Gu","Xiang An","Haiqiang Jiang","Xiangzi Dai","Ziyong Feng","Weidong Cai","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2408.09441v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11668v1","updated":"2024-12-16T11:19:22Z","published":"2024-12-16T11:19:22Z","title":"Online Writer Retrieval with Chinese Handwritten Phrases: A Synergistic\n  Temporal-Frequency Representation Learning Approach","summary":"  Currently, the prevalence of online handwriting has spurred a critical need\nfor effective retrieval systems to accurately search relevant handwriting\ninstances from specific writers, known as online writer retrieval. Despite the\ngrowing demand, this field suffers from a scarcity of well-established\nmethodologies and public large-scale datasets. This paper tackles these\nchallenges with a focus on Chinese handwritten phrases. First, we propose\nDOLPHIN, a novel retrieval model designed to enhance handwriting\nrepresentations through synergistic temporal-frequency analysis. For frequency\nfeature learning, we propose the HFGA block, which performs gated\ncross-attention between the vanilla temporal handwriting sequence and its\nhigh-frequency sub-bands to amplify salient writing details. For temporal\nfeature learning, we propose the CAIR block, tailored to promote channel\ninteraction and reduce channel redundancy. Second, to address data deficit, we\nintroduce OLIWER, a large-scale online writer retrieval dataset encompassing\nover 670,000 Chinese handwritten phrases from 1,731 individuals. Through\nextensive evaluations, we demonstrate the superior performance of DOLPHIN over\nexisting methods. In addition, we explore cross-domain writer retrieval and\nreveal the pivotal role of increasing feature alignment in bridging the\ndistributional gap between different handwriting data. Our findings emphasize\nthe significance of point sampling frequency and pressure features in improving\nhandwriting representation quality and retrieval performance. Code and dataset\nare available at https://github.com/SCUT-DLVCLab/DOLPHIN.\n","authors":["Peirong Zhang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2412.11668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11663v1","updated":"2024-12-16T11:11:23Z","published":"2024-12-16T11:11:23Z","title":"LMM-Regularized CLIP Embeddings for Image Classification","summary":"  In this paper we deal with image classification tasks using the powerful CLIP\nvision-language model. Our goal is to advance the classification performance\nusing the CLIP's image encoder, by proposing a novel Large Multimodal Model\n(LMM) based regularization method. The proposed method uses an LMM to extract\nsemantic descriptions for the images of the dataset. Then, it uses the CLIP's\ntext encoder, frozen, in order to obtain the corresponding text embeddings and\ncompute the mean semantic class descriptions. Subsequently, we adapt the CLIP's\nimage encoder by adding a classification head, and we train it along with the\nimage encoder output, apart from the main classification objective, with an\nadditional auxiliary objective. The additional objective forces the embeddings\nat the image encoder's output to become similar to their corresponding\nLMM-generated mean semantic class descriptions. In this way, it produces\nembeddings with enhanced discrimination ability, leading to improved\nclassification performance. The effectiveness of the proposed regularization\nmethod is validated through extensive experiments on three image classification\ndatasets.\n","authors":["Maria Tzelepi","Vasileios Mezaris"],"pdf_url":"https://arxiv.org/pdf/2412.11663v1.pdf","comment":"Accepted for publication, 26th Int. Symp. on Multimedia (IEEE ISM\n  2024), Tokyo, Japan, Dec. 2024. This is the authors' \"accepted version\""},{"id":"http://arxiv.org/abs/2411.14280v2","updated":"2024-12-16T11:06:19Z","published":"2024-11-21T16:33:35Z","title":"EasyHOI: Unleashing the Power of Large Models for Reconstructing\n  Hand-Object Interactions in the Wild","summary":"  Our work aims to reconstruct hand-object interactions from a single-view\nimage, which is a fundamental but ill-posed task. Unlike methods that\nreconstruct from videos, multi-view images, or predefined 3D templates,\nsingle-view reconstruction faces significant challenges due to inherent\nambiguities and occlusions. These challenges are further amplified by the\ndiverse nature of hand poses and the vast variety of object shapes and sizes.\nOur key insight is that current foundational models for segmentation,\ninpainting, and 3D reconstruction robustly generalize to in-the-wild images,\nwhich could provide strong visual and geometric priors for reconstructing\nhand-object interactions. Specifically, given a single image, we first design a\nnovel pipeline to estimate the underlying hand pose and object shape using\noff-the-shelf large models. Furthermore, with the initial reconstruction, we\nemploy a prior-guided optimization scheme, which optimizes hand pose to comply\nwith 3D physical constraints and the 2D input image content. We perform\nexperiments across several datasets and show that our method consistently\noutperforms baselines and faithfully reconstructs a diverse set of hand-object\ninteractions. Here is the link of our project page:\nhttps://lym29.github.io/EasyHOI-page/\n","authors":["Yumeng Liu","Xiaoxiao Long","Zemin Yang","Yuan Liu","Marc Habermann","Christian Theobalt","Yuexin Ma","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14280v2.pdf","comment":"Project page: https://lym29.github.io/EasyHOI-page/"},{"id":"http://arxiv.org/abs/2412.11657v1","updated":"2024-12-16T11:00:02Z","published":"2024-12-16T11:00:02Z","title":"CNNtention: Can CNNs do better with Attention?","summary":"  Convolutional Neural Networks (CNNs) have been the standard for image\nclassification tasks for a long time, but more recently attention-based\nmechanisms have gained traction. This project aims to compare traditional CNNs\nwith attention-augmented CNNs across an image classification task. By\nevaluating and comparing their performance, accuracy and computational\nefficiency, the project will highlight benefits and trade-off of the localized\nfeature extraction of traditional CNNs and the global context capture in\nattention-augmented CNNs. By doing this, we can reveal further insights into\ntheir respective strengths and weaknesses, guide the selection of models based\non specific application needs and ultimately, enhance understanding of these\narchitectures in the deep learning community.\n  This was our final project for CS7643 Deep Learning course at Georgia Tech.\n","authors":["Julian Glattki","Nikhil Kapila","Tejas Rathi"],"pdf_url":"https://arxiv.org/pdf/2412.11657v1.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.15500v2","updated":"2024-12-16T10:53:08Z","published":"2024-07-22T09:31:30Z","title":"TextureCrop: Enhancing Synthetic Image Detection through Texture-based\n  Cropping","summary":"  Generative AI technologies produce increasingly realistic imagery, which,\ndespite its potential for creative applications, can also be misused to produce\nmisleading and harmful content. This renders Synthetic Image Detection (SID)\nmethods essential for identifying AI-generated content online. State-of-the-art\nSID methods typically resize or center-crop input images due to architectural\nor computational constraints, which hampers the detection of artifacts that\nappear in high-resolution images. To address this limitation, we propose\nTextureCrop, an image pre-processing component that can be plugged in any\npre-trained SID model to improve its performance. By focusing on high-frequency\nimage parts where generative artifacts are prevalent, TextureCrop enhances SID\nperformance with manageable memory requirements. Experimental results\ndemonstrate a consistent improvement in AUC across various detectors by 6.1%\ncompared to center cropping and by 15% compared to resizing, across\nhigh-resolution images from the Forensynths, Synthbuster and TWIGMA datasets.\n","authors":["Despina Konstantinidou","Christos Koutlis","Symeon Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.15500v2.pdf","comment":"10 pages, 6 images"},{"id":"http://arxiv.org/abs/2412.11650v1","updated":"2024-12-16T10:50:52Z","published":"2024-12-16T10:50:52Z","title":"Image Gradient-Aided Photometric Stereo Network","summary":"  Photometric stereo (PS) endeavors to ascertain surface normals using shading\nclues from photometric images under various illuminations. Recent deep\nlearning-based PS methods often overlook the complexity of object surfaces.\nThese neural network models, which exclusively rely on photometric images for\ntraining, often produce blurred results in high-frequency regions characterized\nby local discontinuities, such as wrinkles and edges with significant gradient\nchanges. To address this, we propose the Image Gradient-Aided Photometric\nStereo Network (IGA-PSN), a dual-branch framework extracting features from both\nphotometric images and their gradients. Furthermore, we incorporate an\nhourglass regression network along with supervision to regularize normal\nregression. Experiments on DiLiGenT benchmarks show that IGA-PSN outperforms\nprevious methods in surface normal estimation, achieving a mean angular error\nof 6.46 while preserving textures and geometric shapes in complex regions.\n","authors":["Kaixuan Wang","Lin Qi","Shiyu Qin","Kai Luo","Yakun Ju","Xia Li","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2412.11650v1.pdf","comment":"13 pages, 5 figures, published to Springer"},{"id":"http://arxiv.org/abs/2412.09319v2","updated":"2024-12-16T10:45:46Z","published":"2024-12-12T14:44:05Z","title":"FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot\n  Medical Image Segmentation","summary":"  Existing few-shot medical image segmentation (FSMIS) models fail to address a\npractical issue in medical imaging: the domain shift caused by different\nimaging techniques, which limits the applicability to current FSMIS tasks. To\novercome this limitation, we focus on the cross-domain few-shot medical image\nsegmentation (CD-FSMIS) task, aiming to develop a generalized model capable of\nadapting to a broader range of medical image segmentation scenarios with\nlimited labeled data from the novel target domain. Inspired by the\ncharacteristics of frequency domain similarity across different domains, we\npropose a Frequency-aware Matching Network (FAMNet), which includes two key\ncomponents: a Frequency-aware Matching (FAM) module and a Multi-Spectral Fusion\n(MSF) module. The FAM module tackles two problems during the meta-learning\nphase: 1) intra-domain variance caused by the inherent support-query bias, due\nto the different appearances of organs and lesions, and 2) inter-domain\nvariance caused by different medical imaging techniques. Additionally, we\ndesign an MSF module to integrate the different frequency features decoupled by\nthe FAM module, and further mitigate the impact of inter-domain variance on the\nmodel's segmentation performance. Combining these two modules, our FAMNet\nsurpasses existing FSMIS models and Cross-domain Few-shot Semantic Segmentation\nmodels on three cross-domain datasets, achieving state-of-the-art performance\nin the CD-FSMIS task.\n","authors":["Yuntian Bo","Yazhou Zhu","Lunbo Li","Haofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09319v2.pdf","comment":"Accepted by the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2411.13623v2","updated":"2024-12-16T10:34:06Z","published":"2024-11-20T13:12:43Z","title":"Unsupervised Foundation Model-Agnostic Slide-Level Representation\n  Learning","summary":"  Representation learning of pathology whole-slide images(WSIs) has primarily\nrelied on weak supervision with Multiple Instance Learning (MIL). This approach\nleads to slide representations highly tailored to a specific clinical task.\nSelf-supervised learning (SSL) has been successfully applied to train\nhistopathology foundation models (FMs) for patch embedding generation. However,\ngenerating patient or slide level embeddings remains challenging. Existing\napproaches for slide representation learning extend the principles of SSL from\npatch level learning to entire slides by aligning different augmentations of\nthe slide or by utilizing multimodal data. By integrating tile embeddings from\nmultiple FMs, we propose a new single modality SSL method in feature space that\ngenerates useful slide representations. Our contrastive pretraining strategy,\ncalled COBRA, employs multiple FMs and an architecture based on Mamba-2. COBRA\nexceeds performance of state-of-the-art slide encoders on four different public\nClinical Protemic Tumor Analysis Consortium (CPTAC) cohorts on average by at\nleast +4.5% AUC, despite only being pretrained on 3048 WSIs from The Cancer\nGenome Atlas (TCGA). Additionally, COBRA is readily compatible at inference\ntime with previously unseen feature extractors. Code available at\nhttps://github.com/KatherLab/COBRA.\n","authors":["Tim Lenz","Peter Neidlinger","Marta Ligero","Georg Wölflein","Marko van Treeck","Jakob Nikolas Kather"],"pdf_url":"https://arxiv.org/pdf/2411.13623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11639v1","updated":"2024-12-16T10:33:10Z","published":"2024-12-16T10:33:10Z","title":"High-speed and High-quality Vision Reconstruction of Spike Camera with\n  Spike Stability Theorem","summary":"  Neuromorphic vision sensors, such as the dynamic vision sensor (DVS) and\nspike camera, have gained increasing attention in recent years. The spike\ncamera can detect fine textures by mimicking the fovea in the human visual\nsystem, and output a high-frequency spike stream. Real-time high-quality vision\nreconstruction from the spike stream can build a bridge to high-level vision\ntask applications of the spike camera. To realize high-speed and high-quality\nvision reconstruction of the spike camera, we propose a new spike stability\ntheorem that reveals the relationship between spike stream characteristics and\nstable light intensity. Based on the spike stability theorem, two\nparameter-free algorithms are designed for the real-time vision reconstruction\nof the spike camera. To demonstrate the performances of our algorithms, two\ndatasets (a public dataset PKU-Spike-High-Speed and a newly constructed dataset\nSpikeCityPCL) are used to compare the reconstruction quality and speed of\nvarious reconstruction methods. Experimental results show that, compared with\nthe current state-of-the-art (SOTA) reconstruction methods, our reconstruction\nmethods obtain the best tradeoff between the reconstruction quality and speed.\nAdditionally, we design the FPGA implementation method of our algorithms to\nrealize the real-time (running at 20,000 FPS) visual reconstruction. Our work\nprovides new theorem and algorithm foundations for the real-time edge-end\nvision processing of the spike camera.\n","authors":["Wei Zhang","Weiquan Yan","Yun Zhao","Wenxiang Cheng","Gang Chen","Huihui Zhou","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2412.11639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08925v2","updated":"2024-12-16T10:28:02Z","published":"2024-10-11T15:50:31Z","title":"HyperPg -- Prototypical Gaussians on the Hypersphere for Interpretable\n  Deep Learning","summary":"  Prototype Learning methods provide an interpretable alternative to black-box\ndeep learning models. Approaches such as ProtoPNet learn, which part of a test\nimage \"look like\" known prototypical parts from training images, combining\npredictive power with the inherent interpretability of case-based reasoning.\nHowever, existing approaches have two main drawbacks: A) They rely solely on\ndeterministic similarity scores without statistical confidence. B) The\nprototypes are learned in a black-box manner without human input. This work\nintroduces HyperPg, a new prototype representation leveraging Gaussian\ndistributions on a hypersphere in latent space, with learnable mean and\nvariance. HyperPg prototypes adapt to the spread of clusters in the latent\nspace and output likelihood scores. The new architecture, HyperPgNet, leverages\nHyperPg to learn prototypes aligned with human concepts from pixel-level\nannotations. Consequently, each prototype represents a specific concept such as\ncolor, image texture, or part of the image subject. A concept extraction\npipeline built on foundation models provides pixel-level annotations,\nsignificantly reducing human labeling effort. Experiments on CUB-200-2011 and\nStanford Cars datasets demonstrate that HyperPgNet outperforms other prototype\nlearning architectures while using fewer parameters and training steps.\nAdditionally, the concept-aligned HyperPg prototypes are learned transparently,\nenhancing model interpretability.\n","authors":["Maximilian Xiling Li","Korbinian Franz Rudolf","Nils Blank","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2410.08925v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11638v1","updated":"2024-12-16T10:27:48Z","published":"2024-12-16T10:27:48Z","title":"IDProtector: An Adversarial Noise Encoder to Protect Against\n  ID-Preserving Image Generation","summary":"  Recently, zero-shot methods like InstantID have revolutionized\nidentity-preserving generation. Unlike multi-image finetuning approaches such\nas DreamBooth, these zero-shot methods leverage powerful facial encoders to\nextract identity information from a single portrait photo, enabling efficient\nidentity-preserving generation through a single inference pass. However, this\nconvenience introduces new threats to the facial identity protection. This\npaper aims to safeguard portrait photos from unauthorized encoder-based\ncustomization. We introduce IDProtector, an adversarial noise encoder that\napplies imperceptible adversarial noise to portrait photos in a single forward\npass. Our approach offers universal protection for portraits against multiple\nstate-of-the-art encoder-based methods, including InstantID, IP-Adapter, and\nPhotoMaker, while ensuring robustness to common image transformations such as\nJPEG compression, resizing, and affine transformations. Experiments across\ndiverse portrait datasets and generative models reveal that IDProtector\ngeneralizes effectively to unseen data and even closed-source proprietary\nmodels.\n","authors":["Yiren Song","Pei Yang","Hai Ci","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2412.11638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17821v2","updated":"2024-12-16T10:27:35Z","published":"2024-05-28T04:41:02Z","title":"RITUAL: Random Image Transformations as a Universal Anti-hallucination\n  Lever in Large Vision Language Models","summary":"  Recent advancements in Large Vision Language Models (LVLMs) have\nrevolutionized how machines understand and generate textual responses based on\nvisual inputs, yet they often produce \"hallucinatory\" outputs that misinterpret\nvisual information, posing challenges in reliability and trustworthiness. We\npropose RITUAL, a simple decoding method that reduces hallucinations by\nleveraging randomly transformed images as complementary inputs during decoding,\nadjusting the output probability distribution without additional training or\nexternal models. Our key insight is that random transformations expose the\nmodel to diverse visual perspectives, enabling it to correct misinterpretations\nthat lead to hallucinations. Specifically, when a model hallucinates based on\nthe original image, the transformed images -- altered in aspects such as\norientation, scale, or color -- provide alternative viewpoints that help\nrecalibrate the model's predictions. By integrating the probability\ndistributions from both the original and transformed images, RITUAL effectively\nreduces hallucinations. To further improve reliability and address potential\ninstability from arbitrary transformations, we introduce RITUAL+, an extension\nthat selects image transformations based on self-feedback from the LVLM.\nInstead of applying transformations randomly, RITUAL+ uses the LVLM to evaluate\nand choose transformations that are most beneficial for reducing hallucinations\nin a given context. This self-adaptive approach mitigates the potential\nnegative impact of certain transformations on specific tasks, ensuring more\nconsistent performance across different scenarios. Experiments demonstrate that\nRITUAL and RITUAL+ significantly reduce hallucinations across several object\nhallucination benchmarks.\n","authors":["Sangmin Woo","Jaehyuk Jang","Donguk Kim","Yubin Choi","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2405.17821v2.pdf","comment":"Project: https://sangminwoo.github.io/RITUAL/"},{"id":"http://arxiv.org/abs/2412.11634v1","updated":"2024-12-16T10:25:03Z","published":"2024-12-16T10:25:03Z","title":"Predicting the Original Appearance of Damaged Historical Documents","summary":"  Historical documents encompass a wealth of cultural treasures but suffer from\nsevere damages including character missing, paper damage, and ink erosion over\ntime. However, existing document processing methods primarily focus on\nbinarization, enhancement, etc., neglecting the repair of these damages. To\nthis end, we present a new task, termed Historical Document Repair (HDR), which\naims to predict the original appearance of damaged historical documents. To\nfill the gap in this field, we propose a large-scale dataset HDR28K and a\ndiffusion-based network DiffHDR for historical document repair. Specifically,\nHDR28K contains 28,552 damaged-repaired image pairs with character-level\nannotations and multi-style degradations. Moreover, DiffHDR augments the\nvanilla diffusion framework with semantic and spatial information and a\nmeticulously designed character perceptual loss for contextual and visual\ncoherence. Experimental results demonstrate that the proposed DiffHDR trained\nusing HDR28K significantly surpasses existing approaches and exhibits\nremarkable performance in handling real damaged documents. Notably, DiffHDR can\nalso be extended to document editing and text block generation, showcasing its\nhigh flexibility and generalization capacity. We believe this study could\npioneer a new direction of document processing and contribute to the\ninheritance of invaluable cultures and civilizations. The dataset and code is\navailable at https://github.com/yeungchenwa/HDR.\n","authors":["Zhenhua Yang","Dezhi Peng","Yongxin Shi","Yuyi Zhang","Chongyu Liu","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2412.11634v1.pdf","comment":"Accepted to AAAI 2025; Github Page:\n  https://github.com/yeungchenwa/HDR"},{"id":"http://arxiv.org/abs/2408.05092v2","updated":"2024-12-16T10:10:10Z","published":"2024-08-09T14:33:34Z","title":"PriPHiT: Privacy-Preserving Hierarchical Training of Deep Neural\n  Networks","summary":"  The training phase of deep neural networks requires substantial resources and\nas such is often performed on cloud servers. However, this raises privacy\nconcerns when the training dataset contains sensitive content, e.g., facial or\nmedical images. In this work, we propose a method to perform the training phase\nof a deep learning model on both an edge device and a cloud server that\nprevents sensitive content being transmitted to the cloud while retaining the\ndesired information. The proposed privacy-preserving method uses adversarial\nearly exits to suppress the sensitive content at the edge and transmits the\ntask-relevant information to the cloud. This approach incorporates noise\naddition during the training phase to provide a differential privacy guarantee.\nWe extensively test our method on different facial and medical datasets with\ndiverse attributes using various deep learning architectures, showcasing its\noutstanding performance. We also demonstrate the effectiveness of privacy\npreservation through successful defenses against different white-box, deep and\nGAN-based reconstruction attacks. This approach is designed for\nresource-constrained edge devices, ensuring minimal memory usage and\ncomputational overhead.\n","authors":["Yamin Sepehri","Pedram Pad","Pascal Frossard","L. Andrea Dunbar"],"pdf_url":"https://arxiv.org/pdf/2408.05092v2.pdf","comment":"21 pages, 19 figures, 11 tables"},{"id":"http://arxiv.org/abs/2412.11621v1","updated":"2024-12-16T10:08:38Z","published":"2024-12-16T10:08:38Z","title":"VG-TVP: Multimodal Procedural Planning via Visually Grounded Text-Video\n  Prompting","summary":"  Large Language Model (LLM)-based agents have shown promise in procedural\ntasks, but the potential of multimodal instructions augmented by texts and\nvideos to assist users remains under-explored. To address this gap, we propose\nthe Visually Grounded Text-Video Prompting (VG-TVP) method which is a novel\nLLM-empowered Multimodal Procedural Planning (MPP) framework. It generates\ncohesive text and video procedural plans given a specified high-level\nobjective. The main challenges are achieving textual and visual\ninformativeness, temporal coherence, and accuracy in procedural plans. VG-TVP\nleverages the zero-shot reasoning capability of LLMs, the video-to-text\ngeneration ability of the video captioning models, and the text-to-video\ngeneration ability of diffusion models. VG-TVP improves the interaction between\nmodalities by proposing a novel Fusion of Captioning (FoC) method and using\nText-to-Video Bridge (T2V-B) and Video-to-Text Bridge (V2T-B). They allow LLMs\nto guide the generation of visually-grounded text plans and textual-grounded\nvideo plans. To address the scarcity of datasets suitable for MPP, we have\ncurated a new dataset called Daily-Life Task Procedural Plans (Daily-PP). We\nconduct comprehensive experiments and benchmarks to evaluate human preferences\n(regarding textual and visual informativeness, temporal coherence, and plan\naccuracy). Our VG-TVP method outperforms unimodal baselines on the Daily-PP\ndataset.\n","authors":["Muhammet Furkan Ilaslan","Ali Koksal","Kevin Qinhong Lin","Burak Satar","Mike Zheng Shou","Qianli Xu"],"pdf_url":"https://arxiv.org/pdf/2412.11621v1.pdf","comment":"Accepted for The 39th Annual AAAI Conference on Artificial\n  Intelligence 2025 in Main Track, 19 pages, 24 figures"},{"id":"http://arxiv.org/abs/2412.11620v1","updated":"2024-12-16T10:07:15Z","published":"2024-12-16T10:07:15Z","title":"Combating Semantic Contamination in Learning with Label Noise","summary":"  Noisy labels can negatively impact the performance of deep neural networks.\nOne common solution is label refurbishment, which involves reconstructing noisy\nlabels through predictions and distributions. However, these methods may\nintroduce problematic semantic associations, a phenomenon that we identify as\nSemantic Contamination. Through an analysis of Robust LR, a representative\nlabel refurbishment method, we found that utilizing the logits of views for\nrefurbishment does not adequately balance the semantic information of\nindividual classes. Conversely, using the logits of models fails to maintain\nconsistent semantic relationships across models, which explains why label\nrefurbishment methods frequently encounter issues related to Semantic\nContamination. To address this issue, we propose a novel method called\nCollaborative Cross Learning, which utilizes semi-supervised learning on\nrefurbished labels to extract appropriate semantic associations from embeddings\nacross views and models. Experimental results show that our method outperforms\nexisting approaches on both synthetic and real-world noisy datasets,\neffectively mitigating the impact of label noise and Semantic Contamination.\n","authors":["Wenxiao Fan","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2412.11620v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2308.14930v2","updated":"2024-12-16T10:04:38Z","published":"2023-08-28T23:08:32Z","title":"Application of Quantum Pre-Processing Filter for Binary Image\n  Classification with Small Samples","summary":"  Over the past few years, there has been significant interest in Quantum\nMachine Learning (QML) among researchers, as it has the potential to transform\nthe field of machine learning. Several models that exploit the properties of\nquantum mechanics have been developed for practical applications. In this\nstudy, we investigated the application of our previously proposed quantum\npre-processing filter (QPF) to binary image classification. We evaluated the\nQPF on four datasets: MNIST (handwritten digits), EMNIST (handwritten digits\nand alphabets), CIFAR-10 (photographic images) and GTSRB (real-life traffic\nsign images). Similar to our previous multi-class classification results, the\napplication of QPF improved the binary image classification accuracy using\nneural network against MNIST, EMNIST, and CIFAR-10 from 98.9% to 99.2%, 97.8%\nto 98.3%, and 71.2% to 76.1%, respectively, but degraded it against GTSRB from\n93.5% to 92.0%. We then applied QPF in cases using a smaller number of training\nand testing samples, i.e. 80 and 20 samples per class, respectively. In order\nto derive statistically stable results, we conducted the experiment with 100\ntrials choosing randomly different training and testing samples and averaging\nthe results. The result showed that the application of QPF did not improve the\nimage classification accuracy against MNIST and EMNIST but improved it against\nCIFAR-10 and GTSRB from 65.8% to 67.2% and 90.5% to 91.8%, respectively.\nFurther research will be conducted as part of future work to investigate the\npotential of QPF to assess the scalability of the proposed approach to larger\nand complex datasets.\n","authors":["Farina Riaz","Shahab Abdulla","Hajime Suzuki","Srinjoy Ganguly","Ravinesh C. Deo","Susan Hopkins"],"pdf_url":"https://arxiv.org/pdf/2308.14930v2.pdf","comment":"This paper is accepted by Journal of Data Science and Intelligent\n  Systems (JDSIS)"},{"id":"http://arxiv.org/abs/2412.08221v2","updated":"2024-12-16T09:54:46Z","published":"2024-12-11T09:17:39Z","title":"Generate Any Scene: Evaluating and Improving Text-to-Vision Generation\n  with Scene Graph Programming","summary":"  DALL-E and Sora have gained attention by producing implausible images, such\nas \"astronauts riding a horse in space.\" Despite the proliferation of\ntext-to-vision models that have inundated the internet with synthetic visuals,\nfrom images to 3D assets, current benchmarks predominantly evaluate these\nmodels on real-world scenes paired with captions. We introduce Generate Any\nScene, a framework that systematically enumerates scene graphs representing a\nvast array of visual scenes, spanning realistic to imaginative compositions.\nGenerate Any Scene leverages 'scene graph programming', a method for\ndynamically constructing scene graphs of varying complexity from a structured\ntaxonomy of visual elements. This taxonomy includes numerous objects,\nattributes, and relations, enabling the synthesis of an almost infinite variety\nof scene graphs. Using these structured representations, Generate Any Scene\ntranslates each scene graph into a caption, enabling scalable evaluation of\ntext-to-vision models through standard metrics. We conduct extensive\nevaluations across multiple text-to-image, text-to-video, and text-to-3D\nmodels, presenting key findings on model performance. We find that DiT-backbone\ntext-to-image models align more closely with input captions than UNet-backbone\nmodels. Text-to-video models struggle with balancing dynamics and consistency,\nwhile both text-to-video and text-to-3D models show notable gaps in human\npreference alignment. We demonstrate the effectiveness of Generate Any Scene by\nconducting three practical applications leveraging captions generated by\nGenerate Any Scene: 1) a self-improving framework where models iteratively\nenhance their performance using generated data, 2) a distillation process to\ntransfer specific strengths from proprietary models to open-source\ncounterparts, and 3) improvements in content moderation by identifying and\ngenerating challenging synthetic data.\n","authors":["Ziqi Gao","Weikai Huang","Jieyu Zhang","Aniruddha Kembhavi","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2412.08221v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06618v2","updated":"2024-12-16T09:52:32Z","published":"2024-10-09T07:14:49Z","title":"Text Proxy: Decomposing Retrieval from a 1-to-N Relationship into N\n  1-to-1 Relationships for Text-Video Retrieval","summary":"  Text-video retrieval (TVR) has seen substantial advancements in recent years,\nfueled by the utilization of pre-trained models and large language models\n(LLMs). Despite these advancements, achieving accurate matching in TVR remains\nchallenging due to inherent disparities between video and textual modalities\nand irregularities in data representation. In this paper, we propose\nText-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the\nconventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.\nBy replacing a single text query with a series of text proxies, TV-ProxyNet not\nonly broadens the query scope but also achieves a more precise expansion. Each\ntext proxy is crafted through a refined iterative process, controlled by\nmechanisms we term as the director and dash, which regulate the proxy's\ndirection and distance relative to the original text query. This setup not only\nfacilitates more precise semantic alignment but also effectively manages the\ndisparities and noise inherent in multimodal data. Our experiments on three\nrepresentative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet\nCaptions, demonstrate the effectiveness of TV-ProxyNet. The results show an\nimprovement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved\nstate-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%\nimprovement on DiDeMo compared to existing methods, validating our approach's\nability to enhance semantic mapping and reduce error propensity.\n","authors":["Jian Xiao","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2410.06618v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11609v1","updated":"2024-12-16T09:50:09Z","published":"2024-12-16T09:50:09Z","title":"CLIP-SR: Collaborative Linguistic and Image Processing for\n  Super-Resolution","summary":"  Convolutional Neural Networks (CNNs) have advanced Image Super-Resolution\n(SR), but most CNN-based methods rely solely on pixel-based transformations,\noften leading to artifacts and blurring, particularly with severe downsampling\n(e.g., 8x or 16x). Recent text-guided SR methods attempt to leverage textual\ninformation for enhanced detail, but they frequently struggle with effective\nalignment, resulting in inconsistent semantic coherence. To address these\nlimitations, we introduce a multi-modal semantic enhancement approach that\ncombines textual semantics with visual features, effectively tackling semantic\nmismatches and detail loss in highly degraded LR images. Our proposed\nmulti-modal collaborative framework enables the production of realistic and\nhigh-quality SR images at significant up-scaling factors. The framework\nintegrates text and image inputs, employing a prompt predictor, Text-Image\nFusion Block (TIFBlock), and Iterative Refinement Module alongside CLIP\n(Contrastive Language-Image Pretraining) features to guide a progressive\nenhancement process with fine-grained alignment. This alignment produces\nhigh-resolution outputs with crisp details and semantic coherence, even at\nlarge scaling factors. Through extensive comparative experiments and ablation\nstudies, we validate the effectiveness of our approach. Additionally, by\nincorporating textual semantic guidance, our technique enables a degree of\nsuper-resolution editability while maintaining semantic coherence.\n","authors":["Bingwen Hu","Heng Liu","Zhedong Zheng","Ping Liu"],"pdf_url":"https://arxiv.org/pdf/2412.11609v1.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.11608v1","updated":"2024-12-16T09:49:59Z","published":"2024-12-16T09:49:59Z","title":"Towards Adversarial Robustness of Model-Level Mixture-of-Experts\n  Architectures for Semantic Segmentation","summary":"  Vulnerability to adversarial attacks is a well-known deficiency of deep\nneural networks. Larger networks are generally more robust, and ensembling is\none method to increase adversarial robustness: each model's weaknesses are\ncompensated by the strengths of others. While an ensemble uses a deterministic\nrule to combine model outputs, a mixture of experts (MoE) includes an\nadditional learnable gating component that predicts weights for the outputs of\nthe expert models, thus determining their contributions to the final\nprediction. MoEs have been shown to outperform ensembles on specific tasks, yet\ntheir susceptibility to adversarial attacks has not been studied yet. In this\nwork, we evaluate the adversarial vulnerability of MoEs for semantic\nsegmentation of urban and highway traffic scenes. We show that MoEs are, in\nmost cases, more robust to per-instance and universal white-box adversarial\nattacks and can better withstand transfer attacks. Our code is available at\n\\url{https://github.com/KASTEL-MobilityLab/mixtures-of-experts/}.\n","authors":["Svetlana Pavlitska","Enrico Eisen","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2412.11608v1.pdf","comment":"Accepted for publication at ICMLA 2024"},{"id":"http://arxiv.org/abs/2309.08927v4","updated":"2024-12-16T09:49:16Z","published":"2023-09-16T08:46:59Z","title":"DynaMoN: Motion-Aware Fast and Robust Camera Localization for Dynamic\n  Neural Radiance Fields","summary":"  The accurate reconstruction of dynamic scenes with neural radiance fields is\nsignificantly dependent on the estimation of camera poses. Widely used\nstructure-from-motion pipelines encounter difficulties in accurately tracking\nthe camera trajectory when faced with separate dynamics of the scene content\nand the camera movement. To address this challenge, we propose Dynamic\nMotion-Aware Fast and Robust Camera Localization for Dynamic Neural Radiance\nFields (DynaMoN). DynaMoN utilizes semantic segmentation and generic motion\nmasks to handle dynamic content for initial camera pose estimation and\nstatics-focused ray sampling for fast and accurate novel-view synthesis. Our\nnovel iterative learning scheme switches between training the NeRF and updating\nthe pose parameters for an improved reconstruction and trajectory estimation\nquality. The proposed pipeline shows significant acceleration of the training\nprocess. We extensively evaluate our approach on two real-world dynamic\ndatasets, the TUM RGB-D dataset and the BONN RGB-D Dynamic dataset. DynaMoN\nimproves over the state-of-the-art both in terms of reconstruction quality and\ntrajectory accuracy. We plan to make our code public to enhance research in\nthis area.\n","authors":["Nicolas Schischka","Hannah Schieber","Mert Asim Karaoglu","Melih Görgülü","Florian Grötzner","Alexander Ladikos","Daniel Roth","Nassir Navab","Benjamin Busam"],"pdf_url":"https://arxiv.org/pdf/2309.08927v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11599v1","updated":"2024-12-16T09:37:52Z","published":"2024-12-16T09:37:52Z","title":"3D$^2$-Actor: Learning Pose-Conditioned 3D-Aware Denoiser for Realistic\n  Gaussian Avatar Modeling","summary":"  Advancements in neural implicit representations and differentiable rendering\nhave markedly improved the ability to learn animatable 3D avatars from sparse\nmulti-view RGB videos. However, current methods that map observation space to\ncanonical space often face challenges in capturing pose-dependent details and\ngeneralizing to novel poses. While diffusion models have demonstrated\nremarkable zero-shot capabilities in 2D image generation, their potential for\ncreating animatable 3D avatars from 2D inputs remains underexplored. In this\nwork, we introduce 3D$^2$-Actor, a novel approach featuring a pose-conditioned\n3D-aware human modeling pipeline that integrates iterative 2D denoising and 3D\nrectifying steps. The 2D denoiser, guided by pose cues, generates detailed\nmulti-view images that provide the rich feature set necessary for high-fidelity\n3D reconstruction and pose rendering. Complementing this, our Gaussian-based 3D\nrectifier renders images with enhanced 3D consistency through a two-stage\nprojection strategy and a novel local coordinate representation. Additionally,\nwe propose an innovative sampling strategy to ensure smooth temporal continuity\nacross frames in video synthesis. Our method effectively addresses the\nlimitations of traditional numerical solutions in handling ill-posed mappings,\nproducing realistic and animatable 3D human avatars. Experimental results\ndemonstrate that 3D$^2$-Actor excels in high-fidelity avatar modeling and\nrobustly generalizes to novel poses. Code is available at:\nhttps://github.com/silence-tang/GaussianActor.\n","authors":["Zichen Tang","Hongyu Yang","Hanchen Zhang","Jiaxin Chen","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11599v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2405.01124v5","updated":"2024-12-16T09:37:17Z","published":"2024-05-02T09:38:07Z","title":"Investigating Self-Supervised Image Denoising with Denaturation","summary":"  Self-supervised learning for image denoising problems in the presence of\ndenaturation for noisy data is a crucial approach in machine learning. However,\ntheoretical understanding of the performance of the approach that uses\ndenatured data is lacking. To provide better understanding of the approach, in\nthis paper, we analyze a self-supervised denoising algorithm that uses\ndenatured data in depth through theoretical analysis and numerical experiments.\nThrough the theoretical analysis, we discuss that the algorithm finds desired\nsolutions to the optimization problem with the population risk, while the\nguarantee for the empirical risk depends on the hardness of the denoising task\nin terms of denaturation levels. We also conduct several experiments to\ninvestigate the performance of an extended algorithm in practice. The results\nindicate that the algorithm training with denatured images works, and the\nempirical performance aligns with the theoretical results. These results\nsuggest several insights for further improvement of self-supervised image\ndenoising that uses denatured data in future directions.\n","authors":["Hiroki Waida","Kimihiro Yamazaki","Atsushi Tokuhisa","Mutsuyo Wada","Yuichiro Wada"],"pdf_url":"https://arxiv.org/pdf/2405.01124v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11596v1","updated":"2024-12-16T09:35:08Z","published":"2024-12-16T09:35:08Z","title":"MeshArt: Generating Articulated Meshes with Structure-guided\n  Transformers","summary":"  Articulated 3D object generation is fundamental for creating realistic,\nfunctional, and interactable virtual assets which are not simply static. We\nintroduce MeshArt, a hierarchical transformer-based approach to generate\narticulated 3D meshes with clean, compact geometry, reminiscent of\nhuman-crafted 3D models. We approach articulated mesh generation in a\npart-by-part fashion across two stages. First, we generate a high-level\narticulation-aware object structure; then, based on this structural\ninformation, we synthesize each part's mesh faces. Key to our approach is\nmodeling both articulation structures and part meshes as sequences of quantized\ntriangle embeddings, leading to a unified hierarchical framework with\ntransformers for autoregressive generation. Object part structures are first\ngenerated as their bounding primitives and articulation modes; a second\ntransformer, guided by these articulation structures, then generates each\npart's mesh triangles. To ensure coherency among generated parts, we introduce\nstructure-guided conditioning that also incorporates local part mesh\nconnectivity. MeshArt shows significant improvements over state of the art,\nwith 57.1% improvement in structure coverage and a 209-point improvement in\nmesh generation FID.\n","authors":["Daoyi Gao","Yawar Siddiqui","Lei Li","Angela Dai"],"pdf_url":"https://arxiv.org/pdf/2412.11596v1.pdf","comment":"Project Page: https://daoyig.github.io/Mesh_Art/"},{"id":"http://arxiv.org/abs/2412.11594v1","updated":"2024-12-16T09:32:23Z","published":"2024-12-16T09:32:23Z","title":"VersaGen: Unleashing Versatile Visual Control for Text-to-Image\n  Synthesis","summary":"  Despite the rapid advancements in text-to-image (T2I) synthesis, enabling\nprecise visual control remains a significant challenge. Existing works\nattempted to incorporate multi-facet controls (text and sketch), aiming to\nenhance the creative control over generated images. However, our pilot study\nreveals that the expressive power of humans far surpasses the capabilities of\ncurrent methods. Users desire a more versatile approach that can accommodate\ntheir diverse creative intents, ranging from controlling individual subjects to\nmanipulating the entire scene composition. We present VersaGen, a generative AI\nagent that enables versatile visual control in T2I synthesis. VersaGen admits\nfour types of visual controls: i) single visual subject; ii) multiple visual\nsubjects; iii) scene background; iv) any combination of the three above or\nmerely no control at all. We train an adaptor upon a frozen T2I model to\naccommodate the visual information into the text-dominated diffusion process.\nWe introduce three optimization strategies during the inference phase of\nVersaGen to improve generation results and enhance user experience.\nComprehensive experiments on COCO and Sketchy validate the effectiveness and\nflexibility of VersaGen, as evidenced by both qualitative and quantitative\nresults.\n","authors":["Zhipeng Chen","Lan Yang","Yonggang Qi","Honggang Zhang","Kaiyue Pang","Ke Li","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2412.11594v1.pdf","comment":"The paper has been accepted by AAAI 2025. Paper code:\n  https://github.com/FelixChan9527/VersaGen_official"},{"id":"http://arxiv.org/abs/2412.07147v2","updated":"2024-12-16T09:28:53Z","published":"2024-12-10T03:12:35Z","title":"MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation","summary":"  Image Translation (IT) holds immense potential across diverse domains,\nenabling the translation of textual content within images into various\nlanguages. However, existing datasets often suffer from limitations in scale,\ndiversity, and quality, hindering the development and evaluation of IT models.\nTo address this issue, we introduce MIT-10M, a large-scale parallel corpus of\nmultilingual image translation with over 10M image-text pairs derived from\nreal-world data, which has undergone extensive data cleaning and multilingual\ntranslation validation. It contains 840K images in three sizes, 28 categories,\ntasks with three levels of difficulty and 14 languages image-text pairs, which\nis a considerable improvement on existing datasets. We conduct extensive\nexperiments to evaluate and train models on MIT-10M. The experimental results\nclearly indicate that our dataset has higher adaptability when it comes to\nevaluating the performance of the models in tackling challenging and complex\nimage translation tasks in the real world. Moreover, the performance of the\nmodel fine-tuned with MIT-10M has tripled compared to the baseline model,\nfurther confirming its superiority.\n","authors":["Bo Li","Shaolin Zhu","Lijie Wen"],"pdf_url":"https://arxiv.org/pdf/2412.07147v2.pdf","comment":"Accepted in COLING 2025"},{"id":"http://arxiv.org/abs/2412.06365v2","updated":"2024-12-16T09:25:31Z","published":"2024-12-09T10:35:39Z","title":"Is Self-Supervision Enough? Benchmarking Foundation Models Against\n  End-to-End Training for Mitotic Figure Classification","summary":"  Foundation models (FMs), i.e., models trained on a vast amount of typically\nunlabeled data, have become popular and available recently for the domain of\nhistopathology. The key idea is to extract semantically rich vectors from any\ninput patch, allowing for the use of simple subsequent classification networks\npotentially reducing the required amounts of labeled data, and increasing\ndomain robustness. In this work, we investigate to which degree this also holds\nfor mitotic figure classification. Utilizing two popular public mitotic figure\ndatasets, we compared linear probing of five publicly available FMs against\nmodels trained on ImageNet and a simple ResNet50 end-to-end-trained baseline.\nWe found that the end-to-end-trained baseline outperformed all FM-based\nclassifiers, regardless of the amount of data provided. Additionally, we did\nnot observe the FM-based classifiers to be more robust against domain shifts,\nrendering both of the above assumptions incorrect.\n","authors":["Jonathan Ganz","Jonas Ammeling","Emely Rosbach","Ludwig Lausser","Christof A. Bertram","Katharina Breininger","Marc Aubreville"],"pdf_url":"https://arxiv.org/pdf/2412.06365v2.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2404.16471v6","updated":"2024-12-16T09:21:03Z","published":"2024-04-25T09:55:35Z","title":"COBRA -- COnfidence score Based on shape Regression Analysis for\n  method-independent quality assessment of object pose estimation from single\n  images","summary":"  We propose a generic procedure for assessing 6D object pose estimates. Our\napproach relies on the evaluation of discrepancies in the geometry of the\nobserved object, in particular its respective estimated back-projection in 3D,\nagainst a putative functional shape representation comprising mixtures of\nGaussian Processes, that act as a template. Each Gaussian Process is trained to\nyield a fragment of the object's surface in a radial fashion with respect to\ndesignated reference points. We further define a pose confidence measure as the\naverage probability of pixel back-projections in the Gaussian mixture. The goal\nof our experiments is two-fold. a) We demonstrate that our functional\nrepresentation is sufficiently accurate as a shape template on which the\nprobability of back-projected object points can be evaluated, and, b) we show\nthat the resulting confidence scores based on these probabilities are indeed a\nconsistent quality measure of pose.\n","authors":["Panagiotis Sapoutzoglou","Georgios Giapitzakis","Georgios Floros","George Terzakis","Maria Pateraki"],"pdf_url":"https://arxiv.org/pdf/2404.16471v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09126v5","updated":"2024-12-16T09:18:51Z","published":"2024-08-17T07:27:14Z","title":"Barbie: Text to Barbie-Style 3D Avatars","summary":"  Recent advances in text-guided 3D avatar generation have made substantial\nprogress by distilling knowledge from diffusion models. Despite the plausible\ngenerated appearance, existing methods cannot achieve fine-grained\ndisentanglement or high-fidelity modeling between inner body and outfit. In\nthis paper, we propose Barbie, a novel framework for generating 3D avatars that\ncan be dressed in diverse and high-quality Barbie-like garments and\naccessories. Instead of relying on a holistic model, Barbie achieves\nfine-grained disentanglement on avatars by semantic-aligned separated models\nfor human body and outfits. These disentangled 3D representations are then\noptimized by different expert models to guarantee the domain-specific fidelity.\nTo balance geometry diversity and reasonableness, we propose a series of losses\nfor template-preserving and human-prior evolving. The final avatar is enhanced\nby unified texture refinement for superior texture consistency. Extensive\nexperiments demonstrate that Barbie outperforms existing methods in both\ndressed human and outfit generation, supporting flexible apparel combination\nand animation. Our project page is:\nhttps://xiaokunsun.github.io/Barbie.github.io.\n","authors":["Xiaokun Sun","Zhenyu Zhang","Ying Tai","Qian Wang","Hao Tang","Zili Yi","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2408.09126v5.pdf","comment":"Project page: https://xiaokunsun.github.io/Barbie.github.io"},{"id":"http://arxiv.org/abs/2405.03959v4","updated":"2024-12-16T09:18:28Z","published":"2024-05-07T02:45:50Z","title":"Joint Identity Verification and Pose Alignment for Partial Fingerprints","summary":"  Currently, portable electronic devices are becoming more and more popular.\nFor lightweight considerations, their fingerprint recognition modules usually\nuse limited-size sensors. However, partial fingerprints have few matchable\nfeatures, especially when there are differences in finger pressing posture or\nimage quality, which makes partial fingerprint verification challenging. Most\nexisting methods regard fingerprint position rectification and identity\nverification as independent tasks, ignoring the coupling relationship between\nthem -- relative pose estimation typically relies on paired features as\nanchors, and authentication accuracy tends to improve with more precise pose\nalignment. In this paper, we propose a novel framework for joint identity\nverification and pose alignment of partial fingerprint pairs, aiming to\nleverage their inherent correlation to improve each other. To achieve this, we\npresent a multi-task CNN (Convolutional Neural Network)-Transformer hybrid\nnetwork, and design a pre-training task to enhance the feature extraction\ncapability. Experiments on multiple public datasets (NIST SD14, FVC2002 DB1A &\nDB3A, FVC2004 DB1A & DB2A, FVC2006 DB1A) and an in-house dataset show that our\nmethod achieves state-of-the-art performance in both partial fingerprint\nverification and relative pose estimation, while being more efficient than\nprevious methods.\n","authors":["Xiongjun Guan","Zhiyu Pan","Jianjiang Feng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2405.03959v4.pdf","comment":"15 pages, in IEEE Transactions on Information Forensics and Security,\n  2024"},{"id":"http://arxiv.org/abs/2412.11586v1","updated":"2024-12-16T09:17:36Z","published":"2024-12-16T09:17:36Z","title":"StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair\n  Geometric Priors","summary":"  While haircut indicates distinct personality, existing avatar generation\nmethods fail to model practical hair due to the general or entangled\nrepresentation. We propose StrandHead, a novel text to 3D head avatar\ngeneration method capable of generating disentangled 3D hair with strand\nrepresentation. Without using 3D data for supervision, we demonstrate that\nrealistic hair strands can be generated from prompts by distilling 2D\ngenerative diffusion models. To this end, we propose a series of reliable\npriors on shape initialization, geometric primitives, and statistical haircut\nfeatures, leading to a stable optimization and text-aligned performance.\nExtensive experiments show that StrandHead achieves the state-of-the-art\nreality and diversity of generated 3D head and hair. The generated 3D hair can\nalso be easily implemented in the Unreal Engine for physical simulation and\nother applications. The code will be available at\nhttps://xiaokunsun.github.io/StrandHead.github.io.\n","authors":["Xiaokun Sun","Zeyu Cai","Zhenyu Zhang","Ying Tai","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11586v1.pdf","comment":"Project page: https://xiaokunsun.github.io/StrandHead.github.io"},{"id":"http://arxiv.org/abs/2412.05271v2","updated":"2024-12-16T09:14:43Z","published":"2024-12-06T18:57:08Z","title":"Expanding Performance Boundaries of Open-Source Multimodal Models with\n  Model, Data, and Test-Time Scaling","summary":"  We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)\nseries that builds upon InternVL 2.0, maintaining its core model architecture\nwhile introducing significant enhancements in training and testing strategies\nas well as data quality. In this work, we delve into the relationship between\nmodel scaling and performance, systematically exploring the performance trends\nin vision encoders, language models, dataset sizes, and test-time\nconfigurations. Through extensive evaluations on a wide range of benchmarks,\nincluding multi-discipline reasoning, document understanding, multi-image /\nvideo understanding, real-world comprehension, multimodal hallucination\ndetection, visual grounding, multilingual capabilities, and pure language\nprocessing, InternVL 2.5 exhibits competitive performance, rivaling leading\ncommercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is\nthe first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a\n3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing\nstrong potential for test-time scaling. We hope this model contributes to the\nopen-source community by setting new standards for developing and applying\nmultimodal AI systems. HuggingFace demo see\nhttps://huggingface.co/spaces/OpenGVLab/InternVL\n","authors":["Zhe Chen","Weiyun Wang","Yue Cao","Yangzhou Liu","Zhangwei Gao","Erfei Cui","Jinguo Zhu","Shenglong Ye","Hao Tian","Zhaoyang Liu","Lixin Gu","Xuehui Wang","Qingyun Li","Yimin Ren","Zixuan Chen","Jiapeng Luo","Jiahao Wang","Tan Jiang","Bo Wang","Conghui He","Botian Shi","Xingcheng Zhang","Han Lv","Yi Wang","Wenqi Shao","Pei Chu","Zhongying Tu","Tong He","Zhiyong Wu","Huipeng Deng","Jiaye Ge","Kai Chen","Min Dou","Lewei Lu","Xizhou Zhu","Tong Lu","Dahua Lin","Yu Qiao","Jifeng Dai","Wenhai Wang"],"pdf_url":"https://arxiv.org/pdf/2412.05271v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.11582v1","updated":"2024-12-16T09:14:32Z","published":"2024-12-16T09:14:32Z","title":"Oriented Tiny Object Detection: A Dataset, Benchmark, and Dynamic\n  Unbiased Learning","summary":"  Detecting oriented tiny objects, which are limited in appearance information\nyet prevalent in real-world applications, remains an intricate and\nunder-explored problem. To address this, we systemically introduce a new\ndataset, benchmark, and a dynamic coarse-to-fine learning scheme in this study.\nOur proposed dataset, AI-TOD-R, features the smallest object sizes among all\noriented object detection datasets. Based on AI-TOD-R, we present a benchmark\nspanning a broad range of detection paradigms, including both fully-supervised\nand label-efficient approaches. Through investigation, we identify a learning\nbias presents across various learning pipelines: confident objects become\nincreasingly confident, while vulnerable oriented tiny objects are further\nmarginalized, hindering their detection performance. To mitigate this issue, we\npropose a Dynamic Coarse-to-Fine Learning (DCFL) scheme to achieve unbiased\nlearning. DCFL dynamically updates prior positions to better align with the\nlimited areas of oriented tiny objects, and it assigns samples in a way that\nbalances both quantity and quality across different object shapes, thus\nmitigating biases in prior settings and sample selection. Extensive experiments\nacross eight challenging object detection datasets demonstrate that DCFL\nachieves state-of-the-art accuracy, high efficiency, and remarkable\nversatility. The dataset, benchmark, and code are available at\nhttps://chasel-tsui.github.io/AI-TOD-R/.\n","authors":["Chang Xu","Ruixiang Zhang","Wen Yang","Haoran Zhu","Fang Xu","Jian Ding","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2412.11582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11579v1","updated":"2024-12-16T09:09:42Z","published":"2024-12-16T09:09:42Z","title":"SweepEvGS: Event-Based 3D Gaussian Splatting for Macro and Micro\n  Radiance Field Rendering from a Single Sweep","summary":"  Recent advancements in 3D Gaussian Splatting (3D-GS) have demonstrated the\npotential of using 3D Gaussian primitives for high-speed, high-fidelity, and\ncost-efficient novel view synthesis from continuously calibrated input views.\nHowever, conventional methods require high-frame-rate dense and high-quality\nsharp images, which are time-consuming and inefficient to capture, especially\nin dynamic environments. Event cameras, with their high temporal resolution and\nability to capture asynchronous brightness changes, offer a promising\nalternative for more reliable scene reconstruction without motion blur. In this\npaper, we propose SweepEvGS, a novel hardware-integrated method that leverages\nevent cameras for robust and accurate novel view synthesis across various\nimaging settings from a single sweep. SweepEvGS utilizes the initial static\nframe with dense event streams captured during a single camera sweep to\neffectively reconstruct detailed scene views. We also introduce different\nreal-world hardware imaging systems for real-world data collection and\nevaluation for future research. We validate the robustness and efficiency of\nSweepEvGS through experiments in three different imaging settings: synthetic\nobjects, real-world macro-level, and real-world micro-level view synthesis. Our\nresults demonstrate that SweepEvGS surpasses existing methods in visual\nrendering quality, rendering speed, and computational efficiency, highlighting\nits potential for dynamic practical applications.\n","authors":["Jingqian Wu","Shuo Zhu","Chutian Wang","Boxin Shi","Edmund Y. Lam"],"pdf_url":"https://arxiv.org/pdf/2412.11579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11578v1","updated":"2024-12-16T09:09:10Z","published":"2024-12-16T09:09:10Z","title":"DVP-MVS: Synergize Depth-Edge and Visibility Prior for Multi-View Stereo","summary":"  Patch deformation-based methods have recently exhibited substantial\neffectiveness in multi-view stereo, due to the incorporation of deformable and\nexpandable perception to reconstruct textureless areas. However, such\napproaches typically focus on exploring correlative reliable pixels to\nalleviate match ambiguity during patch deformation, but ignore the deformation\ninstability caused by mistaken edge-skipping and visibility occlusion, leading\nto potential estimation deviation. To remedy the above issues, we propose\nDVP-MVS, which innovatively synergizes depth-edge aligned and cross-view prior\nfor robust and visibility-aware patch deformation. Specifically, to avoid\nunexpected edge-skipping, we first utilize Depth Anything V2 followed by the\nRoberts operator to initialize coarse depth and edge maps respectively, both of\nwhich are further aligned through an erosion-dilation strategy to generate\nfine-grained homogeneous boundaries for guiding patch deformation. In addition,\nwe reform view selection weights as visibility maps and restore visible areas\nby cross-view depth reprojection, then regard them as cross-view prior to\nfacilitate visibility-aware patch deformation. Finally, we improve propagation\nand refinement with multi-view geometry consistency by introducing aggregated\nvisible hemispherical normals based on view selection and local projection\ndepth differences based on epipolar lines, respectively. Extensive evaluations\non ETH3D and Tanks & Temples benchmarks demonstrate that our method can achieve\nstate-of-the-art performance with excellent robustness and generalization.\n","authors":["Zhenlong Yuan","Jinguo Luo","Fei Shen","Zhaoxin Li","Cong Liu","Tianlu Mao","Zhaoqi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11576v1","updated":"2024-12-16T09:04:58Z","published":"2024-12-16T09:04:58Z","title":"Aligning Visual and Semantic Interpretability through Visually Grounded\n  Concept Bottleneck Models","summary":"  The performance of neural networks increases steadily, but our understanding\nof their decision-making lags behind. Concept Bottleneck Models (CBMs) address\nthis issue by incorporating human-understandable concepts into the prediction\nprocess, thereby enhancing transparency and interpretability. Since existing\napproaches often rely on large language models (LLMs) to infer concepts, their\nresults may contain inaccurate or incomplete mappings, especially in complex\nvisual domains. We introduce visually Grounded Concept Bottleneck Models\n(GCBM), which derive concepts on the image level using segmentation and\ndetection foundation models. Our method generates inherently interpretable\nconcepts, which can be grounded in the input image using attribution methods,\nallowing interpretations to be traced back to the image plane. We show that\nGCBM concepts are meaningful interpretability vehicles, which aid our\nunderstanding of model embedding spaces. GCBMs allow users to control the\ngranularity, number, and naming of concepts, providing flexibility and are\neasily adaptable to new datasets without pre-training or additional data\nneeded. Prediction accuracy is within 0.3-6% of the linear probe and GCBMs\nperform especially well for fine-grained classification interpretability on\nCUB, due to their dataset specificity. Our code is available on\nhttps://github.com/KathPra/GCBM.\n","authors":["Patrick Knab","Katharina Prasse","Sascha Marton","Christian Bartelt","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2412.11576v1.pdf","comment":"*Equal contribution"},{"id":"http://arxiv.org/abs/2407.00676v2","updated":"2024-12-16T09:04:24Z","published":"2024-06-30T12:13:34Z","title":"Instruct-IPT: All-in-One Image Processing Transformer via Weight\n  Modulation","summary":"  Due to the unaffordable size and intensive computation costs of low-level\nvision models, All-in-One models that are designed to address a handful of\nlow-level vision tasks simultaneously have been popular. However, existing\nAll-in-One models are limited in terms of the range of tasks and performance.\nTo overcome these limitations, we propose Instruct-IPT -- an All-in-One Image\nProcessing Transformer (IPT) that could effectively address manifold image\nrestoration tasks with large inter-task gaps, such as denoising, deblurring,\nderaining, dehazing, and desnowing. While most research propose feature\nadaptation methods, we reveal their failure in addressing highly distinct\ntasks, and suggest weight modulation that adapts weights to specific tasks.\nFirstly, we search for task-sensitive weights and introduce task-specific\nbiases on top of them. Secondly, we conduct rank analysis for a good\ncompression strategy and perform low-rank decomposition on the biases. Thirdly,\nwe propose synchronous training that updates the task-general backbone model\nand the task-specific biases simultaneously. In this way, the model is\ninstructed to learn both general and task-specific knowledge. Via our simple\nyet effective method that instructs the IPT to be task experts, Instruct-IPT\ncould better cooperate between tasks with distinct characteristics at humble\ncosts. As an additional feature, we enable Instruct-IPT to receive human\nprompts. We have conducted experiments on Instruct-IPT to demonstrate the\neffectiveness of our method on manifold tasks, and we have effectively extended\nour method to diffusion denoisers as well. The code is available at\nhttps://github.com/huawei-noah/Pretrained-IPT.\n","authors":["Yuchuan Tian","Jianhong Han","Hanting Chen","Yuanyuan Xi","Ning Ding","Jie Hu","Chao Xu","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2407.00676v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.11574v1","updated":"2024-12-16T09:01:32Z","published":"2024-12-16T09:01:32Z","title":"PyPotteryLens: An Open-Source Deep Learning Framework for Automated\n  Digitisation of Archaeological Pottery Documentation","summary":"  Archaeological pottery documentation and study represents a crucial but\ntime-consuming aspect of archaeology. While recent years have seen advances in\ndigital documentation methods, vast amounts of legacy data remain locked in\ntraditional publications. This paper introduces PyPotteryLens, an open-source\nframework that leverages deep learning to automate the digitisation and\nprocessing of archaeological pottery drawings from published sources. The\nsystem combines state-of-the-art computer vision models (YOLO for instance\nsegmentation and EfficientNetV2 for classification) with an intuitive user\ninterface, making advanced digital methods accessible to archaeologists\nregardless of technical expertise. The framework achieves over 97\\% precision\nand recall in pottery detection and classification tasks, while reducing\nprocessing time by up to 5x to 20x compared to manual methods. Testing across\ndiverse archaeological contexts demonstrates robust generalisation\ncapabilities. Also, the system's modular architecture facilitates extension to\nother archaeological materials, while its standardised output format ensures\nlong-term preservation and reusability of digitised data as well as solid basis\nfor training machine learning algorithms. The software, documentation, and\nexamples are available on GitHub\n(https://github.com/lrncrd/PyPottery/tree/PyPotteryLens).\n","authors":["Lorenzo Cardarelli"],"pdf_url":"https://arxiv.org/pdf/2412.11574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10261v2","updated":"2024-12-16T08:54:43Z","published":"2024-12-13T16:30:35Z","title":"MVQ:Towards Efficient DNN Compression and Acceleration with Masked\n  Vector Quantization","summary":"  Vector quantization(VQ) is a hardware-friendly DNN compression method that\ncan reduce the storage cost and weight-loading datawidth of hardware\naccelerators. However, conventional VQ techniques lead to significant accuracy\nloss because the important weights are not well preserved. To tackle this\nproblem, a novel approach called MVQ is proposed, which aims at better\napproximating important weights with a limited number of codewords. At the\nalgorithm level, our approach removes the less important weights through N:M\npruning and then minimizes the vector clustering error between the remaining\nweights and codewords by the masked k-means algorithm. Only distances between\nthe unpruned weights and the codewords are computed, which are then used to\nupdate the codewords. At the architecture level, our accelerator implements\nvector quantization on an EWS (Enhanced weight stationary) CNN accelerator and\nproposes a sparse systolic array design to maximize the benefits brought by\nmasked vector quantization.\\\\ Our algorithm is validated on various models for\nimage classification, object detection, and segmentation tasks. Experimental\nresults demonstrate that MVQ not only outperforms conventional vector\nquantization methods at comparable compression ratios but also reduces FLOPs.\nUnder ASIC evaluation, our MVQ accelerator boosts energy efficiency by\n2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared\nwith the base EWS accelerator. Compared to the previous sparse accelerators,\nMVQ achieves 1.73$\\times$ higher energy efficiency.\n","authors":["Shuaiting Li","Chengxuan Wang","Juncan Deng","Zeyu Wang","Zewen Ye","Zongsheng Wang","Haibin Shen","Kejie Huang"],"pdf_url":"https://arxiv.org/pdf/2412.10261v2.pdf","comment":"Accepted by ASPLOS '25"},{"id":"http://arxiv.org/abs/2412.11561v1","updated":"2024-12-16T08:47:55Z","published":"2024-12-16T08:47:55Z","title":"RADARSAT Constellation Mission Compact Polarisation SAR Data for Burned\n  Area Mapping with Deep Learning","summary":"  Monitoring wildfires has become increasingly critical due to the sharp rise\nin wildfire incidents in recent years. Optical satellites like Sentinel-2 and\nLandsat are extensively utilized for mapping burned areas. However, the\neffectiveness of optical sensors is compromised by clouds and smoke, which\nobstruct the detection of burned areas. Thus, satellites equipped with\nSynthetic Aperture Radar (SAR), such as dual-polarization Sentinel-1 and\nquad-polarization RADARSAT-1/-2 C-band SAR, which can penetrate clouds and\nsmoke, are investigated for mapping burned areas. However, there is limited\nresearch on using compact polarisation (compact-pol) C-band RADARSAT\nConstellation Mission (RCM) SAR data for this purpose. This study aims to\ninvestigate the capacity of compact polarisation RCM data for burned area\nmapping through deep learning. Compact-pol m-chi decomposition and Compact-pol\nRadar Vegetation Index (CpRVI) are derived from the RCM Multi-look Complex\nproduct. A deep-learning-based processing pipeline incorporating ConvNet-based\nand Transformer-based models is applied for burned area mapping, with three\ndifferent input settings: using only log-ratio dual-polarization intensity\nimages images, using only compact-pol decomposition plus CpRVI, and using all\nthree data sources. The results demonstrate that compact-pol m-chi\ndecomposition and CpRVI images significantly complement log-ratio images for\nburned area mapping. The best-performing Transformer-based model, UNETR,\ntrained with log-ratio, m-chi decomposition, and CpRVI data, achieved an F1\nScore of 0.718 and an IoU Score of 0.565, showing a notable improvement\ncompared to the same model trained using only log-ratio images.\n","authors":["Yu Zhao","Yifang Ban"],"pdf_url":"https://arxiv.org/pdf/2412.11561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11555v1","updated":"2024-12-16T08:40:12Z","published":"2024-12-16T08:40:12Z","title":"TS-SatFire: A Multi-Task Satellite Image Time-Series Dataset for\n  Wildfire Detection and Prediction","summary":"  Wildfire monitoring and prediction are essential for understanding wildfire\nbehaviour. With extensive Earth observation data, these tasks can be integrated\nand enhanced through multi-task deep learning models. We present a\ncomprehensive multi-temporal remote sensing dataset for active fire detection,\ndaily wildfire monitoring, and next-day wildfire prediction. Covering wildfire\nevents in the contiguous U.S. from January 2017 to October 2021, the dataset\nincludes 3552 surface reflectance images and auxiliary data such as weather,\ntopography, land cover, and fuel information, totalling 71 GB. The lifecycle of\neach wildfire is documented, with labels for active fires (AF) and burned areas\n(BA), supported by manual quality assurance of AF and BA test labels. The\ndataset supports three tasks: a) active fire detection, b) daily burned area\nmapping, and c) wildfire progression prediction. Detection tasks use pixel-wise\nclassification of multi-spectral, multi-temporal images, while prediction tasks\nintegrate satellite and auxiliary data to model fire dynamics. This dataset and\nits benchmarks provide a foundation for advancing wildfire research using deep\nlearning.\n","authors":["Yu Zhao","Sebastian Gerard","Yifang Ban"],"pdf_url":"https://arxiv.org/pdf/2412.11555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11553v1","updated":"2024-12-16T08:37:58Z","published":"2024-12-16T08:37:58Z","title":"Training Strategies for Isolated Sign Language Recognition","summary":"  This paper introduces a comprehensive model training pipeline for Isolated\nSign Language Recognition (ISLR) designed to accommodate the distinctive\ncharacteristics and constraints of the Sign Language (SL) domain. The\nconstructed pipeline incorporates carefully selected image and video\naugmentations to tackle the challenges of low data quality and varying sign\nspeeds. Including an additional regression head combined with IoU-balanced\nclassification loss enhances the model's awareness of the gesture and\nsimplifies capturing temporal information. Extensive experiments demonstrate\nthat the developed training pipeline easily adapts to different datasets and\narchitectures. Additionally, the ablation study shows that each proposed\ncomponent expands the potential to consider ISLR task specifics. The presented\nstrategies improve recognition performance on a broad set of ISLR benchmarks.\nMoreover, we achieved a state-of-the-art result on the WLASL and Slovo\nbenchmarks with 1.63% and 14.12% improvements compared to the previous best\nsolution, respectively.\n","authors":["Karina Kvanchiani","Roman Kraynov","Elizaveta Petrova","Petr Surovcev","Aleksandr Nagaev","Alexander Kapitanov"],"pdf_url":"https://arxiv.org/pdf/2412.11553v1.pdf","comment":"sign language recognition, training strategies, computer vision"},{"id":"http://arxiv.org/abs/2310.19180v3","updated":"2024-12-16T08:34:52Z","published":"2023-10-29T22:51:49Z","title":"JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music\n  Generation","summary":"  With rapid advances in generative artificial intelligence, the text-to-music\nsynthesis task has emerged as a promising direction for music generation.\nNevertheless, achieving precise control over multi-track generation remains an\nopen challenge. While existing models excel in directly generating multi-track\nmix, their limitations become evident when it comes to composing individual\ntracks and integrating them in a controllable manner. This departure from the\ntypical workflows of professional composers hinders the ability to refine\ndetails in specific tracks. To address this gap, we propose JEN-1 Composer, a\nunified framework designed to efficiently model marginal, conditional, and\njoint distributions over multi-track music using a single model. Building upon\nan audio latent diffusion model, JEN-1 Composer extends the versatility of\nmulti-track music generation. We introduce a progressive curriculum training\nstrategy, which gradually escalates the difficulty of training tasks while\nensuring the model's generalization ability and facilitating smooth transitions\nbetween different scenarios. During inference, users can iteratively generate\nand select music tracks, thus incrementally composing entire musical pieces in\naccordance with the Human-AI co-composition workflow. Our approach demonstrates\nstate-of-the-art performance in controllable and high-fidelity multi-track\nmusic synthesis, marking a significant advancement in interactive AI-assisted\nmusic creation. Our demo pages are available at www.jenmusic.ai/research.\n","authors":["Yao Yao","Peike Li","Boyu Chen","Alex Wang"],"pdf_url":"https://arxiv.org/pdf/2310.19180v3.pdf","comment":"9 pages, 3 figures, accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11549v1","updated":"2024-12-16T08:31:55Z","published":"2024-12-16T08:31:55Z","title":"MPQ-DM: Mixed Precision Quantization for Extremely Low Bit Diffusion\n  Models","summary":"  Diffusion models have received wide attention in generation tasks. However,\nthe expensive computation cost prevents the application of diffusion models in\nresource-constrained scenarios. Quantization emerges as a practical solution\nthat significantly saves storage and computation by reducing the bit-width of\nparameters. However, the existing quantization methods for diffusion models\nstill cause severe degradation in performance, especially under extremely low\nbit-widths (2-4 bit). The primary decrease in performance comes from the\nsignificant discretization of activation values at low bit quantization. Too\nfew activation candidates are unfriendly for outlier significant weight channel\nquantization, and the discretized features prevent stable learning over\ndifferent time steps of the diffusion model. This paper presents MPQ-DM, a\nMixed-Precision Quantization method for Diffusion Models. The proposed MPQ-DM\nmainly relies on two techniques:(1) To mitigate the quantization error caused\nby outlier severe weight channels, we propose an Outlier-Driven Mixed\nQuantization (OMQ) technique that uses $Kurtosis$ to quantify outlier salient\nchannels and apply optimized intra-layer mixed-precision bit-width allocation\nto recover accuracy performance within target efficiency.(2) To robustly learn\nrepresentations crossing time steps, we construct a Time-Smoothed Relation\nDistillation (TRD) scheme between the quantized diffusion model and its\nfull-precision counterpart, transferring discrete and continuous latent to a\nunified relation space to reduce the representation inconsistency.\nComprehensive experiments demonstrate that MPQ-DM achieves significant accuracy\ngains under extremely low bit-widths compared with SOTA quantization methods.\nMPQ-DM achieves a 58\\% FID decrease under W2A4 setting compared with baseline,\nwhile all other methods even collapse.\n","authors":["Weilun Feng","Haotong Qin","Chuanguang Yang","Zhulin An","Libo Huang","Boyu Diao","Fei Wang","Renshuai Tao","Yongjun Xu","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2412.11549v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11542v1","updated":"2024-12-16T08:22:23Z","published":"2024-12-16T08:22:23Z","title":"Meta Curvature-Aware Minimization for Domain Generalization","summary":"  Domain generalization (DG) aims to enhance the ability of models trained on\nsource domains to generalize effectively to unseen domains. Recently,\nSharpness-Aware Minimization (SAM) has shown promise in this area by reducing\nthe sharpness of the loss landscape to obtain more generalized models. However,\nSAM and its variants sometimes fail to guide the model toward a flat minimum,\nand their training processes exhibit limitations, hindering further\nimprovements in model generalization. In this paper, we first propose an\nimproved model training process aimed at encouraging the model to converge to a\nflat minima. To achieve this, we design a curvature metric that has a minimal\neffect when the model is far from convergence but becomes increasingly\ninfluential in indicating the curvature of the minima as the model approaches a\nlocal minimum. Then we derive a novel algorithm from this metric, called Meta\nCurvature-Aware Minimization (MeCAM), to minimize the curvature around the\nlocal minima. Specifically, the optimization objective of MeCAM simultaneously\nminimizes the regular training loss, the surrogate gap of SAM, and the\nsurrogate gap of meta-learning. We provide theoretical analysis on MeCAM's\ngeneralization error and convergence rate, and demonstrate its superiority over\nexisting DG methods through extensive experiments on five benchmark DG\ndatasets, including PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet. Code\nwill be available on GitHub.\n","authors":["Ziyang Chen","Yiwen Ye","Feilong Tang","Yongsheng Pan","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2412.11542v1.pdf","comment":"21 pages, 5 figures, 17 tables"},{"id":"http://arxiv.org/abs/2412.11540v1","updated":"2024-12-16T08:21:09Z","published":"2024-12-16T08:21:09Z","title":"SP$^2$T: Sparse Proxy Attention for Dual-stream Point Transformer","summary":"  In 3D understanding, point transformers have yielded significant advances in\nbroadening the receptive field. However, further enhancement of the receptive\nfield is hindered by the constraints of grouping attention. The proxy-based\nmodel, as a hot topic in image and language feature extraction, uses global or\nlocal proxies to expand the model's receptive field. But global proxy-based\nmethods fail to precisely determine proxy positions and are not suited for\ntasks like segmentation and detection in the point cloud, and exist local\nproxy-based methods for image face difficulties in global-local balance, proxy\nsampling in various point clouds, and parallel cross-attention computation for\nsparse association. In this paper, we present SP$^2$T, a local proxy-based dual\nstream point transformer, which promotes global receptive field while\nmaintaining a balance between local and global information. To tackle robust 3D\nproxy sampling, we propose a spatial-wise proxy sampling with vertex-based\npoint proxy associations, ensuring robust point-cloud sampling in many scales\nof point cloud. To resolve economical association computation, we introduce\nsparse proxy attention combined with table-based relative bias, which enables\nlow-cost and precise interactions between proxy and point features.\nComprehensive experiments across multiple datasets reveal that our model\nachieves SOTA performance in downstream tasks. The code has been released in\nhttps://github.com/TerenceWallel/Sparse-Proxy-Point-Transformer .\n","authors":["Jiaxu Wan","Hong Zhang","Ziqi He","Qishu Wang","Ding Yuan","Yifan Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11540v1.pdf","comment":"13 pages, 14 figures, 14 tables"},{"id":"http://arxiv.org/abs/2412.11535v1","updated":"2024-12-16T08:13:05Z","published":"2024-12-16T08:13:05Z","title":"Near Large Far Small: Relative Distance Based Partition Learning for\n  UAV-view Geo-Localization","summary":"  UAV-view Geo-Localization (UVGL) presents substantial challenges, primarily\ndue to appearance differences between drone-view and satellite-view. Existing\nmethods develop partition learning strategies aimed at mining more\ncomprehensive information by constructing diverse part-level feature\nrepresentations, which rely on consistent cross-view scales. However,\nvariations of UAV flight state leads to the scale mismatch of cross-views,\nresulting in serious performance degradation of partition-based methods. To\novercome this issue, we propose a partition learning framework based on\nrelative distance, which alleviates the dependence on scale consistency while\nmining fine-grained features. Specifically, we propose a distance guided\ndynamic partition learning strategy (DGDPL), consisting of a square partition\nstrategy and a dynamic-guided adjustment strategy. The former is utilized to\nextract fine-grained features and global features in a simple manner. The\nlatter calculates the relative distance ratio between drone- and satellite-view\nto adjust the partition size, thereby aligning the semantic information between\npartition pairs. Furthermore, we propose a saliency-guided refinement strategy\nto refine part-level features, so as to further improve the retrieval accuracy.\nExtensive experiments show that our approach achieves superior geo-localization\naccuracy across various scale-inconsistent scenarios, and exhibits remarkable\nrobustness against scale variations. The code will be released.\n","authors":["Quan Chen","Tingyu Wang","Rongfeng Lu","Bolun Zheng","Zhedong Zheng","Chenggang Yan"],"pdf_url":"https://arxiv.org/pdf/2412.11535v1.pdf","comment":"In Peer Review"},{"id":"http://arxiv.org/abs/2411.18203v3","updated":"2024-12-16T08:12:17Z","published":"2024-11-27T10:28:57Z","title":"Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning","summary":"  Vision-language models (VLMs) have shown remarkable advancements in\nmultimodal reasoning tasks. However, they still often generate inaccurate or\nirrelevant responses due to issues like hallucinated image understandings or\nunrefined reasoning paths. To address these challenges, we introduce Critic-V,\na novel framework inspired by the Actor-Critic paradigm to boost the reasoning\ncapability of VLMs. This framework decouples the reasoning process and critic\nprocess by integrating two independent components: the Reasoner, which\ngenerates reasoning paths based on visual and textual inputs, and the Critic,\nwhich provides constructive critique to refine these paths. In this approach,\nthe Reasoner generates reasoning responses according to text prompts, which can\nevolve iteratively as a policy based on feedback from the Critic. This\ninteraction process was theoretically driven by a reinforcement learning\nframework where the Critic offers natural language critiques instead of scalar\nrewards, enabling more nuanced feedback to boost the Reasoner's capability on\ncomplex reasoning tasks. The Critic model is trained using Direct Preference\nOptimization (DPO), leveraging a preference dataset of critiques ranked by\nRule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results\nshow that the Critic-V framework significantly outperforms existing methods,\nincluding GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning\naccuracy and efficiency. Combining a dynamic text-based policy for the Reasoner\nand constructive feedback from the preference-optimized Critic enables a more\nreliable and context-sensitive multimodal reasoning process. Our approach\nprovides a promising solution to enhance the reliability of VLMs, improving\ntheir performance in real-world reasoning-heavy multimodal applications such as\nautonomous driving and embodied intelligence.\n","authors":["Di Zhang","Junxian Li","Jingdi Lei","Xunzhi Wang","Yujie Liu","Zonglin Yang","Jiatong Li","Weida Wang","Suorong Yang","Jianbo Wu","Peng Ye","Wanli Ouyang","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.18203v3.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.11530v1","updated":"2024-12-16T08:08:35Z","published":"2024-12-16T08:08:35Z","title":"RoMeO: Robust Metric Visual Odometry","summary":"  Visual odometry (VO) aims to estimate camera poses from visual inputs -- a\nfundamental building block for many applications such as VR/AR and robotics.\nThis work focuses on monocular RGB VO where the input is a monocular RGB video\nwithout IMU or 3D sensors. Existing approaches lack robustness under this\nchallenging scenario and fail to generalize to unseen data (especially\noutdoors); they also cannot recover metric-scale poses. We propose Robust\nMetric Visual Odometry (RoMeO), a novel method that resolves these issues\nleveraging priors from pre-trained depth models. RoMeO incorporates both\nmonocular metric depth and multi-view stereo (MVS) models to recover\nmetric-scale, simplify correspondence search, provide better initialization and\nregularize optimization. Effective strategies are proposed to inject noise\nduring training and adaptively filter noisy depth priors, which ensure the\nrobustness of RoMeO on in-the-wild data. As shown in Fig.1, RoMeO advances the\nstate-of-the-art (SOTA) by a large margin across 6 diverse datasets covering\nboth indoor and outdoor scenes. Compared to the current SOTA DPVO, RoMeO\nreduces the relative (align the trajectory scale with GT) and absolute\ntrajectory errors both by >50%. The performance gain also transfers to the full\nSLAM pipeline (with global BA & loop closure). Code will be released upon\nacceptance.\n","authors":["Junda Cheng","Zhipeng Cai","Zhaoxing Zhang","Wei Yin","Matthias Muller","Michael Paulitsch","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11529v1","updated":"2024-12-16T08:07:53Z","published":"2024-12-16T08:07:53Z","title":"Cross-View Geo-Localization with Street-View and VHR Satellite Imagery\n  in Decentrality Settings","summary":"  Cross-View Geo-Localization tackles the problem of image geo-localization in\nGNSS-denied environments by matching street-view query images with geo-tagged\naerial-view reference images. However, existing datasets and methods often\nassume center-aligned settings or only consider limited decentrality (i.e., the\noffset of the query image from the reference image center). This assumption\noverlooks the challenges present in real-world applications, where large\ndecentrality can significantly enhance localization efficiency but\nsimultaneously lead to a substantial degradation in localization accuracy. To\naddress this limitation, we introduce CVSat, a novel dataset designed to\nevaluate cross-view geo-localization with a large geographic scope and diverse\nlandscapes, emphasizing the decentrality issue. Meanwhile, we propose AuxGeo\n(Auxiliary Enhanced Geo-Localization), which leverages a multi-metric\noptimization strategy with two novel modules: the Bird's-eye view Intermediary\nModule (BIM) and the Position Constraint Module (PCM). BIM uses bird's-eye view\nimages derived from street-view panoramas as an intermediary, simplifying the\ncross-view challenge with decentrality to a cross-view problem and a\ndecentrality problem. PCM leverages position priors between cross-view images\nto establish multi-grained alignment constraints. These modules improve the\nperformance of cross-view geo-localization with the decentrality problem.\nExtensive experiments demonstrate that AuxGeo outperforms previous methods on\nour proposed CVSat dataset, mitigating the issue of large decentrality, and\nalso achieves state-of-the-art performance on existing public datasets such as\nCVUSA, CVACT, and VIGOR.\n","authors":["Panwang Xia","Lei Yu","Yi Wan","Qiong Wu","Peiqi Chen","Liheng Zhong","Yongxiang Yao","Dong Wei","Xinyi Liu","Lixiang Ru","Yingying Zhang","Jiangwei Lao","Jingdong Chen","Ming Yang","Yongjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11525v1","updated":"2024-12-16T08:00:50Z","published":"2024-12-16T08:00:50Z","title":"Sequence Matters: Harnessing Video Models in Super-Resolution","summary":"  3D super-resolution aims to reconstruct high-fidelity 3D models from\nlow-resolution (LR) multi-view images. Early studies primarily focused on\nsingle-image super-resolution (SISR) models to upsample LR images into\nhigh-resolution images. However, these methods often lack view consistency\nbecause they operate independently on each image. Although various\npost-processing techniques have been extensively explored to mitigate these\ninconsistencies, they have yet to fully resolve the issues. In this paper, we\nperform a comprehensive study of 3D super-resolution by leveraging video\nsuper-resolution (VSR) models. By utilizing VSR models, we ensure a higher\ndegree of spatial consistency and can reference surrounding spatial\ninformation, leading to more accurate and detailed reconstructions. Our\nfindings reveal that VSR models can perform remarkably well even on sequences\nthat lack precise spatial alignment. Given this observation, we propose a\nsimple yet practical approach to align LR images without involving fine-tuning\nor generating 'smooth' trajectory from the trained 3D models over LR images.\nThe experimental results show that the surprisingly simple algorithms can\nachieve the state-of-the-art results of 3D super-resolution tasks on standard\nbenchmark datasets, such as the NeRF-synthetic and MipNeRF-360 datasets.\nProject page: https://ko-lani.github.io/Sequence-Matters\n","authors":["Hyun-kyu Ko","Dongheok Park","Youngin Park","Byeonghyeon Lee","Juhee Han","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2412.11525v1.pdf","comment":"Project page: https://ko-lani.github.io/Sequence-Matters"},{"id":"http://arxiv.org/abs/2412.08929v2","updated":"2024-12-16T07:56:34Z","published":"2024-12-12T04:34:28Z","title":"CAPrompt: Cyclic Prompt Aggregation for Pre-Trained Model Based Class\n  Incremental Learning","summary":"  Recently, prompt tuning methods for pre-trained models have demonstrated\npromising performance in Class Incremental Learning (CIL). These methods\ntypically involve learning task-specific prompts and predicting the task ID to\nselect the appropriate prompts for inference. However, inaccurate task ID\npredictions can cause severe inconsistencies between the prompts used during\ntraining and inference, leading to knowledge forgetting and performance\ndegradation. Additionally, existing prompt tuning methods rely solely on the\npre-trained model to predict task IDs, without fully leveraging the knowledge\nembedded in the learned prompt parameters, resulting in inferior prediction\nperformance. To address these issues, we propose a novel Cyclic Prompt\nAggregation (CAPrompt) method that eliminates the dependency on task ID\nprediction by cyclically aggregating the knowledge from different prompts.\nSpecifically, rather than predicting task IDs, we introduce an innovative\nprompt aggregation strategy during both training and inference to overcome\nprompt inconsistency by utilizing a weighted sum of different prompts. Thorough\ntheoretical analysis demonstrates that under concave conditions, the aggregated\nprompt achieves lower error compared to selecting a single task-specific\nprompt. Consequently, we incorporate a concave constraint and a linear\nconstraint to guide prompt learning, ensuring compliance with the concave\ncondition requirement. Furthermore, to fully exploit the prompts and achieve\nmore accurate prompt weights, we develop a cyclic weight prediction strategy.\nThis strategy begins with equal weights for each task and automatically adjusts\nthem to more appropriate values in a cyclical manner. Experiments on various\ndatasets demonstrate that our proposed CAPrompt outperforms state-of-the-art\nmethods by 2%-3%. Our code is available at\nhttps://github.com/zhoujiahuan1991/AAAI2025-CAPrompt.\n","authors":["Qiwei Li","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.08929v2.pdf","comment":"in Proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.11520v1","updated":"2024-12-16T07:56:04Z","published":"2024-12-16T07:56:04Z","title":"EditSplat: Multi-View Fusion and Attention-Guided Optimization for\n  View-Consistent 3D Scene Editing with 3D Gaussian Splatting","summary":"  Recent advancements in 3D editing have highlighted the potential of\ntext-driven methods in real-time, user-friendly AR/VR applications. However,\ncurrent methods rely on 2D diffusion models without adequately considering\nmulti-view information, resulting in multi-view inconsistency. While 3D\nGaussian Splatting (3DGS) significantly improves rendering quality and speed,\nits 3D editing process encounters difficulties with inefficient optimization,\nas pre-trained Gaussians retain excessive source information, hindering\noptimization. To address these limitations, we propose \\textbf{EditSplat}, a\nnovel 3D editing framework that integrates Multi-view Fusion Guidance (MFG) and\nAttention-Guided Trimming (AGT). Our MFG ensures multi-view consistency by\nincorporating essential multi-view information into the diffusion process,\nleveraging classifier-free guidance from the text-to-image diffusion model and\nthe geometric properties of 3DGS. Additionally, our AGT leverages the explicit\nrepresentation of 3DGS to selectively prune and optimize 3D Gaussians,\nenhancing optimization efficiency and enabling precise, semantically rich local\nedits. Through extensive qualitative and quantitative evaluations, EditSplat\nachieves superior multi-view consistency and editing quality over existing\nmethods, significantly enhancing overall efficiency.\n","authors":["Dong In Lee","Hyeongcheol Park","Jiyoung Seo","Eunbyung Park","Hyunje Park","Ha Dam Baek","Shin Sangheon","Sangmin kim","Sangpil Kim"],"pdf_url":"https://arxiv.org/pdf/2412.11520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11519v1","updated":"2024-12-16T07:54:45Z","published":"2024-12-16T07:54:45Z","title":"LineArt: A Knowledge-guided Training-free High-quality Appearance\n  Transfer for Design Drawing with Diffusion Model","summary":"  Image rendering from line drawings is vital in design and image generation\ntechnologies reduce costs, yet professional line drawings demand preserving\ncomplex details. Text prompts struggle with accuracy, and image translation\nstruggles with consistency and fine-grained control. We present LineArt, a\nframework that transfers complex appearance onto detailed design drawings,\nfacilitating design and artistic creation. It generates high-fidelity\nappearance while preserving structural accuracy by simulating hierarchical\nvisual cognition and integrating human artistic experience to guide the\ndiffusion process. LineArt overcomes the limitations of current methods in\nterms of difficulty in fine-grained control and style degradation in design\ndrawings. It requires no precise 3D modeling, physical property specs, or\nnetwork training, making it more convenient for design tasks. LineArt consists\nof two stages: a multi-frequency lines fusion module to supplement the input\ndesign drawing with detailed structural information and a two-part painting\nprocess for Base Layer Shaping and Surface Layer Coloring. We also present a\nnew design drawing dataset ProLines for evaluation. The experiments show that\nLineArt performs better in accuracy, realism, and material precision compared\nto SOTAs.\n","authors":["Xi Wang","Hongzhen Li","Heng Fang","Yichen Peng","Haoran Xie","Xi Yang","Chuntao Li"],"pdf_url":"https://arxiv.org/pdf/2412.11519v1.pdf","comment":"Project Page: https://meaoxixi.github.io/LineArt/"},{"id":"http://arxiv.org/abs/2412.09920v2","updated":"2024-12-16T07:50:46Z","published":"2024-12-13T07:15:52Z","title":"Precision-Enhanced Human-Object Contact Detection via Depth-Aware\n  Perspective Interaction and Object Texture Restoration","summary":"  Human-object contact (HOT) is designed to accurately identify the areas where\nhumans and objects come into contact. Current methods frequently fail to\naccount for scenarios where objects are frequently blocking the view, resulting\nin inaccurate identification of contact areas. To tackle this problem, we\nsuggest using a perspective interaction HOT detector called PIHOT, which\nutilizes a depth map generation model to offer depth information of humans and\nobjects related to the camera, thereby preventing false interaction detection.\nFurthermore, we use mask dilatation and object restoration techniques to\nrestore the texture details in covered areas, improve the boundaries between\nobjects, and enhance the perception of humans interacting with objects.\nMoreover, a spatial awareness perception is intended to concentrate on the\ncharacteristic features close to the points of contact. The experimental\nresults show that the PIHOT algorithm achieves state-of-the-art performance on\nthree benchmark datasets for HOT detection tasks. Compared to the most recent\nDHOT, our method enjoys an average improvement of 13%, 27.5%, 16%, and 18.5% on\nSC-Acc., C-Acc., mIoU, and wIoU metrics, respectively.\n","authors":["Yuxiao Wang","Wenpeng Neng","Zhenao Wei","Yu Lei","Weiying Xue","Nan Zhuang","Yanwu Xu","Xinyu Jiang","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09920v2.pdf","comment":"Accepted by AAAl 2025"},{"id":"http://arxiv.org/abs/2412.11513v1","updated":"2024-12-16T07:48:30Z","published":"2024-12-16T07:48:30Z","title":"IGR: Improving Diffusion Model for Garment Restoration from Person Image","summary":"  Garment restoration, the inverse of virtual try-on task, focuses on restoring\nstandard garment from a person image, requiring accurate capture of garment\ndetails. However, existing methods often fail to preserve the identity of the\ngarment or rely on complex processes. To address these limitations, we propose\nan improved diffusion model for restoring authentic garments. Our approach\nemploys two garment extractors to independently capture low-level features and\nhigh-level semantics from the person image. Leveraging a pretrained latent\ndiffusion model, these features are integrated into the denoising process\nthrough garment fusion blocks, which combine self-attention and cross-attention\nlayers to align the restored garment with the person image. Furthermore, a\ncoarse-to-fine training strategy is introduced to enhance the fidelity and\nauthenticity of the generated garments. Experimental results demonstrate that\nour model effectively preserves garment identity and generates high-quality\nrestorations, even in challenging scenarios such as complex garments or those\nwith occlusions.\n","authors":["Le Shen","Rong Huang","Zhijie Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11512v1","updated":"2024-12-16T07:42:49Z","published":"2024-12-16T07:42:49Z","title":"SpatialMe: Stereo Video Conversion Using Depth-Warping and\n  Blend-Inpainting","summary":"  Stereo video conversion aims to transform monocular videos into immersive\nstereo format. Despite the advancements in novel view synthesis, it still\nremains two major challenges: i) difficulty of achieving high-fidelity and\nstable results, and ii) insufficiency of high-quality stereo video data. In\nthis paper, we introduce SpatialMe, a novel stereo video conversion framework\nbased on depth-warping and blend-inpainting. Specifically, we propose a\nmask-based hierarchy feature update (MHFU) refiner, which integrate and refine\nthe outputs from designed multi-branch inpainting module, using feature update\nunit (FUU) and mask mechanism. We also propose a disparity expansion strategy\nto address the problem of foreground bleeding. Furthermore, we conduct a\nhigh-quality real-world stereo video dataset -- StereoV1K, to alleviate the\ndata shortage. It contains 1000 stereo videos captured in real-world at a\nresolution of 1180 x 1180, covering various indoor and outdoor scenes.\nExtensive experiments demonstrate the superiority of our approach in generating\nstereo videos over state-of-the-art methods.\n","authors":["Jiale Zhang","Qianxi Jia","Yang Liu","Wei Zhang","Wei Wei","Xin Tian"],"pdf_url":"https://arxiv.org/pdf/2412.11512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11509v1","updated":"2024-12-16T07:33:23Z","published":"2024-12-16T07:33:23Z","title":"Skip Tuning: Pre-trained Vision-Language Models are Effective and\n  Efficient Adapters Themselves","summary":"  Prompt tuning (PT) has long been recognized as an effective and efficient\nparadigm for transferring large pre-trained vision-language models (VLMs) to\ndownstream tasks by learning a tiny set of context vectors. Nevertheless, in\nthis work, we reveal that freezing the parameters of VLMs during learning the\ncontext vectors neither facilitates the transferability of pre-trained\nknowledge nor improves the memory and time efficiency significantly. Upon\nfurther investigation, we find that reducing both the length and width of the\nfeature-gradient propagation flows of the full fine-tuning (FT) baseline is key\nto achieving effective and efficient knowledge transfer. Motivated by this, we\npropose Skip Tuning, a novel paradigm for adapting VLMs to downstream tasks.\nUnlike existing PT or adapter-based methods, Skip Tuning applies Layer-wise\nSkipping (LSkip) and Class-wise Skipping (CSkip) upon the FT baseline without\nintroducing extra context vectors or adapter modules. Extensive experiments\nacross a wide spectrum of benchmarks demonstrate the superior effectiveness and\nefficiency of our Skip Tuning over both PT and adapter-based methods. Code:\nhttps://github.com/Koorye/SkipTuning.\n","authors":["Shihan Wu","Ji Zhang","Pengpeng Zeng","Lianli Gao","Jingkuan Song","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2412.11509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06367v3","updated":"2024-12-16T07:32:01Z","published":"2024-06-10T15:26:48Z","title":"MVGamba: Unify 3D Content Generation as State Space Sequence Modeling","summary":"  Recent 3D large reconstruction models (LRMs) can generate high-quality 3D\ncontent in sub-seconds by integrating multi-view diffusion models with scalable\nmulti-view reconstructors. Current works further leverage 3D Gaussian Splatting\nas 3D representation for improved visual quality and rendering efficiency.\nHowever, we observe that existing Gaussian reconstruction models often suffer\nfrom multi-view inconsistency and blurred textures. We attribute this to the\ncompromise of multi-view information propagation in favor of adopting powerful\nyet computationally intensive architectures (e.g., Transformers). To address\nthis issue, we introduce MVGamba, a general and lightweight Gaussian\nreconstruction model featuring a multi-view Gaussian reconstructor based on the\nRNN-like State Space Model (SSM). Our Gaussian reconstructor propagates causal\ncontext containing multi-view information for cross-view self-refinement while\ngenerating a long sequence of Gaussians for fine-detail modeling with linear\ncomplexity. With off-the-shelf multi-view diffusion models integrated, MVGamba\nunifies 3D generation tasks from a single image, sparse images, or text\nprompts. Extensive experiments demonstrate that MVGamba outperforms\nstate-of-the-art baselines in all 3D content generation scenarios with\napproximately only $0.1\\times$ of the model size.\n","authors":["Xuanyu Yi","Zike Wu","Qiuhong Shen","Qingshan Xu","Pan Zhou","Joo-Hwee Lim","Shuicheng Yan","Xinchao Wang","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.06367v3.pdf","comment":"Accepted by NeurIPS 2024. Code is included in\n  https://github.com/SkyworkAI/MVGamba"},{"id":"http://arxiv.org/abs/2412.11495v1","updated":"2024-12-16T07:15:13Z","published":"2024-12-16T07:15:13Z","title":"Exploring More from Multiple Gait Modalities for Human Identification","summary":"  The gait, as a kind of soft biometric characteristic, can reflect the\ndistinct walking patterns of individuals at a distance, exhibiting a promising\ntechnique for unrestrained human identification. With largely excluding\ngait-unrelated cues hidden in RGB videos, the silhouette and skeleton, though\nvisually compact, have acted as two of the most prevailing gait modalities for\na long time. Recently, several attempts have been made to introduce more\ninformative data forms like human parsing and optical flow images to capture\ngait characteristics, along with multi-branch architectures. However, due to\nthe inconsistency within model designs and experiment settings, we argue that a\ncomprehensive and fair comparative study among these popular gait modalities,\ninvolving the representational capacity and fusion strategy exploration, is\nstill lacking. From the perspectives of fine vs. coarse-grained shape and whole\nvs. pixel-wise motion modeling, this work presents an in-depth investigation of\nthree popular gait representations, i.e., silhouette, human parsing, and\noptical flow, with various fusion evaluations, and experimentally exposes their\nsimilarities and differences. Based on the obtained insights, we further\ndevelop a C$^2$Fusion strategy, consequently building our new framework\nMultiGait++. C$^2$Fusion preserves commonalities while highlighting differences\nto enrich the learning of gait features. To verify our findings and\nconclusions, extensive experiments on Gait3D, GREW, CCPG, and SUSTech1K are\nconducted. The code is available at https://github.com/ShiqiYu/OpenGait.\n","authors":["Dongyang Jin","Chao Fan","Weihua Chen","Shiqi Yu"],"pdf_url":"https://arxiv.org/pdf/2412.11495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11489v1","updated":"2024-12-16T07:06:17Z","published":"2024-12-16T07:06:17Z","title":"HGSFusion: Radar-Camera Fusion with Hybrid Generation and\n  Synchronization for 3D Object Detection","summary":"  Millimeter-wave radar plays a vital role in 3D object detection for\nautonomous driving due to its all-weather and all-lighting-condition\ncapabilities for perception. However, radar point clouds suffer from pronounced\nsparsity and unavoidable angle estimation errors. To address these limitations,\nincorporating a camera may partially help mitigate the shortcomings.\nNevertheless, the direct fusion of radar and camera data can lead to negative\nor even opposite effects due to the lack of depth information in images and\nlow-quality image features under adverse lighting conditions. Hence, in this\npaper, we present the radar-camera fusion network with Hybrid Generation and\nSynchronization (HGSFusion), designed to better fuse radar potentials and image\nfeatures for 3D object detection. Specifically, we propose the Radar Hybrid\nGeneration Module (RHGM), which fully considers the Direction-Of-Arrival (DOA)\nestimation errors in radar signal processing. This module generates denser\nradar points through different Probability Density Functions (PDFs) with the\nassistance of semantic information. Meanwhile, we introduce the Dual Sync\nModule (DSM), comprising spatial sync and modality sync, to enhance image\nfeatures with radar positional information and facilitate the fusion of\ndistinct characteristics in different modalities. Extensive experiments\ndemonstrate the effectiveness of our approach, outperforming the\nstate-of-the-art methods in the VoD and TJ4DRadSet datasets by $6.53\\%$ and\n$2.03\\%$ in RoI AP and BEV AP, respectively. The code is available at\nhttps://github.com/garfield-cpp/HGSFusion.\n","authors":["Zijian Gu","Jianwei Ma","Yan Huang","Honghao Wei","Zhanye Chen","Hui Zhang","Wei Hong"],"pdf_url":"https://arxiv.org/pdf/2412.11489v1.pdf","comment":"12 pages, 8 figures, 7 tables. Accepted by AAAI 2025 , the 39th\n  Annual AAAI Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2408.07343v3","updated":"2024-12-16T07:06:15Z","published":"2024-08-14T07:37:07Z","title":"Gradient Alignment Improves Test-Time Adaptation for Medical Image\n  Segmentation","summary":"  Although recent years have witnessed significant advancements in medical\nimage segmentation, the pervasive issue of domain shift among medical images\nfrom diverse centres hinders the effective deployment of pre-trained models.\nMany Test-time Adaptation (TTA) methods have been proposed to address this\nissue by fine-tuning pre-trained models with test data during inference. These\nmethods, however, often suffer from less-satisfactory optimization due to\nsuboptimal optimization direction (dictated by the gradient) and fixed\nstep-size (predicated on the learning rate). In this paper, we propose the\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\nthe gradient direction and learning rate in the optimization procedure. Unlike\nconventional TTA methods, which primarily optimize the pseudo gradient derived\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\nenables the model to excavate the similarities between different gradients and\ncorrect the gradient direction to approximate the empirical gradient related to\nthe current segmentation task. Additionally, we design a dynamic learning rate\nbased on the cosine similarity between the pseudo and auxiliary gradients,\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\ntest data. Extensive experiments establish the effectiveness of the proposed\ngradient alignment and dynamic learning rate and substantiate the superiority\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\nmedical image segmentation task. The code and weights of pre-trained source\nmodels are available at https://github.com/Chen-Ziyang/GraTa.\n","authors":["Ziyang Chen","Yiwen Ye","Yongsheng Pan","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2408.07343v3.pdf","comment":"9 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.18844v4","updated":"2024-12-16T06:59:33Z","published":"2024-06-27T02:31:03Z","title":"Revisiting Backdoor Attacks against Large Vision-Language Models from\n  Domain Shift","summary":"  Instruction tuning enhances large vision-language models (LVLMs) but\nincreases their vulnerability to backdoor attacks due to their open design.\nUnlike prior studies in static settings, this paper explores backdoor attacks\nin LVLM instruction tuning across mismatched training and testing domains. We\nintroduce a new evaluation dimension, backdoor domain generalization, to assess\nattack robustness under visual and text domain shifts. Our findings reveal two\ninsights: (1) backdoor generalizability improves when distinctive trigger\npatterns are independent of specific data domains or model architectures, and\n(2) the competitive interaction between trigger patterns and clean semantic\nregions, where guiding the model to predict triggers enhances attack\ngeneralizability. Based on these insights, we propose a multimodal attribution\nbackdoor attack (MABA) that injects domain-agnostic triggers into critical\nareas using attributional interpretation. Experiments with OpenFlamingo,\nBlip-2, and Otter show that MABA significantly boosts the attack success rate\nof generalization by 36.4%, achieving a 97% success rate at a 0.2% poisoning\nrate. This study reveals limitations in current evaluations and highlights how\nenhanced backdoor generalizability poses a security threat to LVLMs, even\nwithout test data access.\n","authors":["Siyuan Liang","Jiawei Liang","Tianyu Pang","Chao Du","Aishan Liu","Mingli Zhu","Xiaochun Cao","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2406.18844v4.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.11484v1","updated":"2024-12-16T06:53:00Z","published":"2024-12-16T06:53:00Z","title":"Efficient Policy Adaptation with Contrastive Prompt Ensemble for\n  Embodied Agents","summary":"  For embodied reinforcement learning (RL) agents interacting with the\nenvironment, it is desirable to have rapid policy adaptation to unseen visual\nobservations, but achieving zero-shot adaptation capability is considered as a\nchallenging problem in the RL context. To address the problem, we present a\nnovel contrastive prompt ensemble (ConPE) framework which utilizes a pretrained\nvision-language model and a set of visual prompts, thus enabling efficient\npolicy learning and adaptation upon a wide range of environmental and physical\nchanges encountered by embodied agents. Specifically, we devise a\nguided-attention-based ensemble approach with multiple visual prompts on the\nvision-language model to construct robust state representations. Each prompt is\ncontrastively learned in terms of an individual domain factor that\nsignificantly affects the agent's egocentric perception and observation. For a\ngiven task, the attention-based ensemble and policy are jointly learned so that\nthe resulting state representations not only generalize to various domains but\nare also optimized for learning the task. Through experiments, we show that\nConPE outperforms other state-of-the-art algorithms for several embodied agent\ntasks including navigation in AI2THOR, manipulation in egocentric-Metaworld,\nand autonomous driving in CARLA, while also improving the sample efficiency of\npolicy learning and adaptation.\n","authors":["Wonje Choi","Woo Kyung Kim","SeungHyun Kim","Honguk Woo"],"pdf_url":"https://arxiv.org/pdf/2412.11484v1.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2412.11480v1","updated":"2024-12-16T06:48:30Z","published":"2024-12-16T06:48:30Z","title":"Data-driven Precipitation Nowcasting Using Satellite Imagery","summary":"  Accurate precipitation forecasting is crucial for early warnings of\ndisasters, such as floods and landslides. Traditional forecasts rely on\nground-based radar systems, which are space-constrained and have high\nmaintenance costs. Consequently, most developing countries depend on a global\nnumerical model with low resolution, instead of operating their own radar\nsystems. To mitigate this gap, we propose the Neural Precipitation Model (NPM),\nwhich uses global-scale geostationary satellite imagery. NPM predicts\nprecipitation for up to six hours, with an update every hour. We take three key\nchannels to discriminate rain clouds as input: infrared radiation (at a\nwavelength of 10.5 $\\mu m$), upper- (6.3 $\\mu m$), and lower- (7.3 $\\mu m$)\nlevel water vapor channels. Additionally, NPM introduces positional encoders to\ncapture seasonal and temporal patterns, accounting for variations in\nprecipitation. Our experimental results demonstrate that NPM can predict\nrainfall in real-time with a resolution of 2 km. The code and dataset are\navailable at\nhttps://github.com/seominseok0429/Data-driven-Precipitation-Nowcasting-Using-Satellite-Imagery.\n","authors":["Young-Jae Park","Doyi Kim","Minseok Seo","Hae-Gon Jeon","Yeji Choi"],"pdf_url":"https://arxiv.org/pdf/2412.11480v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.11475v1","updated":"2024-12-16T06:38:00Z","published":"2024-12-16T06:38:00Z","title":"OmniVLM: A Token-Compressed, Sub-Billion-Parameter Vision-Language Model\n  for Efficient On-Device Inference","summary":"  We present OmniVLM, a sub-billion-parameter vision-language model for\nefficient on-device inference. OmniVLM introduces a token compression mechanism\nthat reduces visual token sequence length from 729 to 81 tokens, significantly\nreducing computational overhead while preserving visual-semantic fidelity.\nThrough a multi-stage training pipeline of pretraining, supervised fine-tuning,\nand minimal-edit Direct Preference Optimization (DPO), OmniVLM matches the\nperformance of larger models. On multiple benchmarks including ScienceQA, POPE,\nand MMMU, OmniVLM outperforms existing baselines like nanoLLAVA within a\n968M-parameter footprint. Empirical results on the same laptop demonstrate 9.1x\nfaster time-to-first-token (0.75s vs 6.82s) and 1.5x higher decoding speed\n(29.41 vs 19.20 tokens/s) compared to nanoLLAVA, enabling efficient deployment\non edge devices. The model weights can be accessed on huggingface:\n\\url{https://huggingface.co/NexaAIDev/OmniVLM-968M}, and the inference examples\ncan be find in Appendix B.\n","authors":["Wei Chen","Zhiyuan Li","Shuo Xin"],"pdf_url":"https://arxiv.org/pdf/2412.11475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12730v3","updated":"2024-12-16T06:16:27Z","published":"2024-07-17T16:49:34Z","title":"RoDE: Linear Rectified Mixture of Diverse Experts for Food Large\n  Multi-Modal Models","summary":"  Large Multi-modal Models (LMMs) have significantly advanced a variety of\nvision-language tasks. The scalability and availability of high-quality\ntraining data play a pivotal role in the success of LMMs. In the realm of food,\nwhile comprehensive food datasets such as Recipe1M offer an abundance of\ningredient and recipe information, they often fall short of providing ample\ndata for nutritional analysis. The Recipe1M+ dataset, despite offering a subset\nfor nutritional evaluation, is limited in the scale and accuracy of nutrition\ninformation. To bridge this gap, we introduce Uni-Food, a unified food dataset\nthat comprises over 100,000 images with various food labels, including\ncategories, ingredients, recipes, and ingredient-level nutritional information.\nUni-Food is designed to provide a more holistic approach to food data analysis,\nthereby enhancing the performance and capabilities of LMMs in this domain. To\nmitigate the conflicts arising from multi-task supervision during fine-tuning\nof LMMs, we introduce a novel Linear Rectification Mixture of Diverse Experts\n(RoDE) approach. RoDE utilizes a diverse array of experts to address tasks of\nvarying complexity, thereby facilitating the coordination of trainable\nparameters, i.e., it allocates more parameters for more complex tasks and,\nconversely, fewer parameters for simpler tasks. RoDE implements linear\nrectification union to refine the router's functionality, thereby enhancing the\nefficiency of sparse task allocation. These design choices endow RoDE with\nfeatures that ensure GPU memory efficiency and ease of optimization. Our\nexperimental results validate the effectiveness of our proposed approach in\naddressing the inherent challenges of food-related multitasking.\n","authors":["Pengkun Jiao","Xinlan Wu","Bin Zhu","Jingjing Chen","Chong-Wah Ngo","Yugang Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.12730v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16387v4","updated":"2024-12-16T06:09:40Z","published":"2023-10-25T05:59:25Z","title":"Frequency-Aware Transformer for Learned Image Compression","summary":"  Learned image compression (LIC) has gained traction as an effective solution\nfor image storage and transmission in recent years. However, existing LIC\nmethods are redundant in latent representation due to limitations in capturing\nanisotropic frequency components and preserving directional details. To\novercome these challenges, we propose a novel frequency-aware transformer (FAT)\nblock that for the first time achieves multiscale directional ananlysis for\nLIC. The FAT block comprises frequency-decomposition window attention (FDWA)\nmodules to capture multiscale and directional frequency components of natural\nimages. Additionally, we introduce frequency-modulation feed-forward network\n(FMFFN) to adaptively modulate different frequency components, improving\nrate-distortion performance. Furthermore, we present a transformer-based\nchannel-wise autoregressive (T-CA) model that effectively exploits channel\ndependencies. Experiments show that our method achieves state-of-the-art\nrate-distortion performance compared to existing LIC methods, and evidently\noutperforms latest standardized codec VTM-12.1 by 14.5%, 15.1%, 13.0% in\nBD-rate on the Kodak, Tecnick, and CLIC datasets.\n","authors":["Han Li","Shaohui Li","Wenrui Dai","Chenglin Li","Junni Zou","Hongkai Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.16387v4.pdf","comment":"ICLR2024 poster"},{"id":"http://arxiv.org/abs/2312.03289v3","updated":"2024-12-16T06:09:01Z","published":"2023-12-06T04:38:02Z","title":"Enhancing Robustness in Incremental Learning with Adversarial Training","summary":"  Adversarial training is one of the most effective approaches against\nadversarial attacks. However, adversarial training has primarily been studied\nin scenarios where data for all classes is provided, with limited research\nconducted in the context of incremental learning where knowledge is introduced\nsequentially. In this study, we investigate Adversarially Robust Class\nIncremental Learning (ARCIL), which deals with adversarial robustness in\nincremental learning. We first explore a series of baselines that integrate\nincremental learning with existing adversarial training methods, finding that\nthey lead to conflicts between acquiring new knowledge and retaining past\nknowledge. Furthermore, we discover that training new knowledge causes the\ndisappearance of a key characteristic in robust models: a flat loss landscape\nin input space. To address such issues, we propose a novel and robust baseline\nfor ARCIL, named \\textbf{FL}atness-preserving \\textbf{A}dversarial\n\\textbf{I}ncremental learning for \\textbf{R}obustness (\\textbf{FLAIR}).\nExperimental results demonstrate that FLAIR significantly outperforms other\nbaselines. To the best of our knowledge, we are the first to comprehensively\ninvestigate the baselines, challenges, and solutions for ARCIL, which we\nbelieve represents a significant advance toward achieving real-world\nrobustness. Codes are available at \\url{https://github.com/HongsinLee/FLAIR}.\n","authors":["Seungju Cho","Hongsin Lee","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2312.03289v3.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11468v1","updated":"2024-12-16T06:03:56Z","published":"2024-12-16T06:03:56Z","title":"Block-Based Multi-Scale Image Rescaling","summary":"  Image rescaling (IR) seeks to determine the optimal low-resolution (LR)\nrepresentation of a high-resolution (HR) image to reconstruct a high-quality\nsuper-resolution (SR) image. Typically, HR images with resolutions exceeding 2K\npossess rich information that is unevenly distributed across the image.\nTraditional image rescaling methods often fall short because they focus solely\non the overall scaling rate, ignoring the varying amounts of information in\ndifferent parts of the image. To address this limitation, we propose a\nBlock-Based Multi-Scale Image Rescaling Framework (BBMR), tailored for IR tasks\ninvolving HR images of 2K resolution and higher. BBMR consists of two main\ncomponents: the Downscaling Module and the Upscaling Module. In the Downscaling\nModule, the HR image is segmented into sub-blocks of equal size, with each\nsub-block receiving a dynamically allocated scaling rate while maintaining a\nconstant overall scaling rate. For the Upscaling Module, we introduce the Joint\nSuper-Resolution method (JointSR), which performs SR on these sub-blocks with\nvarying scaling rates and effectively eliminates blocking artifacts.\nExperimental results demonstrate that BBMR significantly enhances the SR image\nquality on the of 2K and 4K test dataset compared to initial network image\nrescaling methods.\n","authors":["Jian Li","Siwang Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.11468v1.pdf","comment":"This paper has been accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.06499v2","updated":"2024-12-16T06:00:52Z","published":"2024-12-09T13:58:00Z","title":"HYATT-Net is Grand: A Hybrid Attention Network for Performant Anatomical\n  Landmark Detection","summary":"  Anatomical landmark detection (ALD) from a medical image is crucial for a\nwide array of clinical applications. While existing methods achieve quite some\nsuccess in ALD, they often struggle to balance global context with\ncomputational efficiency, particularly with high-resolution images, thereby\nleading to the rise of a natural question: where is the performance limit of\nALD? In this paper, we aim to forge performant ALD by proposing a {\\bf HY}brid\n{\\bf ATT}ention {\\bf Net}work (HYATT-Net) with the following designs: (i) A\nnovel hybrid architecture that integrates CNNs and Transformers. Its core is\nthe BiFormer module, utilizing Bi-Level Routing Attention for efficient\nattention to relevant image regions. This, combined with Attention Residual\nModule(ARM), enables precise local feature refinement guided by the global\ncontext. (ii) A Feature Fusion Correction Module that aggregates multi-scale\nfeatures and thus mitigates a resolution loss. Deep supervision with a\nmean-square error loss on multi-resolution heatmaps optimizes the model.\nExperiments on five diverse datasets demonstrate state-of-the-art performance,\nsurpassing existing methods in accuracy, robustness, and efficiency. The\nHYATT-Net provides a promising solution for accurate and efficient ALD in\ncomplex medical images. Our codes and data are already released at:\n\\url{https://github.com/ECNUACRush/HYATT-Net}.\n","authors":["Xiaoqian Zhou","Zhen Huang","Heqin Zhu","Qingsong Yao","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.06499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16252v3","updated":"2024-12-16T05:53:28Z","published":"2024-07-23T07:40:41Z","title":"LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese\n  Legal Consultation","summary":"  Legal Large Language Models (LLMs) have shown promise in providing legal\nconsultations to non-experts. However, most existing Chinese legal consultation\nmodels are based on single-agent systems, which differ from real-world legal\nconsultations, where multiple professionals collaborate to offer more tailored\nresponses. To better simulate real consultations, we propose LawLuo, a\nmulti-agent framework for multi-turn Chinese legal consultations. LawLuo\nincludes four agents: the receptionist agent, which assesses user intent and\nselects a lawyer agent; the lawyer agent, which interacts with the user; the\nsecretary agent, which organizes conversation records and generates\nconsultation reports; and the boss agent, which evaluates the performance of\nthe lawyer and secretary agents to ensure optimal results. These agents'\ninteractions mimic the operations of real law firms. To train them to follow\ndifferent legal instructions, we developed distinct fine-tuning datasets. We\nalso introduce a case graph-based RAG to help the lawyer agent address vague\nuser inputs. Experimental results show that LawLuo outperforms baselines in\ngenerating more personalized and professional responses, handling ambiguous\nqueries, and following legal instructions in multi-turn conversations. Our full\ncode and constructed datasets will be open-sourced upon paper acceptance.\n","authors":["Jingyun Sun","Chengxiao Dai","Zhongze Luo","Yangbo Chang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2407.16252v3.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2412.09177v2","updated":"2024-12-16T05:51:27Z","published":"2024-12-12T11:09:56Z","title":"Weighted Poisson-disk Resampling on Large-Scale Point Clouds","summary":"  For large-scale point cloud processing, resampling takes the important role\nof controlling the point number and density while keeping the geometric\nconsistency. % in related tasks. However, current methods cannot balance such\ndifferent requirements. Particularly with large-scale point clouds, classical\nmethods often struggle with decreased efficiency and accuracy. To address such\nissues, we propose a weighted Poisson-disk (WPD) resampling method to improve\nthe usability and efficiency for the processing. We first design an initial\nPoisson resampling with a voxel-based estimation strategy. It is able to\nestimate a more accurate radius of the Poisson-disk while maintaining high\nefficiency. Then, we design a weighted tangent smoothing step to further\noptimize the Voronoi diagram for each point. At the same time, sharp features\nare detected and kept in the optimized results with isotropic property.\nFinally, we achieve a resampling copy from the original point cloud with the\nspecified point number, uniform density, and high-quality geometric\nconsistency. Experiments show that our method significantly improves the\nperformance of large-scale point cloud resampling for different applications,\nand provides a highly practical solution.\n","authors":["Xianhe Jiao","Chenlei Lv","Junli Zhao","Ran Yi","Yu-Hui Wen","Zhenkuan Pan","Zhongke Wu","Yong-jin Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09177v2.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11467v1","updated":"2024-12-16T05:48:44Z","published":"2024-12-16T05:48:44Z","title":"Exploring Temporal Event Cues for Dense Video Captioning in Cyclic\n  Co-learning","summary":"  Dense video captioning aims to detect and describe all events in untrimmed\nvideos. This paper presents a dense video captioning network called\nMulti-Concept Cyclic Learning (MCCL), which aims to: (1) detect multiple\nconcepts at the frame level, using these concepts to enhance video features and\nprovide temporal event cues; and (2) design cyclic co-learning between the\ngenerator and the localizer within the captioning network to promote semantic\nperception and event localization. Specifically, we perform weakly supervised\nconcept detection for each frame, and the detected concept embeddings are\nintegrated into the video features to provide event cues. Additionally,\nvideo-level concept contrastive learning is introduced to obtain more\ndiscriminative concept embeddings. In the captioning network, we establish a\ncyclic co-learning strategy where the generator guides the localizer for event\nlocalization through semantic matching, while the localizer enhances the\ngenerator's event semantic perception through location matching, making\nsemantic perception and event localization mutually beneficial. MCCL achieves\nstate-of-the-art performance on the ActivityNet Captions and YouCook2 datasets.\nExtensive experiments demonstrate its effectiveness and interpretability.\n","authors":["Zhuyang Xie","Yan Yang","Yankai Yu","Jie Wang","Yongquan Jiang","Xiao Wu"],"pdf_url":"https://arxiv.org/pdf/2412.11467v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11464v1","updated":"2024-12-16T05:44:45Z","published":"2024-12-16T05:44:45Z","title":"MaskCLIP++: A Mask-Based CLIP Fine-tuning Framework for Open-Vocabulary\n  Image Segmentation","summary":"  Open-vocabulary image segmentation has been advanced through the synergy\nbetween mask generators and vision-language models like Contrastive\nLanguage-Image Pre-training (CLIP). Previous approaches focus on generating\nmasks while aligning mask features with text embeddings during training. In\nthis paper, we observe that relying on generated low-quality masks can weaken\nthe alignment of vision and language in regional representations. This\nmotivates us to present a new fine-tuning framework, named MaskCLIP++, which\nuses ground-truth masks instead of generated masks to enhance the mask\nclassification capability of CLIP. Due to the limited diversity of image\nsegmentation datasets with mask annotations, we propose incorporating a\nconsistency alignment constraint during fine-tuning, which alleviates\ncategorical bias toward the fine-tuning dataset. After low-cost fine-tuning,\ncombining with the mask generator in previous state-of-the-art mask-based open\nvocabulary segmentation methods, we achieve performance improvements of +1.7,\n+2.3, +2.1, +3.1, and +0.3 mIoU on the A-847, PC-459, A-150, PC-59, and PAS-20\ndatasets, respectively.\n","authors":["Quan-Sheng Zeng","Yunheng Li","Daquan Zhou","Guanbin Li","Qibin Hou","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2412.11464v1.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.11463v1","updated":"2024-12-16T05:43:14Z","published":"2024-12-16T05:43:14Z","title":"FedCAR: Cross-client Adaptive Re-weighting for Generative Models in\n  Federated Learning","summary":"  Generative models trained on multi-institutional datasets can provide an\nenriched understanding through diverse data distributions. However, training\nthe models on medical images is often challenging due to hospitals' reluctance\nto share data for privacy reasons. Federated learning(FL) has emerged as a\nprivacy-preserving solution for training distributed datasets across data\ncenters by aggregating model weights from multiple clients instead of sharing\nraw data. Previous research has explored the adaptation of FL to generative\nmodels, yet effective aggregation algorithms specifically tailored for\ngenerative models remain unexplored. We hereby propose a novel algorithm aimed\nat improving the performance of generative models within FL. Our approach\nadaptively re-weights the contribution of each client, resulting in\nwell-trained shared parameters. In each round, the server side measures the\ndistribution distance between fake images generated by clients instead of\ndirectly comparing the Fr\\'echet Inception Distance per client, thereby\nenhancing efficiency of the learning. Experimental results on three public\nchest X-ray datasets show superior performance in medical image generation,\noutperforming both centralized learning and conventional FL algorithms. Our\ncode is available at https://github.com/danny0628/FedCAR.\n","authors":["Minjun Kim","Minjee Kim","Jinhoon Jeong"],"pdf_url":"https://arxiv.org/pdf/2412.11463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11458v1","updated":"2024-12-16T05:32:28Z","published":"2024-12-16T05:32:28Z","title":"HResFormer: Hybrid Residual Transformer for Volumetric Medical Image\n  Segmentation","summary":"  Vision Transformer shows great superiority in medical image segmentation due\nto the ability in learning long-range dependency. For medical image\nsegmentation from 3D data, such as computed tomography (CT), existing methods\ncan be broadly classified into 2D-based and 3D-based methods. One key\nlimitation in 2D-based methods is that the intra-slice information is ignored,\nwhile the limitation in 3D-based methods is the high computation cost and\nmemory consumption, resulting in a limited feature representation for\ninner-slice information. During the clinical examination, radiologists\nprimarily use the axial plane and then routinely review both axial and coronal\nplanes to form a 3D understanding of anatomy. Motivated by this fact, our key\ninsight is to design a hybrid model which can first learn fine-grained\ninner-slice information and then generate a 3D understanding of anatomy by\nincorporating 3D information. We present a novel \\textbf{H}ybrid\n\\textbf{Res}idual trans\\textbf{Former} \\textbf{(HResFormer)} for 3D medical\nimage segmentation. Building upon standard 2D and 3D Transformer backbones,\nHResFormer involves two novel key designs: \\textbf{(1)} a \\textbf{H}ybrid\n\\textbf{L}ocal-\\textbf{G}lobal fusion \\textbf{M}odule \\textbf{(HLGM)} to\neffectively and adaptively fuse inner-slice information from 2D Transformer and\nintra-slice information from 3D volumes for 3D Transformer with local\nfine-grained and global long-range representation. \\textbf{(2)} a residual\nlearning of the hybrid model, which can effectively leverage the inner-slice\nand intra-slice information for better 3D understanding of anatomy. Experiments\nshow that our HResFormer outperforms prior art on widely-used medical image\nsegmentation benchmarks. This paper sheds light on an important but neglected\nway to design Transformers for 3D medical image segmentation.\n","authors":["Sucheng Ren","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2412.11458v1.pdf","comment":"Accepted by TNNLS"},{"id":"http://arxiv.org/abs/2412.11457v1","updated":"2024-12-16T05:23:45Z","published":"2024-12-16T05:23:45Z","title":"MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes","summary":"  Repurposing pre-trained diffusion models has been proven to be effective for\nNVS. However, these methods are mostly limited to a single object; directly\napplying such methods to compositional multi-object scenarios yields inferior\nresults, especially incorrect object placement and inconsistent shape and\nappearance under novel views. How to enhance and systematically evaluate the\ncross-view consistency of such models remains under-explored. To address this\nissue, we propose MOVIS to enhance the structural awareness of the\nview-conditioned diffusion model for multi-object NVS in terms of model inputs,\nauxiliary tasks, and training strategy. First, we inject structure-aware\nfeatures, including depth and object mask, into the denoising U-Net to enhance\nthe model's comprehension of object instances and their spatial relationships.\nSecond, we introduce an auxiliary task requiring the model to simultaneously\npredict novel view object masks, further improving the model's capability in\ndifferentiating and placing objects. Finally, we conduct an in-depth analysis\nof the diffusion sampling process and carefully devise a structure-guided\ntimestep sampling scheduler during training, which balances the learning of\nglobal object placement and fine-grained detail recovery. To systematically\nevaluate the plausibility of synthesized images, we propose to assess\ncross-view consistency and novel view object placement alongside existing\nimage-level NVS metrics. Extensive experiments on challenging synthetic and\nrealistic datasets demonstrate that our method exhibits strong generalization\ncapabilities and produces consistent novel view synthesis, highlighting its\npotential to guide future 3D-aware multi-object NVS tasks.\n","authors":["Ruijie Lu","Yixin Chen","Junfeng Ni","Baoxiong Jia","Yu Liu","Diwen Wan","Gang Zeng","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11452v1","updated":"2024-12-16T05:14:08Z","published":"2024-12-16T05:14:08Z","title":"Multilabel Classification for Lung Disease Detection: Integrating Deep\n  Learning and Natural Language Processing","summary":"  Classifying chest radiographs is a time-consuming and challenging task, even\nfor experienced radiologists. This provides an area for improvement due to the\ndifficulty in precisely distinguishing between conditions such as pleural\neffusion, pneumothorax, and pneumonia. We propose a novel transfer learning\nmodel for multi-label lung disease classification, utilizing the CheXpert\ndataset with over 12,617 images of frontal radiographs being analyzed. By\nintegrating RadGraph parsing for efficient annotation extraction, we enhance\nthe model's ability to accurately classify multiple lung diseases from complex\nmedical images. The proposed model achieved an F1 score of 0.69 and an AUROC of\n0.86, demonstrating its potential for clinical applications. Also explored was\nthe use of Natural Language Processing (NLP) to parse report metadata and\naddress uncertainties in disease classification. By comparing uncertain reports\nwith more certain cases, the NLP-enhanced model improves its ability to\nconclusively classify conditions. This research highlights the connection\nbetween deep learning and NLP, underscoring their potential to enhance\nradiological diagnostics and aid in the efficient analysis of chest\nradiographs.\n","authors":["Maria Efimovich","Jayden Lim","Vedant Mehta","Ethan Poon"],"pdf_url":"https://arxiv.org/pdf/2412.11452v1.pdf","comment":"All authors contributed equally"},{"id":"http://arxiv.org/abs/2412.11450v1","updated":"2024-12-16T05:08:15Z","published":"2024-12-16T05:08:15Z","title":"GroupFace: Imbalanced Age Estimation Based on Multi-hop Attention Graph\n  Convolutional Network and Group-aware Margin Optimization","summary":"  With the recent advances in computer vision, age estimation has significantly\nimproved in overall accuracy. However, owing to the most common methods do not\ntake into account the class imbalance problem in age estimation datasets, they\nsuffer from a large bias in recognizing long-tailed groups. To achieve\nhigh-quality imbalanced learning in long-tailed groups, the dominant solution\nlies in that the feature extractor learns the discriminative features of\ndifferent groups and the classifier is able to provide appropriate and unbiased\nmargins for different groups by the discriminative features. Therefore, in this\nnovel, we propose an innovative collaborative learning framework (GroupFace)\nthat integrates a multi-hop attention graph convolutional network and a dynamic\ngroup-aware margin strategy based on reinforcement learning. Specifically, to\nextract the discriminative features of different groups, we design an enhanced\nmulti-hop attention graph convolutional network. This network is capable of\ncapturing the interactions of neighboring nodes at different distances, fusing\nlocal and global information to model facial deep aging, and exploring diverse\nrepresentations of different groups. In addition, to further address the class\nimbalance problem, we design a dynamic group-aware margin strategy based on\nreinforcement learning to provide appropriate and unbiased margins for\ndifferent groups. The strategy divides the sample into four age groups and\nconsiders identifying the optimum margins for various age groups by employing a\nMarkov decision process. Under the guidance of the agent, the feature\nrepresentation bias and the classification margin deviation between different\ngroups can be reduced simultaneously, balancing inter-class separability and\nintra-class proximity. After joint optimization, our architecture achieves\nexcellent performance on several age estimation benchmark datasets.\n","authors":["Yiping Zhang","Yuntao Shou","Wei Ai","Tao Meng","Keqin Li"],"pdf_url":"https://arxiv.org/pdf/2412.11450v1.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.11443v1","updated":"2024-12-16T04:55:13Z","published":"2024-12-16T04:55:13Z","title":"Universal Domain Adaptive Object Detection via Dual Probabilistic\n  Alignment","summary":"  Domain Adaptive Object Detection (DAOD) transfers knowledge from a labeled\nsource domain to an unannotated target domain under closed-set assumption.\nUniversal DAOD (UniDAOD) extends DAOD to handle open-set, partial-set, and\nclosed-set domain adaptation. In this paper, we first unveil two issues:\ndomain-private category alignment is crucial for global-level features, and the\ndomain probability heterogeneity of features across different levels. To\naddress these issues, we propose a novel Dual Probabilistic Alignment (DPA)\nframework to model domain probability as Gaussian distribution, enabling the\nheterogeneity domain distribution sampling and measurement. The DPA consists of\nthree tailored modules: the Global-level Domain Private Alignment (GDPA), the\nInstance-level Domain Shared Alignment (IDSA), and the Private Class Constraint\n(PCC). GDPA utilizes the global-level sampling to mine domain-private category\nsamples and calculate alignment weight through a cumulative distribution\nfunction to address the global-level private category alignment. IDSA utilizes\ninstance-level sampling to mine domain-shared category samples and calculates\nalignment weight through Gaussian distribution to conduct the domain-shared\ncategory domain alignment to address the feature heterogeneity. The PCC\naggregates domain-private category centroids between feature and probability\nspaces to mitigate negative transfer. Extensive experiments demonstrate that\nour DPA outperforms state-of-the-art UniDAOD and DAOD methods across various\ndatasets and scenarios, including open, partial, and closed sets. Codes are\navailable at \\url{https://github.com/zyfone/DPA}.\n","authors":["Yuanfan Zheng","Jinlin Wu","Wuyang Li","Zhen Chen"],"pdf_url":"https://arxiv.org/pdf/2412.11443v1.pdf","comment":"This work is accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2405.13082v4","updated":"2024-12-16T04:44:47Z","published":"2024-05-21T06:44:40Z","title":"A Survey of Artificial Intelligence in Gait-Based Neurodegenerative\n  Disease Diagnosis","summary":"  Recent years have witnessed an increasing global population affected by\nneurodegenerative diseases (NDs), which traditionally require extensive\nhealthcare resources and human effort for medical diagnosis and monitoring. As\na crucial disease-related motor symptom, human gait can be exploited to\ncharacterize different NDs. The current advances in artificial intelligence\n(AI) models enable automatic gait analysis for NDs identification and\nclassification, opening a new avenue to facilitate faster and more\ncost-effective diagnosis of NDs. In this paper, we provide a comprehensive\nsurvey on recent progress of machine learning and deep learning based AI\ntechniques applied to diagnosis of five typical NDs through gait. We provide an\noverview of the process of AI-assisted NDs diagnosis, and present a systematic\ntaxonomy of existing gait data and AI models. Meanwhile, a novel quality\nevaluation criterion is proposed to quantitatively assess the quality of\nexisting studies. Through an extensive review and analysis of 169 studies, we\npresent recent technical advancements, discuss existing challenges, potential\nsolutions, and future directions in this field. Finally, we envision the\nprospective utilization of 3D skeleton data for human gait representation and\nthe development of more efficient AI models for NDs diagnosis.\n","authors":["Haocong Rao","Minlin Zeng","Xuejiao Zhao","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2405.13082v4.pdf","comment":"Article: 57 pages, citing 290 papers. Appendix: 30 pages. A\n  up-to-date resource (papers, data, etc.) of this survey (AI4NDD) is provided\n  at https://github.com/minlinzeng/AI4NDD-Survey"},{"id":"http://arxiv.org/abs/2412.11435v1","updated":"2024-12-16T04:23:33Z","published":"2024-12-16T04:23:33Z","title":"Learning Implicit Features with Flow Infused Attention for Realistic\n  Virtual Try-On","summary":"  Image-based virtual try-on is challenging since the generated image should\nfit the garment to model images in various poses and keep the characteristics\nand details of the garment simultaneously. A popular research stream warps the\ngarment image firstly to reduce the burden of the generation stage, which\nrelies highly on the performance of the warping module. Other methods without\nexplicit warping often lack sufficient guidance to fit the garment to the model\nimages. In this paper, we propose FIA-VTON, which leverages the implicit warp\nfeature by adopting a Flow Infused Attention module on virtual try-on. The\ndense warp flow map is projected as indirect guidance attention to enhance the\nfeature map warping in the generation process implicitly, which is less\nsensitive to the warping estimation accuracy than an explicit warp of the\ngarment image. To further enhance implicit warp guidance, we incorporate\nhigh-level spatial attention to complement the dense warp. Experimental results\non the VTON-HD and DressCode dataset significantly outperform state-of-the-art\nmethods, demonstrating that FIA-VTON is effective and robust for virtual\ntry-on.\n","authors":["Delong Zhang","Qiwei Huang","Yuanliu Liu","Yang Sun","Wei-Shi Zheng","Pengfei Xiong","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03887v3","updated":"2024-12-16T04:21:55Z","published":"2024-12-05T05:40:40Z","title":"MOANA: Multi-Radar Dataset for Maritime Odometry and Autonomous\n  Navigation Application","summary":"  Maritime environmental sensing requires overcoming challenges from complex\nconditions such as harsh weather, platform perturbations, large dynamic\nobjects, and the requirement for long detection ranges. While cameras and LiDAR\nare commonly used in ground vehicle navigation, their applicability in maritime\nsettings is limited by range constraints and hardware maintenance issues. Radar\nsensors, however, offer robust long-range detection capabilities and resilience\nto physical contamination from weather and saline conditions, making it a\npowerful sensor for maritime navigation. Among various radar types, X-band\nradar (e.g., marine radar) is widely employed for maritime vessel navigation,\nproviding effective long-range detection essential for situational awareness\nand collision avoidance. Nevertheless, it exhibits limitations during berthing\noperations where close-range object detection is critical. To address this\nshortcoming, we incorporate W-band radar (e.g., Navtech imaging radar), which\nexcels in detecting nearby objects with a higher update rate. We present a\ncomprehensive maritime sensor dataset featuring multi-range detection\ncapabilities. This dataset integrates short-range LiDAR data, medium-range\nW-band radar data, and long-range X-band radar data into a unified framework.\nAdditionally, it includes object labels for oceanic object detection usage,\nderived from radar and stereo camera images. The dataset comprises seven\nsequences collected from diverse regions with varying levels of estimation\ndifficulty, ranging from easy to challenging, and includes common locations\nsuitable for global localization tasks. This dataset serves as a valuable\nresource for advancing research in place recognition, odometry estimation,\nSLAM, object detection, and dynamic object elimination within maritime\nenvironments. Dataset can be found in following link:\nhttps://sites.google.com/view/rpmmoana\n","authors":["Hyesu Jang","Wooseong Yang","Hanguen Kim","Dongje Lee","Yongjin Kim","Jinbum Park","Minsoo Jeon","Jaeseong Koh","Yejin Kang","Minwoo Jung","Sangwoo Jung","Chng Zhen Hao","Wong Yu Hin","Chew Yihang","Ayoung Kim"],"pdf_url":"https://arxiv.org/pdf/2412.03887v3.pdf","comment":"9 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2406.16464v5","updated":"2024-12-16T04:13:38Z","published":"2024-06-24T09:13:42Z","title":"InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for\n  Multi-modal Sarcasm Detection","summary":"  Sarcasm in social media, often expressed through text-image combinations,\nposes challenges for sentiment analysis and intention mining. Current\nmulti-modal sarcasm detection methods have been demonstrated to overly rely on\nspurious cues within the textual modality, revealing a limited ability to\ngenuinely identify sarcasm through nuanced text-image interactions. To solve\nthis problem, we propose InterCLIP-MEP, which introduces Interactive CLIP\n(InterCLIP) with an efficient training strategy to extract enriched text-image\nrepresentations by embedding cross-modal information directly into each\nencoder. Additionally, we design a Memory-Enhanced Predictor (MEP) with a\ndynamic dual-channel memory that stores valuable test sample knowledge during\ninference, acting as a non-parametric classifier for robust sarcasm\nrecognition. Experiments on two benchmarks demonstrate that InterCLIP-MEP\nachieves state-of-the-art performance, with significant accuracy and F1 score\nimprovements on MMSD and MMSD2.0. Our code is available at\nhttps://github.com/CoderChen01/InterCLIP-MEP.\n","authors":["Junjie Chen","Hang Yu","Subin Huang","Sanmin Liu","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.16464v5.pdf","comment":"15 pages, 7 figures, 11 tables; Code and data are available at\n  https://github.com/CoderChen01/InterCLIP-MEP"},{"id":"http://arxiv.org/abs/2412.09645v2","updated":"2024-12-16T04:05:05Z","published":"2024-12-10T18:52:39Z","title":"Evaluation Agent: Efficient and Promptable Evaluation Framework for\n  Visual Generative Models","summary":"  Recent advancements in visual generative models have enabled high-quality\nimage and video generation, opening diverse applications. However, evaluating\nthese models often demands sampling hundreds or thousands of images or videos,\nmaking the process computationally expensive, especially for diffusion-based\nmodels with inherently slow sampling. Moreover, existing evaluation methods\nrely on rigid pipelines that overlook specific user needs and provide numerical\nresults without clear explanations. In contrast, humans can quickly form\nimpressions of a model's capabilities by observing only a few samples. To mimic\nthis, we propose the Evaluation Agent framework, which employs human-like\nstrategies for efficient, dynamic, multi-round evaluations using only a few\nsamples per round, while offering detailed, user-tailored analyses. It offers\nfour key advantages: 1) efficiency, 2) promptable evaluation tailored to\ndiverse user needs, 3) explainability beyond single numerical scores, and 4)\nscalability across various models and tools. Experiments show that Evaluation\nAgent reduces evaluation time to 10% of traditional methods while delivering\ncomparable results. The Evaluation Agent framework is fully open-sourced to\nadvance research in visual generative models and their efficient evaluation.\n","authors":["Fan Zhang","Shulin Tian","Ziqi Huang","Yu Qiao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09645v2.pdf","comment":"Equal contributions from first three authors. Project page:\n  https://vchitect.github.io/Evaluation-Agent-project Code:\n  https://github.com/Vchitect/Evaluation-Agent"},{"id":"http://arxiv.org/abs/2412.11428v1","updated":"2024-12-16T03:54:08Z","published":"2024-12-16T03:54:08Z","title":"View Transformation Robustness for Multi-View 3D Object Reconstruction\n  with Reconstruction Error-Guided View Selection","summary":"  View transformation robustness (VTR) is critical for deep-learning-based\nmulti-view 3D object reconstruction models, which indicates the methods'\nstability under inputs with various view transformations. However, existing\nresearch seldom focused on view transformation robustness in multi-view 3D\nobject reconstruction. One direct way to improve the models' VTR is to produce\ndata with more view transformations and add them to model training. Recent\nprogress on large vision models, particularly Stable Diffusion models, has\nprovided great potential for generating 3D models or synthesizing novel view\nimages with only a single image input. Directly deploying these models at\ninference consumes heavy computation resources and their robustness to view\ntransformations is not guaranteed either. To fully utilize the power of Stable\nDiffusion models without extra inference computation burdens, we propose to\ngenerate novel views with Stable Diffusion models for better view\ntransformation robustness. Instead of synthesizing random views, we propose a\nreconstruction error-guided view selection method, which considers the\nreconstruction errors' spatial distribution of the 3D predictions and chooses\nthe views that could cover the reconstruction errors as much as possible. The\nmethods are trained and tested on sets with large view transformations to\nvalidate the 3D reconstruction models' robustness to view transformations.\nExtensive experiments demonstrate that the proposed method can outperform\nstate-of-the-art 3D reconstruction methods and other view transformation\nrobustness comparison methods.\n","authors":["Qi Zhang","Zhouhang Luo","Tao Yu","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11428v1.pdf","comment":"Accepted to AAAI 25"},{"id":"http://arxiv.org/abs/2412.11423v1","updated":"2024-12-16T03:46:45Z","published":"2024-12-16T03:46:45Z","title":"Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion\n  Models","summary":"  Recent advancements in diffusion models revolutionize image generation but\npose risks of misuse, such as replicating artworks or generating deepfakes.\nExisting image protection methods, though effective, struggle to balance\nprotection efficacy, invisibility, and latency, thus limiting practical use. We\nintroduce perturbation pre-training to reduce latency and propose a\nmixture-of-perturbations approach that dynamically adapts to input images to\nminimize performance degradation. Our novel training strategy computes\nprotection loss across multiple VAE feature spaces, while adaptive targeted\nprotection at inference enhances robustness and invisibility. Experiments show\ncomparable protection performance with improved invisibility and drastically\nreduced inference time. The code and demo are available at\n\\url{https://webtoon.github.io/impasto}\n","authors":["Namhyuk Ahn","KiYoon Yoo","Wonhyuk Ahn","Daesik Kim","Seung-Hun Nam"],"pdf_url":"https://arxiv.org/pdf/2412.11423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11420v1","updated":"2024-12-16T03:39:33Z","published":"2024-12-16T03:39:33Z","title":"Category Level 6D Object Pose Estimation from a Single RGB Image using\n  Diffusion","summary":"  Estimating the 6D pose and 3D size of an object from an image is a\nfundamental task in computer vision. Most current approaches are restricted to\nspecific instances with known models or require ground truth depth information\nor point cloud captures from LIDAR. We tackle the harder problem of pose\nestimation for category-level objects from a single RGB image. We propose a\nnovel solution that eliminates the need for specific object models or depth\ninformation. Our method utilises score-based diffusion models to generate\nobject pose hypotheses to model the distribution of possible poses for the\nobject. Unlike previous methods that rely on costly trained likelihood\nestimators to remove outliers before pose aggregation using mean pooling, we\nintroduce a simpler approach using Mean Shift to estimate the mode of the\ndistribution as the final pose estimate. Our approach outperforms the current\nstate-of-the-art on the REAL275 dataset by a significant margin.\n","authors":["Adam Bethell","Ravi Garg","Ian Reid"],"pdf_url":"https://arxiv.org/pdf/2412.11420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10275v2","updated":"2024-12-16T03:32:09Z","published":"2024-12-13T16:52:13Z","title":"TIV-Diffusion: Towards Object-Centric Movement for Text-driven Image to\n  Video Generation","summary":"  Text-driven Image to Video Generation (TI2V) aims to generate controllable\nvideo given the first frame and corresponding textual description. The primary\nchallenges of this task lie in two parts: (i) how to identify the target\nobjects and ensure the consistency between the movement trajectory and the\ntextual description. (ii) how to improve the subjective quality of generated\nvideos. To tackle the above challenges, we propose a new diffusion-based TI2V\nframework, termed TIV-Diffusion, via object-centric textual-visual alignment,\nintending to achieve precise control and high-quality video generation based on\ntextual-described motion for different objects. Concretely, we enable our\nTIV-Diffuion model to perceive the textual-described objects and their motion\ntrajectory by incorporating the fused textual and visual knowledge through\nscale-offset modulation. Moreover, to mitigate the problems of object\ndisappearance and misaligned objects and motion, we introduce an object-centric\ntextual-visual alignment module, which reduces the risk of misaligned\nobjects/motion by decoupling the objects in the reference image and aligning\ntextual features with each object individually. Based on the above innovations,\nour TIV-Diffusion achieves state-of-the-art high-quality video generation\ncompared with existing TI2V methods.\n","authors":["Xingrui Wang","Xin Li","Yaosi Hu","Hanxin Zhu","Chen Hou","Cuiling Lan","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2412.10275v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11412v1","updated":"2024-12-16T03:28:00Z","published":"2024-12-16T03:28:00Z","title":"V-MIND: Building Versatile Monocular Indoor 3D Detector with Diverse 2D\n  Annotations","summary":"  The field of indoor monocular 3D object detection is gaining significant\nattention, fueled by the increasing demand in VR/AR and robotic applications.\nHowever, its advancement is impeded by the limited availability and diversity\nof 3D training data, owing to the labor-intensive nature of 3D data collection\nand annotation processes. In this paper, we present V-MIND (Versatile Monocular\nINdoor Detector), which enhances the performance of indoor 3D detectors across\na diverse set of object classes by harnessing publicly available large-scale 2D\ndatasets. By leveraging well-established monocular depth estimation techniques\nand camera intrinsic predictors, we can generate 3D training data by converting\nlarge-scale 2D images into 3D point clouds and subsequently deriving pseudo 3D\nbounding boxes. To mitigate distance errors inherent in the converted point\nclouds, we introduce a novel 3D self-calibration loss for refining the pseudo\n3D bounding boxes during training. Additionally, we propose a novel ambiguity\nloss to address the ambiguity that arises when introducing new classes from 2D\ndatasets. Finally, through joint training with existing 3D datasets and pseudo\n3D bounding boxes derived from 2D datasets, V-MIND achieves state-of-the-art\nobject detection performance across a wide range of classes on the Omni3D\nindoor dataset.\n","authors":["Jin-Cheng Jhang","Tao Tu","Fu-En Wang","Ke Zhang","Min Sun","Cheng-Hao Kuo"],"pdf_url":"https://arxiv.org/pdf/2412.11412v1.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2412.11409v1","updated":"2024-12-16T03:25:23Z","published":"2024-12-16T03:25:23Z","title":"Multi-modal and Multi-scale Spatial Environment Understanding for\n  Immersive Visual Text-to-Speech","summary":"  Visual Text-to-Speech (VTTS) aims to take the environmental image as the\nprompt to synthesize the reverberant speech for the spoken content. The\nchallenge of this task lies in understanding the spatial environment from the\nimage. Many attempts have been made to extract global spatial visual\ninformation from the RGB space of an spatial image. However, local and depth\nimage information are crucial for understanding the spatial environment, which\nprevious works have ignored. To address the issues, we propose a novel\nmulti-modal and multi-scale spatial environment understanding scheme to achieve\nimmersive VTTS, termed M2SE-VTTS. The multi-modal aims to take both the RGB and\nDepth spaces of the spatial image to learn more comprehensive spatial\ninformation, and the multi-scale seeks to model the local and global spatial\nknowledge simultaneously. Specifically, we first split the RGB and Depth images\ninto patches and adopt the Gemini-generated environment captions to guide the\nlocal spatial understanding. After that, the multi-modal and multi-scale\nfeatures are integrated by the local-aware global spatial understanding. In\nthis way, M2SE-VTTS effectively models the interactions between local and\nglobal spatial contexts in the multi-modal spatial environment. Objective and\nsubjective evaluations suggest that our model outperforms the advanced\nbaselines in environmental speech generation. The code and audio samples are\navailable at: https://github.com/AI-S2-Lab/M2SE-VTTS.\n","authors":["Rui Liu","Shuwei He","Yifan Hu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2412.11409v1.pdf","comment":"9 pages,2 figures, Accepted by AAAI'2025"},{"id":"http://arxiv.org/abs/2412.11407v1","updated":"2024-12-16T03:21:20Z","published":"2024-12-16T03:21:20Z","title":"An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion\n  for Long-tailed Multispectral Point Clouds","summary":"  Multispectral point cloud (MPC) captures 3D spatial-spectral information from\nthe observed scene, which can be used for scene understanding and has a wide\nrange of applications. However, most of the existing classification methods\nwere extensively tested on indoor datasets, and when applied to outdoor\ndatasets they still face problems including sparse labeled targets, differences\nin land-covers scales, and long-tailed distributions. To address the above\nissues, an enhanced classification method based on adaptive multi-scale fusion\nfor MPCs with long-tailed distributions is proposed. In the training set\ngeneration stage, a grid-balanced sampling strategy is designed to reliably\ngenerate training samples from sparse labeled datasets. In the feature learning\nstage, a multi-scale feature fusion module is proposed to fuse shallow features\nof land-covers at different scales, addressing the issue of losing fine\nfeatures due to scale variations in land-covers. In the classification stage,\nan adaptive hybrid loss module is devised to utilize multi-classification heads\nwith adaptive weights to balance the learning ability of different classes,\nimproving the classification performance of small classes due to various-scales\nand long-tailed distributions in land-covers. Experimental results on three MPC\ndatasets demonstrate the effectiveness of the proposed method compared with the\nstate-of-the-art methods.\n","authors":["TianZhu Liu","BangYan Hu","YanFeng Gu","Xian Li","Aleksandra Pižurica"],"pdf_url":"https://arxiv.org/pdf/2412.11407v1.pdf","comment":"16 pages, 9 figures, 5 tables"},{"id":"http://arxiv.org/abs/2409.08056v2","updated":"2024-12-16T03:19:27Z","published":"2024-09-12T14:05:13Z","title":"Expansive Supervision for Neural Radiance Field","summary":"  Neural Radiance Field (NeRF) has achieved remarkable success in creating\nimmersive media representations through its exceptional reconstruction\ncapabilities. However, the computational demands of dense forward passes and\nvolume rendering during training continue to challenge its real-world\napplications. In this paper, we introduce Expansive Supervision to reduce time\nand memory costs during NeRF training from the perspective of partial ray\nselection for supervision. Specifically, we observe that training errors\nexhibit a long-tail distribution correlated with image content. Based on this\nobservation, our method selectively renders a small but crucial subset of\npixels and expands their values to estimate errors across the entire area for\neach iteration. Compared to conventional supervision, our approach effectively\nbypasses redundant rendering processes, resulting in substantial reductions in\nboth time and memory consumption. Experimental results demonstrate that\nintegrating Expansive Supervision within existing state-of-the-art acceleration\nframeworks achieves 52% memory savings and 16% time savings while maintaining\ncomparable visual quality.\n","authors":["Weixiang Zhang","Shuzhao Xie","Shijia Ge","Wei Yao","Chen Tang","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2409.08056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08879v2","updated":"2024-12-16T03:16:59Z","published":"2024-12-12T02:27:46Z","title":"Video Repurposing from User Generated Content: A Large-scale Dataset and\n  Benchmark","summary":"  The demand for producing short-form videos for sharing on social media\nplatforms has experienced significant growth in recent times. Despite notable\nadvancements in the fields of video summarization and highlight detection,\nwhich can create partially usable short films from raw videos, these approaches\nare often domain-specific and require an in-depth understanding of real-world\nvideo content. To tackle this predicament, we propose Repurpose-10K, an\nextensive dataset comprising over 10,000 videos with more than 120,000\nannotated clips aimed at resolving the video long-to-short task. Recognizing\nthe inherent constraints posed by untrained human annotators, which can result\nin inaccurate annotations for repurposed videos, we propose a two-stage\nsolution to obtain annotations from real-world user-generated content.\nFurthermore, we offer a baseline model to address this challenging task by\nintegrating audio, visual, and caption aspects through a cross-modal fusion and\nalignment framework. We aspire for our work to ignite groundbreaking research\nin the lesser-explored realms of video repurposing.\n","authors":["Yongliang Wu","Wenbo Zhu","Jiawang Cao","Yi Lu","Bozheng Li","Weiheng Chi","Zihan Qiu","Lirian Su","Haolin Zheng","Jay Wu","Xu Yang"],"pdf_url":"https://arxiv.org/pdf/2412.08879v2.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2406.10126v3","updated":"2024-12-16T03:13:09Z","published":"2024-06-14T15:33:00Z","title":"Training-free Camera Control for Video Generation","summary":"  We propose a training-free and robust solution to offer camera movement\ncontrol for off-the-shelf video diffusion models. Unlike previous work, our\nmethod does not require any supervised finetuning on camera-annotated datasets\nor self-supervised training via data augmentation. Instead, it can be plugged\nand played with most pretrained video diffusion models and generate camera\ncontrollable videos with a single image or text prompt as input. The\ninspiration of our work comes from the layout prior that intermediate latents\nhold towards generated results, thus rearranging noisy pixels in them will make\noutput content reallocated as well. As camera move could also be seen as a kind\nof pixel rearrangement caused by perspective change, videos could be\nreorganized following specific camera motion if their noisy latents change\naccordingly. Established on this, we propose our method CamTrol, which enables\nrobust camera control for video diffusion models. It is achieved by a two-stage\nprocess. First, we model image layout rearrangement through explicit camera\nmovement in 3D point cloud space. Second, we generate videos with camera motion\nusing layout prior of noisy latents formed by a series of rearranged images.\nExtensive experiments have demonstrated the robustness our method holds in\ncontrolling camera motion of generated videos. Furthermore, we show that our\nmethod can produce impressive results in generating 3D rotation videos with\ndynamic content. Project page at https://lifedecoder.github.io/CamTrol/.\n","authors":["Chen Hou","Guoqiang Wei","Yan Zeng","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2406.10126v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09907v2","updated":"2024-12-16T03:04:33Z","published":"2024-12-13T06:52:02Z","title":"IQViC: In-context, Question Adaptive Vision Compressor for Long-term\n  Video Understanding LMMs","summary":"  With the increasing complexity of video data and the need for more efficient\nlong-term temporal understanding, existing long-term video understanding\nmethods often fail to accurately capture and analyze extended video sequences.\nThese methods typically struggle to maintain performance over longer durations\nand to handle the intricate dependencies within the video content. To address\nthese limitations, we propose a simple yet effective large multi-modal model\nframework for long-term video understanding that incorporates a novel visual\ncompressor, the In-context, Question Adaptive Visual Compressor (IQViC). The\nkey idea, inspired by humans' selective attention and in-context memory\nmechanisms, is to introduce a novel visual compressor and incorporate efficient\nmemory management techniques to enhance long-term video question answering. Our\nframework utilizes IQViC, a transformer-based visual compressor, enabling\nquestion-conditioned in-context compression, unlike existing methods that rely\non full video visual features. This selectively extracts relevant information,\nsignificantly reducing memory token requirements. Through extensive experiments\non a new dataset based on InfiniBench for long-term video understanding, and\nstandard benchmarks used for existing methods' evaluation, we demonstrate the\neffectiveness of our proposed IQViC framework and its superiority over\nstate-of-the-art methods in terms of video understanding accuracy and memory\nefficiency.\n","authors":["Sosuke Yamao","Natsuki Miyahara","Yuki Harazono","Shun Takeuchi"],"pdf_url":"https://arxiv.org/pdf/2412.09907v2.pdf","comment":"The first and second authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2412.11396v1","updated":"2024-12-16T02:52:19Z","published":"2024-12-16T02:52:19Z","title":"Leveraging Retrieval-Augmented Tags for Large Vision-Language\n  Understanding in Complex Scenes","summary":"  Object-aware reasoning in vision-language tasks poses significant challenges\nfor current models, particularly in handling unseen objects, reducing\nhallucinations, and capturing fine-grained relationships in complex visual\nscenes. To address these limitations, we propose the Vision-Aware\nRetrieval-Augmented Prompting (VRAP) framework, a generative approach that\nenhances Large Vision-Language Models (LVLMs) by integrating\nretrieval-augmented object tags into their prompts. VRAP introduces a novel\npipeline where structured tags, including objects, attributes, and\nrelationships, are extracted using pretrained visual encoders and scene graph\nparsers. These tags are enriched with external knowledge and incorporated into\nthe LLM's input, enabling detailed and accurate reasoning. We evaluate VRAP\nacross multiple vision-language benchmarks, including VQAv2, GQA, VizWiz, and\nCOCO, achieving state-of-the-art performance in fine-grained reasoning and\nmultimodal understanding. Additionally, our ablation studies highlight the\nimportance of retrieval-augmented tags and contrastive learning, while human\nevaluations confirm VRAP's ability to generate accurate, detailed, and\ncontextually relevant responses. Notably, VRAP achieves a 40% reduction in\ninference latency by eliminating runtime retrieval. These results demonstrate\nthat VRAP is a robust and efficient framework for advancing object-aware\nmultimodal reasoning.\n","authors":["Antonio Carlos Rivera","Anthony Moore","Steven Robinson"],"pdf_url":"https://arxiv.org/pdf/2412.11396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11395v1","updated":"2024-12-16T02:48:55Z","published":"2024-12-16T02:48:55Z","title":"Depth-Centric Dehazing and Depth-Estimation from Real-World Hazy Driving\n  Video","summary":"  In this paper, we study the challenging problem of simultaneously removing\nhaze and estimating depth from real monocular hazy videos. These tasks are\ninherently complementary: enhanced depth estimation improves dehazing via the\natmospheric scattering model (ASM), while superior dehazing contributes to more\naccurate depth estimation through the brightness consistency constraint (BCC).\nTo tackle these intertwined tasks, we propose a novel depth-centric learning\nframework that integrates the ASM model with the BCC constraint. Our key idea\nis that both ASM and BCC rely on a shared depth estimation network. This\nnetwork simultaneously exploits adjacent dehazed frames to enhance depth\nestimation via BCC and uses the refined depth cues to more effectively remove\nhaze through ASM. Additionally, we leverage a non-aligned clear video and its\nestimated depth to independently regularize the dehazing and depth estimation\nnetworks. This is achieved by designing two discriminator networks: $D_{MFIR}$\nenhances high-frequency details in dehazed videos, and $D_{MDR}$ reduces the\noccurrence of black holes in low-texture regions. Extensive experiments\ndemonstrate that the proposed method outperforms current state-of-the-art\ntechniques in both video dehazing and depth estimation tasks, especially in\nreal-world hazy scenes. Project page:\nhttps://fanjunkai1.github.io/projectpage/DCL/index.html.\n","authors":["Junkai Fan","Kun Wang","Zhiqiang Yan","Xiang Chen","Shangbing Gao","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11395v1.pdf","comment":"Accepted by AAAI 20205, Project page:\n  https://fanjunkai1.github.io/projectpage/DCL/index.html"},{"id":"http://arxiv.org/abs/2412.03937v3","updated":"2024-12-16T02:39:18Z","published":"2024-12-05T07:35:19Z","title":"AIpparel: A Large Multimodal Generative Model for Digital Garments","summary":"  Apparel is essential to human life, offering protection, mirroring cultural\nidentities, and showcasing personal style. Yet, the creation of garments\nremains a time-consuming process, largely due to the manual work involved in\ndesigning them. To simplify this process, we introduce AIpparel, a large\nmultimodal model for generating and editing sewing patterns. Our model\nfine-tunes state-of-the-art large multimodal models (LMMs) on a custom-curated\nlarge-scale dataset of over 120,000 unique garments, each with multimodal\nannotations including text, images, and sewing patterns. Additionally, we\npropose a novel tokenization scheme that concisely encodes these complex sewing\npatterns so that LLMs can learn to predict them efficiently. AIpparel achieves\nstate-of-the-art performance in single-modal tasks, including text-to-garment\nand image-to-garment prediction, and enables novel multimodal garment\ngeneration applications such as interactive garment editing. The project\nwebsite is at georgenakayama.github.io/AIpparel/.\n","authors":["Kiyohiro Nakayama","Jan Ackermann","Timur Levent Kesdogan","Yang Zheng","Maria Korosteleva","Olga Sorkine-Hornung","Leonidas J. Guibas","Guandao Yang","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2412.03937v3.pdf","comment":"The project website is at georgenakayama.github.io/AIpparel/"},{"id":"http://arxiv.org/abs/2412.11391v1","updated":"2024-12-16T02:37:58Z","published":"2024-12-16T02:37:58Z","title":"Temporal Contrastive Learning for Video Temporal Reasoning in Large\n  Vision-Language Models","summary":"  Temporal reasoning is a critical challenge in video-language understanding,\nas it requires models to align semantic concepts consistently across time.\nWhile existing large vision-language models (LVLMs) and large language models\n(LLMs) excel at static tasks, they struggle to capture dynamic interactions and\ntemporal dependencies in video sequences. In this work, we propose Temporal\nSemantic Alignment via Dynamic Prompting (TSADP), a novel framework that\nenhances temporal reasoning capabilities through dynamic task-specific prompts\nand temporal contrastive learning. TSADP leverages a Dynamic Prompt Generator\n(DPG) to encode fine-grained temporal relationships and a Temporal Contrastive\nLoss (TCL) to align visual and textual embeddings across time. We evaluate our\nmethod on the VidSitu dataset, augmented with enriched temporal annotations,\nand demonstrate significant improvements over state-of-the-art models in tasks\nsuch as Intra-Video Entity Association, Temporal Relationship Understanding,\nand Chronology Prediction. Human evaluations further confirm TSADP's ability to\ngenerate coherent and semantically accurate descriptions. Our analysis\nhighlights the robustness, efficiency, and practical utility of TSADP, making\nit a step forward in the field of video-language understanding.\n","authors":["Rafael Souza","Jia-Hao Lim","Alexander Davis"],"pdf_url":"https://arxiv.org/pdf/2412.11391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10958v2","updated":"2024-12-16T02:37:33Z","published":"2024-09-17T07:52:09Z","title":"Towards Effective User Attribution for Latent Diffusion Models via\n  Watermark-Informed Blending","summary":"  Rapid advancements in multimodal large language models have enabled the\ncreation of hyper-realistic images from textual descriptions. However, these\nadvancements also raise significant concerns about unauthorized use, which\nhinders their broader distribution. Traditional watermarking methods often\nrequire complex integration or degrade image quality. To address these\nchallenges, we introduce a novel framework Towards Effective user Attribution\nfor latent diffusion models via Watermark-Informed Blending (TEAWIB). TEAWIB\nincorporates a unique ready-to-use configuration approach that allows seamless\nintegration of user-specific watermarks into generative models. This approach\nensures that each user can directly apply a pre-configured set of parameters to\nthe model without altering the original model parameters or compromising image\nquality. Additionally, noise and augmentation operations are embedded at the\npixel level to further secure and stabilize watermarked images. Extensive\nexperiments validate the effectiveness of TEAWIB, showcasing the\nstate-of-the-art performance in perceptual quality and attribution accuracy.\n","authors":["Yongyang Pan","Xiaohong Liu","Siqi Luo","Yi Xin","Xiao Guo","Xiaoming Liu","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2409.10958v2.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.10040v2","updated":"2024-12-16T02:31:03Z","published":"2024-12-13T11:00:57Z","title":"RemDet: Rethinking Efficient Model Design for UAV Object Detection","summary":"  Object detection in Unmanned Aerial Vehicle (UAV) images has emerged as a\nfocal area of research, which presents two significant challenges: i) objects\nare typically small and dense within vast images; ii) computational resource\nconstraints render most models unsuitable for real-time deployment. Current\nreal-time object detectors are not optimized for UAV images, and complex\nmethods designed for small object detection often lack real-time capabilities.\nTo address these challenges, we propose a novel detector, RemDet (Reparameter\nefficient multiplication Detector). Our contributions are as follows: 1)\nRethinking the challenges of existing detectors for small and dense UAV images,\nand proposing information loss as a design guideline for efficient models. 2)\nWe introduce the ChannelC2f module to enhance small object detection\nperformance, demonstrating that high-dimensional representations can\neffectively mitigate information loss. 3) We design the GatedFFN module to\nprovide not only strong performance but also low latency, effectively\naddressing the challenges of real-time detection. Our research reveals that\nGatedFFN, through the use of multiplication, is more cost-effective than\nfeed-forward networks for high-dimensional representation. 4) We propose the\nCED module, which combines the advantages of ViT and CNN downsampling to\neffectively reduce information loss. It specifically enhances context\ninformation for small and dense objects. Extensive experiments on large UAV\ndatasets, Visdrone and UAVDT, validate the real-time efficiency and superior\nperformance of our methods. On the challenging UAV dataset VisDrone, our\nmethods not only provided state-of-the-art results, improving detection by more\nthan 3.4%, but also achieve 110 FPS on a single 4090.\n","authors":["Chen Li","Rui Zhao","Zeyu Wang","Huiying Xu","Xinzhong Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.10040v2.pdf","comment":"Accepted to AAAI25"},{"id":"http://arxiv.org/abs/2410.10084v2","updated":"2024-12-16T02:15:53Z","published":"2024-10-14T01:57:06Z","title":"PointNet with KAN versus PointNet with MLP for 3D Classification and\n  Segmentation of Point Sets","summary":"  Kolmogorov-Arnold Networks (KANs) have recently gained attention as an\nalternative to traditional Multilayer Perceptrons (MLPs) in deep learning\nframeworks. KANs have been integrated into various deep learning architectures\nsuch as convolutional neural networks, graph neural networks, and transformers,\nwith their performance evaluated. However, their effectiveness within\npoint-cloud-based neural networks remains unexplored. To address this gap, we\nincorporate KANs into PointNet for the first time to evaluate their performance\non 3D point cloud classification and segmentation tasks. Specifically, we\nintroduce PointNet-KAN, built upon two key components. First, it employs KANs\ninstead of traditional MLPs. Second, it retains the core principle of PointNet\nby using shared KAN layers and applying symmetric functions for global feature\nextraction, ensuring permutation invariance with respect to the input features.\nIn traditional MLPs, the goal is to train the weights and biases with fixed\nactivation functions; however, in KANs, the goal is to train the activation\nfunctions themselves. We use Jacobi polynomials to construct the KAN layers. We\nextensively and systematically evaluate PointNet-KAN across various polynomial\ndegrees and special types such as the Lagrange, Chebyshev, and Gegenbauer\npolynomials. Our results show that PointNet-KAN achieves competitive\nperformance compared to PointNet with MLPs on benchmark datasets for 3D object\nclassification and segmentation, despite employing a shallower and simpler\nnetwork architecture. We hope this work serves as a foundation and provides\nguidance for integrating KANs, as an alternative to MLPs, into more advanced\npoint cloud processing architectures.\n","authors":["Ali Kashefi"],"pdf_url":"https://arxiv.org/pdf/2410.10084v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11381v1","updated":"2024-12-16T02:11:19Z","published":"2024-12-16T02:11:19Z","title":"Adapting Segment Anything Model (SAM) to Experimental Datasets via\n  Fine-Tuning on GAN-based Simulation: A Case Study in Additive Manufacturing","summary":"  Industrial X-ray computed tomography (XCT) is a powerful tool for\nnon-destructive characterization of materials and manufactured components. XCT\ncommonly accompanied by advanced image analysis and computer vision algorithms\nto extract relevant information from the images. Traditional computer vision\nmodels often struggle due to noise, resolution variability, and complex\ninternal structures, particularly in scientific imaging applications.\nState-of-the-art foundational models, like the Segment Anything Model\n(SAM)-designed for general-purpose image segmentation-have revolutionized image\nsegmentation across various domains, yet their application in specialized\nfields like materials science remains under-explored. In this work, we explore\nthe application and limitations of SAM for industrial X-ray CT inspection of\nadditive manufacturing components. We demonstrate that while SAM shows promise,\nit struggles with out-of-distribution data, multiclass segmentation, and\ncomputational efficiency during fine-tuning. To address these issues, we\npropose a fine-tuning strategy utilizing parameter-efficient techniques,\nspecifically Conv-LoRa, to adapt SAM for material-specific datasets.\nAdditionally, we leverage generative adversarial network (GAN)-generated data\nto enhance the training process and improve the model's segmentation\nperformance on complex X-ray CT data. Our experimental results highlight the\nimportance of tailored segmentation models for accurate inspection, showing\nthat fine-tuning SAM on domain-specific scientific imaging data significantly\nimproves performance. However, despite improvements, the model's ability to\ngeneralize across diverse datasets remains limited, highlighting the need for\nfurther research into robust, scalable solutions for domain-specific\nsegmentation tasks.\n","authors":["Anika Tabassum","Amirkoushyar Ziabari"],"pdf_url":"https://arxiv.org/pdf/2412.11381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11380v1","updated":"2024-12-16T02:11:02Z","published":"2024-12-16T02:11:02Z","title":"Relation-Guided Adversarial Learning for Data-free Knowledge Transfer","summary":"  Data-free knowledge distillation transfers knowledge by recovering training\ndata from a pre-trained model. Despite the recent success of seeking global\ndata diversity, the diversity within each class and the similarity among\ndifferent classes are largely overlooked, resulting in data homogeneity and\nlimited performance. In this paper, we introduce a novel Relation-Guided\nAdversarial Learning method with triplet losses, which solves the homogeneity\nproblem from two aspects. To be specific, our method aims to promote both\nintra-class diversity and inter-class confusion of the generated samples. To\nthis end, we design two phases, an image synthesis phase and a student training\nphase. In the image synthesis phase, we construct an optimization process to\npush away samples with the same labels and pull close samples with different\nlabels, leading to intra-class diversity and inter-class confusion,\nrespectively. Then, in the student training phase, we perform an opposite\noptimization, which adversarially attempts to reduce the distance of samples of\nthe same classes and enlarge the distance of samples of different classes. To\nmitigate the conflict of seeking high global diversity and keeping inter-class\nconfusing, we propose a focal weighted sampling strategy by selecting the\nnegative in the triplets unevenly within a finite range of distance. RGAL shows\nsignificant improvement over previous state-of-the-art methods in accuracy and\ndata efficiency. Besides, RGAL can be inserted into state-of-the-art methods on\nvarious data-free knowledge transfer applications. Experiments on various\nbenchmarks demonstrate the effectiveness and generalizability of our proposed\nmethod on various tasks, specially data-free knowledge distillation, data-free\nquantization, and non-exemplar incremental learning. Our code is available at\nhttps://github.com/Sharpiless/RGAL.\n","authors":["Yingping Liang","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2412.11380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11379v1","updated":"2024-12-16T02:09:32Z","published":"2024-12-16T02:09:32Z","title":"Controllable Distortion-Perception Tradeoff Through Latent Diffusion for\n  Neural Image Compression","summary":"  Neural image compression often faces a challenging trade-off among rate,\ndistortion and perception. While most existing methods typically focus on\neither achieving high pixel-level fidelity or optimizing for perceptual\nmetrics, we propose a novel approach that simultaneously addresses both aspects\nfor a fixed neural image codec. Specifically, we introduce a plug-and-play\nmodule at the decoder side that leverages a latent diffusion process to\ntransform the decoded features, enhancing either low distortion or high\nperceptual quality without altering the original image compression codec. Our\napproach facilitates fusion of original and transformed features without\nadditional training, enabling users to flexibly adjust the balance between\ndistortion and perception during inference. Extensive experimental results\ndemonstrate that our method significantly enhances the pretrained codecs with a\nwide, adjustable distortion-perception range while maintaining their original\ncompression capabilities. For instance, we can achieve more than 150%\nimprovement in LPIPS-BDRate without sacrificing more than 1 dB in PSNR.\n","authors":["Chuqin Zhou","Guo Lu","Jiangchuan Li","Xiangyu Chen","Zhengxue Cheng","Li Song","Wenjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11379v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2408.10605v5","updated":"2024-12-16T02:08:29Z","published":"2024-08-20T07:37:23Z","title":"MUSES: 3D-Controllable Image Generation via Multi-Modal Agent\n  Collaboration","summary":"  Despite recent advancements in text-to-image generation, most existing\nmethods struggle to create images with multiple objects and complex spatial\nrelationships in the 3D world. To tackle this limitation, we introduce a\ngeneric AI system, namely MUSES, for 3D-controllable image generation from user\nqueries. Specifically, our MUSES addresses this challenging task by developing\na progressive workflow with three key components, including (1) Layout Manager\nfor 2D-to-3D layout lifting, (2) Model Engineer for 3D object acquisition and\ncalibration, (3) Image Artist for 3D-to-2D image rendering. By mimicking the\ncollaboration of human professionals, this multi-modal agent pipeline\nfacilitates the effective and automatic creation of images with 3D-controllable\nobjects, through an explainable integration of top-down planning and bottom-up\ngeneration. Additionally, we find that existing benchmarks lack detailed\ndescriptions of complex 3D spatial relationships of multiple objects. To fill\nthis gap, we further construct a new benchmark of T2I-3DisBench (3D image\nscene), which describes diverse 3D image scenes with 50 detailed prompts.\nExtensive experiments show the state-of-the-art performance of MUSES on both\nT2I-CompBench and T2I-3DisBench, outperforming recent strong competitors such\nas DALL-E 3 and Stable Diffusion 3. These results demonstrate a significant\nstep of MUSES forward in bridging natural language, 2D image generation, and 3D\nworld. Our codes are available at the following link:\nhttps://github.com/DINGYANB/MUSES.\n","authors":["Yanbo Ding","Shaobin Zhuang","Kunchang Li","Zhengrong Yue","Yu Qiao","Yali Wang"],"pdf_url":"https://arxiv.org/pdf/2408.10605v5.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11377v1","updated":"2024-12-16T02:05:15Z","published":"2024-12-16T02:05:15Z","title":"Improving Automatic Fetal Biometry Measurement with Swoosh Activation\n  Function","summary":"  The measurement of fetal thalamus diameter (FTD) and fetal head circumference\n(FHC) are crucial in identifying abnormal fetal thalamus development as it may\nlead to certain neuropsychiatric disorders in later life. However, manual\nmeasurements from 2D-US images are laborious, prone to high inter-observer\nvariability, and complicated by the high signal-to-noise ratio nature of the\nimages. Deep learning-based landmark detection approaches have shown promise in\nmeasuring biometrics from US images, but the current state-of-the-art (SOTA)\nalgorithm, BiometryNet, is inadequate for FTD and FHC measurement due to its\ninability to account for the fuzzy edges of these structures and the complex\nshape of the FTD structure. To address these inadequacies, we propose a novel\nSwoosh Activation Function (SAF) designed to enhance the regularization of\nheatmaps produced by landmark detection algorithms. Our SAF serves as a\nregularization term to enforce an optimum mean squared error (MSE) level\nbetween predicted heatmaps, reducing the dispersiveness of hotspots in\npredicted heatmaps. Our experimental results demonstrate that SAF significantly\nimproves the measurement performances of FTD and FHC with higher intraclass\ncorrelation coefficient scores in FTD and lower mean difference scores in FHC\nmeasurement than those of the current SOTA algorithm BiometryNet. Moreover, our\nproposed SAF is highly generalizable and architecture-agnostic. The SAF's\ncoefficients can be configured for different tasks, making it highly\ncustomizable. Our study demonstrates that the SAF activation function is a\nnovel method that can improve measurement accuracy in fetal biometry landmark\ndetection. This improvement has the potential to contribute to better fetal\nmonitoring and improved neonatal outcomes.\n","authors":["Shijia Zhou","Euijoon Ahn","Hao Wang","Ann Quinton","Narelle Kennedy","Pradeeba Sridar","Ralph Nanan","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2412.11377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11375v1","updated":"2024-12-16T02:03:45Z","published":"2024-12-16T02:03:45Z","title":"Text and Image Are Mutually Beneficial: Enhancing Training-Free Few-Shot\n  Classification with CLIP","summary":"  Contrastive Language-Image Pretraining (CLIP) has been widely used in vision\ntasks. Notably, CLIP has demonstrated promising performance in few-shot\nlearning (FSL). However, existing CLIP-based methods in training-free FSL\n(i.e., without the requirement of additional training) mainly learn different\nmodalities independently, leading to two essential issues: 1) severe anomalous\nmatch in image modality; 2) varying quality of generated text prompts. To\naddress these issues, we build a mutual guidance mechanism, that introduces an\nImage-Guided-Text (IGT) component to rectify varying quality of text prompts\nthrough image representations, and a Text-Guided-Image (TGI) component to\nmitigate the anomalous match of image modality through text representations. By\nintegrating IGT and TGI, we adopt a perspective of Text-Image Mutual guidance\nOptimization, proposing TIMO. Extensive experiments show that TIMO\nsignificantly outperforms the state-of-the-art (SOTA) training-free method.\nAdditionally, by exploring the extent of mutual guidance, we propose an\nenhanced variant, TIMO-S, which even surpasses the best training-required\nmethods by 0.33% with approximately 100 times less time cost. Our code is\navailable at https://github.com/lyymuwu/TIMO.\n","authors":["Yayuan Li","Jintao Guo","Lei Qi","Wenbin Li","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2412.11375v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2402.11989v3","updated":"2024-12-16T01:54:47Z","published":"2024-02-19T09:32:48Z","title":"Privacy-Preserving Low-Rank Adaptation against Membership Inference\n  Attacks for Latent Diffusion Models","summary":"  Low-rank adaptation (LoRA) is an efficient strategy for adapting latent\ndiffusion models (LDMs) on a private dataset to generate specific images by\nminimizing the adaptation loss. However, the LoRA-adapted LDMs are vulnerable\nto membership inference (MI) attacks that can judge whether a particular data\npoint belongs to the private dataset, thus leading to the privacy leakage. To\ndefend against MI attacks, we first propose a straightforward solution:\nMembership-Privacy-preserving LoRA (MP-LoRA). MP-LoRA is formulated as a\nmin-max optimization problem where a proxy attack model is trained by\nmaximizing its MI gain while the LDM is adapted by minimizing the sum of the\nadaptation loss and the MI gain of the proxy attack model. However, we\nempirically find that MP-LoRA has the issue of unstable optimization, and\ntheoretically analyze that the potential reason is the unconstrained local\nsmoothness, which impedes the privacy-preserving adaptation. To mitigate this\nissue, we further propose a Stable Membership-Privacy-preserving LoRA\n(SMP-LoRA) that adapts the LDM by minimizing the ratio of the adaptation loss\nto the MI gain. Besides, we theoretically prove that the local smoothness of\nSMP-LoRA can be constrained by the gradient norm, leading to improved\nconvergence. Our experimental results corroborate that SMP-LoRA can indeed\ndefend against MI attacks and generate high-quality images. Our Code is\navailable at \\url{https://github.com/WilliamLUO0/StablePrivateLoRA}.\n","authors":["Zihao Luo","Xilie Xu","Feng Liu","Yun Sing Koh","Di Wang","Jingfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.11989v3.pdf","comment":"AAAI 2025 Accept"},{"id":"http://arxiv.org/abs/2412.11365v1","updated":"2024-12-16T01:37:51Z","published":"2024-12-16T01:37:51Z","title":"BiM-VFI: directional Motion Field-Guided Frame Interpolation for Video\n  with Non-uniform Motions","summary":"  Existing Video Frame interpolation (VFI) models tend to suffer from\ntime-to-location ambiguity when trained with video of non-uniform motions, such\nas accelerating, decelerating, and changing directions, which often yield\nblurred interpolated frames. In this paper, we propose (i) a novel motion\ndescription map, Bidirectional Motion field (BiM), to effectively describe\nnon-uniform motions; (ii) a BiM-guided Flow Net (BiMFN) with Content-Aware\nUpsampling Network (CAUN) for precise optical flow estimation; and (iii)\nKnowledge Distillation for VFI-centric Flow supervision (KDVCF) to supervise\nthe motion estimation of VFI model with VFI-centric teacher flows. The proposed\nVFI is called a Bidirectional Motion field-guided VFI (BiM-VFI) model.\nExtensive experiments show that our BiM-VFI model significantly surpasses the\nrecent state-of-the-art VFI methods by 26% and 45% improvements in LPIPS and\nSTLPIPS respectively, yielding interpolated frames with much fewer blurs at\narbitrary time instances.\n","authors":["Wonyong Seo","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2412.11365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11362v1","updated":"2024-12-16T01:28:04Z","published":"2024-12-16T01:28:04Z","title":"VRVVC: Variable-Rate NeRF-Based Volumetric Video Compression","summary":"  Neural Radiance Field (NeRF)-based volumetric video has revolutionized visual\nmedia by delivering photorealistic Free-Viewpoint Video (FVV) experiences that\nprovide audiences with unprecedented immersion and interactivity. However, the\nsubstantial data volumes pose significant challenges for storage and\ntransmission. Existing solutions typically optimize NeRF representation and\ncompression independently or focus on a single fixed rate-distortion (RD)\ntradeoff. In this paper, we propose VRVVC, a novel end-to-end joint\noptimization variable-rate framework for volumetric video compression that\nachieves variable bitrates using a single model while maintaining superior RD\nperformance. Specifically, VRVVC introduces a compact tri-plane implicit\nresidual representation for inter-frame modeling of long-duration dynamic\nscenes, effectively reducing temporal redundancy. We further propose a\nvariable-rate residual representation compression scheme that leverages a\nlearnable quantization and a tiny MLP-based entropy model. This approach\nenables variable bitrates through the utilization of predefined Lagrange\nmultipliers to manage the quantization error of all latent representations.\nFinally, we present an end-to-end progressive training strategy combined with a\nmulti-rate-distortion loss function to optimize the entire framework. Extensive\nexperiments demonstrate that VRVVC achieves a wide range of variable bitrates\nwithin a single model and surpasses the RD performance of existing methods\nacross various datasets.\n","authors":["Qiang Hu","Houqiang Zhong","Zihan Zheng","Xiaoyun Zhang","Zhengxue Cheng","Li Song","Guangtao Zhai","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11360v1","updated":"2024-12-16T01:23:13Z","published":"2024-12-16T01:23:13Z","title":"Visual IRL for Human-Like Robotic Manipulation","summary":"  We present a novel method for collaborative robots (cobots) to learn\nmanipulation tasks and perform them in a human-like manner. Our method falls\nunder the learn-from-observation (LfO) paradigm, where robots learn to perform\ntasks by observing human actions, which facilitates quicker integration into\nindustrial settings compared to programming from scratch. We introduce Visual\nIRL that uses the RGB-D keypoints in each frame of the observed human task\nperformance directly as state features, which are input to inverse\nreinforcement learning (IRL). The inversely learned reward function, which maps\nkeypoints to reward values, is transferred from the human to the cobot using a\nnovel neuro-symbolic dynamics model, which maps human kinematics to the cobot\narm. This model allows similar end-effector positioning while minimizing joint\nadjustments, aiming to preserve the natural dynamics of human motion in robotic\nmanipulation. In contrast with previous techniques that focus on end-effector\nplacement only, our method maps multiple joint angles of the human arm to the\ncorresponding cobot joints. Moreover, it uses an inverse kinematics model to\nthen minimally adjust the joint angles, for accurate end-effector positioning.\nWe evaluate the performance of this approach on two different realistic\nmanipulation tasks. The first task is produce processing, which involves\npicking, inspecting, and placing onions based on whether they are blemished.\nThe second task is liquid pouring, where the robot picks up bottles, pours the\ncontents into designated containers, and disposes of the empty bottles. Our\nresults demonstrate advances in human-like robotic manipulation, leading to\nmore human-robot compatibility in manufacturing applications.\n","authors":["Ehsan Asali","Prashant Doshi"],"pdf_url":"https://arxiv.org/pdf/2412.11360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.02703v3","updated":"2024-12-16T23:42:56Z","published":"2022-02-06T04:18:45Z","title":"Multi-modal Sensor Fusion for Auto Driving Perception: A Survey","summary":"  Multi-modal fusion is a fundamental task for the perception of an autonomous\ndriving system, which has recently intrigued many researchers. However,\nachieving a rather good performance is not an easy task due to the noisy raw\ndata, underutilized information, and the misalignment of multi-modal sensors.\nIn this paper, we provide a literature review of the existing multi-modal-based\nmethods for perception tasks in autonomous driving. Generally, we make a\ndetailed analysis including over 50 papers leveraging perception sensors\nincluding LiDAR and camera trying to solve object detection and semantic\nsegmentation tasks. Different from traditional fusion methodology for\ncategorizing fusion models, we propose an innovative way that divides them into\ntwo major classes, four minor classes by a more reasonable taxonomy in the view\nof the fusion stage. Moreover, we dive deep into the current fusion methods,\nfocusing on the remaining problems and open-up discussions on the potential\nresearch opportunities. In conclusion, what we expect to do in this paper is to\npresent a new taxonomy of multi-modal fusion methods for the autonomous driving\nperception tasks and provoke thoughts of the fusion-based techniques in the\nfuture.\n","authors":["Keli Huang","Botian Shi","Xiang Li","Xin Li","Siyuan Huang","Yikang Li"],"pdf_url":"https://arxiv.org/pdf/2202.02703v3.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2306.09301v5","updated":"2024-12-16T23:09:28Z","published":"2023-06-15T17:28:00Z","title":"OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection","summary":"  Out-of-Distribution (OOD) detection is critical for the reliable operation of\nopen-world intelligent systems. Despite the emergence of an increasing number\nof OOD detection methods, the evaluation inconsistencies present challenges for\ntracking the progress in this field. OpenOOD v1 initiated the unification of\nthe OOD detection evaluation but faced limitations in scalability and scope. In\nresponse, this paper presents OpenOOD v1.5, a significant improvement from its\npredecessor that ensures accurate and standardized evaluation of OOD detection\nmethodologies at large scale. Notably, OpenOOD v1.5 extends its evaluation\ncapabilities to large-scale data sets (ImageNet) and foundation models (e.g.,\nCLIP and DINOv2), and expands its scope to investigate full-spectrum OOD\ndetection which considers semantic and covariate distribution shifts at the\nsame time. This work also contributes in-depth analysis and insights derived\nfrom comprehensive experimental results, thereby enriching the knowledge pool\nof OOD detection methodologies. With these enhancements, OpenOOD v1.5 aims to\ndrive advancements and offer a more robust and comprehensive evaluation\nbenchmark for OOD detection research.\n","authors":["Jingyang Zhang","Jingkang Yang","Pengyun Wang","Haoqi Wang","Yueqian Lin","Haoran Zhang","Yiyou Sun","Xuefeng Du","Yixuan Li","Ziwei Liu","Yiran Chen","Hai Li"],"pdf_url":"https://arxiv.org/pdf/2306.09301v5.pdf","comment":"Accepted by DMLR. See code at https://github.com/Jingkang50/OpenOOD/\n  and leaderboard at https://zjysteven.github.io/OpenOOD/"},{"id":"http://arxiv.org/abs/2412.12392v1","updated":"2024-12-16T23:00:05Z","published":"2024-12-16T23:00:05Z","title":"MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors","summary":"  We present a real-time monocular dense SLAM system designed bottom-up from\nMASt3R, a two-view 3D reconstruction and matching prior. Equipped with this\nstrong prior, our system is robust on in-the-wild video sequences despite\nmaking no assumption on a fixed or parametric camera model beyond a unique\ncamera centre. We introduce efficient methods for pointmap matching, camera\ntracking and local fusion, graph construction and loop closure, and\nsecond-order global optimisation. With known calibration, a simple modification\nto the system achieves state-of-the-art performance across various benchmarks.\nAltogether, we propose a plug-and-play monocular SLAM system capable of\nproducing globally-consistent poses and dense geometry while operating at 15\nFPS.\n","authors":["Riku Murai","Eric Dexheimer","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2412.12392v1.pdf","comment":"The first two authors contributed equally to this work. Project Page:\n  https://edexheim.github.io/mast3r-slam/"},{"id":"http://arxiv.org/abs/2412.12391v1","updated":"2024-12-16T22:59:26Z","published":"2024-12-16T22:59:26Z","title":"Efficient Scaling of Diffusion Transformers for Text-to-Image Generation","summary":"  We empirically study the scaling properties of various Diffusion Transformers\n(DiTs) for text-to-image generation by performing extensive and rigorous\nablations, including training scaled DiTs ranging from 0.3B upto 8B parameters\non datasets up to 600M images. We find that U-ViT, a pure self-attention based\nDiT model provides a simpler design and scales more effectively in comparison\nwith cross-attention based DiT variants, which allows straightforward expansion\nfor extra conditions and other modalities. We identify a 2.3B U-ViT model can\nget better performance than SDXL UNet and other DiT variants in controlled\nsetting. On the data scaling side, we investigate how increasing dataset size\nand enhanced long caption improve the text-image alignment performance and the\nlearning efficiency.\n","authors":["Hao Li","Shamit Lal","Zhiheng Li","Yusheng Xie","Ying Wang","Yang Zou","Orchid Majumder","R. Manmatha","Zhuowen Tu","Stefano Ermon","Stefano Soatto","Ashwin Swaminathan"],"pdf_url":"https://arxiv.org/pdf/2412.12391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09624v2","updated":"2024-12-16T22:17:55Z","published":"2024-12-12T18:59:57Z","title":"GenEx: Generating an Explorable World","summary":"  Understanding, navigating, and exploring the 3D physical real world has long\nbeen a central challenge in the development of artificial intelligence. In this\nwork, we take a step toward this goal by introducing GenEx, a system capable of\nplanning complex embodied world exploration, guided by its generative\nimagination that forms priors (expectations) about the surrounding\nenvironments. GenEx generates an entire 3D-consistent imaginative environment\nfrom as little as a single RGB image, bringing it to life through panoramic\nvideo streams. Leveraging scalable 3D world data curated from Unreal Engine,\nour generative model is rounded in the physical world. It captures a continuous\n360-degree environment with little effort, offering a boundless landscape for\nAI agents to explore and interact with. GenEx achieves high-quality world\ngeneration, robust loop consistency over long trajectories, and demonstrates\nstrong 3D capabilities such as consistency and active 3D mapping. Powered by\ngenerative imagination of the world, GPT-assisted agents are equipped to\nperform complex embodied tasks, including both goal-agnostic exploration and\ngoal-driven navigation. These agents utilize predictive expectation regarding\nunseen parts of the physical world to refine their beliefs, simulate different\noutcomes based on potential decisions, and make more informed choices. In\nsummary, we demonstrate that GenEx provides a transformative platform for\nadvancing embodied AI in imaginative spaces and brings potential for extending\nthese capabilities to real-world exploration.\n","authors":["Taiming Lu","Tianmin Shu","Junfei Xiao","Luoxin Ye","Jiahao Wang","Cheng Peng","Chen Wei","Daniel Khashabi","Rama Chellappa","Alan Yuille","Jieneng Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09624v2.pdf","comment":"Website: GenEx.world"},{"id":"http://arxiv.org/abs/2311.07184v3","updated":"2024-12-16T21:43:18Z","published":"2023-11-13T09:19:14Z","title":"Cross-Axis Transformer with 3D Rotary Positional Embeddings","summary":"  Despite lagging behind their modal cousins in many respects, Vision\nTransformers have provided an interesting opportunity to bridge the gap between\nsequence modeling and image modeling. Up until now however, vision transformers\nhave largely been held back, due to both computational inefficiency, and lack\nof proper handling of spatial dimensions. In this paper, we introduce the\nCross-Axis Transformer. CAT is a model inspired by both Axial Transformers, and\nMicrosoft's recent Retentive Network, that drastically reduces the required\nnumber of floating point operations required to process an image, while\nsimultaneously converging faster and more accurately than the Vision\nTransformers it replaces.\n","authors":["Lily Erickson"],"pdf_url":"https://arxiv.org/pdf/2311.07184v3.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.05617v3","updated":"2024-12-16T21:35:54Z","published":"2024-08-10T19:31:21Z","title":"Residual-INR: Communication Efficient On-Device Learning Using Implicit\n  Neural Representation","summary":"  Edge computing is a distributed computing paradigm that collects and\nprocesses data at or near the source of data generation. The on-device learning\nat edge relies on device-to-device wireless communication to facilitate\nreal-time data sharing and collaborative decision-making among multiple\ndevices. This significantly improves the adaptability of the edge computing\nsystem to the changing environments. However, as the scale of the edge\ncomputing system is getting larger, communication among devices is becoming the\nbottleneck because of the limited bandwidth of wireless communication leads to\nlarge data transfer latency. To reduce the amount of device-to-device data\ntransmission and accelerate on-device learning, in this paper, we propose\nResidual-INR, a fog computing-based communication-efficient on-device learning\nframework by utilizing implicit neural representation (INR) to compress\nimages/videos into neural network weights. Residual-INR enhances data transfer\nefficiency by collecting JPEG images from edge devices, compressing them into\nINR format at the fog node, and redistributing them for on-device learning. By\nusing a smaller INR for full image encoding and a separate object INR for\nhigh-quality object region reconstruction through residual encoding, our\ntechnique can reduce the encoding redundancy while maintaining the object\nquality. Residual-INR is a promising solution for edge on-device learning\nbecause it reduces data transmission by up to 5.16 x across a network of 10\nedge devices. It also facilitates CPU-free accelerated on-device learning,\nachieving up to 2.9 x speedup without sacrificing accuracy. Our code is\navailable at: https://github.com/sharc-lab/Residual-INR.\n","authors":["Hanqiu Chen","Xuebin Yao","Pradeep Subedi","Cong Hao"],"pdf_url":"https://arxiv.org/pdf/2408.05617v3.pdf","comment":"This paper has been accepted by ICCAD 2024"}]},"2024-12-17T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.13194v1","updated":"2024-12-17T18:59:50Z","published":"2024-12-17T18:59:50Z","title":"Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation\n  Model Internet Agents","summary":"  The vision of a broadly capable and goal-directed agent, such as an\nInternet-browsing agent in the digital world and a household humanoid in the\nphysical world, has rapidly advanced, thanks to the generalization capability\nof foundation models. Such a generalist agent needs to have a large and diverse\nskill repertoire, such as finding directions between two travel locations and\nbuying specific items from the Internet. If each skill needs to be specified\nmanually through a fixed set of human-annotated instructions, the agent's skill\nrepertoire will necessarily be limited due to the quantity and diversity of\nhuman-annotated instructions. In this work, we address this challenge by\nproposing Proposer-Agent-Evaluator, an effective learning system that enables\nfoundation model agents to autonomously discover and practice skills in the\nwild. At the heart of PAE is a context-aware task proposer that autonomously\nproposes tasks for the agent to practice with context information of the\nenvironment such as user demos or even just the name of the website itself for\nInternet-browsing agents. Then, the agent policy attempts those tasks with\nthoughts and actual grounded operations in the real world with resulting\ntrajectories evaluated by an autonomous VLM-based success evaluator. The\nsuccess evaluation serves as the reward signal for the agent to refine its\npolicies through RL. We validate PAE on challenging vision-based web\nnavigation, using both real-world and self-hosted websites from WebVoyager and\nWebArena.To the best of our knowledge, this work represents the first effective\nlearning system to apply autonomous task proposal with RL for agents that\ngeneralizes real-world human-annotated benchmarks with SOTA performances. Our\nopen-source checkpoints and code can be found in https://yanqval.github.io/PAE/\n","authors":["Yifei Zhou","Qianlan Yang","Kaixiang Lin","Min Bai","Xiong Zhou","Yu-Xiong Wang","Sergey Levine","Erran Li"],"pdf_url":"https://arxiv.org/pdf/2412.13194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13195v1","updated":"2024-12-17T18:59:50Z","published":"2024-12-17T18:59:50Z","title":"CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion\n  Models","summary":"  Text-to-image diffusion models excel at generating photorealistic images, but\ncommonly struggle to render accurate spatial relationships described in text\nprompts. We identify two core issues underlying this common failure: 1) the\nambiguous nature of spatial-related data in existing datasets, and 2) the\ninability of current text encoders to accurately interpret the spatial\nsemantics of input descriptions. We address these issues with CoMPaSS, a\nversatile training framework that enhances spatial understanding of any T2I\ndiffusion model. CoMPaSS solves the ambiguity of spatial-related data with the\nSpatial Constraints-Oriented Pairing (SCOP) data engine, which curates\nspatially-accurate training data through a set of principled spatial\nconstraints. To better exploit the curated high-quality spatial priors, CoMPaSS\nfurther introduces a Token ENcoding ORdering (TENOR) module to allow better\nexploitation of high-quality spatial priors, effectively compensating for the\nshortcoming of text encoders. Extensive experiments on four popular open-weight\nT2I diffusion models covering both UNet- and MMDiT-based architectures\ndemonstrate the effectiveness of CoMPaSS by setting new state-of-the-arts with\nsubstantial relative gains across well-known benchmarks on spatial\nrelationships generation, including VISOR (+98%), T2I-CompBench Spatial (+67%),\nand GenEval Position (+131%). Code will be available at\nhttps://github.com/blurgyy/CoMPaSS.\n","authors":["Gaoyang Zhang","Bingtao Fu","Qingnan Fan","Qi Zhang","Runxing Liu","Hong Gu","Huaqi Zhang","Xinguo Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13195v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.13193v1","updated":"2024-12-17T18:59:46Z","published":"2024-12-17T18:59:46Z","title":"GaussTR: Foundation Model-Aligned Gaussian Transformer for\n  Self-Supervised 3D Spatial Understanding","summary":"  3D Semantic Occupancy Prediction is fundamental for spatial understanding as\nit provides a comprehensive semantic cognition of surrounding environments.\nHowever, prevalent approaches primarily rely on extensive labeled data and\ncomputationally intensive voxel-based modeling, restricting the scalability and\ngeneralizability of 3D representation learning. In this paper, we introduce\nGaussTR, a novel Gaussian Transformer that leverages alignment with foundation\nmodels to advance self-supervised 3D spatial understanding. GaussTR adopts a\nTransformer architecture to predict sparse sets of 3D Gaussians that represent\nscenes in a feed-forward manner. Through aligning rendered Gaussian features\nwith diverse knowledge from pre-trained foundation models, GaussTR facilitates\nthe learning of versatile 3D representations and enables open-vocabulary\noccupancy prediction without explicit annotations. Empirical evaluations on the\nOcc3D-nuScenes dataset showcase GaussTR's state-of-the-art zero-shot\nperformance, achieving 11.70 mIoU while reducing training duration by\napproximately 50%. These experimental results highlight the significant\npotential of GaussTR for scalable and holistic 3D spatial understanding, with\npromising implications for autonomous driving and embodied agents. Code is\navailable at https://github.com/hustvl/GaussTR.\n","authors":["Haoyi Jiang","Liu Liu","Tianheng Cheng","Xinjie Wang","Tianwei Lin","Zhizhong Su","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13190v1","updated":"2024-12-17T18:59:33Z","published":"2024-12-17T18:59:33Z","title":"MotionBridge: Dynamic Video Inbetweening with Flexible Controls","summary":"  By generating plausible and smooth transitions between two image frames,\nvideo inbetweening is an essential tool for video editing and long video\nsynthesis. Traditional works lack the capability to generate complex large\nmotions. While recent video generation techniques are powerful in creating\nhigh-quality results, they often lack fine control over the details of\nintermediate frames, which can lead to results that do not align with the\ncreative mind. We introduce MotionBridge, a unified video inbetweening\nframework that allows flexible controls, including trajectory strokes,\nkeyframes, masks, guide pixels, and text. However, learning such multi-modal\ncontrols in a unified framework is a challenging task. We thus design two\ngenerators to extract the control signal faithfully and encode feature through\ndual-branch embedders to resolve ambiguities. We further introduce a curriculum\ntraining strategy to smoothly learn various controls. Extensive qualitative and\nquantitative experiments have demonstrated that such multi-modal controls\nenable a more dynamic, customizable, and contextually accurate visual\nnarrative.\n","authors":["Maham Tanveer","Yang Zhou","Simon Niklaus","Ali Mahdavi Amiri","Hao Zhang","Krishna Kumar Singh","Nanxuan Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.13190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20309v4","updated":"2024-12-17T18:59:07Z","published":"2024-03-29T17:29:58Z","title":"InstantSplat: Sparse-view SfM-free Gaussian Splatting in Seconds","summary":"  While neural 3D reconstruction has advanced substantially, it typically\nrequires densely captured multi-view data with carefully initialized poses\n(e.g., using COLMAP). However, this requirement limits its broader\napplicability, as Structure-from-Motion (SfM) is often unreliable in\nsparse-view scenarios where feature matches are limited, resulting in\ncumulative errors. In this paper, we introduce InstantSplat, a novel and\nlightning-fast neural reconstruction system that builds accurate 3D\nrepresentations from as few as 2-3 images. InstantSplat adopts a\nself-supervised framework that bridges the gap between 2D images and 3D\nrepresentations using Gaussian Bundle Adjustment (GauBA) and can be optimized\nin an end-to-end manner. InstantSplat integrates dense stereo priors and\nco-visibility relationships between frames to initialize pixel-aligned geometry\nby progressively expanding the scene avoiding redundancy. Gaussian Bundle\nAdjustment is used to adapt both the scene representation and camera parameters\nquickly by minimizing gradient-based photometric error. Overall, InstantSplat\nachieves large-scale 3D reconstruction in mere seconds by reducing the required\nnumber of input views. It achieves an acceleration of over 20 times in\nreconstruction, improves visual quality (SSIM) from 0.3755 to 0.7624 than\nCOLMAP with 3D-GS, and is compatible with multiple 3D representations (3D-GS,\n2D-GS, and Mip-Splatting).\n","authors":["Zhiwen Fan","Kairun Wen","Wenyan Cong","Kevin Wang","Jian Zhang","Xinghao Ding","Danfei Xu","Boris Ivanovic","Marco Pavone","Georgios Pavlakos","Zhangyang Wang","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2403.20309v4.pdf","comment":"Project Page: https://instantsplat.github.io/"},{"id":"http://arxiv.org/abs/2412.13188v1","updated":"2024-12-17T18:58:55Z","published":"2024-12-17T18:58:55Z","title":"StreetCrafter: Street View Synthesis with Controllable Video Diffusion\n  Models","summary":"  This paper aims to tackle the problem of photorealistic view synthesis from\nvehicle sensor data. Recent advancements in neural scene representation have\nachieved notable success in rendering high-quality autonomous driving scenes,\nbut the performance significantly degrades as the viewpoint deviates from the\ntraining trajectory. To mitigate this problem, we introduce StreetCrafter, a\nnovel controllable video diffusion model that utilizes LiDAR point cloud\nrenderings as pixel-level conditions, which fully exploits the generative prior\nfor novel view synthesis, while preserving precise camera control. Moreover,\nthe utilization of pixel-level LiDAR conditions allows us to make accurate\npixel-level edits to target scenes. In addition, the generative prior of\nStreetCrafter can be effectively incorporated into dynamic scene\nrepresentations to achieve real-time rendering. Experiments on Waymo Open\nDataset and PandaSet demonstrate that our model enables flexible control over\nviewpoint changes, enlarging the view synthesis regions for satisfying\nrendering, which outperforms existing methods.\n","authors":["Yunzhi Yan","Zhen Xu","Haotong Lin","Haian Jin","Haoyu Guo","Yida Wang","Kun Zhan","Xianpeng Lang","Hujun Bao","Xiaowei Zhou","Sida Peng"],"pdf_url":"https://arxiv.org/pdf/2412.13188v1.pdf","comment":"Project page: https://zju3dv.github.io/street_crafter"},{"id":"http://arxiv.org/abs/2412.13187v1","updated":"2024-12-17T18:58:33Z","published":"2024-12-17T18:58:33Z","title":"HandsOnVLM: Vision-Language Models for Hand-Object Interaction\n  Prediction","summary":"  How can we predict future interaction trajectories of human hands in a scene\ngiven high-level colloquial task specifications in the form of natural\nlanguage? In this paper, we extend the classic hand trajectory prediction task\nto two tasks involving explicit or implicit language queries. Our proposed\ntasks require extensive understanding of human daily activities and reasoning\nabilities about what should be happening next given cues from the current\nscene. We also develop new benchmarks to evaluate the proposed two tasks,\nVanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). We\nenable solving these tasks by integrating high-level world knowledge and\nreasoning capabilities of Vision-Language Models (VLMs) with the\nauto-regressive nature of low-level ego-centric hand trajectories. Our model,\nHandsOnVLM is a novel VLM that can generate textual responses and produce\nfuture hand trajectories through natural-language conversations. Our\nexperiments show that HandsOnVLM outperforms existing task-specific methods and\nother VLM baselines on proposed tasks, and demonstrates its ability to\neffectively utilize world knowledge for reasoning about low-level human hand\ntrajectories based on the provided context. Our website contains code and\ndetailed video results \\url{https://www.chenbao.tech/handsonvlm/}\n","authors":["Chen Bao","Jiarui Xu","Xiaolong Wang","Abhinav Gupta","Homanga Bharadhwaj"],"pdf_url":"https://arxiv.org/pdf/2412.13187v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2412.13185v1","updated":"2024-12-17T18:58:07Z","published":"2024-12-17T18:58:07Z","title":"Move-in-2D: 2D-Conditioned Human Motion Generation","summary":"  Generating realistic human videos remains a challenging task, with the most\neffective methods currently relying on a human motion sequence as a control\nsignal. Existing approaches often use existing motion extracted from other\nvideos, which restricts applications to specific motion types and global scene\nmatching. We propose Move-in-2D, a novel approach to generate human motion\nsequences conditioned on a scene image, allowing for diverse motion that adapts\nto different scenes. Our approach utilizes a diffusion model that accepts both\na scene image and text prompt as inputs, producing a motion sequence tailored\nto the scene. To train this model, we collect a large-scale video dataset\nfeaturing single-human activities, annotating each video with the corresponding\nhuman motion as the target output. Experiments demonstrate that our method\neffectively predicts human motion that aligns with the scene image after\nprojection. Furthermore, we show that the generated motion sequence improves\nhuman motion quality in video synthesis tasks.\n","authors":["Hsin-Ping Huang","Yang Zhou","Jui-Hsien Wang","Difan Liu","Feng Liu","Ming-Hsuan Yang","Zhan Xu"],"pdf_url":"https://arxiv.org/pdf/2412.13185v1.pdf","comment":"Project page: https://hhsinping.github.io/Move-in-2D/"},{"id":"http://arxiv.org/abs/2412.13183v1","updated":"2024-12-17T18:57:38Z","published":"2024-12-17T18:57:38Z","title":"Real-time Free-view Human Rendering from Sparse-view RGB Videos using\n  Double Unprojected Textures","summary":"  Real-time free-view human rendering from sparse-view RGB inputs is a\nchallenging task due to the sensor scarcity and the tight time budget. To\nensure efficiency, recent methods leverage 2D CNNs operating in texture space\nto learn rendering primitives. However, they either jointly learn geometry and\nappearance, or completely ignore sparse image information for geometry\nestimation, significantly harming visual quality and robustness to unseen body\nposes. To address these issues, we present Double Unprojected Textures, which\nat the core disentangles coarse geometric deformation estimation from\nappearance synthesis, enabling robust and photorealistic 4K rendering in\nreal-time. Specifically, we first introduce a novel image-conditioned template\ndeformation network, which estimates the coarse deformation of the human\ntemplate from a first unprojected texture. This updated geometry is then used\nto apply a second and more accurate texture unprojection. The resulting texture\nmap has fewer artifacts and better alignment with input views, which benefits\nour learning of finer-level geometry and appearance represented by Gaussian\nsplats. We validate the effectiveness and efficiency of the proposed method in\nquantitative and qualitative experiments, which significantly surpasses other\nstate-of-the-art methods.\n","authors":["Guoxing Sun","Rishabh Dabral","Heming Zhu","Pascal Fua","Christian Theobalt","Marc Habermann"],"pdf_url":"https://arxiv.org/pdf/2412.13183v1.pdf","comment":"Project page: https://vcai.mpi-inf.mpg.de/projects/DUT/"},{"id":"http://arxiv.org/abs/2412.13180v1","updated":"2024-12-17T18:56:50Z","published":"2024-12-17T18:56:50Z","title":"Feather the Throttle: Revisiting Visual Token Pruning for\n  Vision-Language Model Acceleration","summary":"  Recent works on accelerating Vision-Language Models show that strong\nperformance can be maintained across a variety of vision-language tasks despite\nhighly compressing visual information. In this work, we examine the popular\nacceleration approach of early pruning of visual tokens inside the language\nmodel and find that its strong performance across many tasks is not due to an\nexceptional ability to compress visual information, but rather the benchmarks'\nlimited ability to assess fine-grained visual capabilities. Namely, we\ndemonstrate a core issue with the acceleration approach where most tokens\ntowards the top of the image are pruned away. Yet, this issue is only reflected\nin performance for a small subset of tasks such as localization. For the other\nevaluated tasks, strong performance is maintained with the flawed pruning\nstrategy. Noting the limited visual capabilities of the studied acceleration\ntechnique, we propose FEATHER (Fast and Effective Acceleration wiTH Ensemble\ncRiteria), a straightforward approach that (1) resolves the identified issue\nwith early-layer pruning, (2) incorporates uniform sampling to ensure coverage\nacross all image regions, and (3) applies pruning in two stages to allow the\ncriteria to become more effective at a later layer while still achieving\nsignificant speedup through early-layer pruning. With comparable computational\nsavings, we find that FEATHER has more than $5\\times$ performance improvement\non the vision-centric localization benchmarks compared to the original\nacceleration approach.\n","authors":["Mark Endo","Xiaohan Wang","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2412.13180v1.pdf","comment":"Project page:\n  https://web.stanford.edu/~markendo/projects/feather.html"},{"id":"http://arxiv.org/abs/2412.13176v1","updated":"2024-12-17T18:54:28Z","published":"2024-12-17T18:54:28Z","title":"NFL-BA: Improving Endoscopic SLAM with Near-Field Light Bundle\n  Adjustment","summary":"  Simultaneous Localization And Mapping (SLAM) from a monocular endoscopy video\ncan enable autonomous navigation, guidance to unsurveyed regions, and 3D\nvisualizations, which can significantly improve endoscopy experience for\nsurgeons and patient outcomes. Existing dense SLAM algorithms often assume\ndistant and static lighting and textured surfaces, and alternate between\noptimizing scene geometry and camera parameters by minimizing a photometric\nrendering loss, often called Photometric Bundle Adjustment. However, endoscopic\nenvironments exhibit dynamic near-field lighting due to the co-located light\nand camera moving extremely close to the surface, textureless surfaces, and\nstrong specular reflections due to mucus layers. When not considered, these\nnear-field lighting effects can cause significant performance reductions for\nexisting SLAM algorithms from indoor/outdoor scenes when applied to endoscopy\nvideos. To mitigate this problem, we introduce a new Near-Field Lighting Bundle\nAdjustment Loss $(L_{NFL-BA})$ that can also be alternatingly optimized, along\nwith the Photometric Bundle Adjustment loss, such that the captured images'\nintensity variations match the relative distance and orientation between the\nsurface and the co-located light and camera. We derive a general NFL-BA loss\nfunction for 3D Gaussian surface representations and demonstrate that adding\n$L_{NFL-BA}$ can significantly improve the tracking and mapping performance of\ntwo state-of-the-art 3DGS-SLAM systems, MonoGS (35% improvement in tracking,\n48% improvement in mapping with predicted depth maps) and EndoGSLAM (22%\nimprovement in tracking, marginal improvement in mapping with predicted\ndepths), on the C3VD endoscopy dataset for colons. The project page is\navailable at https://asdunnbe.github.io/NFL-BA/\n","authors":["Andrea Dunn Beltran","Daniel Rho","Marc Niethammer","Roni Sengupta"],"pdf_url":"https://arxiv.org/pdf/2412.13176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13174v1","updated":"2024-12-17T18:53:43Z","published":"2024-12-17T18:53:43Z","title":"ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark\n  Detection","summary":"  Although facial landmark detection (FLD) has gained significant progress,\nexisting FLD methods still suffer from performance drops on partially\nnon-visible faces, such as faces with occlusions or under extreme lighting\nconditions or poses. To address this issue, we introduce ORFormer, a novel\ntransformer-based method that can detect non-visible regions and recover their\nmissing features from visible parts. Specifically, ORFormer associates each\nimage patch token with one additional learnable token called the messenger\ntoken. The messenger token aggregates features from all but its patch. This\nway, the consensus between a patch and other patches can be assessed by\nreferring to the similarity between its regular and messenger embeddings,\nenabling non-visible region identification. Our method then recovers occluded\npatches with features aggregated by the messenger tokens. Leveraging the\nrecovered features, ORFormer compiles high-quality heatmaps for the downstream\nFLD task. Extensive experiments show that our method generates heatmaps\nresilient to partial occlusions. By integrating the resultant heatmaps into\nexisting FLD methods, our method performs favorably against the state of the\narts on challenging datasets such as WFLW and COFW.\n","authors":["Jui-Che Chiang","Hou-Ning Hu","Bo-Syuan Hou","Chia-Yu Tseng","Yu-Lun Liu","Min-Hung Chen","Yen-Yu Lin"],"pdf_url":"https://arxiv.org/pdf/2412.13174v1.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2412.13173v1","updated":"2024-12-17T18:52:30Z","published":"2024-12-17T18:52:30Z","title":"Locate n' Rotate: Two-stage Openable Part Detection with Foundation\n  Model Priors","summary":"  Detecting the openable parts of articulated objects is crucial for downstream\napplications in intelligent robotics, such as pulling a drawer. This task poses\na multitasking challenge due to the necessity of understanding object\ncategories and motion. Most existing methods are either category-specific or\ntrained on specific datasets, lacking generalization to unseen environments and\nobjects. In this paper, we propose a Transformer-based Openable Part Detection\n(OPD) framework named Multi-feature Openable Part Detection (MOPD) that\nincorporates perceptual grouping and geometric priors, outperforming previous\nmethods in performance. In the first stage of the framework, we introduce a\nperceptual grouping feature model that provides perceptual grouping feature\npriors for openable part detection, enhancing detection results through a\ncross-attention mechanism. In the second stage, a geometric understanding\nfeature model offers geometric feature priors for predicting motion parameters.\nCompared to existing methods, our proposed approach shows better performance in\nboth detection and motion parameter prediction. Codes and models are publicly\navailable at https://github.com/lisiqi-zju/MOPD\n","authors":["Siqi Li","Xiaoxue Chen","Haoyu Cheng","Guyue Zhou","Hao Zhao","Guanzhong Tian"],"pdf_url":"https://arxiv.org/pdf/2412.13173v1.pdf","comment":"ACCV 2024 Oral, Project: https://github.com/lisiqi-zju/MOPD"},{"id":"http://arxiv.org/abs/2412.12095v2","updated":"2024-12-17T18:45:55Z","published":"2024-12-16T18:59:29Z","title":"Causal Diffusion Transformers for Generative Modeling","summary":"  We introduce Causal Diffusion as the autoregressive (AR) counterpart of\nDiffusion models. It is a next-token(s) forecasting framework that is friendly\nto both discrete and continuous modalities and compatible with existing\nnext-token prediction models like LLaMA and GPT. While recent works attempt to\ncombine diffusion with AR models, we show that introducing sequential\nfactorization to a diffusion model can substantially improve its performance\nand enables a smooth transition between AR and diffusion generation modes.\nHence, we propose CausalFusion - a decoder-only transformer that\ndual-factorizes data across sequential tokens and diffusion noise levels,\nleading to state-of-the-art results on the ImageNet generation benchmark while\nalso enjoying the AR advantage of generating an arbitrary number of tokens for\nin-context reasoning. We further demonstrate CausalFusion's multimodal\ncapabilities through a joint image generation and captioning model, and\nshowcase CausalFusion's ability for zero-shot in-context image manipulations.\nWe hope that this work could provide the community with a fresh perspective on\ntraining multimodal models over discrete and continuous data.\n","authors":["Chaorui Deng","Deyao Zhu","Kunchang Li","Shi Guang","Haoqi Fan"],"pdf_url":"https://arxiv.org/pdf/2412.12095v2.pdf","comment":"22 figures, 21 pages"},{"id":"http://arxiv.org/abs/2412.13168v1","updated":"2024-12-17T18:45:53Z","published":"2024-12-17T18:45:53Z","title":"Lifting Scheme-Based Implicit Disentanglement of Emotion-Related Facial\n  Dynamics in the Wild","summary":"  In-the-wild Dynamic facial expression recognition (DFER) encounters a\nsignificant challenge in recognizing emotion-related expressions, which are\noften temporally and spatially diluted by emotion-irrelevant expressions and\nglobal context respectively. Most of the prior DFER methods model tightly\ncoupled spatiotemporal representations which may incorporate weakly relevant\nfeatures, leading to information redundancy and emotion-irrelevant context\nbias. Several DFER methods have highlighted the significance of dynamic\ninformation, but utilize explicit manners to extract dynamic features with\noverly strong prior knowledge. In this paper, we propose a novel Implicit\nFacial Dynamics Disentanglement framework (IFDD). Through expanding wavelet\nlifting scheme to fully learnable framework, IFDD disentangles emotion-related\ndynamic information from emotion-irrelevant global context in an implicit\nmanner, i.e., without exploit operations and external guidance. The\ndisentanglement process of IFDD contains two stages, i.e., Inter-frame\nStatic-dynamic Splitting Module (ISSM) for rough disentanglement estimation and\nLifting-based Aggregation-Disentanglement Module (LADM) for further refinement.\nSpecifically, ISSM explores inter-frame correlation to generate content-aware\nsplitting indexes on-the-fly. We preliminarily utilize these indexes to split\nframe features into two groups, one with greater global similarity, and the\nother with more unique dynamic features. Subsequently, LADM first aggregates\nthese two groups of features to obtain fine-grained global context features by\nan updater, and then disentangles emotion-related facial dynamic features from\nthe global context by a predictor. Extensive experiments on in-the-wild\ndatasets have demonstrated that IFDD outperforms prior supervised DFER methods\nwith higher recognition accuracy and comparable efficiency.\n","authors":["Xingjian Wang","Li Chai"],"pdf_url":"https://arxiv.org/pdf/2412.13168v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.13161v1","updated":"2024-12-17T18:39:10Z","published":"2024-12-17T18:39:10Z","title":"BanglishRev: A Large-Scale Bangla-English and Code-mixed Dataset of\n  Product Reviews in E-Commerce","summary":"  This work presents the BanglishRev Dataset, the largest e-commerce product\nreview dataset to date for reviews written in Bengali, English, a mixture of\nboth and Banglish, Bengali words written with English alphabets. The dataset\ncomprises of 1.74 million written reviews from 3.2 million ratings information\ncollected from a total of 128k products being sold in online e-commerce\nplatforms targeting the Bengali population. It includes an extensive array of\nrelated metadata for each of the reviews including the rating given by the\nreviewer, date the review was posted and date of purchase, number of likes,\ndislikes, response from the seller, images associated with the review etc. With\nsentiment analysis being the most prominent usage of review datasets,\nexperimentation with a binary sentiment analysis model with the review rating\nserving as an indicator of positive or negative sentiment was conducted to\nevaluate the effectiveness of the large amount of data presented in BanglishRev\nfor sentiment analysis tasks. A BanglishBERT model is trained on the data from\nBanglishRev with reviews being considered labeled positive if the rating is\ngreater than 3 and negative if the rating is less than or equal to 3. The model\nis evaluated by being testing against a previously published manually annotated\ndataset for e-commerce reviews written in a mixture of Bangla, English and\nBanglish. The experimental model achieved an exceptional accuracy of 94\\% and\nF1 score of 0.94, demonstrating the dataset's efficacy for sentiment analysis.\nSome of the intriguing patterns and observations seen within the dataset and\nfuture research directions where the dataset can be utilized is also discussed\nand explored. The dataset can be accessed through\nhttps://huggingface.co/datasets/BanglishRev/bangla-english-and-code-mixed-ecommerce-review-dataset.\n","authors":["Mohammad Nazmush Shamael","Sabila Nawshin","Swakkhar Shatabda","Salekul Islam"],"pdf_url":"https://arxiv.org/pdf/2412.13161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03665v3","updated":"2024-12-17T18:39:00Z","published":"2024-10-04T17:59:57Z","title":"Estimating Body and Hand Motion in an Ego-sensed World","summary":"  We present EgoAllo, a system for human motion estimation from a head-mounted\ndevice. Using only egocentric SLAM poses and images, EgoAllo guides sampling\nfrom a conditional diffusion model to estimate 3D body pose, height, and hand\nparameters that capture a device wearer's actions in the allocentric coordinate\nframe of the scene. To achieve this, our key insight is in representation: we\npropose spatial and temporal invariance criteria for improving model\nperformance, from which we derive a head motion conditioning parameterization\nthat improves estimation by up to 18%. We also show how the bodies estimated by\nour system can improve hand estimation: the resulting kinematic and temporal\nconstraints can reduce world-frame errors in single-frame estimates by 40%.\nProject page: https://egoallo.github.io/\n","authors":["Brent Yi","Vickie Ye","Maya Zheng","Yunqi Li","Lea Müller","Georgios Pavlakos","Yi Ma","Jitendra Malik","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2410.03665v3.pdf","comment":"Project page: https://egoallo.github.io/"},{"id":"http://arxiv.org/abs/2412.13156v1","updated":"2024-12-17T18:30:22Z","published":"2024-12-17T18:30:22Z","title":"S2S2: Semantic Stacking for Robust Semantic Segmentation in Medical\n  Imaging","summary":"  Robustness and generalizability in medical image segmentation are often\nhindered by scarcity and limited diversity of training data, which stands in\ncontrast to the variability encountered during inference. While conventional\nstrategies -- such as domain-specific augmentation, specialized architectures,\nand tailored training procedures -- can alleviate these issues, they depend on\nthe availability and reliability of domain knowledge. When such knowledge is\nunavailable, misleading, or improperly applied, performance may deteriorate. In\nresponse, we introduce a novel, domain-agnostic, add-on, and data-driven\nstrategy inspired by image stacking in image denoising. Termed ``semantic\nstacking,'' our method estimates a denoised semantic representation that\ncomplements the conventional segmentation loss during training. This method\ndoes not depend on domain-specific assumptions, making it broadly applicable\nacross diverse image modalities, model architectures, and augmentation\ntechniques. Through extensive experiments, we validate the superiority of our\napproach in improving segmentation performance under diverse conditions. Code\nis available at https://github.com/ymp5078/Semantic-Stacking.\n","authors":["Yimu Pan","Sitao Zhang","Alison D. Gernand","Jeffery A. Goldstein","James Z. Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13156v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.13155v1","updated":"2024-12-17T18:28:48Z","published":"2024-12-17T18:28:48Z","title":"F-Bench: Rethinking Human Preference Evaluation Metrics for Benchmarking\n  Face Generation, Customization, and Restoration","summary":"  Artificial intelligence generative models exhibit remarkable capabilities in\ncontent creation, particularly in face image generation, customization, and\nrestoration. However, current AI-generated faces (AIGFs) often fall short of\nhuman preferences due to unique distortions, unrealistic details, and\nunexpected identity shifts, underscoring the need for a comprehensive quality\nevaluation framework for AIGFs. To address this need, we introduce FaceQ, a\nlarge-scale, comprehensive database of AI-generated Face images with\nfine-grained Quality annotations reflecting human preferences. The FaceQ\ndatabase comprises 12,255 images generated by 29 models across three tasks: (1)\nface generation, (2) face customization, and (3) face restoration. It includes\n32,742 mean opinion scores (MOSs) from 180 annotators, assessed across multiple\ndimensions: quality, authenticity, identity (ID) fidelity, and text-image\ncorrespondence. Using the FaceQ database, we establish F-Bench, a benchmark for\ncomparing and evaluating face generation, customization, and restoration\nmodels, highlighting strengths and weaknesses across various prompts and\nevaluation dimensions. Additionally, we assess the performance of existing\nimage quality assessment (IQA), face quality assessment (FQA), AI-generated\ncontent image quality assessment (AIGCIQA), and preference evaluation metrics,\nmanifesting that these standard metrics are relatively ineffective in\nevaluating authenticity, ID fidelity, and text-image correspondence. The FaceQ\ndatabase will be publicly available upon publication.\n","authors":["Lu Liu","Huiyu Duan","Qiang Hu","Liu Yang","Chunlei Cai","Tianxiao Ye","Huayu Liu","Xiaoyun Zhang","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2412.13155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13152v1","updated":"2024-12-17T18:23:33Z","published":"2024-12-17T18:23:33Z","title":"Continuous Patient Monitoring with AI: Real-Time Analysis of Video in\n  Hospital Care Settings","summary":"  This study introduces an AI-driven platform for continuous and passive\npatient monitoring in hospital settings, developed by LookDeep Health.\nLeveraging advanced computer vision, the platform provides real-time insights\ninto patient behavior and interactions through video analysis, securely storing\ninference results in the cloud for retrospective evaluation. The dataset,\ncompiled in collaboration with 11 hospital partners, encompasses over 300\nhigh-risk fall patients and over 1,000 days of inference, enabling applications\nsuch as fall detection and safety monitoring for vulnerable patient\npopulations. To foster innovation and reproducibility, an anonymized subset of\nthis dataset is publicly available. The AI system detects key components in\nhospital rooms, including individual presence and role, furniture location,\nmotion magnitude, and boundary crossings. Performance evaluation demonstrates\nstrong accuracy in object detection (macro F1-score = 0.92) and patient-role\nclassification (F1-score = 0.98), as well as reliable trend analysis for the\n\"patient alone\" metric (mean logistic regression accuracy = 0.82 \\pm 0.15).\nThese capabilities enable automated detection of patient isolation, wandering,\nor unsupervised movement-key indicators for fall risk and other adverse events.\nThis work establishes benchmarks for validating AI-driven patient monitoring\nsystems, highlighting the platform's potential to enhance patient safety and\ncare by providing continuous, data-driven insights into patient behavior and\ninteractions.\n","authors":["Paolo Gabriel","Peter Rehani","Tyler Troy","Tiffany Wyatt","Michael Choma","Narinder Singh"],"pdf_url":"https://arxiv.org/pdf/2412.13152v1.pdf","comment":"21 pages, 9 figures, 3 tables, submitted to Frontiers in Imaging >\n  Imaging Applications > (Research Topic) Deep Learning for Medical Imaging\n  Applications for publication"},{"id":"http://arxiv.org/abs/2412.13140v1","updated":"2024-12-17T18:06:28Z","published":"2024-12-17T18:06:28Z","title":"Label Errors in the Tobacco3482 Dataset","summary":"  Tobacco3482 is a widely used document classification benchmark dataset.\nHowever, our manual inspection of the entire dataset uncovers widespread\nontological issues, especially large amounts of annotation label problems in\nthe dataset. We establish data label guidelines and find that 11.7% of the\ndataset is improperly annotated and should either have an unknown label or a\ncorrected label, and 16.7% of samples in the dataset have multiple valid\nlabels. We then analyze the mistakes of a top-performing model and find that\n35% of the model's mistakes can be directly attributed to these label issues,\nhighlighting the inherent problems with using a noisily labeled dataset as a\nbenchmark. Supplementary material, including dataset annotations and code, is\navailable at https://github.com/gordon-lim/tobacco3482-mistakes/.\n","authors":["Gordon Lim","Stefan Larson","Kevin Leach"],"pdf_url":"https://arxiv.org/pdf/2412.13140v1.pdf","comment":"WACV VisionDocs Workshop 2025"},{"id":"http://arxiv.org/abs/2412.13137v1","updated":"2024-12-17T18:04:33Z","published":"2024-12-17T18:04:33Z","title":"Unlocking the Potential of Digital Pathology: Novel Baselines for\n  Compression","summary":"  Digital pathology offers a groundbreaking opportunity to transform clinical\npractice in histopathological image analysis, yet faces a significant hurdle:\nthe substantial file sizes of pathological Whole Slide Images (WSI). While\ncurrent digital pathology solutions rely on lossy JPEG compression to address\nthis issue, lossy compression can introduce color and texture disparities,\npotentially impacting clinical decision-making. While prior research addresses\nperceptual image quality and downstream performance independently of each\nother, we jointly evaluate compression schemes for perceptual and downstream\ntask quality on four different datasets. In addition, we collect an initially\nuncompressed dataset for an unbiased perceptual evaluation of compression\nschemes. Our results show that deep learning models fine-tuned for perceptual\nquality outperform conventional compression schemes like JPEG-XL or WebP for\nfurther compression of WSI. However, they exhibit a significant bias towards\nthe compression artifacts present in the training data and struggle to\ngeneralize across various compression schemes. We introduce a novel evaluation\nmetric based on feature similarity between original files and compressed files\nthat aligns very well with the actual downstream performance on the compressed\nWSI. Our metric allows for a general and standardized evaluation of lossy\ncompression schemes and mitigates the requirement to independently assess\ndifferent downstream tasks. Our study provides novel insights for the\nassessment of lossy compression schemes for WSI and encourages a unified\nevaluation of lossy compression schemes to accelerate the clinical uptake of\ndigital pathology.\n","authors":["Maximilian Fischer","Peter Neher","Peter Schüffler","Sebastian Ziegler","Shuhan Xiao","Robin Peretzke","David Clunie","Constantin Ulrich","Michael Baumgartner","Alexander Muckenhuber","Silvia Dias Almeida","Michael Götz","Jens Kleesiek","Marco Nolden","Rickmer Braren","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2412.13137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13126v1","updated":"2024-12-17T17:45:21Z","published":"2024-12-17T17:45:21Z","title":"A Knowledge-enhanced Pathology Vision-language Foundation Model for\n  Cancer Diagnosis","summary":"  Deep learning has enabled the development of highly robust foundation models\nfor various pathological tasks across diverse diseases and patient cohorts.\nAmong these models, vision-language pre-training, which leverages large-scale\npaired data to align pathology image and text embedding spaces, and provides a\nnovel zero-shot paradigm for downstream tasks. However, existing models have\nbeen primarily data-driven and lack the incorporation of domain-specific\nknowledge, which limits their performance in cancer diagnosis, especially for\nrare tumor subtypes. To address this limitation, we establish a\nKnowledge-enhanced Pathology (KEEP) foundation model that harnesses disease\nknowledge to facilitate vision-language pre-training. Specifically, we first\nconstruct a disease knowledge graph (KG) that covers 11,454 human diseases with\n139,143 disease attributes, including synonyms, definitions, and hypernym\nrelations. We then systematically reorganize the millions of publicly available\nnoisy pathology image-text pairs, into 143K well-structured semantic groups\nlinked through the hierarchical relations of the disease KG. To derive more\nnuanced image and text representations, we propose a novel knowledge-enhanced\nvision-language pre-training approach that integrates disease knowledge into\nthe alignment within hierarchical semantic groups instead of unstructured\nimage-text pairs. Validated on 18 diverse benchmarks with more than 14,000\nwhole slide images (WSIs), KEEP achieves state-of-the-art performance in\nzero-shot cancer diagnostic tasks. Notably, for cancer detection, KEEP\ndemonstrates an average sensitivity of 89.8% at a specificity of 95.0% across 7\ncancer types. For cancer subtyping, KEEP achieves a median balanced accuracy of\n0.456 in subtyping 30 rare brain cancers, indicating strong generalizability\nfor diagnosing rare tumors.\n","authors":["Xiao Zhou","Luoyi Sun","Dexuan He","Wenbin Guan","Ruifen Wang","Lifeng Wang","Xin Sun","Kun Sun","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2412.13126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05271v3","updated":"2024-12-17T17:44:38Z","published":"2024-12-06T18:57:08Z","title":"Expanding Performance Boundaries of Open-Source Multimodal Models with\n  Model, Data, and Test-Time Scaling","summary":"  We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)\nseries that builds upon InternVL 2.0, maintaining its core model architecture\nwhile introducing significant enhancements in training and testing strategies\nas well as data quality. In this work, we delve into the relationship between\nmodel scaling and performance, systematically exploring the performance trends\nin vision encoders, language models, dataset sizes, and test-time\nconfigurations. Through extensive evaluations on a wide range of benchmarks,\nincluding multi-discipline reasoning, document understanding, multi-image /\nvideo understanding, real-world comprehension, multimodal hallucination\ndetection, visual grounding, multilingual capabilities, and pure language\nprocessing, InternVL 2.5 exhibits competitive performance, rivaling leading\ncommercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is\nthe first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a\n3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing\nstrong potential for test-time scaling. We hope this model contributes to the\nopen-source community by setting new standards for developing and applying\nmultimodal AI systems. HuggingFace demo see\nhttps://huggingface.co/spaces/OpenGVLab/InternVL\n","authors":["Zhe Chen","Weiyun Wang","Yue Cao","Yangzhou Liu","Zhangwei Gao","Erfei Cui","Jinguo Zhu","Shenglong Ye","Hao Tian","Zhaoyang Liu","Lixin Gu","Xuehui Wang","Qingyun Li","Yimin Ren","Zixuan Chen","Jiapeng Luo","Jiahao Wang","Tan Jiang","Bo Wang","Conghui He","Botian Shi","Xingcheng Zhang","Han Lv","Yi Wang","Wenqi Shao","Pei Chu","Zhongying Tu","Tong He","Zhiyong Wu","Huipeng Deng","Jiaye Ge","Kai Chen","Min Dou","Lewei Lu","Xizhou Zhu","Tong Lu","Dahua Lin","Yu Qiao","Jifeng Dai","Wenhai Wang"],"pdf_url":"https://arxiv.org/pdf/2412.05271v3.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.13111v1","updated":"2024-12-17T17:34:52Z","published":"2024-12-17T17:34:52Z","title":"Motion-2-to-3: Leveraging 2D Motion Data to Boost 3D Motion Generation","summary":"  Text-driven human motion synthesis is capturing significant attention for its\nability to effortlessly generate intricate movements from abstract text cues,\nshowcasing its potential for revolutionizing motion design not only in film\nnarratives but also in virtual reality experiences and computer game\ndevelopment. Existing methods often rely on 3D motion capture data, which\nrequire special setups resulting in higher costs for data acquisition,\nultimately limiting the diversity and scope of human motion. In contrast, 2D\nhuman videos offer a vast and accessible source of motion data, covering a\nwider range of styles and activities. In this paper, we explore leveraging 2D\nhuman motion extracted from videos as an alternative data source to improve\ntext-driven 3D motion generation. Our approach introduces a novel framework\nthat disentangles local joint motion from global movements, enabling efficient\nlearning of local motion priors from 2D data. We first train a single-view 2D\nlocal motion generator on a large dataset of text-motion pairs. To enhance this\nmodel to synthesize 3D motion, we fine-tune the generator with 3D data,\ntransforming it into a multi-view generator that predicts view-consistent local\njoint motion and root dynamics. Experiments on the HumanML3D dataset and novel\ntext prompts demonstrate that our method efficiently utilizes 2D data,\nsupporting realistic 3D human motion generation and broadening the range of\nmotion types it supports. Our code will be made publicly available at\nhttps://zju3dv.github.io/Motion-2-to-3/.\n","authors":["Huaijin Pi","Ruoxi Guo","Zehong Shen","Qing Shuai","Zechen Hu","Zhumei Wang","Yajiao Dong","Ruizhen Hu","Taku Komura","Sida Peng","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.13111v1.pdf","comment":"Project page: https://zju3dv.github.io/Motion-2-to-3/"},{"id":"http://arxiv.org/abs/2412.13099v1","updated":"2024-12-17T17:10:02Z","published":"2024-12-17T17:10:02Z","title":"Accuracy Limits as a Barrier to Biometric System Security","summary":"  Biometric systems are widely used for identity verification and\nidentification, including authentication (i.e., one-to-one matching to verify a\nclaimed identity) and identification (i.e., one-to-many matching to find a\nsubject in a database). The matching process relies on measuring similarities\nor dissimilarities between a fresh biometric template and enrolled templates.\nThe False Match Rate FMR is a key metric for assessing the accuracy and\nreliability of such systems. This paper analyzes biometric systems based on\ntheir FMR, with two main contributions. First, we explore untargeted attacks,\nwhere an adversary aims to impersonate any user within a database. We determine\nthe number of trials required for an attacker to successfully impersonate a\nuser and derive the critical population size (i.e., the maximum number of users\nin the database) required to maintain a given level of security. Furthermore,\nwe compute the critical FMR value needed to ensure resistance against\nuntargeted attacks as the database size increases. Second, we revisit the\nbiometric birthday problem to evaluate the approximate and exact probabilities\nthat two users in a database collide (i.e., can impersonate each other). Based\non this analysis, we derive both the approximate critical population size and\nthe critical FMR value needed to bound the likelihood of such collisions\noccurring with a given probability. These thresholds offer insights for\ndesigning systems that mitigate the risk of impersonation and collisions,\nparticularly in large-scale biometric databases. Our findings indicate that\ncurrent biometric systems fail to deliver sufficient accuracy to achieve an\nadequate security level against untargeted attacks, even in small-scale\ndatabases. Moreover, state-of-the-art systems face significant challenges in\naddressing the biometric birthday problem, especially as database sizes grow.\n","authors":["Axel Durbet","Paul-Marie Grollemund","Pascal Lafourcade","Kevin Thiry-Atighehchi"],"pdf_url":"https://arxiv.org/pdf/2412.13099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13096v1","updated":"2024-12-17T17:06:33Z","published":"2024-12-17T17:06:33Z","title":"Incremental Online Learning of Randomized Neural Network with Forward\n  Regularization","summary":"  Online learning of deep neural networks suffers from challenges such as\nhysteretic non-incremental updating, increasing memory usage, past\nretrospective retraining, and catastrophic forgetting. To alleviate these\ndrawbacks and achieve progressive immediate decision-making, we propose a novel\nIncremental Online Learning (IOL) process of Randomized Neural Networks\n(Randomized NN), a framework facilitating continuous improvements to Randomized\nNN performance in restrictive online scenarios. Within the framework, we\nfurther introduce IOL with ridge regularization (-R) and IOL with forward\nregularization (-F). -R generates stepwise incremental updates without\nretrospective retraining and avoids catastrophic forgetting. Moreover, we\nsubstituted -R with -F as it enhanced precognition learning ability using\nsemi-supervision and realized better online regrets to offline global experts\ncompared to -R during IOL. The algorithms of IOL for Randomized NN with -R/-F\non non-stationary batch stream were derived respectively, featuring recursive\nweight updates and variable learning rates. Additionally, we conducted a\ndetailed analysis and theoretically derived relative cumulative regret bounds\nof the Randomized NN learners with -R/-F in IOL under adversarial assumptions\nusing a novel methodology and presented several corollaries, from which we\nobserved the superiority on online learning acceleration and regret bounds of\nemploying -F in IOL. Finally, our proposed methods were rigorously examined\nacross regression and classification tasks on diverse datasets, which\ndistinctly validated the efficacy of IOL frameworks of Randomized NN and the\nadvantages of forward regularization.\n","authors":["Junda Wang","Minghui Hu","Ning Li","Abdulaziz Al-Ali","Ponnuthurai Nagaratnam Suganthan"],"pdf_url":"https://arxiv.org/pdf/2412.13096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13081v1","updated":"2024-12-17T16:54:05Z","published":"2024-12-17T16:54:05Z","title":"Prompt Augmentation for Self-supervised Text-guided Image Manipulation","summary":"  Text-guided image editing finds applications in various creative and\npractical fields. While recent studies in image generation have advanced the\nfield, they often struggle with the dual challenges of coherent image\ntransformation and context preservation. In response, our work introduces\nprompt augmentation, a method amplifying a single input prompt into several\ntarget prompts, strengthening textual context and enabling localised image\nediting. Specifically, we use the augmented prompts to delineate the intended\nmanipulation area. We propose a Contrastive Loss tailored to driving effective\nimage editing by displacing edited areas and drawing preserved regions closer.\nAcknowledging the continuous nature of image manipulations, we further refine\nour approach by incorporating the similarity concept, creating a Soft\nContrastive Loss. The new losses are incorporated to the diffusion model,\ndemonstrating improved or competitive image editing results on public datasets\nand generated images over state-of-the-art approaches.\n","authors":["Rumeysa Bodur","Binod Bhattarai","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2412.13081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13079v1","updated":"2024-12-17T16:51:44Z","published":"2024-12-17T16:51:44Z","title":"Identifying Bias in Deep Neural Networks Using Image Transforms","summary":"  CNNs have become one of the most commonly used computational tool in the past\ntwo decades. One of the primary downsides of CNNs is that they work as a\n``black box\", where the user cannot necessarily know how the image data are\nanalyzed, and therefore needs to rely on empirical evaluation to test the\nefficacy of a trained CNN. This can lead to hidden biases that affect the\nperformance evaluation of neural networks, but are difficult to identify. Here\nwe discuss examples of such hidden biases in common and widely used benchmark\ndatasets, and propose techniques for identifying dataset biases that can affect\nthe standard performance evaluation metrics. One effective approach to identify\ndataset bias is to perform image classification by using merely blank\nbackground parts of the original images. However, in some situations a blank\nbackground in the images is not available, making it more difficult to separate\nforeground or contextual information from the bias. To overcome this, we\npropose a method to identify dataset bias without the need to crop background\ninformation from the images. That method is based on applying several image\ntransforms to the original images, including Fourier transform, wavelet\ntransforms, median filter, and their combinations. These transforms were\napplied to recover background bias information that CNNs use to classify\nimages. This transformations affect the contextual visual information in a\ndifferent manner than it affects the systemic background bias. Therefore, the\nmethod can distinguish between contextual information and the bias, and alert\non the presence of background bias even without the need to separate sub-images\nparts from the blank background of the original images. Code used in the\nexperiments is publicly available.\n","authors":["Sai Teja Erukude","Akhil Joshi","Lior Shamir"],"pdf_url":"https://arxiv.org/pdf/2412.13079v1.pdf","comment":"Computers, published"},{"id":"http://arxiv.org/abs/2410.14672v2","updated":"2024-12-17T16:47:41Z","published":"2024-10-18T17:59:04Z","title":"BiGR: Harnessing Binary Latent Codes for Image Generation and Improved\n  Visual Representation Capabilities","summary":"  We introduce BiGR, a novel conditional image generation model using compact\nbinary latent codes for generative training, focusing on enhancing both\ngeneration and representation capabilities. BiGR is the first conditional\ngenerative model that unifies generation and discrimination within the same\nframework. BiGR features a binary tokenizer, a masked modeling mechanism, and a\nbinary transcoder for binary code prediction. Additionally, we introduce a\nnovel entropy-ordered sampling method to enable efficient image generation.\nExtensive experiments validate BiGR's superior performance in generation\nquality, as measured by FID-50k, and representation capabilities, as evidenced\nby linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization\nacross various vision tasks, enabling applications such as image inpainting,\noutpainting, editing, interpolation, and enrichment, without the need for\nstructural modifications. Our findings suggest that BiGR unifies generative and\ndiscriminative tasks effectively, paving the way for further advancements in\nthe field. We further enable BiGR to perform text-to-image generation,\nshowcasing its potential for broader applications.\n","authors":["Shaozhe Hao","Xuantong Liu","Xianbiao Qi","Shihao Zhao","Bojia Zi","Rong Xiao","Kai Han","Kwan-Yee K. Wong"],"pdf_url":"https://arxiv.org/pdf/2410.14672v2.pdf","comment":"Updated with additional T2I results; Project page:\n  https://haoosz.github.io/BiGR"},{"id":"http://arxiv.org/abs/2412.13070v1","updated":"2024-12-17T16:34:32Z","published":"2024-12-17T16:34:32Z","title":"Learning of Patch-Based Smooth-Plus-Sparse Models for Image\n  Reconstruction","summary":"  We aim at the solution of inverse problems in imaging, by combining a\npenalized sparse representation of image patches with an unconstrained smooth\none. This allows for a straightforward interpretation of the reconstruction. We\nformulate the optimization as a bilevel problem. The inner problem deploys\nclassical algorithms while the outer problem optimizes the dictionary and the\nregularizer parameters through supervised learning. The process is carried out\nvia implicit differentiation and gradient-based optimization. We evaluate our\nmethod for denoising, super-resolution, and compressed-sensing\nmagnetic-resonance imaging. We compare it to other classical models as well as\ndeep-learning-based methods and show that it always outperforms the former and\nalso the latter in some instances.\n","authors":["Stanislas Ducotterd","Sebastian Neumayer","Michael Unser"],"pdf_url":"https://arxiv.org/pdf/2412.13070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13063v1","updated":"2024-12-17T16:28:08Z","published":"2024-12-17T16:28:08Z","title":"Smartphone-based Iris Recognition through High-Quality Visible Spectrum\n  Iris Capture","summary":"  Iris recognition is widely acknowledged for its exceptional accuracy in\nbiometric authentication, traditionally relying on near-infrared (NIR) imaging.\nRecently, visible spectrum (VIS) imaging via accessible smartphone cameras has\nbeen explored for biometric capture. However, a thorough study of iris\nrecognition using smartphone-captured 'High-Quality' VIS images and\ncross-spectral matching with previously enrolled NIR images has not been\nconducted. The primary challenge lies in capturing high-quality biometrics, a\nknown limitation of smartphone cameras. This study introduces a novel Android\napplication designed to consistently capture high-quality VIS iris images\nthrough automated focus and zoom adjustments. The application integrates a\nYOLOv3-tiny model for precise eye and iris detection and a lightweight\nGhost-Attention U-Net (G-ATTU-Net) for segmentation, while adhering to ISO/IEC\n29794-6 standards for image quality. The approach was validated using\nsmartphone-captured VIS and NIR iris images from 47 subjects, achieving a True\nAcceptance Rate (TAR) of 96.57% for VIS images and 97.95% for NIR images, with\nconsistent performance across various capture distances and iris colors. This\nrobust solution is expected to significantly advance the field of iris\nbiometrics, with important implications for enhancing smartphone security.\n","authors":["Naveenkumar G Venkataswamy","Yu Liu","Surendra Singh","Soumyabrata Dey","Stephanie Schuckers","Masudul H Imtiaz"],"pdf_url":"https://arxiv.org/pdf/2412.13063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11017v2","updated":"2024-12-17T16:27:21Z","published":"2024-12-15T02:10:18Z","title":"On Distilling the Displacement Knowledge for Few-Shot Class-Incremental\n  Learning","summary":"  Few-shot Class-Incremental Learning (FSCIL) addresses the challenges of\nevolving data distributions and the difficulty of data acquisition in\nreal-world scenarios. To counteract the catastrophic forgetting typically\nencountered in FSCIL, knowledge distillation is employed as a way to maintain\nthe knowledge from learned data distribution. Recognizing the limitations of\ngenerating discriminative feature representations in a few-shot context, our\napproach incorporates structural information between samples into knowledge\ndistillation. This structural information serves as a remedy for the low\nquality of features. Diverging from traditional structured distillation methods\nthat compute sample similarity, we introduce the Displacement Knowledge\nDistillation (DKD) method. DKD utilizes displacement rather than similarity\nbetween samples, incorporating both distance and angular information to\nsignificantly enhance the information density retained through knowledge\ndistillation. Observing performance disparities in feature distribution between\nbase and novel classes, we propose the Dual Distillation Network (DDNet). This\nnetwork applies traditional knowledge distillation to base classes and DKD to\nnovel classes, challenging the conventional integration of novel classes with\nbase classes. Additionally, we implement an instance-aware sample selector\nduring inference to dynamically adjust dual branch weights, thereby leveraging\nthe complementary strengths of each approach. Extensive testing on three\nbenchmarks demonstrates that DDNet achieves state-of-the-art results. Moreover,\nthrough rigorous experimentation and comparison, we establish the robustness\nand general applicability of our proposed DKD method.\n","authors":["Pengfei Fang","Yongchun Qin","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2412.11017v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13061v1","updated":"2024-12-17T16:27:11Z","published":"2024-12-17T16:27:11Z","title":"VidTok: A Versatile and Open-Source Video Tokenizer","summary":"  Encoding video content into compact latent tokens has become a fundamental\nstep in video generation and understanding, driven by the need to address the\ninherent redundancy in pixel-level representations. Consequently, there is a\ngrowing demand for high-performance, open-source video tokenizers as\nvideo-centric research gains prominence. We introduce VidTok, a versatile video\ntokenizer that delivers state-of-the-art performance in both continuous and\ndiscrete tokenizations. VidTok incorporates several key advancements over\nexisting approaches: 1) model architecture such as convolutional layers and\nup/downsampling modules; 2) to address the training instability and codebook\ncollapse commonly associated with conventional Vector Quantization (VQ), we\nintegrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3)\nimproved training strategies, including a two-stage training process and the\nuse of reduced frame rates. By integrating these advancements, VidTok achieves\nsubstantial improvements over existing methods, demonstrating superior\nperformance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD,\nunder standardized evaluation settings.\n","authors":["Anni Tang","Tianyu He","Junliang Guo","Xinle Cheng","Li Song","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2412.13061v1.pdf","comment":"Code & Models: https://github.com/microsoft/VidTok"},{"id":"http://arxiv.org/abs/2412.13059v1","updated":"2024-12-17T16:25:40Z","published":"2024-12-17T16:25:40Z","title":"3D MedDiffusion: A 3D Medical Diffusion Model for Controllable and\n  High-quality Medical Image Generation","summary":"  The generation of medical images presents significant challenges due to their\nhigh-resolution and three-dimensional nature. Existing methods often yield\nsuboptimal performance in generating high-quality 3D medical images, and there\nis currently no universal generative framework for medical imaging. In this\npaper, we introduce the 3D Medical Diffusion (3D MedDiffusion) model for\ncontrollable, high-quality 3D medical image generation. 3D MedDiffusion\nincorporates a novel, highly efficient Patch-Volume Autoencoder that compresses\nmedical images into latent space through patch-wise encoding and recovers back\ninto image space through volume-wise decoding. Additionally, we design a new\nnoise estimator to capture both local details and global structure information\nduring diffusion denoising process. 3D MedDiffusion can generate fine-detailed,\nhigh-resolution images (up to 512x512x512) and effectively adapt to various\ndownstream tasks as it is trained on large-scale datasets covering CT and MRI\nmodalities and different anatomical regions (from head to leg). Experimental\nresults demonstrate that 3D MedDiffusion surpasses state-of-the-art methods in\ngenerative quality and exhibits strong generalizability across tasks such as\nsparse-view CT reconstruction, fast MRI reconstruction, and data augmentation.\n","authors":["Haoshen Wang","Zhentao Liu","Kaicong Sun","Xiaodong Wang","Dinggang Shen","Zhiming Cui"],"pdf_url":"https://arxiv.org/pdf/2412.13059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13058v1","updated":"2024-12-17T16:22:56Z","published":"2024-12-17T16:22:56Z","title":"CondiMen: Conditional Multi-Person Mesh Recovery","summary":"  Multi-person human mesh recovery (HMR) consists in detecting all individuals\nin a given input image, and predicting the body shape, pose, and 3D location\nfor each detected person. The dominant approaches to this task rely on neural\nnetworks trained to output a single prediction for each detected individual. In\ncontrast, we propose CondiMen, a method that outputs a joint parametric\ndistribution over likely poses, body shapes, intrinsics and distances to the\ncamera, using a Bayesian network. This approach offers several advantages.\nFirst, a probability distribution can handle some inherent ambiguities of this\ntask -- such as the uncertainty between a person's size and their distance to\nthe camera, or simply the loss of information when projecting 3D data onto the\n2D image plane. Second, the output distribution can be combined with additional\ninformation to produce better predictions, by using e.g. known camera or body\nshape parameters, or by exploiting multi-view observations. Third, one can\nefficiently extract the most likely predictions from the output distribution,\nmaking our proposed approach suitable for real-time applications. Empirically\nwe find that our model i) achieves performance on par with or better than the\nstate-of-the-art, ii) captures uncertainties and correlations inherent in pose\nestimation and iii) can exploit additional information at test time, such as\nmulti-view consistency or body shape priors. CondiMen spices up the modeling of\nambiguity, using just the right ingredients on hand.\n","authors":["Brégier Romain","Baradel Fabien","Lucas Thomas","Galaaoui Salma","Armando Matthieu","Weinzaepfel Philippe","Rogez Grégory"],"pdf_url":"https://arxiv.org/pdf/2412.13058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16268v2","updated":"2024-12-17T16:22:55Z","published":"2024-10-21T17:59:19Z","title":"SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a\n  Training-Free Memory Tree","summary":"  The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundation\nmodel for object segmentation in both images and videos, paving the way for\nvarious downstream video applications. The crucial design of SAM 2 for video\nsegmentation is its memory module, which prompts object-aware memories from\nprevious frames for current frame prediction. However, its greedy-selection\nmemory design suffers from the \"error accumulation\" problem, where an errored\nor missed mask will cascade and influence the segmentation of the subsequent\nframes, which limits the performance of SAM 2 toward complex long-term videos.\nTo this end, we introduce SAM2Long, an improved training-free video object\nsegmentation strategy, which considers the segmentation uncertainty within each\nframe and chooses the video-level optimal results from multiple segmentation\npathways in a constrained tree search manner. In practice, we maintain a fixed\nnumber of segmentation pathways throughout the video. For each frame, multiple\nmasks are proposed based on the existing pathways, creating various candidate\nbranches. We then select the same fixed number of branches with higher\ncumulative scores as the new pathways for the next frame. After processing the\nfinal frame, the pathway with the highest cumulative score is chosen as the\nfinal segmentation result. Benefiting from its heuristic search design,\nSAM2Long is robust toward occlusions and object reappearances, and can\neffectively segment and track objects for complex long-term videos. Notably,\nSAM2Long achieves an average improvement of 3.0 points across all 24\nhead-to-head comparisons, with gains of up to 5.3 points in J&F on long-term\nvideo object segmentation benchmarks such as SA-V and LVOS. The code is\nreleased at https://github.com/Mark12Ding/SAM2Long.\n","authors":["Shuangrui Ding","Rui Qian","Xiaoyi Dong","Pan Zhang","Yuhang Zang","Yuhang Cao","Yuwei Guo","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16268v2.pdf","comment":"update results including single VOT, Project page:\n  https://mark12ding.github.io/project/SAM2Long/"},{"id":"http://arxiv.org/abs/2408.08495v2","updated":"2024-12-17T16:21:31Z","published":"2024-08-16T02:33:55Z","title":"FunEditor: Achieving Complex Image Edits via Function Aggregation with\n  Diffusion Models","summary":"  Diffusion models have demonstrated outstanding performance in generative\ntasks, making them ideal candidates for image editing. Recent studies highlight\ntheir ability to apply desired edits effectively by following textual\ninstructions, yet with two key challenges remaining. First, these models\nstruggle to apply multiple edits simultaneously, resulting in computational\ninefficiencies due to their reliance on sequential processing. Second, relying\non textual prompts to determine the editing region can lead to unintended\nalterations to the image. We introduce FunEditor, an efficient diffusion model\ndesigned to learn atomic editing functions and perform complex edits by\naggregating simpler functions. This approach enables complex editing tasks,\nsuch as object movement, by aggregating multiple functions and applying them\nsimultaneously to specific areas. Our experiments demonstrate that FunEditor\nsignificantly outperforms recent inference-time optimization methods and\nfine-tuned models, either quantitatively across various metrics or through\nvisual comparisons or both, on complex tasks like object movement and object\npasting. In the meantime, with only 4 steps of inference, FunEditor achieves\n5-24x inference speedups over existing popular methods. The code is available\nat: mhmdsmdi.github.io/funeditor/.\n","authors":["Mohammadreza Samadi","Fred X. Han","Mohammad Salameh","Hao Wu","Fengyu Sun","Chunhua Zhou","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2408.08495v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13050v1","updated":"2024-12-17T16:13:56Z","published":"2024-12-17T16:13:56Z","title":"Modality-Inconsistent Continual Learning of Multimodal Large Language\n  Models","summary":"  In this paper, we introduce Modality-Inconsistent Continual Learning (MICL),\na new continual learning scenario for Multimodal Large Language Models (MLLMs)\nthat involves tasks with inconsistent modalities (image, audio, or video) and\nvarying task types (captioning or question-answering). Unlike existing\nvision-only or modality-incremental settings, MICL combines modality and task\ntype shifts, both of which drive catastrophic forgetting. To address these\nchallenges, we propose MoInCL, which employs a Pseudo Targets Generation Module\nto mitigate forgetting caused by task type shifts in previously seen\nmodalities. It also incorporates Instruction-based Knowledge Distillation to\npreserve the model's ability to handle previously learned modalities when new\nones are introduced. We benchmark MICL using a total of six tasks and conduct\nexperiments to validate the effectiveness of our proposed MoInCL. The\nexperimental results highlight the superiority of MoInCL, showing significant\nimprovements over representative and state-of-the-art continual learning\nbaselines.\n","authors":["Weiguo Pian","Shijian Deng","Shentong Mo","Yunhui Guo","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2412.13050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10488v2","updated":"2024-12-17T16:13:15Z","published":"2024-12-13T15:24:11Z","title":"SVGBuilder: Component-Based Colored SVG Generation with Text-Guided\n  Autoregressive Transformers","summary":"  Scalable Vector Graphics (SVG) are essential XML-based formats for versatile\ngraphics, offering resolution independence and scalability. Unlike raster\nimages, SVGs use geometric shapes and support interactivity, animation, and\nmanipulation via CSS and JavaScript. Current SVG generation methods face\nchallenges related to high computational costs and complexity. In contrast,\nhuman designers use component-based tools for efficient SVG creation. Inspired\nby this, SVGBuilder introduces a component-based, autoregressive model for\ngenerating high-quality colored SVGs from textual input. It significantly\nreduces computational overhead and improves efficiency compared to traditional\nmethods. Our model generates SVGs up to 604 times faster than\noptimization-based approaches. To address the limitations of existing SVG\ndatasets and support our research, we introduce ColorSVG-100K, the first\nlarge-scale dataset of colored SVGs, comprising 100,000 graphics. This dataset\nfills the gap in color information for SVG generation models and enhances\ndiversity in model training. Evaluation against state-of-the-art models\ndemonstrates SVGBuilder's superior performance in practical applications,\nhighlighting its efficiency and quality in generating complex SVG graphics.\n","authors":["Zehao Chen","Rong Pan"],"pdf_url":"https://arxiv.org/pdf/2412.10488v2.pdf","comment":"Project: https://svgbuilder.github.io"},{"id":"http://arxiv.org/abs/2412.13047v1","updated":"2024-12-17T16:11:14Z","published":"2024-12-17T16:11:14Z","title":"EOGS: Gaussian Splatting for Earth Observation","summary":"  Recently, Gaussian splatting has emerged as a strong alternative to NeRF,\ndemonstrating impressive 3D modeling capabilities while requiring only a\nfraction of the training and rendering time. In this paper, we show how the\nstandard Gaussian splatting framework can be adapted for remote sensing,\nretaining its high efficiency. This enables us to achieve state-of-the-art\nperformance in just a few minutes, compared to the day-long optimization\nrequired by the best-performing NeRF-based Earth observation methods. The\nproposed framework incorporates remote-sensing improvements from EO-NeRF, such\nas radiometric correction and shadow modeling, while introducing novel\ncomponents, including sparsity, view consistency, and opacity regularizations.\n","authors":["Luca Savant Aira","Gabriele Facciolo","Thibaud Ehret"],"pdf_url":"https://arxiv.org/pdf/2412.13047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07841v2","updated":"2024-12-17T15:54:07Z","published":"2024-07-10T17:00:57Z","title":"Benchmarking Embedding Aggregation Methods in Computational Pathology: A\n  Clinical Data Perspective","summary":"  Recent advances in artificial intelligence (AI), in particular\nself-supervised learning of foundation models (FMs), are revolutionizing\nmedical imaging and computational pathology (CPath). A constant challenge in\nthe analysis of digital Whole Slide Images (WSIs) is the problem of aggregating\ntens of thousands of tile-level image embeddings to a slide-level\nrepresentation. Due to the prevalent use of datasets created for genomic\nresearch, such as TCGA, for method development, the performance of these\ntechniques on diagnostic slides from clinical practice has been inadequately\nexplored. This study conducts a thorough benchmarking analysis of ten\nslide-level aggregation techniques across nine clinically relevant tasks,\nincluding diagnostic assessment, biomarker classification, and outcome\nprediction. The results yield following key insights: (1) Embeddings derived\nfrom domain-specific (histological images) FMs outperform those from generic\nImageNet-based models across aggregation methods. (2) Spatial-aware aggregators\nenhance the performance significantly when using ImageNet pre-trained models\nbut not when using FMs. (3) No single model excels in all tasks and\nspatially-aware models do not show general superiority as it would be expected.\nThese findings underscore the need for more adaptable and universally\napplicable aggregation techniques, guiding future research towards tools that\nbetter meet the evolving needs of clinical-AI in pathology. The code used in\nthis work is available at\n\\url{https://github.com/fuchs-lab-public/CPath_SABenchmark}.\n","authors":["Shengjia Chen","Gabriele Campanella","Abdulkadir Elmas","Aryeh Stock","Jennifer Zeng","Alexandros D. Polydorides","Adam J. Schoenfeld","Kuan-lin Huang","Jane Houldsworth","Chad Vanderbilt","Thomas J. Fuchs"],"pdf_url":"https://arxiv.org/pdf/2407.07841v2.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2412.13026v1","updated":"2024-12-17T15:48:25Z","published":"2024-12-17T15:48:25Z","title":"NAVCON: A Cognitively Inspired and Linguistically Grounded Corpus for\n  Vision and Language Navigation","summary":"  We present NAVCON, a large-scale annotated Vision-Language Navigation (VLN)\ncorpus built on top of two popular datasets (R2R and RxR). The paper introduces\nfour core, cognitively motivated and linguistically grounded, navigation\nconcepts and an algorithm for generating large-scale silver annotations of\nnaturally occurring linguistic realizations of these concepts in navigation\ninstructions. We pair the annotated instructions with video clips of an agent\nacting on these instructions. NAVCON contains 236, 316 concept annotations for\napproximately 30, 0000 instructions and 2.7 million aligned images (from\napproximately 19, 000 instructions) showing what the agent sees when executing\nan instruction. To our knowledge, this is the first comprehensive resource of\nnavigation concepts. We evaluated the quality of the silver annotations by\nconducting human evaluation studies on NAVCON samples. As further validation of\nthe quality and usefulness of the resource, we trained a model for detecting\nnavigation concepts and their linguistic realizations in unseen instructions.\nAdditionally, we show that few-shot learning with GPT-4o performs well on this\ntask using large-scale silver annotations of NAVCON.\n","authors":["Karan Wanchoo","Xiaoye Zuo","Hannah Gonzalez","Soham Dan","Georgios Georgakis","Dan Roth","Kostas Daniilidis","Eleni Miltsakaki"],"pdf_url":"https://arxiv.org/pdf/2412.13026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13017v1","updated":"2024-12-17T15:36:55Z","published":"2024-12-17T15:36:55Z","title":"A New Adversarial Perspective for LiDAR-based 3D Object Detection","summary":"  Autonomous vehicles (AVs) rely on LiDAR sensors for environmental perception\nand decision-making in driving scenarios. However, ensuring the safety and\nreliability of AVs in complex environments remains a pressing challenge. To\naddress this issue, we introduce a real-world dataset (ROLiD) comprising\nLiDAR-scanned point clouds of two random objects: water mist and smoke. In this\npaper, we introduce a novel adversarial perspective by proposing an attack\nframework that utilizes water mist and smoke to simulate environmental\ninterference. Specifically, we propose a point cloud sequence generation method\nusing a motion and content decomposition generative adversarial network named\nPCS-GAN to simulate the distribution of random objects. Furthermore, leveraging\nthe simulated LiDAR scanning characteristics implemented with Range Image, we\nexamine the effects of introducing random object perturbations at various\npositions on the target vehicle. Extensive experiments demonstrate that\nadversarial perturbations based on random objects effectively deceive vehicle\ndetection and reduce the recognition rate of 3D object detection models.\n","authors":["Shijun Zheng","Weiquan Liu","Yu Guo","Yu Zang","Siqi Shen","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13017v1.pdf","comment":"11 pages, 7 figures, AAAI2025"},{"id":"http://arxiv.org/abs/2412.13010v1","updated":"2024-12-17T15:32:12Z","published":"2024-12-17T15:32:12Z","title":"Measurement of Medial Elbow Joint Space using Landmark Detection","summary":"  Ultrasound imaging of the medial elbow is crucial for the early\nidentification of Ulnar Collateral Ligament (UCL) injuries. Specifically,\nmeasuring the elbow joint space in ultrasound images is used to assess the\nvalgus instability of elbow. To automate this measurement, a precisely\nannotated dataset is necessary; however, no publicly available dataset has been\nproposed thus far. This study introduces a novel ultrasound medial elbow\ndataset for measuring joint space to diagnose Ulnar Collateral Ligament (UCL)\ninjuries. The dataset comprises 4,201 medial elbow ultrasound images from 22\nsubjects, with landmark annotations on the humerus and ulna. The annotations\nare made precisely by the authors under the supervision of three orthopedic\nsurgeons. We evaluated joint space measurement methods using our proposed\ndataset with several landmark detection approaches, including ViTPose, HRNet,\nPCT, YOLOv8, and U-Net. In addition, we propose using Shape Subspace (SS) for\nlandmark refinement in heatmap-based landmark detection. The results show that\nthe mean Euclidean distance error of joint space is 0.116 mm when using HRNet.\nFurthermore, the SS landmark refinement improves the mean absolute error of\nlandmark positions by 0.010 mm with HRNet and by 0.103 mm with ViTPose on\naverage. These highlight the potential for high-precision, real-time diagnosis\nof UCL injuries and associated risks, which could be leveraged in large-scale\nscreening. Lastly, we demonstrate point-based segmentation of the humerus and\nulna using the detected landmarks as input. The dataset will be made publicly\navailable upon acceptance of this paper at:\nhttps://github.com/Akahori000/Ultrasound-Medial-Elbow-Dataset.\n","authors":["Shizuka Akahori","Shotaro Teruya","Pragyan Shrestha","Yuichi Yoshii","Ryuhei Michinobu","Satoshi Iizuka","Itaru Kitahara"],"pdf_url":"https://arxiv.org/pdf/2412.13010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05966v3","updated":"2024-12-17T15:31:49Z","published":"2024-03-09T17:17:07Z","title":"Can Generative Models Improve Self-Supervised Representation Learning?","summary":"  The rapid advancement in self-supervised representation learning has\nhighlighted its potential to leverage unlabeled data for learning rich visual\nrepresentations. However, the existing techniques, particularly those employing\ndifferent augmentations of the same image, often rely on a limited set of\nsimple transformations that cannot fully capture variations in the real world.\nThis constrains the diversity and quality of samples, which leads to\nsub-optimal representations. In this paper, we introduce a framework that\nenriches the self-supervised learning (SSL) paradigm by utilizing generative\nmodels to produce semantically consistent image augmentations. By directly\nconditioning generative models on a source image, our method enables the\ngeneration of diverse augmentations while maintaining the semantics of the\nsource image, thus offering a richer set of data for SSL. Our extensive\nexperimental results on various joint-embedding SSL techniques demonstrate that\nour framework significantly enhances the quality of learned visual\nrepresentations by up to 10\\% Top-1 accuracy in downstream tasks. This research\ndemonstrates that incorporating generative models into the joint-embedding SSL\nworkflow opens new avenues for exploring the potential of synthetic data. This\ndevelopment paves the way for more robust and versatile representation learning\ntechniques.\n","authors":["Sana Ayromlou","Vahid Reza Khazaie","Fereshteh Forghani","Arash Afkanpour"],"pdf_url":"https://arxiv.org/pdf/2403.05966v3.pdf","comment":"To be published in AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13006v1","updated":"2024-12-17T15:26:15Z","published":"2024-12-17T15:26:15Z","title":"What is YOLOv6? A Deep Insight into the Object Detection Model","summary":"  This work explores the YOLOv6 object detection model in depth, concentrating\non its design framework, optimization techniques, and detection capabilities.\nYOLOv6's core elements consist of the EfficientRep Backbone for robust feature\nextraction and the Rep-PAN Neck for seamless feature aggregation, ensuring\nhigh-performance object detection. Evaluated on the COCO dataset, YOLOv6-N\nachieves 37.5\\% AP at 1187 FPS on an NVIDIA Tesla T4 GPU. YOLOv6-S reaches\n45.0\\% AP at 484 FPS, outperforming models like PPYOLOE-S, YOLOv5-S, YOLOX-S,\nand YOLOv8-S in the same class. Moreover, YOLOv6-M and YOLOv6-L also show\nbetter accuracy (50.0\\% and 52.8\\%) while maintaining comparable inference\nspeeds to other detectors. With an upgraded backbone and neck structure,\nYOLOv6-L6 delivers cutting-edge accuracy in real-time.\n","authors":["Athulya Sundaresan Geetha"],"pdf_url":"https://arxiv.org/pdf/2412.13006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12990v1","updated":"2024-12-17T15:07:50Z","published":"2024-12-17T15:07:50Z","title":"Future Aspects in Human Action Recognition: Exploring Emerging\n  Techniques and Ethical Influences","summary":"  Visual-based human action recognition can be found in various application\nfields, e.g., surveillance systems, sports analytics, medical assistive\ntechnologies, or human-robot interaction frameworks, and it concerns the\nidentification and classification of individuals' activities within a video.\nSince actions typically occur over a sequence of consecutive images, it is\nparticularly challenging due to the inclusion of temporal analysis, which\nintroduces an extra layer of complexity. However, although multiple approaches\ntry to handle temporal analysis, there are still difficulties because of their\ncomputational cost and lack of adaptability. Therefore, different types of\nvision data, containing transition information between consecutive images,\nprovided by next-generation hardware sensors will guide the robotics community\nin tackling the problem of human action recognition. On the other hand, while\nthere is a plethora of still-image datasets, that researchers can adopt to\ntrain new artificial intelligence models, videos representing human activities\nare of limited capabilities, e.g., small and unbalanced datasets or selected\nwithout control from multiple sources. To this end, generating new and\nrealistic synthetic videos is possible since labeling is performed throughout\nthe data creation process, while reinforcement learning techniques can permit\nthe avoidance of considerable dataset dependence. At the same time, human\nfactors' involvement raises ethical issues for the research community, as\ndoubts and concerns about new technologies already exist.\n","authors":["Antonios Gasteratos","Stavros N. Moutsis","Konstantinos A. Tsintotas","Yiannis Aloimonos"],"pdf_url":"https://arxiv.org/pdf/2412.12990v1.pdf","comment":"2 pages, 1 figure, 40th Anniversary of the IEEE Conference on\n  Robotics and Automation (ICRA@40), Rotterdam, Netherlands | September 23-26,\n  2024"},{"id":"http://arxiv.org/abs/2205.10691v2","updated":"2024-12-17T15:04:46Z","published":"2022-05-21T23:04:20Z","title":"Producing Histopathology Phantom Images using Generative Adversarial\n  Networks to improve Tumor Detection","summary":"  Advance in medical imaging is an important part in deep learning research.\nOne of the goals of computer vision is development of a holistic, comprehensive\nmodel which can identify tumors from histology slides obtained via biopsies. A\nmajor problem that stands in the way is lack of data for a few cancer-types. In\nthis paper, we ascertain that data augmentation using GANs can be a viable\nsolution to reduce the unevenness in the distribution of different cancer types\nin our dataset. Our demonstration showed that a dataset augmented to a 50%\nincrease causes an increase in tumor detection from 80% to 87.5%\n","authors":["Vidit Gautam"],"pdf_url":"https://arxiv.org/pdf/2205.10691v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12982v1","updated":"2024-12-17T15:01:35Z","published":"2024-12-17T15:01:35Z","title":"Stable Diffusion is a Natural Cross-Modal Decoder for Layered\n  AI-generated Image Compression","summary":"  Recent advances in Artificial Intelligence Generated Content (AIGC) have\ngarnered significant interest, accompanied by an increasing need to transmit\nand compress the vast number of AI-generated images (AIGIs). However, there is\na noticeable deficiency in research focused on compression methods for AIGIs.\nTo address this critical gap, we introduce a scalable cross-modal compression\nframework that incorporates multiple human-comprehensible modalities, designed\nto efficiently capture and relay essential visual information for AIGIs. In\nparticular, our framework encodes images into a layered bitstream consisting of\na semantic layer that delivers high-level semantic information through text\nprompts; a structural layer that captures spatial details using edge or\nskeleton maps; and a texture layer that preserves local textures via a\ncolormap. Utilizing Stable Diffusion as the backend, the framework effectively\nleverages these multimodal priors for image generation, effectively functioning\nas a decoder when these priors are encoded. Qualitative and quantitative\nresults show that our method proficiently restores both semantic and visual\ndetails, competing against baseline approaches at extremely low bitrates (\n<0.02 bpp). Additionally, our framework facilitates downstream editing\napplications without requiring full decoding, thereby paving a new direction\nfor future research in AIGI compression.\n","authors":["Ruijie Chen","Qi Mao","Zhengxue Cheng"],"pdf_url":"https://arxiv.org/pdf/2412.12982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12974v1","updated":"2024-12-17T14:56:59Z","published":"2024-12-17T14:56:59Z","title":"Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential\n  via Self-Attention Redirection Guidance","summary":"  Recently, diffusion models have emerged as promising newcomers in the field\nof generative models, shining brightly in image generation. However, when\nemployed for object removal tasks, they still encounter issues such as\ngenerating random artifacts and the incapacity to repaint foreground object\nareas with appropriate content after removal. To tackle these problems, we\npropose Attentive Eraser, a tuning-free method to empower pre-trained diffusion\nmodels for stable and effective object removal. Firstly, in light of the\nobservation that the self-attention maps influence the structure and shape\ndetails of the generated images, we propose Attention Activation and\nSuppression (ASS), which re-engineers the self-attention mechanism within the\npre-trained diffusion models based on the given mask, thereby prioritizing the\nbackground over the foreground object during the reverse generation process.\nMoreover, we introduce Self-Attention Redirection Guidance (SARG), which\nutilizes the self-attention redirected by ASS to guide the generation process,\neffectively removing foreground objects within the mask while simultaneously\ngenerating content that is both plausible and coherent. Experiments demonstrate\nthe stability and effectiveness of Attentive Eraser in object removal across a\nvariety of pre-trained diffusion models, outperforming even training-based\nmethods. Furthermore, Attentive Eraser can be implemented in various diffusion\nmodel architectures and checkpoints, enabling excellent scalability. Code is\navailable at https://github.com/Anonym0u3/AttentiveEraser.\n","authors":["Wenhao Sun","Benlei Cui","Jingqun Tang","Xue-Mei Dong"],"pdf_url":"https://arxiv.org/pdf/2412.12974v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12966v1","updated":"2024-12-17T14:51:13Z","published":"2024-12-17T14:51:13Z","title":"Fruit Deformity Classification through Single-Input and Multi-Input\n  Architectures based on CNN Models using Real and Synthetic Images","summary":"  The present study focuses on detecting the degree of deformity in fruits such\nas apples, mangoes, and strawberries during the process of inspecting their\nexternal quality, employing Single-Input and Multi-Input architectures based on\nconvolutional neural network (CNN) models using sets of real and synthetic\nimages. The datasets are segmented using the Segment Anything Model (SAM),\nwhich provides the silhouette of the fruits. Regarding the single-input\narchitecture, the evaluation of the CNN models is performed only with real\nimages, but a methodology is proposed to improve these results using a\npre-trained model with synthetic images. In the Multi-Input architecture,\nbranches with RGB images and fruit silhouettes are implemented as inputs for\nevaluating CNN models such as VGG16, MobileNetV2, and CIDIS. However, the\nresults revealed that the Multi-Input architecture with the MobileNetV2 model\nwas the most effective in identifying deformities in the fruits, achieving\naccuracies of 90\\%, 94\\%, and 92\\% for apples, mangoes, and strawberries,\nrespectively. In conclusion, the Multi-Input architecture with the MobileNetV2\nmodel is the most accurate for classifying levels of deformity in fruits.\n","authors":["Tommy D. Beltran","Raul J. Villao","Luis E. Chuquimarca","Boris X. Vintimilla","Sergio A. Velastin"],"pdf_url":"https://arxiv.org/pdf/2412.12966v1.pdf","comment":"15 pages, 9 figures, CIARP 2024"},{"id":"http://arxiv.org/abs/2412.00876v3","updated":"2024-12-17T14:45:12Z","published":"2024-12-01T16:32:31Z","title":"Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification","summary":"  Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .\n","authors":["Wenxuan Huang","Zijie Zhai","Yunhang Shen","Shaosheng Cao","Fei Zhao","Xiangfeng Xu","Zheyu Ye","Shaohui Lin"],"pdf_url":"https://arxiv.org/pdf/2412.00876v3.pdf","comment":"Code is available at https://github.com/Osilly/dynamic_llava"},{"id":"http://arxiv.org/abs/2403.15698v3","updated":"2024-12-17T14:39:07Z","published":"2024-03-23T03:23:29Z","title":"SceneX: Procedural Controllable Large-scale Scene Generation","summary":"  Developing comprehensive explicit world models is crucial for understanding\nand simulating real-world scenarios. Recently, Procedural Controllable\nGeneration (PCG) has gained significant attention in large-scale scene\ngeneration by enabling the creation of scalable, high-quality assets. However,\nPCG faces challenges such as limited modular diversity, high expertise\nrequirements, and challenges in managing the diverse elements and structures in\ncomplex scenes. In this paper, we introduce a large-scale scene generation\nframework, SceneX, which can automatically produce high-quality procedural\nmodels according to designers' textual descriptions. Specifically, the proposed\nmethod comprises two components, PCGHub and PCGPlanner. The former encompasses\nan extensive collection of accessible procedural assets and thousands of\nhand-craft API documents to perform as a standard protocol for PCG controller.\nThe latter aims to generate executable actions for Blender to produce\ncontrollable and precise 3D assets guided by the user's instructions. Extensive\nexperiments demonstrated the capability of our method in controllable\nlarge-scale scene generation, including nature scenes and unbounded cities, as\nwell as scene editing such as asset placement and season translation.\n","authors":["Mengqi Zhou","Yuxi Wang","Jun Hou","Shougao Zhang","Yiwei Li","Chuanchen Luo","Junran Peng","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.15698v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10702v2","updated":"2024-12-17T14:37:34Z","published":"2024-12-14T06:21:24Z","title":"Memory Efficient Matting with Adaptive Token Routing","summary":"  Transformer-based models have recently achieved outstanding performance in\nimage matting. However, their application to high-resolution images remains\nchallenging due to the quadratic complexity of global self-attention. To\naddress this issue, we propose MEMatte, a \\textbf{m}emory-\\textbf{e}fficient\n\\textbf{m}atting framework for processing high-resolution images. MEMatte\nincorporates a router before each global attention block, directing informative\ntokens to the global attention while routing other tokens to a Lightweight\nToken Refinement Module (LTRM). Specifically, the router employs a local-global\nstrategy to predict the routing probability of each token, and the LTRM\nutilizes efficient modules to simulate global attention. Additionally, we\nintroduce a Batch-constrained Adaptive Token Routing (BATR) mechanism, which\nallows each router to dynamically route tokens based on image content and the\nstages of attention block in the network. Furthermore, we construct an ultra\nhigh-resolution image matting dataset, UHR-395, comprising 35,500 training\nimages and 1,000 test images, with an average resolution of $4872\\times6017$.\nThis dataset is created by compositing 395 different alpha mattes across 11\ncategories onto various backgrounds, all with high-quality manual annotation.\nExtensive experiments demonstrate that MEMatte outperforms existing methods on\nboth high-resolution and real-world datasets, significantly reducing memory\nusage by approximately 88% and latency by 50% on the Composition-1K benchmark.\nOur code is available at https://github.com/linyiheng123/MEMatte.\n","authors":["Yiheng Lin","Yihan Hu","Chenyi Zhang","Ting Liu","Xiaochao Qu","Luoqi Liu","Yao Zhao","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2412.10702v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12949v1","updated":"2024-12-17T14:29:12Z","published":"2024-12-17T14:29:12Z","title":"Synthetic Data Generation for Anomaly Detection on Table Grapes","summary":"  Early detection of illnesses and pest infestations in fruit cultivation is\ncritical for maintaining yield quality and plant health. Computer vision and\nrobotics are increasingly employed for the automatic detection of such issues,\nparticularly using data-driven solutions. However, the rarity of these problems\nmakes acquiring and processing the necessary data to train such algorithms a\nsignificant obstacle. One solution to this scarcity is the generation of\nsynthetic high-quality anomalous samples. While numerous methods exist for this\ntask, most require highly trained individuals for setup.\n  This work addresses the challenge of generating synthetic anomalies in an\nautomatic fashion that requires only an initial collection of normal and\nanomalous samples from the user - a task that is straightforward for farmers.\nWe demonstrate the approach in the context of table grape cultivation.\nSpecifically, based on the observation that normal berries present relatively\nsmooth surfaces, while defects result in more complex textures, we introduce a\nDual-Canny Edge Detection (DCED) filter. This filter emphasizes the additional\ntexture indicative of diseases, pest infestations, or other defects. Using\nsegmentation masks provided by the Segment Anything Model, we then select and\nseamlessly blend anomalous berries onto normal ones. We show that the proposed\ndataset augmentation technique improves the accuracy of an anomaly classifier\nfor table grapes and that the approach can be generalized to other fruit types.\n","authors":["Ionut Marian Motoi","Valerio Belli","Alberto Carpineto","Daniele Nardi","Thomas Alessandro Ciarfuglia"],"pdf_url":"https://arxiv.org/pdf/2412.12949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12944v1","updated":"2024-12-17T14:22:44Z","published":"2024-12-17T14:22:44Z","title":"Online optimisation for dynamic electrical impedance tomography","summary":"  Online optimisation studies the convergence of optimisation methods as the\ndata embedded in the problem changes. Based on this idea, we propose a primal\ndual online method for nonlinear time-discrete inverse problems. We analyse the\nmethod through regret theory and demonstrate its performance in real-time\nmonitoring of moving bodies in a fluid with Electrical Impedance Tomography\n(EIT). To do so, we also prove the second-order differentiability of the\nComplete Electrode Model (CEM) solution operator on $L^\\infty$.\n","authors":["Neil Dizon","Jyrki Jauhiainen","Tuomo Valkonen"],"pdf_url":"https://arxiv.org/pdf/2412.12944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16424v2","updated":"2024-12-17T14:17:51Z","published":"2024-07-23T12:21:23Z","title":"ESOD: Efficient Small Object Detection on High-Resolution Images","summary":"  Enlarging input images is a straightforward and effective approach to promote\nsmall object detection. However, simple image enlargement is significantly\nexpensive on both computations and GPU memory. In fact, small objects are\nusually sparsely distributed and locally clustered. Therefore, massive feature\nextraction computations are wasted on the non-target background area of images.\nRecent works have tried to pick out target-containing regions using an extra\nnetwork and perform conventional object detection, but the newly introduced\ncomputation limits their final performance. In this paper, we propose to reuse\nthe detector's backbone to conduct feature-level object-seeking and\npatch-slicing, which can avoid redundant feature extraction and reduce the\ncomputation cost. Incorporating a sparse detection head, we are able to detect\nsmall objects on high-resolution inputs (e.g., 1080P or larger) for superior\nperformance. The resulting Efficient Small Object Detection (ESOD) approach is\na generic framework, which can be applied to both CNN- and ViT-based detectors\nto save the computation and GPU memory costs. Extensive experiments demonstrate\nthe efficacy and efficiency of our method. In particular, our method\nconsistently surpasses the SOTA detectors by a large margin (e.g., 8% gains on\nAP) on the representative VisDrone, UAVDT, and TinyPerson datasets. Code is\navailable at https://github.com/alibaba/esod.\n","authors":["Kai Liu","Zhihang Fu","Sheng Jin","Ze Chen","Fan Zhou","Rongxin Jiang","Yaowu Chen","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2407.16424v2.pdf","comment":"This paper has been recerived by IEEE TIP 2024. Code is available at\n  https://github.com/alibaba/esod"},{"id":"http://arxiv.org/abs/2412.11974v2","updated":"2024-12-17T14:12:56Z","published":"2024-12-16T16:58:28Z","title":"Emma-X: An Embodied Multimodal Action Model with Grounded Chain of\n  Thought and Look-ahead Spatial Reasoning","summary":"  Traditional reinforcement learning-based robotic control methods are often\ntask-specific and fail to generalize across diverse environments or unseen\nobjects and instructions. Visual Language Models (VLMs) demonstrate strong\nscene understanding and planning capabilities but lack the ability to generate\nactionable policies tailored to specific robotic embodiments. To address this,\nVisual-Language-Action (VLA) models have emerged, yet they face challenges in\nlong-horizon spatial reasoning and grounded task planning. In this work, we\npropose the Embodied Multimodal Action Model with Grounded Chain of Thought and\nLook-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed\nhierarchical embodiment dataset based on BridgeV2, containing 60,000 robot\nmanipulation trajectories auto-annotated with grounded task reasoning and\nspatial guidance. Additionally, we introduce a trajectory segmentation strategy\nbased on gripper states and motion trajectories, which can help mitigate\nhallucination in grounding subtask reasoning generation. Experimental results\ndemonstrate that Emma-X achieves superior performance over competitive\nbaselines, particularly in real-world robotic tasks requiring spatial\nreasoning.\n","authors":["Qi Sun","Pengfei Hong","Tej Deep Pala","Vernon Toh","U-Xuan Tan","Deepanway Ghosal","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2412.11974v2.pdf","comment":"https://github.com/declare-lab/Emma-X,\n  https://huggingface.co/declare-lab/Emma-X"},{"id":"http://arxiv.org/abs/2412.12932v1","updated":"2024-12-17T14:10:16Z","published":"2024-12-17T14:10:16Z","title":"CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large\n  Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) have recently demonstrated amazing\nsuccess in multi-modal tasks, including advancements in Multi-modal\nChain-of-Thought (MCoT) reasoning. Despite these successes, current benchmarks\nstill follow a traditional paradigm with multi-modal input and text-modal\noutput, which leads to significant drawbacks such as missing visual operations\nand vague expressions. Motivated by this, we introduce a novel Chain of\nMulti-modal Thought (CoMT) benchmark to address these limitations. Different\nfrom the traditional MCoT benchmark, CoMT requires both multi-modal input and\nmulti-modal reasoning output, aiming to mimic human-like reasoning that\ninherently integrates visual operation. Specifically, CoMT consists of four\ncategories: (1) Visual Creation, (2) Visual Deletion, (3) Visual Update, and\n(4) Visual Selection to comprehensively explore complex visual operations and\nconcise expression in real scenarios. We evaluate various LVLMs and strategies\non CoMT, revealing some key insights into the capabilities and limitations of\nthe current approaches. We hope that CoMT can inspire more research on\nintroducing multi-modal generation into the reasoning process.\n","authors":["Zihui Cheng","Qiguang Chen","Jin Zhang","Hao Fei","Xiaocheng Feng","Wanxiang Che","Min Li","Libo Qin"],"pdf_url":"https://arxiv.org/pdf/2412.12932v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.10908v2","updated":"2024-12-17T14:06:48Z","published":"2024-12-14T17:35:27Z","title":"Do large language vision models understand 3D shapes?","summary":"  Large vision language models (LVLM) are the leading A.I approach for\nachieving a general visual understanding of the world. Models such as GPT,\nClaude, Gemini, and LLama can use images to understand and analyze complex\nvisual scenes. 3D objects and shapes are the basic building blocks of the\nworld, recognizing them is a fundamental part of human perception. The goal of\nthis work is to test whether LVLMs truly understand 3D shapes by testing the\nmodels ability to identify and match objects of the exact same 3D shapes but\nwith different orientations and materials/textures. Test images were created\nusing CGI with a huge number of highly diverse objects, materials, and scenes.\nThe results of this test show that the ability of such models to match 3D\nshapes is significantly below humans but much higher than random guesses.\nSuggesting that the models have gained some abstract understanding of 3D shapes\nbut still trail far beyond humans in this task. Mainly it seems that the models\ncan easily identify the same object with a different orientation as well as\nmatching identical 3D shapes of the same orientation but with different\nmaterial textures. However, when both the object material and orientation are\nchanged, all models perform poorly relative to humans.\n","authors":["Sagi Eppel"],"pdf_url":"https://arxiv.org/pdf/2412.10908v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12919v1","updated":"2024-12-17T13:51:56Z","published":"2024-12-17T13:51:56Z","title":"4DRGS: 4D Radiative Gaussian Splatting for Efficient 3D Vessel\n  Reconstruction from Sparse-View Dynamic DSA Images","summary":"  Reconstructing 3D vessel structures from sparse-view dynamic digital\nsubtraction angiography (DSA) images enables accurate medical assessment while\nreducing radiation exposure. Existing methods often produce suboptimal results\nor require excessive computation time. In this work, we propose 4D radiative\nGaussian splatting (4DRGS) to achieve high-quality reconstruction efficiently.\nIn detail, we represent the vessels with 4D radiative Gaussian kernels. Each\nkernel has time-invariant geometry parameters, including position, rotation,\nand scale, to model static vessel structures. The time-dependent central\nattenuation of each kernel is predicted from a compact neural network to\ncapture the temporal varying response of contrast agent flow. We splat these\nGaussian kernels to synthesize DSA images via X-ray rasterization and optimize\nthe model with real captured ones. The final 3D vessel volume is voxelized from\nthe well-trained kernels. Moreover, we introduce accumulated attenuation\npruning and bounded scaling activation to improve reconstruction quality.\nExtensive experiments on real-world patient data demonstrate that 4DRGS\nachieves impressive results in 5 minutes training, which is 32x faster than the\nstate-of-the-art method. This underscores the potential of 4DRGS for real-world\nclinics.\n","authors":["Zhentao Liu","Ruyi Zha","Huangxuan Zhao","Hongdong Li","Zhiming Cui"],"pdf_url":"https://arxiv.org/pdf/2412.12919v1.pdf","comment":"Zhentao Liu and Ruyi Zha made equal contributions"},{"id":"http://arxiv.org/abs/2412.06418v2","updated":"2024-12-17T13:49:59Z","published":"2024-12-09T11:51:28Z","title":"Continual Learning for Segment Anything Model Adaptation","summary":"  Although the current different types of SAM adaptation methods have achieved\npromising performance for various downstream tasks, such as prompt-based ones\nand adapter-based ones, most of them belong to the one-step adaptation\nparadigm. In real-world scenarios, we are generally confronted with the dynamic\nscenario where the data comes in a streaming manner. Driven by the practical\nneed, in this paper, we first propose a novel Continual SAM adaptation (CoSAM)\nbenchmark with 8 different task domains and carefully analyze the limitations\nof the existing SAM one-step adaptation methods in the continual segmentation\nscenario. Then we propose a novel simple-yet-effective Mixture of Domain\nAdapters (MoDA) algorithm which utilizes the Global Feature Tokens (GFT) and\nGlobal Assistant Tokens (GAT) modules to help the SAM encoder extract\nwell-separated features for different task domains, and then provide the\naccurate task-specific information for continual learning. Extensive\nexperiments demonstrate that our proposed MoDA obviously surpasses the existing\nclassic continual learning methods, as well as prompt-based and adapter-based\napproaches for continual segmentation. Moreover, after sequential learning on\nthe CoSAM benchmark with diverse data distributions, our MoDA maintains highly\ncompetitive results in the natural image domain, approaching the zero-shot\nperformance of the original SAM, demonstrating its superior capability in\nknowledge preservation. Notably, the proposed MoDA can be seamlessly integrated\ninto various one-step adaptation methods of SAM, which can consistently bring\nobvious performance gains. Code is available at\n\\url{https://github.com/yangjl1215/CoSAM}\n","authors":["Jinglong Yang","Yichen Wu","Jun Cen","Wenjian Huang","Hong Wang","Jianguo Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.06418v2.pdf","comment":"Code is available at \\url{https://github.com/yangjl1215/CoSAM}"},{"id":"http://arxiv.org/abs/2412.12912v1","updated":"2024-12-17T13:46:12Z","published":"2024-12-17T13:46:12Z","title":"Unsupervised Region-Based Image Editing of Denoising Diffusion Models","summary":"  Although diffusion models have achieved remarkable success in the field of\nimage generation, their latent space remains under-explored. Current methods\nfor identifying semantics within latent space often rely on external\nsupervision, such as textual information and segmentation masks. In this paper,\nwe propose a method to identify semantic attributes in the latent space of\npre-trained diffusion models without any further training. By projecting the\nJacobian of the targeted semantic region into a low-dimensional subspace which\nis orthogonal to the non-masked regions, our approach facilitates precise\nsemantic discovery and control over local masked areas, eliminating the need\nfor annotations. We conducted extensive experiments across multiple datasets\nand various architectures of diffusion models, achieving state-of-the-art\nperformance. In particular, for some specific face attributes, the performance\nof our proposed method even surpasses that of supervised approaches,\ndemonstrating its superior ability in editing local image properties.\n","authors":["Zixiang Li","Yue Song","Renshuai Tao","Xiaohong Jia","Yao Zhao","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10710v2","updated":"2024-12-17T13:41:32Z","published":"2024-12-14T06:50:10Z","title":"Virtual Trial Room with Computer Vision and Machine Learning","summary":"  Online shopping has revolutionized the retail industry, providing customers\nwith convenience and accessibility. However, customers often hesitate to\npurchase wearable products such as watches, jewelry, glasses, shoes, and\nclothes due to the lack of certainty regarding fit and suitability. This leads\nto significant return rates, causing problems for both customers and vendors.\nTo address this issue, a platform called the Virtual Trial Room with Computer\nVision and Machine Learning is designed which enables customers to easily check\nwhether a product will fit and suit them or not. To achieve this, an\nAI-generated 3D model of the human head was created from a single 2D image\nusing the DECA model. This 3D model was then superimposed with a custom-made 3D\nmodel of glass which is based on real-world measurements and fitted over the\nhuman head. To replicate the real-world look and feel, the model was retouched\nwith textures, lightness, and smoothness. Furthermore, a full-stack application\nwas developed utilizing various fornt-end and back-end technologies. This\napplication enables users to view 3D-generated results on the website,\nproviding an immersive and interactive experience.\n","authors":["Tulashi Prasad Joshi","Amrendra Kumar Yadav","Arjun Chhetri","Suraj Agrahari","Umesh Kanta Ghimire"],"pdf_url":"https://arxiv.org/pdf/2412.10710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12906v1","updated":"2024-12-17T13:32:04Z","published":"2024-12-17T13:32:04Z","title":"CATSplat: Context-Aware Transformer with Spatial Guidance for\n  Generalizable 3D Gaussian Splatting from A Single-View Image","summary":"  Recently, generalizable feed-forward methods based on 3D Gaussian Splatting\nhave gained significant attention for their potential to reconstruct 3D scenes\nusing finite resources. These approaches create a 3D radiance field,\nparameterized by per-pixel 3D Gaussian primitives, from just a few images in a\nsingle forward pass. However, unlike multi-view methods that benefit from\ncross-view correspondences, 3D scene reconstruction with a single-view image\nremains an underexplored area. In this work, we introduce CATSplat, a novel\ngeneralizable transformer-based framework designed to break through the\ninherent constraints in monocular settings. First, we propose leveraging\ntextual guidance from a visual-language model to complement insufficient\ninformation from a single image. By incorporating scene-specific contextual\ndetails from text embeddings through cross-attention, we pave the way for\ncontext-aware 3D scene reconstruction beyond relying solely on visual cues.\nMoreover, we advocate utilizing spatial guidance from 3D point features toward\ncomprehensive geometric understanding under single-view settings. With 3D\npriors, image features can capture rich structural insights for predicting 3D\nGaussians without multi-view techniques. Extensive experiments on large-scale\ndatasets demonstrate the state-of-the-art performance of CATSplat in\nsingle-view 3D scene reconstruction with high-quality novel view synthesis.\n","authors":["Wonseok Roh","Hwanhee Jung","Jong Wook Kim","Seunggwan Lee","Innfarn Yoo","Andreas Lugmayr","Seunggeun Chi","Karthik Ramani","Sangpil Kim"],"pdf_url":"https://arxiv.org/pdf/2412.12906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12902v1","updated":"2024-12-17T13:26:31Z","published":"2024-12-17T13:26:31Z","title":"DoPTA: Improving Document Layout Analysis using Patch-Text Alignment","summary":"  The advent of multimodal learning has brought a significant improvement in\ndocument AI. Documents are now treated as multimodal entities, incorporating\nboth textual and visual information for downstream analysis. However, works in\nthis space are often focused on the textual aspect, using the visual space as\nauxiliary information. While some works have explored pure vision based\ntechniques for document image understanding, they require OCR identified text\nas input during inference, or do not align with text in their learning\nprocedure. Therefore, we present a novel image-text alignment technique\nspecially designed for leveraging the textual information in document images to\nimprove performance on visual tasks. Our document encoder model DoPTA - trained\nwith this technique demonstrates strong performance on a wide range of document\nimage understanding tasks, without requiring OCR during inference. Combined\nwith an auxiliary reconstruction objective, DoPTA consistently outperforms\nlarger models, while using significantly lesser pre-training compute. DoPTA\nalso sets new state-of-the art results on D4LA, and FUNSD, two challenging\ndocument visual analysis benchmarks.\n","authors":["Nikitha SR","Tarun Ram Menta","Mausoom Sarkar"],"pdf_url":"https://arxiv.org/pdf/2412.12902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12892v1","updated":"2024-12-17T13:18:41Z","published":"2024-12-17T13:18:41Z","title":"SAUGE: Taming SAM for Uncertainty-Aligned Multi-Granularity Edge\n  Detection","summary":"  Edge labels are typically at various granularity levels owing to the varying\npreferences of annotators, thus handling the subjectivity of per-pixel labels\nhas been a focal point for edge detection. Previous methods often employ a\nsimple voting strategy to diminish such label uncertainty or impose a strong\nassumption of labels with a pre-defined distribution, e.g., Gaussian. In this\nwork, we unveil that the segment anything model (SAM) provides strong prior\nknowledge to model the uncertainty in edge labels. Our key insight is that the\nintermediate SAM features inherently correspond to object edges at various\ngranularities, which reflects different edge options due to uncertainty.\nTherefore, we attempt to align uncertainty with granularity by regressing\nintermediate SAM features from different layers to object edges at\nmulti-granularity levels. In doing so, the model can fully and explicitly\nexplore diverse ``uncertainties'' in a data-driven fashion. Specifically, we\ninject a lightweight module (~ 1.5% additional parameters) into the frozen SAM\nto progressively fuse and adapt its intermediate features to estimate edges\nfrom coarse to fine. It is crucial to normalize the granularity level of human\nedge labels to match their innate uncertainty. For this, we simply perform\nlinear blending to the real edge labels at hand to create pseudo labels with\nvarying granularities. Consequently, our uncertainty-aligned edge detector can\nflexibly produce edges at any desired granularity (including an optimal one).\nThanks to SAM, our model uniquely demonstrates strong generalizability for\ncross-dataset edge detection. Extensive experimental results on BSDS500,\nMuticue and NYUDv2 validate our model's superiority.\n","authors":["Xing Liufu","Chaolei Tan","Xiaotong Lin","Yonggang Qi","Jinxuan Li","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2412.12892v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12890v1","updated":"2024-12-17T13:17:19Z","published":"2024-12-17T13:17:19Z","title":"Suppressing Uncertainty in Gaze Estimation","summary":"  Uncertainty in gaze estimation manifests in two aspects: 1) low-quality\nimages caused by occlusion, blurriness, inconsistent eye movements, or even\nnon-face images; 2) incorrect labels resulting from the misalignment between\nthe labeled and actual gaze points during the annotation process. Allowing\nthese uncertainties to participate in training hinders the improvement of gaze\nestimation. To tackle these challenges, in this paper, we propose an effective\nsolution, named Suppressing Uncertainty in Gaze Estimation (SUGE), which\nintroduces a novel triplet-label consistency measurement to estimate and reduce\nthe uncertainties. Specifically, for each training sample, we propose to\nestimate a novel ``neighboring label'' calculated by a linearly weighted\nprojection from the neighbors to capture the similarity relationship between\nimage features and their corresponding labels, which can be incorporated with\nthe predicted pseudo label and ground-truth label for uncertainty estimation.\nBy modeling such triplet-label consistency, we can measure the qualities of\nboth images and labels, and further largely reduce the negative effects of\nunqualified images and wrong labels through our designed sample weighting and\nlabel correction strategies. Experimental results on the gaze estimation\nbenchmarks indicate that our proposed SUGE achieves state-of-the-art\nperformance.\n","authors":["Shijing Wang","Yaping Huang"],"pdf_url":"https://arxiv.org/pdf/2412.12890v1.pdf","comment":"This paper has been accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2412.12888v1","updated":"2024-12-17T13:12:31Z","published":"2024-12-17T13:12:31Z","title":"ArtAug: Enhancing Text-to-Image Generation through\n  Synthesis-Understanding Interaction","summary":"  The emergence of diffusion models has significantly advanced image synthesis.\nThe recent studies of model interaction and self-corrective reasoning approach\nin large language models offer new insights for enhancing text-to-image models.\nInspired by these studies, we propose a novel method called ArtAug for\nenhancing text-to-image models in this paper. To the best of our knowledge,\nArtAug is the first one that improves image synthesis models via model\ninteractions with understanding models. In the interactions, we leverage human\npreferences implicitly learned by image understanding models to provide\nfine-grained suggestions for image synthesis models. The interactions can\nmodify the image content to make it aesthetically pleasing, such as adjusting\nexposure, changing shooting angles, and adding atmospheric effects. The\nenhancements brought by the interaction are iteratively fused into the\nsynthesis model itself through an additional enhancement module. This enables\nthe synthesis model to directly produce aesthetically pleasing images without\nany extra computational cost. In the experiments, we train the ArtAug\nenhancement module on existing text-to-image models. Various evaluation metrics\nconsistently demonstrate that ArtAug enhances the generative capabilities of\ntext-to-image models without incurring additional computational costs. The\nsource code and models will be released publicly.\n","authors":["Zhongjie Duan","Qianyi Zhao","Cen Chen","Daoyuan Chen","Wenmeng Zhou","Yaliang Li","Yingda Chen"],"pdf_url":"https://arxiv.org/pdf/2412.12888v1.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.12887v1","updated":"2024-12-17T13:11:48Z","published":"2024-12-17T13:11:48Z","title":"Learning Coarse-to-Fine Pruning of Graph Convolutional Networks for\n  Skeleton-based Recognition","summary":"  Magnitude Pruning is a staple lightweight network design method which seeks\nto remove connections with the smallest magnitude. This process is either\nachieved in a structured or unstructured manner. While structured pruning\nallows reaching high efficiency, unstructured one is more flexible and leads to\nbetter accuracy, but this is achieved at the expense of low computational\nperformance. In this paper, we devise a novel coarse-to-fine (CTF) method that\ngathers the advantages of structured and unstructured pruning while discarding\ntheir inconveniences to some extent. Our method relies on a novel CTF\nparametrization that models the mask of each connection as the Hadamard product\ninvolving four parametrizations which capture channel-wise, column-wise,\nrow-wise and entry-wise pruning respectively. Hence, fine-grained pruning is\nenabled only when the coarse-grained one is disabled, and this leads to highly\nefficient networks while being effective. Extensive experiments conducted on\nthe challenging task of skeleton-based recognition, using the standard SBU and\nFPHA datasets, show the clear advantage of our CTF approach against different\nbaselines as well as the related work.\n","authors":["Hichem Sahbi"],"pdf_url":"https://arxiv.org/pdf/2412.12887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14322v2","updated":"2024-12-17T13:02:10Z","published":"2024-11-21T17:12:47Z","title":"SplatR : Experience Goal Visual Rearrangement with 3D Gaussian Splatting\n  and Dense Feature Matching","summary":"  Experience Goal Visual Rearrangement task stands as a foundational challenge\nwithin Embodied AI, requiring an agent to construct a robust world model that\naccurately captures the goal state. The agent uses this world model to restore\na shuffled scene to its original configuration, making an accurate\nrepresentation of the world essential for successfully completing the task. In\nthis work, we present a novel framework that leverages on 3D Gaussian Splatting\nas a 3D scene representation for experience goal visual rearrangement task.\nRecent advances in volumetric scene representation like 3D Gaussian Splatting,\noffer fast rendering of high quality and photo-realistic novel views. Our\napproach enables the agent to have consistent views of the current and the goal\nsetting of the rearrangement task, which enables the agent to directly compare\nthe goal state and the shuffled state of the world in image space. To compare\nthese views, we propose to use a dense feature matching method with visual\nfeatures extracted from a foundation model, leveraging its advantages of a more\nuniversal feature representation, which facilitates robustness, and\ngeneralization. We validate our approach on the AI2-THOR rearrangement\nchallenge benchmark and demonstrate improvements over the current state of the\nart methods\n","authors":["Arjun P S","Andrew Melnik","Gora Chand Nandi"],"pdf_url":"https://arxiv.org/pdf/2411.14322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12877v1","updated":"2024-12-17T13:00:04Z","published":"2024-12-17T13:00:04Z","title":"MIVE: New Design and Benchmark for Multi-Instance Video Editing","summary":"  Recent AI-based video editing has enabled users to edit videos through simple\ntext prompts, significantly simplifying the editing process. However, recent\nzero-shot video editing techniques primarily focus on global or single-object\nedits, which can lead to unintended changes in other parts of the video. When\nmultiple objects require localized edits, existing methods face challenges,\nsuch as unfaithful editing, editing leakage, and lack of suitable evaluation\ndatasets and metrics. To overcome these limitations, we propose a zero-shot\n$\\textbf{M}$ulti-$\\textbf{I}$nstance $\\textbf{V}$ideo $\\textbf{E}$diting\nframework, called MIVE. MIVE is a general-purpose mask-based framework, not\ndedicated to specific objects (e.g., people). MIVE introduces two key modules:\n(i) Disentangled Multi-instance Sampling (DMS) to prevent editing leakage and\n(ii) Instance-centric Probability Redistribution (IPR) to ensure precise\nlocalization and faithful editing. Additionally, we present our new MIVE\nDataset featuring diverse video scenarios and introduce the Cross-Instance\nAccuracy (CIA) Score to evaluate editing leakage in multi-instance video\nediting tasks. Our extensive qualitative, quantitative, and user study\nevaluations demonstrate that MIVE significantly outperforms recent\nstate-of-the-art methods in terms of editing faithfulness, accuracy, and\nleakage prevention, setting a new benchmark for multi-instance video editing.\nThe project page is available at https://kaist-viclab.github.io/mive-site/\n","authors":["Samuel Teodoro","Agus Gunawan","Soo Ye Kim","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2412.12877v1.pdf","comment":"The first two authors contributed equally to this work. The last two\n  authors are co-corresponding authors. Please visit our project page at\n  https://kaist-viclab.github.io/mive-site/"},{"id":"http://arxiv.org/abs/2312.16476v6","updated":"2024-12-17T12:55:57Z","published":"2023-12-27T08:50:01Z","title":"SVGDreamer: Text Guided SVG Generation with Diffusion Model","summary":"  Recently, text-guided scalable vector graphics (SVGs) synthesis has shown\npromise in domains such as iconography and sketch. However, existing\ntext-to-SVG generation methods lack editability and struggle with visual\nquality and result diversity. To address these limitations, we propose a novel\ntext-guided vector graphics synthesis method called SVGDreamer. SVGDreamer\nincorporates a semantic-driven image vectorization (SIVE) process that enables\nthe decomposition of synthesis into foreground objects and background, thereby\nenhancing editability. Specifically, the SIVE process introduces\nattention-based primitive control and an attention-mask loss function for\neffective control and manipulation of individual elements. Additionally, we\npropose a Vectorized Particle-based Score Distillation (VPSD) approach to\naddress issues of shape over-smoothing, color over-saturation, limited\ndiversity, and slow convergence of the existing text-to-SVG generation methods\nby modeling SVGs as distributions of control points and colors. Furthermore,\nVPSD leverages a reward model to re-weight vector particles, which improves\naesthetic appeal and accelerates convergence. Extensive experiments are\nconducted to validate the effectiveness of SVGDreamer, demonstrating its\nsuperiority over baseline methods in terms of editability, visual quality, and\ndiversity. Project page: https://ximinng.github.io/SVGDreamer-project/\n","authors":["Ximing Xing","Haitao Zhou","Chuang Wang","Jing Zhang","Dong Xu","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2312.16476v6.pdf","comment":"Accepted by CVPR 2024. project link:\n  https://ximinng.github.io/SVGDreamer-project/"},{"id":"http://arxiv.org/abs/2412.12861v1","updated":"2024-12-17T12:43:10Z","published":"2024-12-17T12:43:10Z","title":"Dyn-HaMR: Recovering 4D Interacting Hand Motion from a Dynamic Camera","summary":"  We propose Dyn-HaMR, to the best of our knowledge, the first approach to\nreconstruct 4D global hand motion from monocular videos recorded by dynamic\ncameras in the wild. Reconstructing accurate 3D hand meshes from monocular\nvideos is a crucial task for understanding human behaviour, with significant\napplications in augmented and virtual reality (AR/VR). However, existing\nmethods for monocular hand reconstruction typically rely on a weak perspective\ncamera model, which simulates hand motion within a limited camera frustum. As a\nresult, these approaches struggle to recover the full 3D global trajectory and\noften produce noisy or incorrect depth estimations, particularly when the video\nis captured by dynamic or moving cameras, which is common in egocentric\nscenarios. Our Dyn-HaMR consists of a multi-stage, multi-objective optimization\npipeline, that factors in (i) simultaneous localization and mapping (SLAM) to\nrobustly estimate relative camera motion, (ii) an interacting-hand prior for\ngenerative infilling and to refine the interaction dynamics, ensuring plausible\nrecovery under (self-)occlusions, and (iii) hierarchical initialization through\na combination of state-of-the-art hand tracking methods. Through extensive\nevaluations on both in-the-wild and indoor datasets, we show that our approach\nsignificantly outperforms state-of-the-art methods in terms of 4D global mesh\nrecovery. This establishes a new benchmark for hand motion reconstruction from\nmonocular video with moving cameras. Our project page is at\nhttps://dyn-hamr.github.io/.\n","authors":["Zhengdi Yu","Stefanos Zafeiriou","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2412.12861v1.pdf","comment":"Project page is available at https://dyn-hamr.github.io/"},{"id":"http://arxiv.org/abs/2412.12853v1","updated":"2024-12-17T12:29:32Z","published":"2024-12-17T12:29:32Z","title":"Automatic Left Ventricular Cavity Segmentation via Deep Spatial\n  Sequential Network in 4D Computed Tomography Studies","summary":"  Automated segmentation of left ventricular cavity (LVC) in temporal cardiac\nimage sequences (multiple time points) is a fundamental requirement for\nquantitative analysis of its structural and functional changes. Deep learning\nbased methods for the segmentation of LVC are the state of the art; however,\nthese methods are generally formulated to work on single time points, and fails\nto exploit the complementary information from the temporal image sequences that\ncan aid in segmentation accuracy and consistency among the images across the\ntime points. Furthermore, these segmentation methods perform poorly in\nsegmenting the end-systole (ES) phase images, where the left ventricle deforms\nto the smallest irregular shape, and the boundary between the blood chamber and\nmyocardium becomes inconspicuous. To overcome these limitations, we propose a\nnew method to automatically segment temporal cardiac images where we introduce\na spatial sequential (SS) network to learn the deformation and motion\ncharacteristics of the LVC in an unsupervised manner; these characteristics\nwere then integrated with sequential context information derived from\nbi-directional learning (BL) where both chronological and reverse-chronological\ndirections of the image sequence were used. Our experimental results on a\ncardiac computed tomography (CT) dataset demonstrated that our\nspatial-sequential network with bi-directional learning (SS-BL) method\noutperformed existing methods for LVC segmentation. Our method was also applied\nto MRI cardiac dataset and the results demonstrated the generalizability of our\nmethod.\n","authors":["Yuyu Guo","Lei Bi","Zhengbin Zhu","David Dagan Feng","Ruiyan Zhang","Qian Wang","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2412.12853v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2412.12850v1","updated":"2024-12-17T12:24:08Z","published":"2024-12-17T12:24:08Z","title":"Boosting Fine-Grained Visual Anomaly Detection with\n  Coarse-Knowledge-Aware Adversarial Learning","summary":"  Many unsupervised visual anomaly detection methods train an auto-encoder to\nreconstruct normal samples and then leverage the reconstruction error map to\ndetect and localize the anomalies. However, due to the powerful modeling and\ngeneralization ability of neural networks, some anomalies can also be well\nreconstructed, resulting in unsatisfactory detection and localization accuracy.\nIn this paper, a small coarsely-labeled anomaly dataset is first collected.\nThen, a coarse-knowledge-aware adversarial learning method is developed to\nalign the distribution of reconstructed features with that of normal features.\nThe alignment can effectively suppress the auto-encoder's reconstruction\nability on anomalies and thus improve the detection accuracy. Considering that\nanomalies often only occupy very small areas in anomalous images, a patch-level\nadversarial learning strategy is further developed. Although no patch-level\nanomalous information is available, we rigorously prove that by simply viewing\nany patch features from anomalous images as anomalies, the proposed\nknowledge-aware method can also align the distribution of reconstructed patch\nfeatures with the normal ones. Experimental results on four medical datasets\nand two industrial datasets demonstrate the effectiveness of our method in\nimproving the detection and localization performance.\n","authors":["Qingqing Fang","Qinliang Su","Wenxi Lv","Wenchao Xu","Jianxing Yu"],"pdf_url":"https://arxiv.org/pdf/2412.12850v1.pdf","comment":"The paper is accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12849v1","updated":"2024-12-17T12:23:07Z","published":"2024-12-17T12:23:07Z","title":"HyperGS: Hyperspectral 3D Gaussian Splatting","summary":"  We introduce HyperGS, a novel framework for Hyperspectral Novel View\nSynthesis (HNVS), based on a new latent 3D Gaussian Splatting (3DGS) technique.\nOur approach enables simultaneous spatial and spectral renderings by encoding\nmaterial properties from multi-view 3D hyperspectral datasets. HyperGS\nreconstructs high-fidelity views from arbitrary perspectives with improved\naccuracy and speed, outperforming currently existing methods. To address the\nchallenges of high-dimensional data, we perform view synthesis in a learned\nlatent space, incorporating a pixel-wise adaptive density function and a\npruning technique for increased training stability and efficiency.\nAdditionally, we introduce the first HNVS benchmark, implementing a number of\nnew baselines based on recent SOTA RGB-NVS techniques, alongside the small\nnumber of prior works on HNVS. We demonstrate HyperGS's robustness through\nextensive evaluation of real and simulated hyperspectral scenes with a 14db\naccuracy improvement upon previously published models.\n","authors":["Christopher Thirgood","Oscar Mendez","Erin Chao Ling","Jon Storey","Simon Hadfield"],"pdf_url":"https://arxiv.org/pdf/2412.12849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12843v1","updated":"2024-12-17T12:11:04Z","published":"2024-12-17T12:11:04Z","title":"Efficient Event-based Semantic Segmentation with Spike-driven\n  Lightweight Transformer-based Networks","summary":"  Event-based semantic segmentation has great potential in autonomous driving\nand robotics due to the advantages of event cameras, such as high dynamic\nrange, low latency, and low power cost. Unfortunately, current artificial\nneural network (ANN)-based segmentation methods suffer from high computational\ndemands, the requirements for image frames, and massive energy consumption,\nlimiting their efficiency and application on resource-constrained edge/mobile\nplatforms. To address these problems, we introduce SLTNet, a spike-driven\nlightweight transformer-based network designed for event-based semantic\nsegmentation. Specifically, SLTNet is built on efficient spike-driven\nconvolution blocks (SCBs) to extract rich semantic features while reducing the\nmodel's parameters. Then, to enhance the long-range contextural feature\ninteraction, we propose novel spike-driven transformer blocks (STBs) with\nbinary mask operations. Based on these basic blocks, SLTNet employs a\nhigh-efficiency single-branch architecture while maintaining the low energy\nconsumption of the Spiking Neural Network (SNN). Finally, extensive experiments\non DDD17 and DSEC-Semantic datasets demonstrate that SLTNet outperforms\nstate-of-the-art (SOTA) SNN-based methods by at least 7.30% and 3.30% mIoU,\nrespectively, with extremely 5.48x lower energy consumption and 1.14x faster\ninference speed.\n","authors":["Xiaxin Zhu","Fangming Guo","Xianlei Long","Qingyi Gu","Chao Chen","Fuqiang Gu"],"pdf_url":"https://arxiv.org/pdf/2412.12843v1.pdf","comment":"Submitted to IEEE ICRA 2025"},{"id":"http://arxiv.org/abs/2409.03487v3","updated":"2024-12-17T11:57:52Z","published":"2024-09-05T12:52:24Z","title":"ScreenMark: Watermarking Arbitrary Visual Content on Screen","summary":"  Digital watermarking has shown its effectiveness in protecting multimedia\ncontent. However, existing watermarking is predominantly tailored for specific\nmedia types, rendering them less effective for the protection of content\ndisplayed on computer screens, which is often multi-modal and dynamic. Visual\nScreen Content (VSC), is particularly susceptible to theft and leakage through\nscreenshots, a vulnerability that current watermarking methods fail to\nadequately address.To address these challenges, we propose ScreenMark, a robust\nand practical watermarking method designed specifically for arbitrary VSC\nprotection. ScreenMark utilizes a three-stage progressive watermarking\nframework. Initially, inspired by diffusion principles, we initialize the\nmutual transformation between regular watermark information and irregular\nwatermark patterns. Subsequently, these patterns are integrated with screen\ncontent using a pre-multiplication alpha blending technique, supported by a\npre-trained screen decoder for accurate watermark retrieval. The progressively\ncomplex distorter enhances the robustness of the watermark in real-world\nscreenshot scenarios. Finally, the model undergoes fine-tuning guided by a\njoint-level distorter to ensure optimal performance. To validate the\neffectiveness of ScreenMark, we compiled a dataset comprising 100,000\nscreenshots from various devices and resolutions. Extensive experiments on\ndifferent datasets confirm the superior robustness, imperceptibility, and\npractical applicability of the method.\n","authors":["Xiujian Liang","Gaozhi Liu","Yichao Si","Xiaoxiao Hu","Zhenxing Qian"],"pdf_url":"https://arxiv.org/pdf/2409.03487v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12833v1","updated":"2024-12-17T11:54:47Z","published":"2024-12-17T11:54:47Z","title":"FocusChat: Text-guided Long Video Understanding via Spatiotemporal\n  Information Filtering","summary":"  Recently, multi-modal large language models have made significant progress.\nHowever, visual information lacking of guidance from the user's intention may\nlead to redundant computation and involve unnecessary visual noise, especially\nin long, untrimmed videos. To address this issue, we propose FocusChat, a\ntext-guided multi-modal large language model (LLM) that emphasizes visual\ninformation correlated to the user's prompt. In detail, Our model first\nundergoes the semantic extraction module, which comprises a visual semantic\nbranch and a text semantic branch to extract image and text semantics,\nrespectively. The two branches are combined using the Spatial-Temporal\nFiltering Module (STFM). STFM enables explicit spatial-level information\nfiltering and implicit temporal-level feature filtering, ensuring that the\nvisual tokens are closely aligned with the user's query. It lowers the\nessential number of visual tokens inputted into the LLM. FocusChat\nsignificantly outperforms Video-LLaMA in zero-shot experiments, using an order\nof magnitude less training data with only 16 visual tokens occupied. It\nachieves results comparable to the state-of-the-art in few-shot experiments,\nwith only 0.72M pre-training data.\n","authors":["Zheng Cheng","Rendong Wang","Zhicheng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12833v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.12830v1","updated":"2024-12-17T11:52:10Z","published":"2024-12-17T11:52:10Z","title":"Differential Alignment for Domain Adaptive Object Detection","summary":"  Domain adaptive object detection (DAOD) aims to generalize an object detector\ntrained on labeled source-domain data to a target domain without annotations,\nthe core principle of which is \\emph{source-target feature alignment}.\nTypically, existing approaches employ adversarial learning to align the\ndistributions of the source and target domains as a whole, barely considering\nthe varying significance of distinct regions, say instances under different\ncircumstances and foreground \\emph{vs} background areas, during feature\nalignment. To overcome the shortcoming, we investigates a differential feature\nalignment strategy. Specifically, a prediction-discrepancy feedback instance\nalignment module (dubbed PDFA) is designed to adaptively assign higher weights\nto instances of higher teacher-student detection discrepancy, effectively\nhandling heavier domain-specific information. Additionally, an\nuncertainty-based foreground-oriented image alignment module (UFOA) is proposed\nto explicitly guide the model to focus more on regions of interest. Extensive\nexperiments on widely-used DAOD datasets together with ablation studies are\nconducted to demonstrate the efficacy of our proposed method and reveal its\nsuperiority over other SOTA alternatives. Our code is available at\nhttps://github.com/EstrellaXyu/Differential-Alignment-for-DAOD.\n","authors":["Xinyu He","Xinhui Li","Xiaojie Guo"],"pdf_url":"https://arxiv.org/pdf/2412.12830v1.pdf","comment":"11 pages, 8 figures, accepted by aaai25"},{"id":"http://arxiv.org/abs/2412.12829v1","updated":"2024-12-17T11:49:36Z","published":"2024-12-17T11:49:36Z","title":"2by2: Weakly-Supervised Learning for Global Action Segmentation","summary":"  This paper presents a simple yet effective approach for the poorly\ninvestigated task of global action segmentation, aiming at grouping frames\ncapturing the same action across videos of different activities. Unlike the\ncase of videos depicting all the same activity, the temporal order of actions\nis not roughly shared among all videos, making the task even more challenging.\nWe propose to use activity labels to learn, in a weakly-supervised fashion,\naction representations suitable for global action segmentation. For this\npurpose, we introduce a triadic learning approach for video pairs, to ensure\nintra-video action discrimination, as well as inter-video and inter-activity\naction association. For the backbone architecture, we use a Siamese network\nbased on sparse transformers that takes as input video pairs and determine\nwhether they belong to the same activity. The proposed approach is validated on\ntwo challenging benchmark datasets: Breakfast and YouTube Instructions,\noutperforming state-of-the-art methods.\n","authors":["Elena Bueno-Benito","Mariella Dimiccoli"],"pdf_url":"https://arxiv.org/pdf/2412.12829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12827v1","updated":"2024-12-17T11:47:59Z","published":"2024-12-17T11:47:59Z","title":"TabSniper: Towards Accurate Table Detection & Structure Recognition for\n  Bank Statements","summary":"  Extraction of transaction information from bank statements is required to\nassess one's financial well-being for credit rating and underwriting decisions.\nUnlike other financial documents such as tax forms or financial statements,\nextracting the transaction descriptions from bank statements can provide a\ncomprehensive and recent view into the cash flows and spending patterns. With\nmultiple variations in layout and templates across several banks, extracting\ntransactional level information from different table categories is an arduous\ntask. Existing table structure recognition approaches produce sub optimal\nresults for long, complex tables and are unable to capture all transactions\naccurately. This paper proposes TabSniper, a novel approach for efficient table\ndetection, categorization and structure recognition from bank statements. The\npipeline starts with detecting and categorizing tables of interest from the\nbank statements. The extracted table regions are then processed by the table\nstructure recognition model followed by a post-processing module to transform\nthe transactional data into a structured and standardised format. The detection\nand structure recognition architectures are based on DETR, fine-tuned with\ndiverse bank statements along with additional feature enhancements. Results on\nchallenging datasets demonstrate that TabSniper outperforms strong baselines\nand produces high-quality extraction of transaction information from bank and\nother financial documents across multiple layouts and templates.\n","authors":["Abhishek Trivedi","Sourajit Mukherjee","Rajat Kumar Singh","Vani Agarwal","Sriranjani Ramakrishnan","Himanshu S. Bhatt"],"pdf_url":"https://arxiv.org/pdf/2412.12827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12821v1","updated":"2024-12-17T11:41:49Z","published":"2024-12-17T11:41:49Z","title":"ComprehendEdit: A Comprehensive Dataset and Evaluation Framework for\n  Multimodal Knowledge Editing","summary":"  Large multimodal language models (MLLMs) have revolutionized natural language\nprocessing and visual understanding, but often contain outdated or inaccurate\ninformation. Current multimodal knowledge editing evaluations are limited in\nscope and potentially biased, focusing on narrow tasks and failing to assess\nthe impact on in-domain samples. To address these issues, we introduce\nComprehendEdit, a comprehensive benchmark comprising eight diverse tasks from\nmultiple datasets. We propose two novel metrics: Knowledge Generalization Index\n(KGI) and Knowledge Preservation Index (KPI), which evaluate editing effects on\nin-domain samples without relying on AI-synthetic samples. Based on insights\nfrom our framework, we establish Hierarchical In-Context Editing (HICE), a\nbaseline method employing a two-stage approach that balances performance across\nall metrics. This study provides a more comprehensive evaluation framework for\nmultimodal knowledge editing, reveals unique challenges in this field, and\noffers a baseline method demonstrating improved performance. Our work opens new\nperspectives for future research and provides a foundation for developing more\nrobust and effective editing techniques for MLLMs. The ComprehendEdit benchmark\nand implementation code are available at\nhttps://github.com/yaohui120/ComprehendEdit.\n","authors":["Yaohui Ma","Xiaopeng Hong","Shizhou Zhang","Huiyun Li","Zhilin Zhu","Wei Luo","Zhiheng Ma"],"pdf_url":"https://arxiv.org/pdf/2412.12821v1.pdf","comment":"Extended version for paper accepted to AAAI 2025. Project Page:\n  https://github.com/yaohui120/ComprehendEdit"},{"id":"http://arxiv.org/abs/2412.12801v1","updated":"2024-12-17T11:10:46Z","published":"2024-12-17T11:10:46Z","title":"Multi-View Incremental Learning with Structured Hebbian Plasticity for\n  Enhanced Fusion Efficiency","summary":"  The rapid evolution of multimedia technology has revolutionized human\nperception, paving the way for multi-view learning. However, traditional\nmulti-view learning approaches are tailored for scenarios with fixed data\nviews, falling short of emulating the intricate cognitive procedures of the\nhuman brain processing signals sequentially. Our cerebral architecture\nseamlessly integrates sequential data through intricate feed-forward and\nfeedback mechanisms. In stark contrast, traditional methods struggle to\ngeneralize effectively when confronted with data spanning diverse domains,\nhighlighting the need for innovative strategies that can mimic the brain's\nadaptability and dynamic integration capabilities. In this paper, we propose a\nbio-neurologically inspired multi-view incremental framework named MVIL aimed\nat emulating the brain's fine-grained fusion of sequentially arriving views.\nMVIL lies two fundamental modules: structured Hebbian plasticity and synaptic\npartition learning. The structured Hebbian plasticity reshapes the structure of\nweights to express the high correlation between view representations,\nfacilitating a fine-grained fusion of view representations. Moreover, synaptic\npartition learning is efficient in alleviating drastic changes in weights and\nalso retaining old knowledge by inhibiting partial synapses. These modules\nbionically play a central role in reinforcing crucial associations between\nnewly acquired information and existing knowledge repositories, thereby\nenhancing the network's capacity for generalization. Experimental results on\nsix benchmark datasets show MVIL's effectiveness over state-of-the-art methods.\n","authors":["Yuhong Chen","Ailin Song","Huifeng Yin","Shuai Zhong","Fuhai Chen","Qi Xu","Shiping Wang","Mingkun Xu"],"pdf_url":"https://arxiv.org/pdf/2412.12801v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2412.12799v1","updated":"2024-12-17T11:02:36Z","published":"2024-12-17T11:02:36Z","title":"RCTrans: Radar-Camera Transformer via Radar Densifier and Sequential\n  Decoder for 3D Object Detection","summary":"  In radar-camera 3D object detection, the radar point clouds are sparse and\nnoisy, which causes difficulties in fusing camera and radar modalities. To\nsolve this, we introduce a novel query-based detection method named\nRadar-Camera Transformer (RCTrans). Specifically, we first design a Radar Dense\nEncoder to enrich the sparse valid radar tokens, and then concatenate them with\nthe image tokens. By doing this, we can fully explore the 3D information of\neach interest region and reduce the interference of empty tokens during the\nfusing stage. We then design a Pruning Sequential Decoder to predict 3D boxes\nbased on the obtained tokens and random initialized queries. To alleviate the\neffect of elevation ambiguity in radar point clouds, we gradually locate the\nposition of the object via a sequential fusion structure. It helps to get more\nprecise and flexible correspondences between tokens and queries. A pruning\ntraining strategy is adopted in the decoder, which can save much time during\ninference and inhibit queries from losing their distinctiveness. Extensive\nexperiments on the large-scale nuScenes dataset prove the superiority of our\nmethod, and we also achieve new state-of-the-art radar-camera 3D detection\nresults. Our implementation is available at https://github.com/liyih/RCTrans.\n","authors":["Yiheng Li","Yang Yang","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2412.12799v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12798v1","updated":"2024-12-17T11:00:56Z","published":"2024-12-17T11:00:56Z","title":"ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation","summary":"  Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.\n","authors":["Shiqi Huang","Shuting He","Bihan Wen"],"pdf_url":"https://arxiv.org/pdf/2412.12798v1.pdf","comment":"AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI"},{"id":"http://arxiv.org/abs/2412.12793v1","updated":"2024-12-17T10:56:18Z","published":"2024-12-17T10:56:18Z","title":"CRoF: CLIP-based Robust Few-shot Learning on Noisy Labels","summary":"  Noisy labels threaten the robustness of few-shot learning (FSL) due to the\ninexact features in a new domain. CLIP, a large-scale vision-language model,\nperforms well in FSL on image-text embedding similarities, but it is\nsusceptible to misclassification caused by noisy labels. How to enhance domain\ngeneralization of CLIP on noisy data within FSL tasks is a critical challenge.\nIn this paper, we provide a novel view to mitigate the influence of noisy\nlabels, CLIP-based Robust Few-shot learning (CRoF). CRoF is a general plug-in\nmodule for CLIP-based models. To avoid misclassification and confused label\nembedding, we design the few-shot task-oriented prompt generator to give more\ndiscriminative descriptions of each category. The proposed prompt achieves\nlarger distances of inter-class textual embedding. Furthermore, rather than\nfully trusting zero-shot classification by CLIP, we fine-tune CLIP on noisy\nfew-shot data in a new domain with a weighting strategy like label-smooth. The\nweights for multiple potentially correct labels consider the relationship\nbetween CLIP's prior knowledge and original label information to ensure\nreliability. Our multiple label loss function further supports robust training\nunder this paradigm. Comprehensive experiments show that CRoF, as a plug-in,\noutperforms fine-tuned and vanilla CLIP models on different noise types and\nnoise ratios.\n","authors":["Shizhuo Deng","Bowen Han","Jiaqi Chen","Hao Wang","Dongyue Chen","Tong Jia"],"pdf_url":"https://arxiv.org/pdf/2412.12793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12791v1","updated":"2024-12-17T10:52:50Z","published":"2024-12-17T10:52:50Z","title":"Implicit Location-Caption Alignment via Complementary Masking for\n  Weakly-Supervised Dense Video Captioning","summary":"  Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and\ndescribe all events of interest in a video without requiring annotations of\nevent boundaries. This setting poses a great challenge in accurately locating\nthe temporal location of event, as the relevant supervision is unavailable.\nExisting methods rely on explicit alignment constraints between event locations\nand captions, which involve complex event proposal procedures during both\ntraining and inference. To tackle this problem, we propose a novel implicit\nlocation-caption alignment paradigm by complementary masking, which simplifies\nthe complex event proposal and localization process while maintaining\neffectiveness. Specifically, our model comprises two components: a dual-mode\nvideo captioning module and a mask generation module. The dual-mode video\ncaptioning module captures global event information and generates descriptive\ncaptions, while the mask generation module generates differentiable positive\nand negative masks for localizing the events. These masks enable the implicit\nalignment of event locations and captions by ensuring that captions generated\nfrom positively and negatively masked videos are complementary, thereby forming\na complete video description. In this way, even under weak supervision, the\nevent location and event caption can be aligned implicitly. Extensive\nexperiments on the public datasets demonstrate that our method outperforms\nexisting weakly-supervised methods and achieves competitive results compared to\nfully-supervised methods.\n","authors":["Shiping Ge","Qiang Chen","Zhiwei Jiang","Yafeng Yin","Liu Qin","Ziyao Chen","Qing Gu"],"pdf_url":"https://arxiv.org/pdf/2412.12791v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12788v1","updated":"2024-12-17T10:47:13Z","published":"2024-12-17T10:47:13Z","title":"RA-SGG: Retrieval-Augmented Scene Graph Generation Framework via\n  Multi-Prototype Learning","summary":"  Scene Graph Generation (SGG) research has suffered from two fundamental\nchallenges: the long-tailed predicate distribution and semantic ambiguity\nbetween predicates. These challenges lead to a bias towards head predicates in\nSGG models, favoring dominant general predicates while overlooking fine-grained\npredicates. In this paper, we address the challenges of SGG by framing it as\nmulti-label classification problem with partial annotation, where relevant\nlabels of fine-grained predicates are missing. Under the new frame, we propose\nRetrieval-Augmented Scene Graph Generation (RA-SGG), which identifies potential\ninstances to be multi-labeled and enriches the single-label with multi-labels\nthat are semantically similar to the original label by retrieving relevant\nsamples from our established memory bank. Based on augmented relations (i.e.,\ndiscovered multi-labels), we apply multi-prototype learning to train our SGG\nmodel. Several comprehensive experiments have demonstrated that RA-SGG\noutperforms state-of-the-art baselines by up to 3.6% on VG and 5.9% on GQA,\nparticularly in terms of F@K, showing that RA-SGG effectively alleviates the\nissue of biased prediction caused by the long-tailed distribution and semantic\nambiguity of predicates.\n","authors":["Kanghoon Yoon","Kibum Kim","Jaehyung Jeon","Yeonjun In","Donghyun Kim","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2412.12788v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2310.12848v2","updated":"2024-12-17T10:44:49Z","published":"2023-10-19T15:59:24Z","title":"Neural Degradation Representation Learning for All-In-One Image\n  Restoration","summary":"  Existing methods have demonstrated effective performance on a single\ndegradation type. In practical applications, however, the degradation is often\nunknown, and the mismatch between the model and the degradation will result in\na severe performance drop. In this paper, we propose an all-in-one image\nrestoration network that tackles multiple degradations. Due to the\nheterogeneous nature of different types of degradations, it is difficult to\nprocess multiple degradations in a single network. To this end, we propose to\nlearn a neural degradation representation (NDR) that captures the underlying\ncharacteristics of various degradations. The learned NDR decomposes different\ntypes of degradations adaptively, similar to a neural dictionary that\nrepresents basic degradation components. Subsequently, we develop a degradation\nquery module and a degradation injection module to effectively recognize and\nutilize the specific degradation based on NDR, enabling the all-in-one\nrestoration ability for multiple degradations. Moreover, we propose a\nbidirectional optimization strategy to effectively drive NDR to learn the\ndegradation representation by optimizing the degradation and restoration\nprocesses alternately. Comprehensive experiments on representative types of\ndegradations (including noise, haze, rain, and downsampling) demonstrate the\neffectiveness and generalization capability of our method.\n","authors":["Mingde Yao","Ruikang Xu","Yuanshen Guan","Jie Huang","Zhiwei Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.12848v2.pdf","comment":"Code: https://github.com/mdyao/NDR-Restore"},{"id":"http://arxiv.org/abs/2412.12785v1","updated":"2024-12-17T10:44:47Z","published":"2024-12-17T10:44:47Z","title":"Activating Distributed Visual Region within LLMs for Efficient and\n  Effective Vision-Language Training and Inference","summary":"  Large Vision-Language Models (LVLMs) typically learn visual capacity through\nvisual instruction tuning, involving updates to both a projector and their LLM\nbackbones. Drawing inspiration from the concept of visual region in the human\nbrain, we investigate the existence of an analogous \\textit{visual region}\nwithin LLMs that functions as a cognitive core, and explore the possibility of\nefficient training of LVLMs via selective layers tuning. We use\nBunny-Llama-3-8B-V for detailed experiments and LLaVA-1.5-7B and LLaVA-1.5-13B\nfor validation across a range of visual and textual tasks. Our findings reveal\nthat selectively updating 25\\% of LLMs layers, when sparsely and uniformly\ndistributed, can preserve nearly 99\\% of visual performance while maintaining\nor enhancing textual task results, and also effectively reducing training time.\nBased on this targeted training approach, we further propose a novel visual\nregion-based pruning paradigm, removing non-critical layers outside the visual\nregion, which can achieve minimal performance loss. This study offers an\neffective and efficient strategy for LVLM training and inference by activating\na layer-wise visual region within LLMs, which is consistently effective across\ndifferent models and parameter scales.\n","authors":["Siyuan Wang","Dianyi Wang","Chengxing Zhou","Zejun Li","Zhihao Fan","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2412.12785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12782v1","updated":"2024-12-17T10:42:19Z","published":"2024-12-17T10:42:19Z","title":"Bidirectional Logits Tree: Pursuing Granularity Reconcilement in\n  Fine-Grained Classification","summary":"  This paper addresses the challenge of Granularity Competition in fine-grained\nclassification tasks, which arises due to the semantic gap between\nmulti-granularity labels. Existing approaches typically develop independent\nhierarchy-aware models based on shared features extracted from a common base\nencoder. However, because coarse-grained levels are inherently easier to learn\nthan finer ones, the base encoder tends to prioritize coarse feature\nabstractions, which impedes the learning of fine-grained features. To overcome\nthis challenge, we propose a novel framework called the Bidirectional Logits\nTree (BiLT) for Granularity Reconcilement. The key idea is to develop\nclassifiers sequentially from the finest to the coarsest granularities, rather\nthan parallelly constructing a set of classifiers based on the same input\nfeatures. In this setup, the outputs of finer-grained classifiers serve as\ninputs for coarser-grained ones, facilitating the flow of hierarchical semantic\ninformation across different granularities. On top of this, we further\nintroduce an Adaptive Intra-Granularity Difference Learning (AIGDL) approach to\nuncover subtle semantic differences between classes within the same\ngranularity. Extensive experiments demonstrate the effectiveness of our\nproposed method.\n","authors":["Zhiguang Lu","Qianqian Xu","Shilong Bao","Zhiyong Yang","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2412.12782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12778v1","updated":"2024-12-17T10:37:46Z","published":"2024-12-17T10:37:46Z","title":"Rethinking Diffusion-Based Image Generators for Fundus Fluorescein\n  Angiography Synthesis on Limited Data","summary":"  Fundus imaging is a critical tool in ophthalmology, with different imaging\nmodalities offering unique advantages. For instance, fundus fluorescein\nangiography (FFA) can accurately identify eye diseases. However, traditional\ninvasive FFA involves the injection of sodium fluorescein, which can cause\ndiscomfort and risks. Generating corresponding FFA images from non-invasive\nfundus images holds significant practical value but also presents challenges.\nFirst, limited datasets constrain the performance and effectiveness of models.\nSecond, previous studies have primarily focused on generating FFA for single\ndiseases or single modalities, often resulting in poor performance for patients\nwith various ophthalmic conditions. To address these issues, we propose a novel\nlatent diffusion model-based framework, Diffusion, which introduces a\nfine-tuning protocol to overcome the challenge of limited medical data and\nunleash the generative capabilities of diffusion models. Furthermore, we\ndesigned a new approach to tackle the challenges of generating across different\nmodalities and disease types. On limited datasets, our framework achieves\nstate-of-the-art results compared to existing methods, offering significant\npotential to enhance ophthalmic diagnostics and patient care. Our code will be\nreleased soon to support further research in this field.\n","authors":["Chengzhou Yu","Huihui Fang","Hongqiu Wang","Ting Deng","Qing Du","Yanwu Xu","Weihua Yang"],"pdf_url":"https://arxiv.org/pdf/2412.12778v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.12774v1","updated":"2024-12-17T10:35:27Z","published":"2024-12-17T10:35:27Z","title":"A Framework for Critical Evaluation of Text-to-Image Models: Integrating\n  Art Historical Analysis, Artistic Exploration, and Critical Prompt\n  Engineering","summary":"  This paper proposes a novel interdisciplinary framework for the critical\nevaluation of text-to-image models, addressing the limitations of current\ntechnical metrics and bias studies. By integrating art historical analysis,\nartistic exploration, and critical prompt engineering, the framework offers a\nmore nuanced understanding of these models' capabilities and societal\nimplications. Art historical analysis provides a structured approach to examine\nvisual and symbolic elements, revealing potential biases and\nmisrepresentations. Artistic exploration, through creative experimentation,\nuncovers hidden potentials and limitations, prompting critical reflection on\nthe algorithms' assumptions. Critical prompt engineering actively challenges\nthe model's assumptions, exposing embedded biases. Case studies demonstrate the\nframework's practical application, showcasing how it can reveal biases related\nto gender, race, and cultural representation. This comprehensive approach not\nonly enhances the evaluation of text-to-image models but also contributes to\nthe development of more equitable, responsible, and culturally aware AI\nsystems.\n","authors":["Amalia Foka"],"pdf_url":"https://arxiv.org/pdf/2412.12774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12772v1","updated":"2024-12-17T10:33:36Z","published":"2024-12-17T10:33:36Z","title":"Optimize the Unseen -- Fast NeRF Cleanup with Free Space Prior","summary":"  Neural Radiance Fields (NeRF) have advanced photorealistic novel view\nsynthesis, but their reliance on photometric reconstruction introduces\nartifacts, commonly known as \"floaters\". These artifacts degrade novel view\nquality, especially in areas unseen by the training cameras. We present a fast,\npost-hoc NeRF cleanup method that eliminates such artifacts by enforcing our\nFree Space Prior, effectively minimizing floaters without disrupting the NeRF's\nrepresentation of observed regions. Unlike existing approaches that rely on\neither Maximum Likelihood (ML) estimation to fit the data or a complex, local\ndata-driven prior, our method adopts a Maximum-a-Posteriori (MAP) approach,\nselecting the optimal model parameters under a simple global prior assumption\nthat unseen regions should remain empty. This enables our method to clean\nartifacts in both seen and unseen areas, enhancing novel view quality even in\nchallenging scene regions. Our method is comparable with existing NeRF cleanup\nmodels while being 2.5x faster in inference time, requires no additional memory\nbeyond the original NeRF, and achieves cleanup training in less than 30\nseconds. Our code will be made publically available.\n","authors":["Leo Segre","Shai Avidan"],"pdf_url":"https://arxiv.org/pdf/2412.12772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12771v1","updated":"2024-12-17T10:33:34Z","published":"2024-12-17T10:33:34Z","title":"Guided and Variance-Corrected Fusion with One-shot Style Alignment for\n  Large-Content Image Generation","summary":"  Producing large images using small diffusion models is gaining increasing\npopularity, as the cost of training large models could be prohibitive. A common\napproach involves jointly generating a series of overlapped image patches and\nobtaining large images by merging adjacent patches. However, results from\nexisting methods often exhibit obvious artifacts, e.g., seams and inconsistent\nobjects and styles. To address the issues, we proposed Guided Fusion (GF),\nwhich mitigates the negative impact from distant image regions by applying a\nweighted average to the overlapping regions. Moreover, we proposed\nVariance-Corrected Fusion (VCF), which corrects data variance at\npost-averaging, generating more accurate fusion for the Denoising Diffusion\nProbabilistic Model. Furthermore, we proposed a one-shot Style Alignment (SA),\nwhich generates a coherent style for large images by adjusting the initial\ninput noise without adding extra computational burden. Extensive experiments\ndemonstrated that the proposed fusion methods improved the quality of the\ngenerated image significantly. As a plug-and-play module, the proposed method\ncan be widely applied to enhance other fusion-based methods for large image\ngeneration.\n","authors":["Shoukun Sun","Min Xian","Tiankai Yao","Fei Xu","Luca Capriotti"],"pdf_url":"https://arxiv.org/pdf/2412.12771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12766v1","updated":"2024-12-17T10:31:03Z","published":"2024-12-17T10:31:03Z","title":"Towards a Training Free Approach for 3D Scene Editing","summary":"  Text driven diffusion models have shown remarkable capabilities in editing\nimages. However, when editing 3D scenes, existing works mostly rely on training\na NeRF for 3D editing. Recent NeRF editing methods leverages edit operations by\ndeploying 2D diffusion models and project these edits into 3D space. They\nrequire strong positional priors alongside text prompt to identify the edit\nlocation. These methods are operational on small 3D scenes and are more\ngeneralized to particular scene. They require training for each specific edit\nand cannot be exploited in real-time edits. To address these limitations, we\npropose a novel method, FreeEdit, to make edits in training free manner using\nmesh representations as a substitute for NeRF. Training-free methods are now a\npossibility because of the advances in foundation model's space. We leverage\nthese models to bring a training-free alternative and introduce solutions for\ninsertion, replacement and deletion. We consider insertion, replacement and\ndeletion as basic blocks for performing intricate edits with certain\ncombinations of these operations. Given a text prompt and a 3D scene, our model\nis capable of identifying what object should be inserted/replaced or deleted\nand location where edit should be performed. We also introduce a novel\nalgorithm as part of FreeEdit to find the optimal location on grounding object\nfor placement. We evaluate our model by comparing it with baseline models on a\nwide range of scenes using quantitative and qualitative metrics and showcase\nthe merits of our method with respect to others.\n","authors":["Vivek Madhavaram","Shivangana Rawat","Chaitanya Devaguptapu","Charu Sharma","Manohar Kaul"],"pdf_url":"https://arxiv.org/pdf/2412.12766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12765v1","updated":"2024-12-17T10:30:56Z","published":"2024-12-17T10:30:56Z","title":"Monocular Facial Appearance Capture in the Wild","summary":"  We present a new method for reconstructing the appearance properties of human\nfaces from a lightweight capture procedure in an unconstrained environment. Our\nmethod recovers the surface geometry, diffuse albedo, specular intensity and\nspecular roughness from a monocular video containing a simple head rotation\nin-the-wild. Notably, we make no simplifying assumptions on the environment\nlighting, and we explicitly take visibility and occlusions into account. As a\nresult, our method can produce facial appearance maps that approach the\nfidelity of studio-based multi-view captures, but with a far easier and cheaper\nprocedure.\n","authors":["Yingyan Xu","Kate Gadola","Prashanth Chandran","Sebastian Weiss","Markus Gross","Gaspard Zoss","Derek Bradley"],"pdf_url":"https://arxiv.org/pdf/2412.12765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12755v1","updated":"2024-12-17T10:20:29Z","published":"2024-12-17T10:20:29Z","title":"Progressive Monitoring of Generative Model Training Evolution","summary":"  While deep generative models (DGMs) have gained popularity, their\nsusceptibility to biases and other inefficiencies that lead to undesirable\noutcomes remains an issue. With their growing complexity, there is a critical\nneed for early detection of issues to achieve desired results and optimize\nresources. Hence, we introduce a progressive analysis framework to monitor the\ntraining process of DGMs. Our method utilizes dimensionality reduction\ntechniques to facilitate the inspection of latent representations, the\ngenerated and real distributions, and their evolution across training\niterations. This monitoring allows us to pause and fix the training method if\nthe representations or distributions progress undesirably. This approach allows\nfor the analysis of a models' training dynamics and the timely identification\nof biases and failures, minimizing computational loads. We demonstrate how our\nmethod supports identifying and mitigating biases early in training a\nGenerative Adversarial Network (GAN) and improving the quality of the generated\ndata distribution.\n","authors":["Vidya Prasad","Anna Vilanova","Nicola Pezzotti"],"pdf_url":"https://arxiv.org/pdf/2412.12755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12743v1","updated":"2024-12-17T10:06:42Z","published":"2024-12-17T10:06:42Z","title":"Training a Distributed Acoustic Sensing Traffic Monitoring Network With\n  Video Inputs","summary":"  Distributed Acoustic Sensing (DAS) has emerged as a promising tool for\nreal-time traffic monitoring in densely populated areas. In this paper, we\npresent a novel concept that integrates DAS data with co-located visual\ninformation. We use YOLO-derived vehicle location and classification from\ncamera inputs as labeled data to train a detection and classification neural\nnetwork utilizing DAS data only. Our model achieves a performance exceeding 94%\nfor detection and classification, and about 1.2% false alarm rate. We\nillustrate the model's application in monitoring traffic over a week, yielding\nstatistical insights that could benefit future smart city developments. Our\napproach highlights the potential of combining fiber-optic sensors with visual\ninformation, focusing on practicality and scalability, protecting privacy, and\nminimizing infrastructure costs. To encourage future research, we share our\ndataset.\n","authors":["Khen Cohen","Liav Hen","Ariel Lellouch"],"pdf_url":"https://arxiv.org/pdf/2412.12743v1.pdf","comment":"12 pages, 11 figures, 5 appendices. Shared dataset in:\n  https://zenodo.org/records/14502092"},{"id":"http://arxiv.org/abs/2412.12740v1","updated":"2024-12-17T10:03:39Z","published":"2024-12-17T10:03:39Z","title":"Open-World Panoptic Segmentation","summary":"  Perception is a key building block of autonomously acting vision systems such\nas autonomous vehicles. It is crucial that these systems are able to understand\ntheir surroundings in order to operate safely and robustly. Additionally,\nautonomous systems deployed in unconstrained real-world scenarios must be able\nof dealing with novel situations and object that have never been seen before.\nIn this article, we tackle the problem of open-world panoptic segmentation,\ni.e., the task of discovering new semantic categories and new object instances\nat test time, while enforcing consistency among the categories that we\nincrementally discover. We propose Con2MAV, an approach for open-world panoptic\nsegmentation that extends our previous work, ContMAV, which was developed for\nopen-world semantic segmentation. Through extensive experiments across multiple\ndatasets, we show that our model achieves state-of-the-art results on\nopen-world segmentation tasks, while still performing competitively on the\nknown categories. We will open-source our implementation upon acceptance.\nAdditionally, we propose PANIC (Panoptic ANomalies In Context), a benchmark for\nevaluating open-world panoptic segmentation in autonomous driving scenarios.\nThis dataset, recorded with a multi-modal sensor suite mounted on a car,\nprovides high-quality, pixel-wise annotations of anomalous objects at both\nsemantic and instance level. Our dataset contains 800 images, with more than 50\nunknown classes, i.e., classes that do not appear in the training set, and 4000\nobject instances, making it an extremely challenging dataset for open-world\nsegmentation tasks in the autonomous driving scenario. We provide competitions\nfor multiple open-world tasks on a hidden test set. Our dataset and\ncompetitions are available at https://www.ipb.uni-bonn.de/data/panic.\n","authors":["Matteo Sodano","Federico Magistri","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2412.12740v1.pdf","comment":"Submitted to PAMI"},{"id":"http://arxiv.org/abs/2410.15628v2","updated":"2024-12-17T10:01:59Z","published":"2024-10-21T04:24:10Z","title":"Towards Kriging-informed Conditional Diffusion for Regional Sea-Level\n  Data Downscaling","summary":"  Given coarser-resolution projections from global climate models or satellite\ndata, the downscaling problem aims to estimate finer-resolution regional\nclimate data, capturing fine-scale spatial patterns and variability.\nDownscaling is any method to derive high-resolution data from low-resolution\nvariables, often to provide more detailed and local predictions and analyses.\nThis problem is societally crucial for effective adaptation, mitigation, and\nresilience against significant risks from climate change. The challenge arises\nfrom spatial heterogeneity and the need to recover finer-scale features while\nensuring model generalization. Most downscaling methods \\cite{Li2020} fail to\ncapture the spatial dependencies at finer scales and underperform on real-world\nclimate datasets, such as sea-level rise. We propose a novel Kriging-informed\nConditional Diffusion Probabilistic Model (Ki-CDPM) to capture spatial\nvariability while preserving fine-scale features. Experimental results on\nclimate data show that our proposed method is more accurate than\nstate-of-the-art downscaling techniques.\n","authors":["Subhankar Ghosh","Arun Sharma","Jayant Gupta","Aneesh Subramanian","Shashi Shekhar"],"pdf_url":"https://arxiv.org/pdf/2410.15628v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12737v1","updated":"2024-12-17T09:59:53Z","published":"2024-12-17T09:59:53Z","title":"PolSAM: Polarimetric Scattering Mechanism Informed Segment Anything\n  Model","summary":"  PolSAR data presents unique challenges due to its rich and complex\ncharacteristics. Existing data representations, such as complex-valued data,\npolarimetric features, and amplitude images, are widely used. However, these\nformats often face issues related to usability, interpretability, and data\nintegrity. Most feature extraction networks for PolSAR are small, limiting\ntheir ability to capture features effectively. To address these issues, We\npropose the Polarimetric Scattering Mechanism-Informed SAM (PolSAM), an\nenhanced Segment Anything Model (SAM) that integrates domain-specific\nscattering characteristics and a novel prompt generation strategy. PolSAM\nintroduces Microwave Vision Data (MVD), a lightweight and interpretable data\nrepresentation derived from polarimetric decomposition and semantic\ncorrelations. We propose two key components: the Feature-Level Fusion Prompt\n(FFP), which fuses visual tokens from pseudo-colored SAR images and MVD to\naddress modality incompatibility in the frozen SAM encoder, and the\nSemantic-Level Fusion Prompt (SFP), which refines sparse and dense segmentation\nprompts using semantic information. Experimental results on the PhySAR-Seg\ndatasets demonstrate that PolSAM significantly outperforms existing SAM-based\nand multimodal fusion models, improving segmentation accuracy, reducing data\nstorage, and accelerating inference time. The source code and datasets will be\nmade publicly available at \\url{https://github.com/XAI4SAR/PolSAM}.\n","authors":["Yuqing Wang","Zhongling Huang","Shuxin Yang","Hao Tang","Xiaolan Qiu","Junwei Han","Dingwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12737v1.pdf","comment":"The manuscript is 15 pages long, includes 14 figures and 5 tables"},{"id":"http://arxiv.org/abs/2412.12735v1","updated":"2024-12-17T09:57:21Z","published":"2024-12-17T09:57:21Z","title":"GIRAFFE: Design Choices for Extending the Context Length of Visual\n  Language Models","summary":"  Visual Language Models (VLMs) demonstrate impressive capabilities in\nprocessing multimodal inputs, yet applications such as visual agents, which\nrequire handling multiple images and high-resolution videos, demand enhanced\nlong-range modeling. Moreover, existing open-source VLMs lack systematic\nexploration into extending their context length, and commercial models often\nprovide limited details. To tackle this, we aim to establish an effective\nsolution that enhances long context performance of VLMs while preserving their\ncapacities in short context scenarios. Towards this goal, we make the best\ndesign choice through extensive experiment settings from data curation to\ncontext window extending and utilizing: (1) we analyze data sources and length\ndistributions to construct ETVLM - a data recipe to balance the performance\nacross scenarios; (2) we examine existing position extending methods, identify\ntheir limitations and propose M-RoPE++ as an enhanced approach; we also choose\nto solely instruction-tune the backbone with mixed-source data; (3) we discuss\nhow to better utilize extended context windows and propose hybrid-resolution\ntraining. Built on the Qwen-VL series model, we propose Giraffe, which is\neffectively extended to 128K lengths. Evaluated on extensive long context VLM\nbenchmarks such as VideoMME and Viusal Haystacks, our Giraffe achieves\nstate-of-the-art performance among similarly sized open-source long VLMs and is\ncompetitive with commercial model GPT-4V. We will open-source the code, data,\nand models.\n","authors":["Mukai Li","Lei Li","Shansan Gong","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.12735v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2412.12734v1","updated":"2024-12-17T09:57:04Z","published":"2024-12-17T09:57:04Z","title":"Gaussian Billboards: Expressive 2D Gaussian Splatting with Textures","summary":"  Gaussian Splatting has recently emerged as the go-to representation for\nreconstructing and rendering 3D scenes. The transition from 3D to 2D Gaussian\nprimitives has further improved multi-view consistency and surface\nreconstruction accuracy. In this work we highlight the similarity between 2D\nGaussian Splatting (2DGS) and billboards from traditional computer graphics.\nBoth use flat semi-transparent 2D geometry that is positioned, oriented and\nscaled in 3D space. However 2DGS uses a solid color per splat and an opacity\nmodulated by a Gaussian distribution, where billboards are more expressive,\nmodulating the color with a uv-parameterized texture. We propose to unify these\nconcepts by presenting Gaussian Billboards, a modification of 2DGS to add\nspatially-varying color achieved using per-splat texture interpolation. The\nresult is a mixture of the two representations, which benefits from both the\nrobust scene optimization power of 2DGS and the expressiveness of texture\nmapping. We show that our method can improve the sharpness and quality of the\nscene representation in a wide range of qualitative and quantitative\nevaluations compared to the original 2DGS implementation.\n","authors":["Sebastian Weiss","Derek Bradley"],"pdf_url":"https://arxiv.org/pdf/2412.12734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12725v1","updated":"2024-12-17T09:47:48Z","published":"2024-12-17T09:47:48Z","title":"RaCFormer: Towards High-Quality 3D Object Detection via Query-based\n  Radar-Camera Fusion","summary":"  We propose Radar-Camera fusion transformer (RaCFormer) to boost the accuracy\nof 3D object detection by the following insight. The Radar-Camera fusion in\noutdoor 3D scene perception is capped by the image-to-BEV transformation--if\nthe depth of pixels is not accurately estimated, the naive combination of BEV\nfeatures actually integrates unaligned visual content. To avoid this problem,\nwe propose a query-based framework that enables adaptively sample\ninstance-relevant features from both the BEV and the original image view.\nFurthermore, we enhance system performance by two key designs: optimizing query\ninitialization and strengthening the representational capacity of BEV. For the\nformer, we introduce an adaptive circular distribution in polar coordinates to\nrefine the initialization of object queries, allowing for a distance-based\nadjustment of query density. For the latter, we initially incorporate a\nradar-guided depth head to refine the transformation from image view to BEV.\nSubsequently, we focus on leveraging the Doppler effect of radar and introduce\nan implicit dynamic catcher to capture the temporal elements within the BEV.\nExtensive experiments on nuScenes and View-of-Delft (VoD) datasets validate the\nmerits of our design. Remarkably, our method achieves superior results of 64.9%\nmAP and 70.2% NDS on nuScenes, even outperforming several LiDAR-based\ndetectors. RaCFormer also secures the 1st ranking on the VoD dataset. The code\nwill be released.\n","authors":["Xiaomeng Chu","Jiajun Deng","Guoliang You","Yifan Duan","Houqiang Li","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13362v2","updated":"2024-12-17T09:46:19Z","published":"2024-06-19T09:07:31Z","title":"VisualRWKV: Exploring Recurrent Neural Networks for Visual Language\n  Models","summary":"  Visual Language Models (VLMs) have rapidly progressed with the recent success\nof large language models. However, there have been few attempts to incorporate\nefficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In\nthis study, we introduce VisualRWKV, the first application of a linear RNN\nmodel to multimodal learning tasks, leveraging the pre-trained RWKV language\nmodel. We propose a data-dependent recurrence and sandwich prompts to enhance\nour modeling capabilities, along with a 2D image scanning mechanism to enrich\nthe processing of visual sequences. Extensive experiments demonstrate that\nVisualRWKV achieves competitive performance compared to Transformer-based\nmodels like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV\nhas a speed advantage of 3.98 times and can save 54% of GPU memory when\nreaching an inference length of 24K tokens. To facilitate further research and\nanalysis, we have made the checkpoints and the associated code publicly\naccessible at the following GitHub repository: see\nhttps://github.com/howard-hou/VisualRWKV.\n","authors":["Haowen Hou","Peigen Zeng","Fei Ma","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2406.13362v2.pdf","comment":"Accepted at COLING 2025 main conference"},{"id":"http://arxiv.org/abs/2412.12722v1","updated":"2024-12-17T09:38:58Z","published":"2024-12-17T09:38:58Z","title":"Defending LVLMs Against Vision Attacks through Partial-Perception\n  Supervision","summary":"  Recent studies have raised significant concerns regarding the vulnerability\nof Large Vision Language Models (LVLMs) to maliciously injected or perturbed\ninput images, which can mislead their responses. Existing defense methods show\nthat such vision attacks are sensitive to image modifications especially\ncropping, using majority voting across responses of modified images as\ncorrected responses. However, these modifications often result in partial\nimages and distort the semantics, which reduces response quality on clean\nimages after voting. Instead of directly using responses from partial images\nfor voting, we investigate using them to supervise the LVLM's responses to the\noriginal images. We propose a black-box, training-free method called DPS\n(Defense through Partial-Perception Supervision). In this approach, the model\nis prompted using the responses generated by a model that perceives only a\npartial image. With DPS, the model can adjust its response based on partial\nimage understanding when under attack, while confidently maintaining its\noriginal response for clean input. Our findings show that the weak model can\nsupervise the strong model: when faced with an attacked input, the strong model\nbecomes less confident and adjusts its response based on the weak model's\npartial understanding, effectively defending against the attack. With clean\ninput, it confidently maintains its original response. Empirical experiments\nshow our method outperforms the baseline, cutting the average attack success\nrate by 76.3% across six datasets on three popular models.\n","authors":["Qi Zhou","Tianlin Li","Qing Guo","Dongxia Wang","Yun Lin","Yang Liu","Jin Song Dong"],"pdf_url":"https://arxiv.org/pdf/2412.12722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18263v2","updated":"2024-12-17T09:34:49Z","published":"2024-11-27T12:01:08Z","title":"TSD-SR: One-Step Diffusion with Target Score Distillation for Real-World\n  Image Super-Resolution","summary":"  Pre-trained text-to-image diffusion models are increasingly applied to\nreal-world image super-resolution (Real-ISR) task. Given the iterative\nrefinement nature of diffusion models, most existing approaches are\ncomputationally expensive. While methods such as SinSR and OSEDiff have emerged\nto condense inference steps via distillation, their performance in image\nrestoration or details recovery is not satisfied. To address this, we propose\nTSD-SR, a novel distillation framework specifically designed for real-world\nimage super-resolution, aiming to construct an efficient and effective one-step\nmodel. We first introduce the Target Score Distillation, which leverages the\npriors of diffusion models and real image references to achieve more realistic\nimage restoration. Secondly, we propose a Distribution-Aware Sampling Module to\nmake detail-oriented gradients more readily accessible, addressing the\nchallenge of recovering fine details. Extensive experiments demonstrate that\nour TSD-SR has superior restoration results (most of the metrics perform the\nbest) and the fastest inference speed (e.g. 40 times faster than SeeSR)\ncompared to the past Real-ISR approaches based on pre-trained diffusion priors.\n","authors":["Linwei Dong","Qingnan Fan","Yihong Guo","Zhonghao Wang","Qi Zhang","Jinwei Chen","Yawei Luo","Changqing Zou"],"pdf_url":"https://arxiv.org/pdf/2411.18263v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12718v1","updated":"2024-12-17T09:33:06Z","published":"2024-12-17T09:33:06Z","title":"ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation\n  Detecting and Grounding","summary":"  We present ASAP, a new framework for detecting and grounding multi-modal\nmedia manipulation (DGM4).Upon thorough examination, we observe that accurate\nfine-grained cross-modal semantic alignment between the image and text is vital\nfor accurately manipulation detection and grounding. While existing DGM4\nmethods pay rare attention to the cross-modal alignment, hampering the accuracy\nof manipulation detecting to step further. To remedy this issue, this work\ntargets to advance the semantic alignment learning to promote this task.\nParticularly, we utilize the off-the-shelf Multimodal Large-Language Models\n(MLLMs) and Large Language Models (LLMs) to construct paired image-text pairs,\nespecially for the manipulated instances. Subsequently, a cross-modal alignment\nlearning is performed to enhance the semantic alignment. Besides the explicit\nauxiliary clues, we further design a Manipulation-Guided Cross Attention (MGCA)\nto provide implicit guidance for augmenting the manipulation perceiving. With\nthe grounding truth available during training, MGCA encourages the model to\nconcentrate more on manipulated components while downplaying normal ones,\nenhancing the model's ability to capture manipulations. Extensive experiments\nare conducted on the DGM4 dataset, the results demonstrate that our model can\nsurpass the comparison method with a clear margin.\n","authors":["Zhenxing Zhang","Yaxiong Wang","Lechao Cheng","Zhun Zhong","Dan Guo","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12718v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.12716v1","updated":"2024-12-17T09:30:31Z","published":"2024-12-17T09:30:31Z","title":"Unsupervised UAV 3D Trajectories Estimation with Sparse Point Clouds","summary":"  Compact UAV systems, while advancing delivery and surveillance, pose\nsignificant security challenges due to their small size, which hinders\ndetection by traditional methods. This paper presents a cost-effective,\nunsupervised UAV detection method using spatial-temporal sequence processing to\nfuse multiple LiDAR scans for accurate UAV tracking in real-world scenarios.\nOur approach segments point clouds into foreground and background, analyzes\nspatial-temporal data, and employs a scoring mechanism to enhance detection\naccuracy. Tested on a public dataset, our solution placed 4th in the CVPR 2024\nUG2+ Challenge, demonstrating its practical effectiveness. We plan to\nopen-source all designs, code, and sample data for the research community\ngithub.com/lianghanfang/UnLiDAR-UAV-Est.\n","authors":["Hanfang Liang","Yizhuo Yang","Jinming Hu","Jianfei Yang","Fen Liu","Shenghai Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.12716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09951v2","updated":"2024-12-17T09:27:33Z","published":"2024-12-13T08:14:24Z","title":"WiseAD: Knowledge Augmented End-to-End Autonomous Driving with\n  Vision-Language Model","summary":"  The emergence of general human knowledge and impressive logical reasoning\ncapacity in rapidly progressed vision-language models (VLMs) have driven\nincreasing interest in applying VLMs to high-level autonomous driving tasks,\nsuch as scene understanding and decision-making. However, an in-depth study on\nthe relationship between knowledge proficiency, especially essential driving\nexpertise, and closed-loop autonomous driving performance requires further\nexploration. In this paper, we investigate the effects of the depth and breadth\nof fundamental driving knowledge on closed-loop trajectory planning and\nintroduce WiseAD, a specialized VLM tailored for end-to-end autonomous driving\ncapable of driving reasoning, action justification, object recognition, risk\nanalysis, driving suggestions, and trajectory planning across diverse\nscenarios. We employ joint training on driving knowledge and planning datasets,\nenabling the model to perform knowledge-aligned trajectory planning\naccordingly. Extensive experiments indicate that as the diversity of driving\nknowledge extends, critical accidents are notably reduced, contributing 11.9%\nand 12.4% improvements in the driving score and route completion on the Carla\nclosed-loop evaluations, achieving state-of-the-art performance. Moreover,\nWiseAD also demonstrates remarkable performance in knowledge evaluations on\nboth in-domain and out-of-domain datasets.\n","authors":["Songyan Zhang","Wenhui Huang","Zihui Gao","Hao Chen","Chen Lv"],"pdf_url":"https://arxiv.org/pdf/2412.09951v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12709v1","updated":"2024-12-17T09:23:46Z","published":"2024-12-17T09:23:46Z","title":"Accelerating lensed quasars discovery and modeling with physics-informed\n  variational autoencoders","summary":"  Strongly lensed quasars provide valuable insights into the rate of cosmic\nexpansion, the distribution of dark matter in foreground deflectors, and the\ncharacteristics of quasar hosts. However, detecting them in astronomical images\nis difficult due to the prevalence of non-lensing objects. To address this\nchallenge, we developed a generative deep learning model called VariLens, built\nupon a physics-informed variational autoencoder. This model seamlessly\nintegrates three essential modules: image reconstruction, object\nclassification, and lens modeling, offering a fast and comprehensive approach\nto strong lens analysis. VariLens is capable of rapidly determining both (1)\nthe probability that an object is a lens system and (2) key parameters of a\nsingular isothermal ellipsoid (SIE) mass model -- including the Einstein radius\n($\\theta_\\mathrm{E}$), lens center, and ellipticity -- in just milliseconds\nusing a single CPU. A direct comparison of VariLens estimates with traditional\nlens modeling for 20 known lensed quasars within the Subaru Hyper Suprime-Cam\n(HSC) footprint shows good agreement, with both results consistent within\n$2\\sigma$ for systems with $\\theta_\\mathrm{E}<3$ arcsecs. To identify new\nlensed quasar candidates, we begin with an initial sample of approximately 80\nmillion sources, combining HSC data with multiwavelength information from\nvarious surveys. After applying a photometric preselection aimed at locating\n$z>1.5$ sources, the number of candidates is reduced to 710,966. Subsequently,\nVariLens highlights 13,831 sources, each showing a high likelihood of being a\nlens. A visual assessment of these objects results in 42 promising candidates\nthat await spectroscopic confirmation. These results underscore the potential\nof automated deep learning pipelines to efficiently detect and model strong\nlenses in large datasets.\n","authors":["Irham T. Andika","Stefan Schuldt","Sherry H. Suyu","Satadru Bag","Raoul Cañameras","Alejandra Melo","Claudio Grillo","James H. H. Chan"],"pdf_url":"https://arxiv.org/pdf/2412.12709v1.pdf","comment":"Submitted to the Astronomy & Astrophysics journal. The paper consists\n  of 17 main pages, 14 figures, and 5 tables. We welcome feedback and comments\n  from readers!"},{"id":"http://arxiv.org/abs/2408.09429v2","updated":"2024-12-17T09:19:46Z","published":"2024-08-18T10:07:02Z","title":"Reefknot: A Comprehensive Benchmark for Relation Hallucination\n  Evaluation, Analysis and Mitigation in Multimodal Large Language Models","summary":"  Hallucination issues continue to affect multimodal large language models\n(MLLMs), with existing research mainly addressing object-level or\nattribute-level hallucinations, neglecting the more complex relation\nhallucinations that require advanced reasoning. Current benchmarks for relation\nhallucinations lack detailed evaluation and effective mitigation, and their\ndatasets often suffer from biases due to systematic annotation processes. To\naddress these challenges, we introduce Reefknot, a comprehensive benchmark\ntargeting relation hallucinations, comprising over 20,000 real-world samples.\nWe provide a systematic definition of relation hallucinations, integrating\nperceptive and cognitive perspectives, and construct a relation-based corpus\nusing the Visual Genome scene graph dataset. Our comparative evaluation reveals\nsignificant limitations in current MLLMs' ability to handle relation\nhallucinations. Additionally, we propose a novel confidence-based mitigation\nstrategy, which reduces the hallucination rate by an average of 9.75% across\nthree datasets, including Reefknot. Our work offers valuable insights for\nachieving trustworthy multimodal intelligence.\n","authors":["Kening Zheng","Junkai Chen","Yibo Yan","Xin Zou","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2408.09429v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12704v1","updated":"2024-12-17T09:19:44Z","published":"2024-12-17T09:19:44Z","title":"MapExpert: Online HD Map Construction with Simple and Efficient Sparse\n  Map Element Expert","summary":"  Constructing online High-Definition (HD) maps is crucial for the static\nenvironment perception of autonomous driving systems (ADS). Existing solutions\ntypically attempt to detect vectorized HD map elements with unified models;\nhowever, these methods often overlook the distinct characteristics of different\nnon-cubic map elements, making accurate distinction challenging. To address\nthese issues, we introduce an expert-based online HD map method, termed\nMapExpert. MapExpert utilizes sparse experts, distributed by our routers, to\ndescribe various non-cubic map elements accurately. Additionally, we propose an\nauxiliary balance loss function to distribute the load evenly across experts.\nFurthermore, we theoretically analyze the limitations of prevalent bird's-eye\nview (BEV) feature temporal fusion methods and introduce an efficient temporal\nfusion module called Learnable Weighted Moving Descentage. This module\neffectively integrates relevant historical information into the final BEV\nfeatures. Combined with an enhanced slice head branch, the proposed MapExpert\nachieves state-of-the-art performance and maintains good efficiency on both\nnuScenes and Argoverse2 datasets.\n","authors":["Dapeng Zhang","Dayu Chen","Peng Zhi","Yinda Chen","Zhenlong Yuan","Chenyang Li"," Sunjing","Rui Zhou","Qingguo Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12696v1","updated":"2024-12-17T09:13:22Z","published":"2024-12-17T09:13:22Z","title":"ALADE-SNN: Adaptive Logit Alignment in Dynamically Expandable Spiking\n  Neural Networks for Class Incremental Learning","summary":"  Inspired by the human brain's ability to adapt to new tasks without erasing\nprior knowledge, we develop spiking neural networks (SNNs) with dynamic\nstructures for Class Incremental Learning (CIL). Our comparative experiments\nreveal that limited datasets introduce biases in logits distributions among\ntasks. Fixed features from frozen past-task extractors can cause overfitting\nand hinder the learning of new tasks. To address these challenges, we propose\nthe ALADE-SNN framework, which includes adaptive logit alignment for balanced\nfeature representation and OtoN suppression to manage weights mapping frozen\nold features to new classes during training, releasing them during fine-tuning.\nThis approach dynamically adjusts the network architecture based on analytical\nobservations, improving feature extraction and balancing performance between\nnew and old tasks. Experiment results show that ALADE-SNN achieves an average\nincremental accuracy of 75.42 on the CIFAR100-B0 benchmark over 10 incremental\nsteps. ALADE-SNN not only matches the performance of DNN-based methods but also\nsurpasses state-of-the-art SNN-based continual learning algorithms. This\nadvancement enhances continual learning in neuromorphic computing, offering a\nbrain-inspired, energy-efficient solution for real-time data processing.\n","authors":["Wenyao Ni","Jiangrong Shen","Qi Xu","Huajin Tang"],"pdf_url":"https://arxiv.org/pdf/2412.12696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12693v1","updated":"2024-12-17T09:10:55Z","published":"2024-12-17T09:10:55Z","title":"SPHERE: A Hierarchical Evaluation on Spatial Perception and Reasoning\n  for Vision-Language Models","summary":"  Current vision-language models may incorporate single-dimensional spatial\ncues, such as depth, object boundary, and basic spatial directions (e.g. left,\nright, front, back), yet often lack the multi-dimensional spatial reasoning\nnecessary for human-like understanding and real-world applications. To address\nthis gap, we develop SPHERE (Spatial Perception and Hierarchical Evaluation of\nREasoning), a hierarchical evaluation framework with a new human-annotated\ndataset to pinpoint model strengths and weaknesses, advancing from single-skill\ntasks to multi-skill tasks, and ultimately to complex reasoning tasks that\nrequire the integration of multiple spatial and visual cues with logical\nreasoning. Benchmark evaluation of state-of-the-art open-source models reveal\nsignificant shortcomings, especially in the abilities to understand distance\nand proximity, to reason from both allocentric and egocentric viewpoints, and\nto perform complex reasoning in a physical context. This work underscores the\nneed for more advanced approaches to spatial understanding and reasoning,\npaving the way for improvements in vision-language models and their alignment\nwith human-like spatial capabilities. The dataset will be open-sourced upon\npublication.\n","authors":["Wenyu Zhang","Wei En Ng","Lixin Ma","Yuwen Wang","Jungqi Zhao","Boyang Li","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12685v1","updated":"2024-12-17T09:02:55Z","published":"2024-12-17T09:02:55Z","title":"SemStereo: Semantic-Constrained Stereo Matching Network for Remote\n  Sensing","summary":"  Semantic segmentation and 3D reconstruction are two fundamental tasks in\nremote sensing, typically treated as separate or loosely coupled tasks. Despite\nattempts to integrate them into a unified network, the constraints between the\ntwo heterogeneous tasks are not explicitly modeled, since the pioneering\nstudies either utilize a loosely coupled parallel structure or engage in only\nimplicit interactions, failing to capture the inherent connections. In this\nwork, we explore the connections between the two tasks and propose a new\nnetwork that imposes semantic constraints on the stereo matching task, both\nimplicitly and explicitly. Implicitly, we transform the traditional parallel\nstructure to a new cascade structure termed Semantic-Guided Cascade structure,\nwhere the deep features enriched with semantic information are utilized for the\ncomputation of initial disparity maps, enhancing semantic guidance. Explicitly,\nwe propose a Semantic Selective Refinement (SSR) module and a Left-Right\nSemantic Consistency (LRSC) module. The SSR refines the initial disparity map\nunder the guidance of the semantic map. The LRSC ensures semantic consistency\nbetween two views via reducing the semantic divergence after transforming the\nsemantic map from one view to the other using the disparity map. Experiments on\nthe US3D and WHU datasets demonstrate that our method achieves state-of-the-art\nperformance for both semantic segmentation and stereo matching.\n","authors":["Chen Chen","Liangjin Zhao","Yuanchun He","Yingxuan Long","Kaiqiang Chen","Zhirui Wang","Yanfeng Hu","Xian Sun"],"pdf_url":"https://arxiv.org/pdf/2412.12685v1.pdf","comment":"9 pages, 6 figures, AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12683v1","updated":"2024-12-17T08:56:59Z","published":"2024-12-17T08:56:59Z","title":"ShiftedBronzes: Benchmarking and Analysis of Domain Fine-Grained\n  Classification in Open-World Settings","summary":"  In real-world applications across specialized domains, addressing complex\nout-of-distribution (OOD) challenges is a common and significant concern. In\nthis study, we concentrate on the task of fine-grained bronze ware dating, a\ncritical aspect in the study of ancient Chinese history, and developed a\nbenchmark dataset named ShiftedBronzes. By extensively expanding the bronze\nDing dataset, ShiftedBronzes incorporates two types of bronze ware data and\nseven types of OOD data, which exhibit distribution shifts commonly encountered\nin bronze ware dating scenarios. We conduct benchmarking experiments on\nShiftedBronzes and five commonly used general OOD datasets, employing a variety\nof widely adopted post-hoc, pre-trained Vision Large Model (VLM)-based and\ngeneration-based OOD detection methods. Through analysis of the experimental\nresults, we validate previous conclusions regarding post-hoc, VLM-based, and\ngeneration-based methods, while also highlighting their distinct behaviors on\nspecialized datasets. These findings underscore the unique challenges of\napplying general OOD detection methods to domain-specific tasks such as bronze\nware dating. We hope that the ShiftedBronzes benchmark provides valuable\ninsights into both the field of bronze ware dating and the and the development\nof OOD detection methods. The dataset and associated code will be available\nlater.\n","authors":["Rixin Zhou","Honglin Pang","Qian Zhang","Ruihua Qi","Xi Yang","Chuntao Li"],"pdf_url":"https://arxiv.org/pdf/2412.12683v1.pdf","comment":"9pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2412.11594v2","updated":"2024-12-17T08:52:51Z","published":"2024-12-16T09:32:23Z","title":"VersaGen: Unleashing Versatile Visual Control for Text-to-Image\n  Synthesis","summary":"  Despite the rapid advancements in text-to-image (T2I) synthesis, enabling\nprecise visual control remains a significant challenge. Existing works\nattempted to incorporate multi-facet controls (text and sketch), aiming to\nenhance the creative control over generated images. However, our pilot study\nreveals that the expressive power of humans far surpasses the capabilities of\ncurrent methods. Users desire a more versatile approach that can accommodate\ntheir diverse creative intents, ranging from controlling individual subjects to\nmanipulating the entire scene composition. We present VersaGen, a generative AI\nagent that enables versatile visual control in T2I synthesis. VersaGen admits\nfour types of visual controls: i) single visual subject; ii) multiple visual\nsubjects; iii) scene background; iv) any combination of the three above or\nmerely no control at all. We train an adaptor upon a frozen T2I model to\naccommodate the visual information into the text-dominated diffusion process.\nWe introduce three optimization strategies during the inference phase of\nVersaGen to improve generation results and enhance user experience.\nComprehensive experiments on COCO and Sketchy validate the effectiveness and\nflexibility of VersaGen, as evidenced by both qualitative and quantitative\nresults.\n","authors":["Zhipeng Chen","Lan Yang","Yonggang Qi","Honggang Zhang","Kaiyue Pang","Ke Li","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2412.11594v2.pdf","comment":"The paper has been accepted by AAAI 2025. Paper code:\n  https://github.com/FelixChan9527/VersaGen_official"},{"id":"http://arxiv.org/abs/2412.12675v1","updated":"2024-12-17T08:44:29Z","published":"2024-12-17T08:44:29Z","title":"ShotVL: Human-Centric Highlight Frame Retrieval via Language Queries","summary":"  Existing works on human-centric video understanding typically focus on\nanalyzing specific moment or entire videos. However, many applications require\nhigher precision at the frame level. In this work, we propose a novel task,\nBestShot, which aims to locate highlight frames within human-centric videos via\nlanguage queries. This task demands not only a deep semantic comprehension of\nhuman actions but also precise temporal localization. To support this task, we\nintroduce the BestShot Benchmark. %The benchmark is meticulously constructed by\ncombining human detection and tracking, potential frame selection based on\nhuman judgment, and detailed textual descriptions crafted by human input to\nensure precision. The benchmark is meticulously constructed by combining\nhuman-annotated highlight frames, detailed textual descriptions and duration\nlabeling. These descriptions encompass three critical elements: (1) Visual\ncontent; (2) Fine-grained action; and (3) Human Pose Description. Together,\nthese elements provide the necessary precision to identify the exact highlight\nframes in videos.\n  To tackle this problem, we have collected two distinct datasets: (i)\nShotGPT4o Dataset, which is algorithmically generated by GPT-4o and (ii)\nImage-SMPLText Dataset, a dataset with large-scale and accurate per-frame pose\ndescription leveraging PoseScript and existing pose estimation datasets. Based\non these datasets, we present a strong baseline model, ShotVL, fine-tuned from\nInternVL, specifically for BestShot. We highlight the impressive zero-shot\ncapabilities of our model and offer comparative analyses with existing SOTA\nmodels. ShotVL demonstrates a significant 52% improvement over InternVL on the\nBestShot Benchmark and a notable 57% improvement on the THUMOS14 Benchmark, all\nwhile maintaining the SOTA performance in general image classification and\nretrieval.\n","authors":["Wangyu Xue","Chen Qian","Jiayi Wu","Yang Zhou","Wentao Liu","Ju Ren","Siming Fan","Yaoxue Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12672v1","updated":"2024-12-17T08:41:50Z","published":"2024-12-17T08:41:50Z","title":"Structural Pruning via Spatial-aware Information Redundancy for Semantic\n  Segmentation","summary":"  In recent years, semantic segmentation has flourished in various\napplications. However, the high computational cost remains a significant\nchallenge that hinders its further adoption. The filter pruning method for\nstructured network slimming offers a direct and effective solution for the\nreduction of segmentation networks. Nevertheless, we argue that most existing\npruning methods, originally designed for image classification, overlook the\nfact that segmentation is a location-sensitive task, which consequently leads\nto their suboptimal performance when applied to segmentation networks. To\naddress this issue, this paper proposes a novel approach, denoted as\nSpatial-aware Information Redundancy Filter Pruning~(SIRFP), which aims to\nreduce feature redundancy between channels. First, we formulate the pruning\nprocess as a maximum edge weight clique problem~(MEWCP) in graph theory,\nthereby minimizing the redundancy among the remaining features after pruning.\nWithin this framework, we introduce a spatial-aware redundancy metric based on\nfeature maps, thus endowing the pruning process with location sensitivity to\nbetter adapt to pruning segmentation networks. Additionally, based on the\nMEWCP, we propose a low computational complexity greedy strategy to solve this\nNP-hard problem, making it feasible and efficient for structured pruning. To\nvalidate the effectiveness of our method, we conducted extensive comparative\nexperiments on various challenging datasets. The results demonstrate the\nsuperior performance of SIRFP for semantic segmentation tasks.\n","authors":["Dongyue Wu","Zilin Guo","Li Yu","Nong Sang","Changxin Gao"],"pdf_url":"https://arxiv.org/pdf/2412.12672v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12669v1","updated":"2024-12-17T08:40:23Z","published":"2024-12-17T08:40:23Z","title":"Adaptive Prototype Replay for Class Incremental Semantic Segmentation","summary":"  Class incremental semantic segmentation (CISS) aims to segment new classes\nduring continual steps while preventing the forgetting of old knowledge.\nExisting methods alleviate catastrophic forgetting by replaying distributions\nof previously learned classes using stored prototypes or features. However,\nthey overlook a critical issue: in CISS, the representation of class knowledge\nis updated continuously through incremental learning, whereas prototype replay\nmethods maintain fixed prototypes. This mismatch between updated representation\nand fixed prototypes limits the effectiveness of the prototype replay strategy.\nTo address this issue, we propose the Adaptive prototype replay (Adapter) for\nCISS in this paper. Adapter comprises an adaptive deviation compen sation (ADC)\nstrategy and an uncertainty-aware constraint (UAC) loss. Specifically, the ADC\nstrategy dynamically updates the stored prototypes based on the estimated\nrepresentation shift distance to match the updated representation of old class.\nThe UAC loss reduces prediction uncertainty, aggregating discriminative\nfeatures to aid in generating compact prototypes. Additionally, we introduce a\ncompensation-based prototype similarity discriminative (CPD) loss to ensure\nadequate differentiation between similar prototypes, thereby enhancing the\nefficiency of the adaptive prototype replay strategy. Extensive experiments on\nPascal VOC and ADE20K datasets demonstrate that Adapter achieves\nstate-of-the-art results and proves effective across various CISS tasks,\nparticularly in challenging multi-step scenarios. The code and model is\navailable at https://github.com/zhu-gl-ux/Adapter.\n","authors":["Guilin Zhu","Dongyue Wu","Changxin Gao","Runmin Wang","Weidong Yang","Nong Sang"],"pdf_url":"https://arxiv.org/pdf/2412.12669v1.pdf","comment":"Accepted by the Main Technical Track of the 39th Annual AAAI\n  Conference on Artificial Intelligence (AAAI-2025)"},{"id":"http://arxiv.org/abs/2412.12667v1","updated":"2024-12-17T08:36:47Z","published":"2024-12-17T08:36:47Z","title":"A Two-Fold Patch Selection Approach for Improved 360-Degree Image\n  Quality Assessment","summary":"  This article presents a novel approach to improving the accuracy of\n360-degree perceptual image quality assessment (IQA) through a two-fold patch\nselection process. Our methodology combines visual patch selection with\nembedding similarity-based refinement. The first stage focuses on selecting\npatches from 360-degree images using three distinct sampling methods to ensure\ncomprehensive coverage of visual content for IQA. The second stage, which is\nthe core of our approach, employs an embedding similarity-based selection\nprocess to filter and prioritize the most informative patches based on their\nembeddings similarity distances. This dual selection mechanism ensures that the\ntraining data is both relevant and informative, enhancing the model's learning\nefficiency. Extensive experiments and statistical analyses using three distance\nmetrics across three benchmark datasets validate the effectiveness of our\nselection algorithm. The results highlight its potential to deliver robust and\naccurate 360-degree IQA, with performance gains of up to 4.5% in accuracy and\nmonotonicity of quality score prediction, while using only 40% to 50% of the\ntraining patches. These improvements are consistent across various\nconfigurations and evaluation metrics, demonstrating the strength of the\nproposed method. The code for the selection process is available at:\nhttps://github.com/sendjasni/patch-selection-360-image-quality.\n","authors":["Abderrezzaq Sendjasni","Seif-Eddine Benkabou","Mohamed-Chaker Larabi"],"pdf_url":"https://arxiv.org/pdf/2412.12667v1.pdf","comment":"Submitted to IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2412.12661v1","updated":"2024-12-17T08:30:00Z","published":"2024-12-17T08:30:00Z","title":"MedMax: Mixed-Modal Instruction Tuning for Training Biomedical\n  Assistants","summary":"  Recent advancements in mixed-modal generative models have enabled flexible\nintegration of information across image-text content. These models have opened\nnew avenues for developing unified biomedical assistants capable of analyzing\nbiomedical images, answering complex questions about them, and predicting the\nimpact of medical procedures on a patient's health. However, existing resources\nface challenges such as limited data availability, narrow domain coverage, and\nrestricted sources (e.g., medical papers). To address these gaps, we present\nMedMax, the first large-scale multimodal biomedical instruction-tuning dataset\nfor mixed-modal foundation models. With 1.47 million instances, MedMax\nencompasses a diverse range of tasks, including multimodal content generation\n(interleaved image-text data), biomedical image captioning and generation,\nvisual chatting, and report understanding. These tasks span diverse medical\ndomains such as radiology and histopathology. Subsequently, we fine-tune a\nmixed-modal foundation model on the MedMax dataset, achieving significant\nperformance improvements: a 26% gain over the Chameleon model and an 18.3%\nimprovement over GPT-4o across 12 downstream biomedical visual\nquestion-answering tasks. Additionally, we introduce a unified evaluation suite\nfor biomedical tasks, providing a robust framework to guide the development of\nnext-generation mixed-modal biomedical AI assistants.\n","authors":["Hritik Bansal","Daniel Israel","Siyan Zhao","Shufan Li","Tung Nguyen","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2412.12661v1.pdf","comment":"12 figures, 15 tables"},{"id":"http://arxiv.org/abs/2412.12660v1","updated":"2024-12-17T08:29:13Z","published":"2024-12-17T08:29:13Z","title":"SEG-SAM: Semantic-Guided SAM for Unified Medical Image Segmentation","summary":"  Recently, developing unified medical image segmentation models gains\nincreasing attention, especially with the advent of the Segment Anything Model\n(SAM). SAM has shown promising binary segmentation performance in natural\ndomains, however, transferring it to the medical domain remains challenging, as\nmedical images often possess substantial inter-category overlaps. To address\nthis, we propose the SEmantic-Guided SAM (SEG-SAM), a unified medical\nsegmentation model that incorporates semantic medical knowledge to enhance\nmedical segmentation performance. First, to avoid the potential conflict\nbetween binary and semantic predictions, we introduce a semantic-aware decoder\nindependent of SAM's original decoder, specialized for both semantic\nsegmentation on the prompted object and classification on unprompted objects in\nimages. To further enhance the model's semantic understanding, we solicit key\ncharacteristics of medical categories from large language models and\nincorporate them into SEG-SAM through a text-to-vision semantic module,\nadaptively transferring the language information into the visual segmentation\ntask. In the end, we introduce the cross-mask spatial alignment strategy to\nencourage greater overlap between the predicted masks from SEG-SAM's two\ndecoders, thereby benefiting both predictions. Extensive experiments\ndemonstrate that SEG-SAM outperforms state-of-the-art SAM-based methods in\nunified binary medical segmentation and task-specific methods in semantic\nmedical segmentation, showcasing promising results and potential for broader\nmedical applications.\n","authors":["Shuangping Huang","Hao Liang","Qingfeng Wang","Chulong Zhong","Zijian Zhou","Miaojing Shi"],"pdf_url":"https://arxiv.org/pdf/2412.12660v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.12654v1","updated":"2024-12-17T08:21:46Z","published":"2024-12-17T08:21:46Z","title":"CALA: A Class-Aware Logit Adapter for Few-Shot Class-Incremental\n  Learning","summary":"  Few-Shot Class-Incremental Learning (FSCIL) defines a practical but\nchallenging task where models are required to continuously learn novel concepts\nwith only a few training samples. Due to data scarcity, existing FSCIL methods\nresort to training a backbone with abundant base data and then keeping it\nfrozen afterward. However, the above operation often causes the backbone to\noverfit to base classes while overlooking the novel ones, leading to severe\nconfusion between them. To address this issue, we propose Class-Aware Logit\nAdapter (CALA). Our method involves a lightweight adapter that learns to\nrectify biased predictions through a pseudo-incremental learning paradigm. In\nthe real FSCIL process, we use the learned adapter to dynamically generate\nrobust balancing factors. These factors can adjust confused novel instances\nback to their true label space based on their similarity to base classes.\nSpecifically, when confusion is more likely to occur in novel instances that\nclosely resemble base classes, greater rectification is required. Notably, CALA\noperates on the classifier level, preserving the original feature space, thus\nit can be flexibly plugged into most of the existing FSCIL works for improved\nperformance. Experiments on three benchmark datasets consistently validate the\neffectiveness and flexibility of CALA. Codes will be available upon acceptance.\n","authors":["Chengyan Liu","Linglan Zhao","Fan Lyu","Kaile Du","Fuyuan Hu","Tao Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12654v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2405.05095v4","updated":"2024-12-17T08:12:37Z","published":"2024-05-08T14:44:34Z","title":"Approximation properties relative to continuous scale space for hybrid\n  discretizations of Gaussian derivative operators","summary":"  This paper presents an analysis of properties of two hybrid discretization\nmethods for Gaussian derivatives, based on convolutions with either the\nnormalized sampled Gaussian kernel or the integrated Gaussian kernel followed\nby central differences. The motivation for studying these discretization\nmethods is that in situations when multiple spatial derivatives of different\norder are needed at the same scale level, they can be computed significantly\nmore efficiently compared to more direct derivative approximations based on\nexplicit convolutions with either sampled Gaussian kernels or integrated\nGaussian kernels.\n  While these computational benefits do also hold for the genuinely discrete\napproach for computing discrete analogues of Gaussian derivatives, based on\nconvolution with the discrete analogue of the Gaussian kernel followed by\ncentral differences, the underlying mathematical primitives for the discrete\nanalogue of the Gaussian kernel, in terms of modified Bessel functions of\ninteger order, may not be available in certain frameworks for image processing,\nsuch as when performing deep learning based on scale-parameterized filters in\nterms of Gaussian derivatives, with learning of the scale levels.\n  In this paper, we present a characterization of the properties of these\nhybrid discretization methods, in terms of quantitative performance measures\nconcerning the amount of spatial smoothing that they imply, as well as the\nrelative consistency of scale estimates obtained from scale-invariant feature\ndetectors with automatic scale selection, with an emphasis on the behaviour for\nvery small values of the scale parameter, which may differ significantly from\ncorresponding results obtained from the fully continuous scale-space theory, as\nwell as between different types of discretization methods.\n","authors":["Tony Lindeberg"],"pdf_url":"https://arxiv.org/pdf/2405.05095v4.pdf","comment":"23 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2311.11317"},{"id":"http://arxiv.org/abs/2408.13854v2","updated":"2024-12-17T08:12:25Z","published":"2024-08-25T14:47:25Z","title":"Tangram: Benchmark for Evaluating Geometric Element Recognition in Large\n  Multimodal Models","summary":"  Significant advancements in Large Multimodal Models (LMMs) have enabled them\nto tackle complex problems involving visual-mathematical reasoning. However,\ntheir ability to identify geometric elements remains underexplored. To address\nthis gap, we introduce Tangram, a novel benchmark designed to evaluate the\nperformance of LMMs on geometric element recognition. Tangram comprises 1,080\ndiverse geometric diagrams sourced from primary and secondary school exams,\ncompetitions, and textbooks, ranging from simple geometric shapes to complex\ncombinations. Each diagram is paired with four questions, resulting in 4,320\nvisual-question-answer pairs. Unlike existing benchmarks that emphasize\nhigher-level cognition and reasoning, Tangram focuses on understanding\ngeometric elements, requiring models to perform a ``simple yet challenging\"\ncounting task. Systematic evaluation of 13 prominent LMMs, such as GPT-4o and\nClaude 3.5 Sonnet, reveals that these models face significant challenges even\nin seemingly straightforward tasks. The top-performing model achieves an\naccuracy of only 53.0%, highlighting a substantial gap compared to human\nperformance. These findings underscore the limitations of current multimodal AI\nsystems in handling basic perception tasks and serve to inspire the development\nof the next generation of expert-level multimodal foundational models. The data\nand code will be released soon.\n","authors":["Chao Zhang","Jiamin Tang","Jing Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.13854v2.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.11024v2","updated":"2024-12-17T07:45:29Z","published":"2024-12-15T02:35:31Z","title":"Exploring Diffusion and Flow Matching Under Generator Matching","summary":"  In this paper, we present a comprehensive theoretical comparison of diffusion\nand flow matching under the Generator Matching framework. Despite their\napparent differences, both diffusion and flow matching can be viewed under the\nunified framework of Generator Matching. By recasting both diffusion and flow\nmatching under the same generative Markov framework, we provide theoretical\ninsights into why flow matching models can be more robust empirically and how\nnovel model classes can be constructed by mixing deterministic and stochastic\ncomponents. Our analysis offers a fresh perspective on the relationships\nbetween state-of-the-art generative modeling paradigms.\n","authors":["Zeeshan Patel","James DeLoye","Lance Mathias"],"pdf_url":"https://arxiv.org/pdf/2412.11024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12629v1","updated":"2024-12-17T07:44:25Z","published":"2024-12-17T07:44:25Z","title":"a2z-1 for Multi-Disease Detection in Abdomen-Pelvis CT: External\n  Validation and Performance Analysis Across 21 Conditions","summary":"  We present a comprehensive evaluation of a2z-1, an artificial intelligence\n(AI) model designed to analyze abdomen-pelvis CT scans for 21 time-sensitive\nand actionable findings. Our study focuses on rigorous assessment of the\nmodel's performance and generalizability. Large-scale retrospective analysis\ndemonstrates an average AUC of 0.931 across 21 conditions. External validation\nacross two distinct health systems confirms consistent performance (AUC 0.923),\nestablishing generalizability to different evaluation scenarios, with notable\nperformance in critical findings such as small bowel obstruction (AUC 0.958)\nand acute pancreatitis (AUC 0.961). Subgroup analysis shows consistent accuracy\nacross patient sex, age groups, and varied imaging protocols, including\ndifferent slice thicknesses and contrast administration types. Comparison of\nhigh-confidence model outputs to radiologist reports reveals instances where\na2z-1 identified overlooked findings, suggesting potential for quality\nassurance applications.\n","authors":["Pranav Rajpurkar","Julian N. Acosta","Siddhant Dogra","Jaehwan Jeong","Deepanshu Jindal","Michael Moritz","Samir Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2412.12629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12628v1","updated":"2024-12-17T07:43:36Z","published":"2024-12-17T07:43:36Z","title":"Dense Audio-Visual Event Localization under Cross-Modal Consistency and\n  Multi-Temporal Granularity Collaboration","summary":"  In the field of audio-visual learning, most research tasks focus exclusively\non short videos. This paper focuses on the more practical Dense Audio-Visual\nEvent Localization (DAVEL) task, advancing audio-visual scene understanding for\nlonger, {untrimmed} videos. This task seeks to identify and temporally pinpoint\nall events simultaneously occurring in both audio and visual streams.\nTypically, each video encompasses dense events of multiple classes, which may\noverlap on the timeline, each exhibiting varied durations. Given these\nchallenges, effectively exploiting the audio-visual relations and the temporal\nfeatures encoded at various granularities becomes crucial. To address these\nchallenges, we introduce a novel \\ul{CC}Net, comprising two core modules: the\nCross-Modal Consistency \\ul{C}ollaboration (CMCC) and the Multi-Temporal\nGranularity \\ul{C}ollaboration (MTGC). Specifically, the CMCC module contains\ntwo branches: a cross-modal interaction branch and a temporal consistency-gated\nbranch. The former branch facilitates the aggregation of consistent event\nsemantics across modalities through the encoding of audio-visual relations,\nwhile the latter branch guides one modality's focus to pivotal event-relevant\ntemporal areas as discerned in the other modality. The MTGC module includes a\ncoarse-to-fine collaboration block and a fine-to-coarse collaboration block,\nproviding bidirectional support among coarse- and fine-grained temporal\nfeatures. Extensive experiments on the UnAV-100 dataset validate our module\ndesign, resulting in a new state-of-the-art performance in dense audio-visual\nevent localization. The code is available at\n\\url{https://github.com/zzhhfut/CCNet-AAAI2025}.\n","authors":["Ziheng Zhou","Jinxing Zhou","Wei Qian","Shengeng Tang","Xiaojun Chang","Dan Guo"],"pdf_url":"https://arxiv.org/pdf/2412.12628v1.pdf","comment":"Accepted by AAAI 2025. Project page:\n  https://github.com/zzhhfut/CCNet-AAAI2025. Jinxing Zhou and Dan Guo are the\n  corresponding authors"},{"id":"http://arxiv.org/abs/2412.12626v1","updated":"2024-12-17T07:41:06Z","published":"2024-12-17T07:41:06Z","title":"Improving the Transferability of 3D Point Cloud Attack via\n  Spectral-aware Admix and Optimization Designs","summary":"  Deep learning models for point clouds have shown to be vulnerable to\nadversarial attacks, which have received increasing attention in various\nsafety-critical applications such as autonomous driving, robotics, and\nsurveillance. Existing 3D attackers generally design various attack strategies\nin the white-box setting, requiring the prior knowledge of 3D model details.\nHowever, real-world 3D applications are in the black-box setting, where we can\nonly acquire the outputs of the target classifier. Although few recent works\ntry to explore the black-box attack, they still achieve limited attack success\nrates (ASR). To alleviate this issue, this paper focuses on attacking the 3D\nmodels in a transfer-based black-box setting, where we first carefully design\nadversarial examples in a white-box surrogate model and then transfer them to\nattack other black-box victim models. Specifically, we propose a novel\nSpectral-aware Admix with Augmented Optimization method (SAAO) to improve the\nadversarial transferability. In particular, since traditional Admix strategy\nare deployed in the 2D domain that adds pixel-wise images for perturbing, we\ncan not directly follow it to merge point clouds in coordinate domain as it\nwill destroy the geometric shapes. Therefore, we design spectral-aware fusion\nthat performs Graph Fourier Transform (GFT) to get spectral features of the\npoint clouds and add them in the spectral domain. Afterward, we run a few steps\nwith spectral-aware weighted Admix to select better optimization paths as well\nas to adjust corresponding learning weights. At last, we run more steps to\ngenerate adversarial spectral feature along the optimization path and perform\nInverse-GFT on the adversarial spectral feature to obtain the adversarial\nexample in the data domain. Experiments show that our SAAO achieves better\ntransferability compared to existing 3D attack methods.\n","authors":["Shiyu Hu","Daizong Liu","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2412.12626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12620v1","updated":"2024-12-17T07:33:07Z","published":"2024-12-17T07:33:07Z","title":"Multi-Domain Features Guided Supervised Contrastive Learning for Radar\n  Target Detection","summary":"  Detecting small targets in sea clutter is challenging due to dynamic maritime\nconditions. Existing solutions either model sea clutter for detection or\nextract target features based on clutter-target echo differences, including\nstatistical and deep features. While more common, the latter often excels in\ncontrolled scenarios but struggles with robust detection and generalization in\ndiverse environments, limiting practical use. In this letter, we propose a\nmulti-domain features guided supervised contrastive learning (MDFG_SCL) method,\nwhich integrates statistical features derived from multi-domain differences\nwith deep features obtained through supervised contrastive learning, thereby\ncapturing both low-level domain-specific variations and high-level semantic\ninformation. This comprehensive feature integration enables the model to\neffectively distinguish between small targets and sea clutter, even under\nchallenging conditions. Experiments conducted on real-world datasets\ndemonstrate that the proposed shallow-to-deep detector not only achieves\neffective identification of small maritime targets but also maintains superior\ndetection performance across varying sea conditions, outperforming the\nmainstream unsupervised contrastive learning and supervised contrastive\nlearning methods.\n","authors":["Junjie Wang","Yuze Gao","Dongying Li","Wenxian Yu"],"pdf_url":"https://arxiv.org/pdf/2412.12620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11248v2","updated":"2024-12-17T07:31:27Z","published":"2024-12-15T16:54:53Z","title":"Multimodal Class-aware Semantic Enhancement Network for Audio-Visual\n  Video Parsing","summary":"  The Audio-Visual Video Parsing task aims to recognize and temporally localize\nall events occurring in either the audio or visual stream, or both. Capturing\naccurate event semantics for each audio/visual segment is vital. Prior works\ndirectly utilize the extracted holistic audio and visual features for intra-\nand cross-modal temporal interactions. However, each segment may contain\nmultiple events, resulting in semantically mixed holistic features that can\nlead to semantic interference during intra- or cross-modal interactions: the\nevent semantics of one segment may incorporate semantics of unrelated events\nfrom other segments. To address this issue, our method begins with a\nClass-Aware Feature Decoupling (CAFD) module, which explicitly decouples the\nsemantically mixed features into distinct class-wise features, including\nmultiple event-specific features and a dedicated background feature. The\ndecoupled class-wise features enable our model to selectively aggregate useful\nsemantics for each segment from clearly matched classes contained in other\nsegments, preventing semantic interference from irrelevant classes.\nSpecifically, we further design a Fine-Grained Semantic Enhancement module for\nencoding intra- and cross-modal relations. It comprises a Segment-wise Event\nCo-occurrence Modeling (SECM) block and a Local-Global Semantic Fusion (LGSF)\nblock. The SECM exploits inter-class dependencies of concurrent events within\nthe same timestamp with the aid of a new event co-occurrence loss. The LGSF\nfurther enhances the event semantics of each segment by incorporating relevant\nsemantics from more informative global video features. Extensive experiments\nvalidate the effectiveness of the proposed modules and loss functions,\nresulting in a new state-of-the-art parsing performance.\n","authors":["Pengcheng Zhao","Jinxing Zhou","Yang Zhao","Dan Guo","Yanxiang Chen"],"pdf_url":"https://arxiv.org/pdf/2412.11248v2.pdf","comment":"Accepted by AAAI-2025"},{"id":"http://arxiv.org/abs/2412.12617v1","updated":"2024-12-17T07:30:09Z","published":"2024-12-17T07:30:09Z","title":"PO3AD: Predicting Point Offsets toward Better 3D Point Cloud Anomaly\n  Detection","summary":"  Point cloud anomaly detection under the anomaly-free setting poses\nsignificant challenges as it requires accurately capturing the features of 3D\nnormal data to identify deviations indicative of anomalies. Current efforts\nfocus on devising reconstruction tasks, such as acquiring normal data\nrepresentations by restoring normal samples from altered, pseudo-anomalous\ncounterparts. Our findings reveal that distributing attention equally across\nnormal and pseudo-anomalous data tends to dilute the model's focus on anomalous\ndeviations. The challenge is further compounded by the inherently disordered\nand sparse nature of 3D point cloud data. In response to those predicaments, we\nintroduce an innovative approach that emphasizes learning point offsets,\ntargeting more informative pseudo-abnormal points, thus fostering more\neffective distillation of normal data representations. We also have crafted an\naugmentation technique that is steered by normal vectors, facilitating the\ncreation of credible pseudo anomalies that enhance the efficiency of the\ntraining process. Our comprehensive experimental evaluation on the\nAnomaly-ShapeNet and Real3D-AD datasets evidences that our proposed method\noutperforms existing state-of-the-art approaches, achieving an average\nenhancement of 9.0% and 1.4% in the AUC-ROC detection metric across these\ndatasets, respectively.\n","authors":["Jianan Ye","Weiguang Zhao","Xi Yang","Guangliang Cheng","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2412.12617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20962v3","updated":"2024-12-17T07:13:38Z","published":"2024-07-30T16:43:24Z","title":"MMTrail: A Multimodal Trailer Video Dataset with Language and Music\n  Descriptions","summary":"  Massive multi-modality datasets play a significant role in facilitating the\nsuccess of large video-language models. However, current video-language\ndatasets primarily provide text descriptions for visual frames, considering\naudio to be weakly related information. They usually overlook exploring the\npotential of inherent audio-visual correlation, leading to monotonous\nannotation within each modality instead of comprehensive and precise\ndescriptions. Such ignorance results in the difficulty of multiple\ncross-modality studies. To fulfill this gap, we present MMTrail, a large-scale\nmulti-modality video-language dataset incorporating more than 20M trailer clips\nwith visual captions, and 2M high-quality clips with multimodal captions.\nTrailers preview full-length video works and integrate context, visual frames,\nand background music. In particular, the trailer has two main advantages: (1)\nthe topics are diverse, and the content characters are of various types, e.g.,\nfilm, news, and gaming. (2) the corresponding background music is\ncustom-designed, making it more coherent with the visual context. Upon these\ninsights, we propose a systemic captioning framework, achieving various\nmodality annotations with more than 27.1k hours of trailer videos. Here, to\nensure the caption retains music perspective while preserving the authority of\nvisual context, we leverage the advanced LLM to merge all annotations\nadaptively. In this fashion, our MMtrail dataset potentially paves the path for\nfine-grained large multimodal-language model training. In experiments, we\nprovide evaluation metrics and benchmark results on our dataset, demonstrating\nthe high quality of our annotation and its effectiveness for model training.\n","authors":["Xiaowei Chi","Yatian Wang","Aosong Cheng","Pengjun Fang","Zeyue Tian","Yingqing He","Zhaoyang Liu","Xingqun Qi","Jiahao Pan","Rongyu Zhang","Mengfei Li","Ruibin Yuan","Yanbing Jiang","Wei Xue","Wenhan Luo","Qifeng Chen","Shanghang Zhang","Qifeng Liu","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2407.20962v3.pdf","comment":"15 Pages. Dataset report"},{"id":"http://arxiv.org/abs/2410.05993v3","updated":"2024-12-17T07:13:18Z","published":"2024-10-08T12:44:57Z","title":"Aria: An Open Multimodal Native Mixture-of-Experts Model","summary":"  Information comes in diverse modalities. Multimodal native AI models are\nessential to integrate real-world information and deliver comprehensive\nunderstanding. While proprietary multimodal native models exist, their lack of\nopenness imposes obstacles for adoptions, let alone adaptations. To fill this\ngap, we introduce Aria, an open multimodal native model with best-in-class\nperformance across a wide range of multimodal, language, and coding tasks. Aria\nis a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual\ntoken and text token, respectively. It outperforms Pixtral-12B and\nLlama3.2-11B, and is competitive against the best proprietary models on various\nmultimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline,\nwhich progressively equips the model with strong capabilities in language\nunderstanding, multimodal understanding, long context window, and instruction\nfollowing. We open-source the model weights along with a codebase that\nfacilitates easy adoptions and adaptations of Aria in real-world applications.\n","authors":["Dongxu Li","Yudong Liu","Haoning Wu","Yue Wang","Zhiqi Shen","Bowen Qu","Xinyao Niu","Fan Zhou","Chengen Huang","Yanpeng Li","Chongyan Zhu","Xiaoyi Ren","Chao Li","Yifan Ye","Lihuan Zhang","Hanshu Yan","Guoyin Wang","Bei Chen","Junnan Li"],"pdf_url":"https://arxiv.org/pdf/2410.05993v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12606v1","updated":"2024-12-17T07:06:10Z","published":"2024-12-17T07:06:10Z","title":"Multi-Dimensional Insights: Benchmarking Real-World Personalization in\n  Large Multimodal Models","summary":"  The rapidly developing field of large multimodal models (LMMs) has led to the\nemergence of diverse models with remarkable capabilities. However, existing\nbenchmarks fail to comprehensively, objectively and accurately evaluate whether\nLMMs align with the diverse needs of humans in real-world scenarios. To bridge\nthis gap, we propose the Multi-Dimensional Insights (MDI) benchmark, which\nincludes over 500 images covering six common scenarios of human life. Notably,\nthe MDI-Benchmark offers two significant advantages over existing evaluations:\n(1) Each image is accompanied by two types of questions: simple questions to\nassess the model's understanding of the image, and complex questions to\nevaluate the model's ability to analyze and reason beyond basic content. (2)\nRecognizing that people of different age groups have varying needs and\nperspectives when faced with the same scenario, our benchmark stratifies\nquestions into three age categories: young people, middle-aged people, and\nolder people. This design allows for a detailed assessment of LMMs'\ncapabilities in meeting the preferences and needs of different age groups. With\nMDI-Benchmark, the strong model like GPT-4o achieve 79% accuracy on age-related\ntasks, indicating that existing LMMs still have considerable room for\nimprovement in addressing real-world applications. Looking ahead, we anticipate\nthat the MDI-Benchmark will open new pathways for aligning real-world\npersonalization in LMMs. The MDI-Benchmark data and evaluation code are\navailable at https://mdi-benchmark.github.io/\n","authors":["YiFan Zhang","Shanglin Lei","Runqi Qiao","Zhuoma GongQue","Xiaoshuai Song","Guanting Dong","Qiuna Tan","Zhe Wei","Peiqing Yang","Ye Tian","Yadong Xue","Xiaofei Wang","Honggang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12606v1.pdf","comment":"33 pages, 33 figures, Work in progress"},{"id":"http://arxiv.org/abs/2412.12603v1","updated":"2024-12-17T07:00:07Z","published":"2024-12-17T07:00:07Z","title":"RemoteTrimmer: Adaptive Structural Pruning for Remote Sensing Image\n  Classification","summary":"  Since high resolution remote sensing image classification often requires a\nrelatively high computation complexity, lightweight models tend to be practical\nand efficient. Model pruning is an effective method for model compression.\nHowever, existing methods rarely take into account the specificity of remote\nsensing images, resulting in significant accuracy loss after pruning. To this\nend, we propose an effective structural pruning approach for remote sensing\nimage classification. Specifically, a pruning strategy that amplifies the\ndifferences in channel importance of the model is introduced. Then an adaptive\nmining loss function is designed for the fine-tuning process of the pruned\nmodel. Finally, we conducted experiments on two remote sensing classification\ndatasets. The experimental results demonstrate that our method achieves minimal\naccuracy loss after compressing remote sensing classification models, achieving\nstate-of-the-art (SoTA) performance.\n","authors":["Guanwenjie Zou","Liang Yao","Fan Liu","Chuanyi Zhang","Xin Li","Ning Chen","Shengxiang Xu","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12596v1","updated":"2024-12-17T06:54:54Z","published":"2024-12-17T06:54:54Z","title":"OpenViewer: Openness-Aware Multi-View Learning","summary":"  Multi-view learning methods leverage multiple data sources to enhance\nperception by mining correlations across views, typically relying on predefined\ncategories. However, deploying these models in real-world scenarios presents\ntwo primary openness challenges. 1) Lack of Interpretability: The integration\nmechanisms of multi-view data in existing black-box models remain poorly\nexplained; 2) Insufficient Generalization: Most models are not adapted to\nmulti-view scenarios involving unknown categories. To address these challenges,\nwe propose OpenViewer, an openness-aware multi-view learning framework with\ntheoretical support. This framework begins with a Pseudo-Unknown Sample\nGeneration Mechanism to efficiently simulate open multi-view environments and\npreviously adapt to potential unknown samples. Subsequently, we introduce an\nExpression-Enhanced Deep Unfolding Network to intuitively promote\ninterpretability by systematically constructing functional prior-mapping\nmodules and effectively providing a more transparent integration mechanism for\nmulti-view data. Additionally, we establish a Perception-Augmented Open-Set\nTraining Regime to significantly enhance generalization by precisely boosting\nconfidences for known categories and carefully suppressing inappropriate\nconfidences for unknown ones. Experimental results demonstrate that OpenViewer\neffectively addresses openness challenges while ensuring recognition\nperformance for both known and unknown samples. The code is released at\nhttps://github.com/dushide/OpenViewer.\n","authors":["Shide Du","Zihan Fang","Yanchao Tan","Changwei Wang","Shiping Wang","Wenzhong Guo"],"pdf_url":"https://arxiv.org/pdf/2412.12596v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2411.16657v2","updated":"2024-12-17T06:52:46Z","published":"2024-11-25T18:41:56Z","title":"DreamRunner: Fine-Grained Storytelling Video Generation with\n  Retrieval-Augmented Motion Adaptation","summary":"  Storytelling video generation (SVG) has recently emerged as a task to create\nlong, multi-motion, multi-scene videos that consistently represent the story\ndescribed in the input text script. SVG holds great potential for diverse\ncontent creation in media and entertainment; however, it also presents\nsignificant challenges: (1) objects must exhibit a range of fine-grained,\ncomplex motions, (2) multiple objects need to appear consistently across\nscenes, and (3) subjects may require multiple motions with seamless transitions\nwithin a single scene. To address these challenges, we propose DreamRunner, a\nnovel story-to-video generation method: First, we structure the input script\nusing a large language model (LLM) to facilitate both coarse-grained scene\nplanning as well as fine-grained object-level layout and motion planning. Next,\nDreamRunner presents retrieval-augmented test-time adaptation to capture target\nmotion priors for objects in each scene, supporting diverse motion\ncustomization based on retrieved videos, thus facilitating the generation of\nnew videos with complex, scripted motions. Lastly, we propose a novel\nspatial-temporal region-based 3D attention and prior injection module SR3AI for\nfine-grained object-motion binding and frame-by-frame semantic control. We\ncompare DreamRunner with various SVG baselines, demonstrating state-of-the-art\nperformance in character consistency, text alignment, and smooth transitions.\nAdditionally, DreamRunner exhibits strong fine-grained condition-following\nability in compositional text-to-video generation, significantly outperforming\nbaselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to\ngenerate multi-object interactions with qualitative examples.\n","authors":["Zun Wang","Jialu Li","Han Lin","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2411.16657v2.pdf","comment":"Project website: https://zunwang1.github.io/DreamRunner"},{"id":"http://arxiv.org/abs/2412.12594v1","updated":"2024-12-17T06:50:23Z","published":"2024-12-17T06:50:23Z","title":"A Simple and Efficient Baseline for Zero-Shot Generative Classification","summary":"  Large diffusion models have become mainstream generative models in both\nacademic studies and industrial AIGC applications. Recently, a number of works\nfurther explored how to employ the power of large diffusion models as zero-shot\nclassifiers. While recent zero-shot diffusion-based classifiers have made\nperformance advancement on benchmark datasets, they still suffered badly from\nextremely slow classification speed (e.g., ~1000 seconds per classifying single\nimage on ImageNet). The extremely slow classification speed strongly prohibits\nexisting zero-shot diffusion-based classifiers from practical applications. In\nthis paper, we propose an embarrassingly simple and efficient zero-shot\nGaussian Diffusion Classifiers (GDC) via pretrained text-to-image diffusion\nmodels and DINOv2. The proposed GDC can not only significantly surpass previous\nzero-shot diffusion-based classifiers by over 10 points (61.40% - 71.44%) on\nImageNet, but also accelerate more than 30000 times (1000 - 0.03 seconds)\nclassifying a single image on ImageNet. Additionally, it provides probability\ninterpretation of the results. Our extensive experiments further demonstrate\nthat GDC can achieve highly competitive zero-shot classification performance\nover various datasets and can promisingly self-improve with stronger diffusion\nmodels. To the best of our knowledge, the proposed GDC is the first zero-shot\ndiffusionbased classifier that exhibits both competitive accuracy and practical\nefficiency.\n","authors":["Zipeng Qi","Buhua Liu","Shiyan Zhang","Bao Li","Zhiqiang Xu","Haoyi Xiong","Zeke Xie"],"pdf_url":"https://arxiv.org/pdf/2412.12594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10638v2","updated":"2024-12-17T06:48:10Z","published":"2024-06-15T13:58:26Z","title":"Unveiling the Ignorance of MLLMs: Seeing Clearly, Answering Incorrectly","summary":"  Multimodal Large Language Models (MLLMs) have displayed remarkable\nperformance in multi-modal tasks, particularly in visual comprehension.\nHowever, we reveal that MLLMs often generate incorrect answers even when they\nunderstand the visual content. To this end, we manually construct a benchmark\nwith 12 categories and design evaluation metrics that assess the degree of\nerror in MLLM responses even when the visual content is seemingly understood.\nBased on this benchmark, we test 15 leading MLLMs and analyze the distribution\nof attention maps and logits of some MLLMs. Our investigation identifies two\nprimary issues: 1) most instruction tuning datasets predominantly feature\nquestions that 'directly' relate to the visual content, leading to a bias in\nMLLMs' responses to other indirect questions, and 2) MLLMs' attention to visual\ntokens is notably lower than to system and question tokens. We further observe\nthat attention scores between questions and visual tokens as well as the\nmodel's confidence in the answers are lower in response to misleading questions\nthan to straightforward ones. To address the first challenge, we introduce a\npaired positive and negative data construction pipeline to diversify the\ndataset. For the second challenge, we propose to enhance the model's focus on\nvisual content during decoding by refining the text and visual prompt. For the\ntext prompt, we propose a content guided refinement strategy that performs\npreliminary visual content analysis to generate structured information before\nanswering the question. Additionally, we employ a visual attention refinement\nstrategy that highlights question-relevant visual tokens to increase the\nmodel's attention to visual content that aligns with the question. Extensive\nexperiments demonstrate that these challenges can be significantly mitigated\nwith our proposed dataset and techniques.\n","authors":["Yexin Liu","Zhengyang Liang","Yueze Wang","Xianfeng Wu","Feilong Tang","Muyang He","Jian Li","Zheng Liu","Harry Yang","Sernam Lim","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.10638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14358v2","updated":"2024-12-17T06:40:05Z","published":"2024-11-21T17:58:07Z","title":"InCrowd-VI: A Realistic Visual-Inertial Dataset for Evaluating SLAM in\n  Indoor Pedestrian-Rich Spaces for Human Navigation","summary":"  Simultaneous localization and mapping (SLAM) techniques can be used to\nnavigate the visually impaired, but the development of robust SLAM solutions\nfor crowded spaces is limited by the lack of realistic datasets. To address\nthis, we introduce InCrowd-VI, a novel visual-inertial dataset specifically\ndesigned for human navigation in indoor pedestrian-rich environments. Recorded\nusing Meta Aria Project glasses, it captures realistic scenarios without\nenvironmental control. InCrowd-VI features 58 sequences totaling a 5 km\ntrajectory length and 1.5 hours of recording time, including RGB, stereo\nimages, and IMU measurements. The dataset captures important challenges such as\npedestrian occlusions, varying crowd densities, complex layouts, and lighting\nchanges. Ground-truth trajectories, accurate to approximately 2 cm, are\nprovided in the dataset, originating from the Meta Aria project machine\nperception SLAM service. In addition, a semi-dense 3D point cloud of scenes is\nprovided for each sequence. The evaluation of state-of-the-art visual odometry\n(VO) and SLAM algorithms on InCrowd-VI revealed severe performance limitations\nin these realistic scenarios. Under challenging conditions, systems exceeded\nthe required localization accuracy of 0.5 meters and the 1\\% drift threshold,\nwith classical methods showing drift up to 5-10\\%. While deep learning-based\napproaches maintained high pose estimation coverage (>90\\%), they failed to\nachieve real-time processing speeds necessary for walking pace navigation.\nThese results demonstrate the need and value of a new dataset to advance SLAM\nresearch for visually impaired navigation in complex indoor environments. The\ndataset and associated tools are publicly available at\nhttps://incrowd-vi.cloudlab.zhaw.ch/.\n","authors":["Marziyeh Bamdad","Hans-Peter Hutter","Alireza Darvishy"],"pdf_url":"https://arxiv.org/pdf/2411.14358v2.pdf","comment":"24 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2412.08939v2","updated":"2024-12-17T06:30:00Z","published":"2024-12-12T05:01:17Z","title":"Dynamic Contrastive Knowledge Distillation for Efficient Image\n  Restoration","summary":"  Knowledge distillation (KD) is a valuable yet challenging approach that\nenhances a compact student network by learning from a high-performance but\ncumbersome teacher model. However, previous KD methods for image restoration\noverlook the state of the student during the distillation, adopting a fixed\nsolution space that limits the capability of KD. Additionally, relying solely\non L1-type loss struggles to leverage the distribution information of images.\nIn this work, we propose a novel dynamic contrastive knowledge distillation\n(DCKD) framework for image restoration. Specifically, we introduce dynamic\ncontrastive regularization to perceive the student's learning state and\ndynamically adjust the distilled solution space using contrastive learning.\nAdditionally, we also propose a distribution mapping module to extract and\nalign the pixel-level category distribution of the teacher and student models.\nNote that the proposed DCKD is a structure-agnostic distillation framework,\nwhich can adapt to different backbones and can be combined with methods that\noptimize upper-bound constraints to further enhance model performance.\nExtensive experiments demonstrate that DCKD significantly outperforms the\nstate-of-the-art KD methods across various image restoration tasks and\nbackbones.\n","authors":["Yunshuai Zhou","Junbo Qiao","Jincheng Liao","Wei Li","Simiao Li","Jiao Xie","Yunhang Shen","Jie Hu","Shaohui Lin"],"pdf_url":"https://arxiv.org/pdf/2412.08939v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09224v3","updated":"2024-12-17T06:21:52Z","published":"2024-12-12T12:26:08Z","title":"DASK: Distribution Rehearsing via Adaptive Style Kernel Learning for\n  Exemplar-Free Lifelong Person Re-Identification","summary":"  Lifelong person re-identification (LReID) is an important but challenging\ntask that suffers from catastrophic forgetting due to significant domain gaps\nbetween training steps. Existing LReID approaches typically rely on data replay\nand knowledge distillation to mitigate this issue. However, data replay methods\ncompromise data privacy by storing historical exemplars, while knowledge\ndistillation methods suffer from limited performance due to the cumulative\nforgetting of undistilled knowledge. To overcome these challenges, we propose a\nnovel paradigm that models and rehearses the distribution of the old domains to\nenhance knowledge consolidation during the new data learning, possessing a\nstrong anti-forgetting capacity without storing any exemplars. Specifically, we\nintroduce an exemplar-free LReID method called Distribution Rehearsing via\nAdaptive Style Kernel Learning (DASK). DASK includes a Distribution Rehearser\nLearning (DRL) mechanism that learns to transform arbitrary distribution data\ninto the current data style at each learning step. To enhance the style\ntransfer capacity of DRL, an Adaptive Kernel Prediction Network (AKPNet) is\nexplored to achieve an instance-specific distribution adjustment. Additionally,\nwe design a Distribution Rehearsing-driven LReID Training (DRRT) module, which\nrehearses old distribution based on the new data via the old AKPNet model,\nachieving effective new-old knowledge accumulation under a joint knowledge\nconsolidation scheme. Experimental results show our DASK outperforms the\nexisting methods by 3.6%-6.8% and 4.5%-6.5% on anti-forgetting and\ngeneralization capacity, respectively. Our code is available at\nhttps://github.com/zhoujiahuan1991/AAAI2025-LReID-DASK\n","authors":["Kunlun Xu","Chenghao Jiang","Peixi Xiong","Yuxin Peng","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.09224v3.pdf","comment":"in Proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.00473v4","updated":"2024-12-17T06:09:08Z","published":"2024-11-30T13:21:15Z","title":"Jailbreak Large Vision-Language Models Through Multi-Modal Linkage","summary":"  With the significant advancement of Large Vision-Language Models (VLMs),\nconcerns about their potential misuse and abuse have grown rapidly. Previous\nstudies have highlighted VLMs' vulnerability to jailbreak attacks, where\ncarefully crafted inputs can lead the model to produce content that violates\nethical and legal standards. However, existing methods struggle against\nstate-of-the-art VLMs like GPT-4o, due to the over-exposure of harmful content\nand lack of stealthy malicious guidance. In this work, we propose a novel\njailbreak attack framework: Multi-Modal Linkage (MML) Attack. Drawing\ninspiration from cryptography, MML utilizes an encryption-decryption process\nacross text and image modalities to mitigate over-exposure of malicious\ninformation. To align the model's output with malicious intent covertly, MML\nemploys a technique called \"evil alignment\", framing the attack within a video\ngame production scenario. Comprehensive experiments demonstrate MML's\neffectiveness. Specifically, MML jailbreaks GPT-4o with attack success rates of\n97.80% on SafeBench, 98.81% on MM-SafeBench and 99.07% on HADES-Dataset. Our\ncode is available at https://github.com/wangyu-ovo/MML\n","authors":["Yu Wang","Xiaofei Zhou","Yichen Wang","Geyuan Zhang","Tianxing He"],"pdf_url":"https://arxiv.org/pdf/2412.00473v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16900v2","updated":"2024-12-17T06:06:02Z","published":"2023-11-28T15:58:13Z","title":"Lane-Keeping Control of Autonomous Vehicles Through a Soft-Constrained\n  Iterative LQR","summary":"  The accurate prediction of smooth steering inputs is crucial for automotive\napplications because control actions with jitter might cause the vehicle system\nto become unstable. To address this problem in automobile lane-keeping control\nwithout the use of additional smoothing algorithms, we developed a novel\nsoft-constrained iterative linear quadratic regulator (soft-CILQR) algorithm by\nintegrating CILQR algorithm and a model predictive control (MPC) constraint\nrelaxation method. We incorporated slack variables into the state and control\nbarrier functions of the soft-CILQR solver to soften the constraints in the\noptimization process such that control input stabilization can be achieved in a\ncomputationally simple manner. Two types of automotive lane-keeping experiments\n(numerical simulations and experiments involving challenging vision-based\nmaneuvers) were conducted with a linear system dynamics model to test the\nperformance of the proposed soft-CILQR algorithm, and its performance was\ncompared with that of the CILQR algorithm. In the numerical simulations, the\nsoft-CILQR and CILQR solvers managed to drive the system toward the reference\nstate asymptotically; however, the soft-CILQR solver obtained smooth steering\ninput trajectories more easily than did the CILQR solver under conditions\ninvolving additive disturbances. The results of the vision-based experiments in\nwhich an ego vehicle drove in perturbed TORCS environments with various road\nfriction settings were consistent with those of the numerical tests. The\nproposed soft-CILQR algorithm achieved an average runtime of 2.55 ms and is\nthus applicable for real-time autonomous driving scenarios.\n","authors":["Der-Hau Lee"],"pdf_url":"https://arxiv.org/pdf/2311.16900v2.pdf","comment":"17 figures, 13 pages"},{"id":"http://arxiv.org/abs/2412.12572v1","updated":"2024-12-17T06:03:42Z","published":"2024-12-17T06:03:42Z","title":"License Plate Detection and Character Recognition Using Deep Learning\n  and Font Evaluation","summary":"  License plate detection (LPD) is essential for traffic management, vehicle\ntracking, and law enforcement but faces challenges like variable lighting and\ndiverse font types, impacting accuracy. Traditionally reliant on image\nprocessing and machine learning, the field is now shifting towards deep\nlearning for its robust performance in various conditions. Current methods,\nhowever, often require tailoring to specific regional datasets. This paper\nproposes a dual deep learning strategy using a Faster R-CNN for detection and a\nCNN-RNN model with Connectionist Temporal Classification (CTC) loss and a\nMobileNet V3 backbone for recognition. This approach aims to improve model\nperformance using datasets from Ontario, Quebec, California, and New York\nState, achieving a recall rate of 92% on the Centre for Pattern Recognition and\nMachine Intelligence (CENPARMI) dataset and 90% on the UFPR-ALPR dataset. It\nincludes a detailed error analysis to identify the causes of false positives.\nAdditionally, the research examines the role of font features in license plate\n(LP) recognition, analyzing fonts like Driver Gothic, Dreadnought, California\nClarendon, and Zurich Extra Condensed with the OpenALPR system. It discovers\nsignificant performance discrepancies influenced by font characteristics,\noffering insights for future LPD system enhancements.\n  Keywords: Deep Learning, License Plate, Font Evaluation\n","authors":["Zahra Ebrahimi Vargoorani","Ching Yee Suen"],"pdf_url":"https://arxiv.org/pdf/2412.12572v1.pdf","comment":"12 pages, 5 figures. This is the pre-Springer final accepted version.\n  The final version is published in Springer, Lecture Notes in Computer Science\n  (LNCS), Volume 14731, 2024. Springer Version of Record"},{"id":"http://arxiv.org/abs/2412.12571v1","updated":"2024-12-17T06:03:05Z","published":"2024-12-17T06:03:05Z","title":"ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting\n  with Diffusion Transformers","summary":"  Recent research arXiv:2410.15027 arXiv:2410.23775 has highlighted the\ninherent in-context generation capabilities of pretrained diffusion\ntransformers (DiTs), enabling them to seamlessly adapt to diverse visual tasks\nwith minimal or no architectural modifications. These capabilities are unlocked\nby concatenating self-attention tokens across multiple input and target images,\ncombined with grouped and masked generation pipelines. Building upon this\nfoundation, we present ChatDiT, a zero-shot, general-purpose, and interactive\nvisual generation framework that leverages pretrained diffusion transformers in\ntheir original form, requiring no additional tuning, adapters, or\nmodifications. Users can interact with ChatDiT to create interleaved text-image\narticles, multi-page picture books, edit images, design IP derivatives, or\ndevelop character design settings, all through free-form natural language\nacross one or more conversational rounds. At its core, ChatDiT employs a\nmulti-agent system comprising three key components: an Instruction-Parsing\nagent that interprets user-uploaded images and instructions, a\nStrategy-Planning agent that devises single-step or multi-step generation\nactions, and an Execution agent that performs these actions using an in-context\ntoolkit of diffusion transformers. We thoroughly evaluate ChatDiT on IDEA-Bench\narXiv:2412.11767, comprising 100 real-world design tasks and 275 cases with\ndiverse instructions and varying numbers of input and target images. Despite\nits simplicity and training-free approach, ChatDiT surpasses all competitors,\nincluding those specifically designed and trained on extensive multi-task\ndatasets. We further identify key limitations of pretrained DiTs in zero-shot\nadapting to tasks. We release all code, agents, results, and intermediate\noutputs to facilitate further research at https://github.com/ali-vilab/ChatDiT\n","authors":["Lianghua Huang","Wei Wang","Zhi-Fan Wu","Yupeng Shi","Chen Liang","Tong Shen","Han Zhang","Huanzhang Dou","Yu Liu","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12571v1.pdf","comment":"Tech report. Project page: https://ali-vilab.github.io/ChatDiT-Page/"},{"id":"http://arxiv.org/abs/2406.16469v2","updated":"2024-12-17T05:51:01Z","published":"2024-06-24T09:18:15Z","title":"Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark\n  with Human-VLM Collaboration","summary":"  To create culturally inclusive vision-language models (VLMs), developing a\nbenchmark that tests their ability to address culturally relevant questions is\nessential. Existing approaches typically rely on human annotators, making the\nprocess labor-intensive and creating a cognitive burden in generating diverse\nquestions. To address this, we propose a semi-automated framework for\nconstructing cultural VLM benchmarks, specifically targeting multiple-choice\nQA. This framework combines human-VLM collaboration, where VLMs generate\nquestions based on guidelines, a small set of annotated examples, and relevant\nknowledge, followed by a verification process by native speakers. We\ndemonstrate the effectiveness of this framework through the creation of\nK-Viscuit, a dataset focused on Korean culture. Our experiments on this dataset\nreveal that open-source models lag behind proprietary ones in understanding\nKorean culture, highlighting key areas for improvement. We also present a\nseries of further analyses, including human evaluation, augmenting VLMs with\nexternal knowledge, and the evaluation beyond multiple-choice QA. Our dataset\nis available at https://huggingface.co/datasets/ddehun/k-viscuit.\n","authors":["Yujin Baek","ChaeHun Park","Jaeseok Kim","Yu-Jung Heo","Du-Seong Chang","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2406.16469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12566v1","updated":"2024-12-17T05:49:26Z","published":"2024-12-17T05:49:26Z","title":"ITP: Instance-Aware Test Pruning for Out-of-Distribution Detection","summary":"  Out-of-distribution (OOD) detection is crucial for ensuring the reliable\ndeployment of deep models in real-world scenarios. Recently, from the\nperspective of over-parameterization, a series of methods leveraging weight\nsparsification techniques have shown promising performance. These methods\ntypically focus on selecting important parameters for in-distribution (ID) data\nto reduce the negative impact of redundant parameters on OOD detection.\nHowever, we empirically find that these selected parameters may behave\noverconfidently toward OOD data and hurt OOD detection. To address this issue,\nwe propose a simple yet effective post-hoc method called Instance-aware Test\nPruning (ITP), which performs OOD detection by considering both coarse-grained\nand fine-grained levels of parameter pruning. Specifically, ITP first estimates\nthe class-specific parameter contribution distribution by exploring the ID\ndata. By using the contribution distribution, ITP conducts coarse-grained\npruning to eliminate redundant parameters. More importantly, ITP further adopts\na fine-grained test pruning process based on the right-tailed Z-score test,\nwhich can adaptively remove instance-level overconfident parameters. Finally,\nITP derives OOD scores from the pruned model to achieve more reliable\npredictions. Extensive experiments on widely adopted benchmarks verify the\neffectiveness of ITP, demonstrating its competitive performance.\n","authors":["Haonan Xu","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2412.12566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12565v1","updated":"2024-12-17T05:49:16Z","published":"2024-12-17T05:49:16Z","title":"PBVS 2024 Solution: Self-Supervised Learning and Sampling Strategies for\n  SAR Classification in Extreme Long-Tail Distribution","summary":"  The Multimodal Learning Workshop (PBVS 2024) aims to improve the performance\nof automatic target recognition (ATR) systems by leveraging both Synthetic\nAperture Radar (SAR) data, which is difficult to interpret but remains\nunaffected by weather conditions and visible light, and Electro-Optical (EO)\ndata for simultaneous learning. The subtask, known as the Multi-modal Aerial\nView Imagery Challenge - Classification, focuses on predicting the class label\nof a low-resolution aerial image based on a set of SAR-EO image pairs and their\nrespective class labels. The provided dataset consists of SAR-EO pairs,\ncharacterized by a severe long-tail distribution with over a 1000-fold\ndifference between the largest and smallest classes, making typical long-tail\nmethods difficult to apply. Additionally, the domain disparity between the SAR\nand EO datasets complicates the effectiveness of standard multimodal methods.\nTo address these significant challenges, we propose a two-stage learning\napproach that utilizes self-supervised techniques, combined with multimodal\nlearning and inference through SAR-to-EO translation for effective EO\nutilization. In the final testing phase of the PBVS 2024 Multi-modal Aerial\nView Image Challenge - Classification (SAR Classification) task, our model\nachieved an accuracy of 21.45%, an AUC of 0.56, and a total score of 0.30,\nplacing us 9th in the competition.\n","authors":["Yuhyun Kim","Minwoo Kim","Hyobin Park","Jinwook Jung","Dong-Geol Choi"],"pdf_url":"https://arxiv.org/pdf/2412.12565v1.pdf","comment":"4 pages, 3 figures, 1 Table"},{"id":"http://arxiv.org/abs/2412.12562v1","updated":"2024-12-17T05:45:48Z","published":"2024-12-17T05:45:48Z","title":"Efficient Oriented Object Detection with Enhanced Small Object\n  Recognition in Aerial Images","summary":"  Achieving a balance between computational efficiency and detection accuracy\nin the realm of rotated bounding box object detection within aerial imagery is\na significant challenge. While prior research has aimed at creating lightweight\nmodels that enhance computational performance and feature extraction, there\nremains a gap in the performance of these networks when it comes to the\ndetection of small and multi-scale objects in remote sensing (RS) imagery. To\naddress these challenges, we present a novel enhancement to the YOLOv8 model,\ntailored for oriented object detection tasks and optimized for environments\nwith limited computational resources. Our model features a wavelet\ntransform-based C2f module for capturing associative features and an Adaptive\nScale Feature Pyramid (ASFP) module that leverages P2 layer details.\nAdditionally, the incorporation of GhostDynamicConv significantly contributes\nto the model's lightweight nature, ensuring high efficiency in aerial imagery\nanalysis. Featuring a parameter count of 21.6M, our approach provides a more\nefficient architectural design than DecoupleNet, which has 23.3M parameters,\nall while maintaining detection accuracy. On the DOTAv1.0 dataset, our model\ndemonstrates a mean Average Precision (mAP) that is competitive with leading\nmethods such as DecoupleNet. The model's efficiency, combined with its reduced\nparameter count, makes it a strong candidate for aerial object detection,\nparticularly in resource-constrained environments.\n","authors":["Zhifei Shi","Zongyao Yin","Sheng Chang","Xiao Yi","Xianchuan Yu"],"pdf_url":"https://arxiv.org/pdf/2412.12562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12561v1","updated":"2024-12-17T05:43:35Z","published":"2024-12-17T05:43:35Z","title":"Tell Me What to Track: Infusing Robust Language Guidance for Enhanced\n  Referring Multi-Object Tracking","summary":"  Referring multi-object tracking (RMOT) is an emerging cross-modal task that\naims to localize an arbitrary number of targets based on a language expression\nand continuously track them in a video. This intricate task involves reasoning\non multi-modal data and precise target localization with temporal association.\nHowever, prior studies overlook the imbalanced data distribution between\nnewborn targets and existing targets due to the nature of the task. In\naddition, they only indirectly fuse multi-modal features, struggling to deliver\nclear guidance on newborn target detection. To solve the above issues, we\nconduct a collaborative matching strategy to alleviate the impact of the\nimbalance, boosting the ability to detect newborn targets while maintaining\ntracking performance. In the encoder, we integrate and enhance the cross-modal\nand multi-scale fusion, overcoming the bottlenecks in previous work, where\nlimited multi-modal information is shared and interacted between feature maps.\nIn the decoder, we also develop a referring-infused adaptation that provides\nexplicit referring guidance through the query tokens. The experiments showcase\nthe superior performance of our model (+3.42%) compared to prior works,\ndemonstrating the effectiveness of our designs.\n","authors":["Wenjun Huang","Yang Ni","Hanning Chen","Yirui He","Ian Bryant","Yezi Liu","Mohsen Imani"],"pdf_url":"https://arxiv.org/pdf/2412.12561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19391v2","updated":"2024-12-17T05:37:37Z","published":"2024-06-27T17:59:40Z","title":"Fibottention: Inceptive Visual Representation Learning with Diverse\n  Attention Across Heads","summary":"  Transformer architectures such as Vision Transformers (ViT) have proven\neffective for solving visual perception tasks. However, they suffer from two\nmajor limitations; first, the quadratic complexity of self-attention limits the\nnumber of tokens that can be processed, and second, Transformers often require\nlarge amounts of training data to attain state-of-the-art performance. In this\npaper, we propose a new multi-head self-attention (MHSA) variant named\nFibottention, which can replace MHSA in Transformer architectures. Fibottention\nis data-efficient and computationally more suitable for processing large\nnumbers of tokens than the standard MHSA. It employs structured sparse\nattention based on dilated Fibonacci sequences, which, uniquely, differ across\nattention heads, resulting in inception-like diverse features across heads. The\nspacing of the Fibonacci sequences follows the Wythoff array, which minimizes\nthe redundancy of token interactions aggregated across different attention\nheads, while still capturing sufficient complementary information through token\npair interactions. These sparse attention patterns are unique among the\nexisting sparse attention and lead to an $O(N \\log N)$ complexity, where $N$ is\nthe number of tokens. Leveraging only 2-6% of the elements in the\nself-attention heads, Fibottention embedded into popular, state-of-the-art\nTransformer architectures can achieve significantly improved predictive\nperformance for domains with limited data such as image classification, video\nunderstanding, and robot learning tasks, and render reduced computational\ncomplexity. We further validated the improved diversity of feature\nrepresentations resulting from different self-attention heads, and our model\ndesign against other sparse attention mechanisms.\n","authors":["Ali Khaleghi Rahimian","Manish Kumar Govind","Subhajit Maity","Dominick Reilly","Christian Kümmerle","Srijan Das","Aritra Dutta"],"pdf_url":"https://arxiv.org/pdf/2406.19391v2.pdf","comment":"The complete implementation, including source code and evaluation\n  scripts, is publicly available at:\n  https://github.com/Charlotte-CharMLab/Fibottention"},{"id":"http://arxiv.org/abs/2412.10718v2","updated":"2024-12-17T05:24:57Z","published":"2024-12-14T07:22:03Z","title":"GridShow: Omni Visual Generation","summary":"  In this paper, we introduce GRID, a novel paradigm that reframes a broad\nrange of visual generation tasks as the problem of arranging grids, akin to\nfilm strips. At its core, GRID transforms temporal sequences into grid layouts,\nenabling image generation models to process visual sequences holistically. To\nachieve both layout consistency and motion coherence, we develop a parallel\nflow-matching training strategy that combines layout matching and temporal\nlosses, guided by a coarse-to-fine schedule that evolves from basic layouts to\nprecise motion control. Our approach demonstrates remarkable efficiency,\nachieving up to 35 faster inference speeds while using 1/1000 of the\ncomputational resources compared to specialized models. Extensive experiments\nshow that GRID exhibits exceptional versatility across diverse visual\ngeneration tasks, from Text-to-Video to 3D Editing, while maintaining its\nfoundational image generation capabilities. This dual strength in both expanded\napplications and preserved core competencies establishes GRID as an efficient\nand versatile omni-solution for visual generation.\n","authors":["Cong Wan","Xiangyang Luo","Zijian Cai","Yiren Song","Yunlong Zhao","Yifan Bai","Yuhang He","Yihong Gong"],"pdf_url":"https://arxiv.org/pdf/2412.10718v2.pdf","comment":"Codes: https://github.com/Should-AI-Lab/GRID"},{"id":"http://arxiv.org/abs/2412.10891v2","updated":"2024-12-17T05:23:42Z","published":"2024-12-14T16:42:41Z","title":"Zigzag Diffusion Sampling: Diffusion Models Can Self-Improve via\n  Self-Reflection","summary":"  Diffusion models, the most popular generative paradigm so far, can inject\nconditional information into the generation path to guide the latent towards\ndesired directions. However, existing text-to-image diffusion models often fail\nto maintain high image quality and high prompt-image alignment for those\nchallenging prompts. To mitigate this issue and enhance existing pretrained\ndiffusion models, we mainly made three contributions in this paper. First, we\npropose diffusion self-reflection that alternately performs denoising and\ninversion and demonstrate that such diffusion self-reflection can leverage the\nguidance gap between denoising and inversion to capture prompt-related semantic\ninformation with theoretical and empirical evidence. Second, motivated by\ntheoretical analysis, we derive Zigzag Diffusion Sampling (Z-Sampling), a novel\nself-reflection-based diffusion sampling method that leverages the guidance gap\nbetween denosing and inversion to accumulate semantic information step by step\nalong the sampling path, leading to improved sampling results. Moreover, as a\nplug-and-play method, Z-Sampling can be generally applied to various diffusion\nmodels (e.g., accelerated ones and Transformer-based ones) with very limited\ncoding and computational costs. Third, our extensive experiments demonstrate\nthat Z-Sampling can generally and significantly enhance generation quality\nacross various benchmark datasets, diffusion models, and performance evaluation\nmetrics. For example, DreamShaper with Z-Sampling can self-improve with the\nHPSv2 winning rate up to 94% over the original results. Moreover, Z-Sampling\ncan further enhance existing diffusion models combined with other orthogonal\nmethods, including Diffusion-DPO.\n","authors":["Lichen Bai","Shitong Shao","Zikai Zhou","Zipeng Qi","Zhiqiang Xu","Haoyi Xiong","Zeke Xie"],"pdf_url":"https://arxiv.org/pdf/2412.10891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12552v1","updated":"2024-12-17T05:23:00Z","published":"2024-12-17T05:23:00Z","title":"SAModified: A Foundation Model-Based Zero-Shot Approach for Refining\n  Noisy Land-Use Land-Cover Maps","summary":"  Land-use and land cover (LULC) analysis is critical in remote sensing, with\nwide-ranging applications across diverse fields such as agriculture, utilities,\nand urban planning. However, automating LULC map generation using machine\nlearning is rendered challenging due to noisy labels. Typically, the ground\ntruths (e.g. ESRI LULC, MapBioMass) have noisy labels that hamper the model's\nability to learn to accurately classify the pixels. Further, these erroneous\nlabels can significantly distort the performance metrics of a model, leading to\nmisleading evaluations. Traditionally, the ambiguous labels are rectified using\nunsupervised algorithms. These algorithms struggle not only with scalability\nbut also with generalization across different geographies. To overcome these\nchallenges, we propose a zero-shot approach using the foundation model, Segment\nAnything Model (SAM), to automatically delineate different land parcels/regions\nand leverage them to relabel the unsure pixels by using the local label\nstatistics within each detected region. We achieve a significant reduction in\nlabel noise and an improvement in the performance of the downstream\nsegmentation model by $\\approx 5\\%$ when trained with denoised labels.\n","authors":["Sparsh Pekhale","Rakshith Sathish","Sathisha Basavaraju","Divya Sharma"],"pdf_url":"https://arxiv.org/pdf/2412.12552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09191v2","updated":"2024-12-17T05:21:52Z","published":"2024-12-12T11:38:46Z","title":"RAD: Region-Aware Diffusion Models for Image Inpainting","summary":"  Diffusion models have achieved remarkable success in image generation, with\napplications broadening across various domains. Inpainting is one such\napplication that can benefit significantly from diffusion models. Existing\nmethods either hijack the reverse process of a pretrained diffusion model or\ncast the problem into a larger framework, \\ie, conditioned generation. However,\nthese approaches often require nested loops in the generation process or\nadditional components for conditioning. In this paper, we present region-aware\ndiffusion models (RAD) for inpainting with a simple yet effective reformulation\nof the vanilla diffusion models. RAD utilizes a different noise schedule for\neach pixel, which allows local regions to be generated asynchronously while\nconsidering the global image context. A plain reverse process requires no\nadditional components, enabling RAD to achieve inference time up to 100 times\nfaster than the state-of-the-art approaches. Moreover, we employ low-rank\nadaptation (LoRA) to fine-tune RAD based on other pretrained diffusion models,\nreducing computational burdens in training as well. Experiments demonstrated\nthat RAD provides state-of-the-art results both qualitatively and\nquantitatively, on the FFHQ, LSUN Bedroom, and ImageNet datasets.\n","authors":["Sora Kim","Sungho Suh","Minsik Lee"],"pdf_url":"https://arxiv.org/pdf/2412.09191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12550v1","updated":"2024-12-17T05:21:16Z","published":"2024-12-17T05:21:16Z","title":"Consistent Diffusion: Denoising Diffusion Model with Data-Consistent\n  Training for Image Restoration","summary":"  In this work, we address the limitations of denoising diffusion models (DDMs)\nin image restoration tasks, particularly the shape and color distortions that\ncan compromise image quality. While DDMs have demonstrated a promising\nperformance in many applications such as text-to-image synthesis, their\neffectiveness in image restoration is often hindered by shape and color\ndistortions. We observe that these issues arise from inconsistencies between\nthe training and testing data used by DDMs. Based on our observation, we\npropose a novel training method, named data-consistent training, which allows\nthe DDMs to access images with accumulated errors during training, thereby\nensuring the model to learn to correct these errors. Experimental results show\nthat, across five image restoration tasks, our method has significant\nimprovements over state-of-the-art methods while effectively minimizing\ndistortions and preserving image fidelity.\n","authors":["Xinlong Cheng","Tiantian Cao","Guoan Cheng","Bangxuan Huang","Xinghan Tian","Ye Wang","Xiaoyu He","Weixin Li","Tianfan Xue","Xuan Dong"],"pdf_url":"https://arxiv.org/pdf/2412.12550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17418v2","updated":"2024-12-17T05:15:54Z","published":"2024-07-24T16:53:17Z","title":"3D Gaussian Splatting: Survey, Technologies, Challenges, and\n  Opportunities","summary":"  3D Gaussian Splatting (3DGS) has emerged as a prominent technique with the\npotential to become a mainstream method for 3D representations. It can\neffectively transform multi-view images into explicit 3D Gaussian through\nefficient training, and achieve real-time rendering of novel views. This survey\naims to analyze existing 3DGS-related works from multiple intersecting\nperspectives, including related tasks, technologies, challenges, and\nopportunities. The primary objective is to provide newcomers with a rapid\nunderstanding of the field and to assist researchers in methodically organizing\nexisting technologies and challenges. Specifically, we delve into the\noptimization, application, and extension of 3DGS, categorizing them based on\ntheir focuses or motivations. Additionally, we summarize and classify nine\ntypes of technical modules and corresponding improvements identified in\nexisting works. Based on these analyses, we further examine the common\nchallenges and technologies across various tasks, proposing potential research\nopportunities.\n","authors":["Yanqi Bao","Tianyu Ding","Jing Huo","Yaoli Liu","Yuxin Li","Wenbin Li","Yang Gao","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2407.17418v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09777v4","updated":"2024-12-17T05:14:03Z","published":"2024-09-15T15:55:24Z","title":"DiFSD: Ego-Centric Fully Sparse Paradigm with Uncertainty Denoising and\n  Iterative Refinement for Efficient End-to-End Self-Driving","summary":"  Current end-to-end autonomous driving methods resort to unifying modular\ndesigns for various tasks (e.g. perception, prediction and planning). Although\noptimized in a planning-oriented spirit with a fully differentiable framework,\nexisting end-to-end driving systems without ego-centric designs still suffer\nfrom unsatisfactory performance and inferior efficiency, owing to the\nrasterized scene representation learning and redundant information\ntransmission. In this paper, we revisit the human driving behavior and propose\nan ego-centric fully sparse paradigm, named DiFSD, for end-to-end self-driving.\nSpecifically, DiFSD mainly consists of sparse perception, hierarchical\ninteraction and iterative motion planner. The sparse perception module performs\ndetection, tracking and online mapping based on sparse representation of the\ndriving scene. The hierarchical interaction module aims to select the Closest\nIn-Path Vehicle / Stationary (CIPV / CIPS) from coarse to fine, benefiting from\nan additional geometric prior. As for the iterative motion planner, both\nselected interactive agents and ego-vehicle are considered for joint motion\nprediction, where the output multi-modal ego-trajectories are optimized in an\niterative fashion. Besides, both position-level motion diffusion and\ntrajectory-level planning denoising are introduced for uncertainty modeling,\nthus facilitating the training stability and convergence of the whole\nframework. Extensive experiments conducted on nuScenes and Bench2Drive datasets\ndemonstrate the superior planning performance and great efficiency of DiFSD.\n","authors":["Haisheng Su","Wei Wu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2409.09777v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17886v2","updated":"2024-12-17T04:59:06Z","published":"2024-11-26T21:16:36Z","title":"The Context of Crash Occurrence: A Complexity-Infused Approach\n  Integrating Semantic, Contextual, and Kinematic Features","summary":"  Understanding the context of crash occurrence in complex driving environments\nis essential for improving traffic safety and advancing automated driving.\nPrevious studies have used statistical models and deep learning to predict\ncrashes based on semantic, contextual, or vehicle kinematic features, but none\nhave examined the combined influence of these factors. In this study, we term\nthe integration of these features ``roadway complexity''. This paper introduces\na two-stage framework that integrates roadway complexity features for crash\nprediction. In the first stage, an encoder extracts hidden contextual\ninformation from these features, generating complexity-infused features. The\nsecond stage uses both original and complexity-infused features to predict\ncrash likelihood, achieving an accuracy of 87.98\\% with original features alone\nand 90.15\\% with the added complexity-infused features. Ablation studies\nconfirm that a combination of semantic, kinematic, and contextual features\nyields the best results, which emphasize their role in capturing roadway\ncomplexity. Additionally, complexity index annotations generated by the Large\nLanguage Model outperform those by Amazon Mechanical Turk, highlighting the\npotential of AI-based tools for accurate, scalable crash prediction systems.\n","authors":["Meng Wang","Zach Noonan","Pnina Gershon","Bruce Mehler","Bryan Reimer","Shannon C. Roberts"],"pdf_url":"https://arxiv.org/pdf/2411.17886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10538v4","updated":"2024-12-17T04:58:18Z","published":"2024-08-20T04:32:50Z","title":"Surgical Workflow Recognition and Blocking Effectiveness Detection in\n  Laparoscopic Liver Resections with Pringle Maneuver","summary":"  Pringle maneuver (PM) in laparoscopic liver resection aims to reduce blood\nloss and provide a clear surgical view by intermittently blocking blood inflow\nof the liver, whereas prolonged PM may cause ischemic injury. To\ncomprehensively monitor this surgical procedure and provide timely warnings of\nineffective and prolonged blocking, we suggest two complementary AI-assisted\nsurgical monitoring tasks: workflow recognition and blocking effectiveness\ndetection in liver resections. The former presents challenges in real-time\ncapturing of short-term PM, while the latter involves the intraoperative\ndiscrimination of long-term liver ischemia states. To address these challenges,\nwe meticulously collect a novel dataset, called PmLR50, consisting of 25,037\nvideo frames covering various surgical phases from 50 laparoscopic liver\nresection procedures. Additionally, we develop an online baseline for PmLR50,\ntermed PmNet. This model embraces Masked Temporal Encoding (MTE) and Compressed\nSequence Modeling (CSM) for efficient short-term and long-term temporal\ninformation modeling, and embeds Contrastive Prototype Separation (CPS) to\nenhance action discrimination between similar intraoperative operations.\nExperimental results demonstrate that PmNet outperforms existing\nstate-of-the-art surgical workflow recognition methods on the PmLR50 benchmark.\nOur research offers potential clinical applications for the laparoscopic liver\nsurgery community. Codes are available at https://github.com/RascalGdd/PmNet.\n","authors":["Diandian Guo","Weixin Si","Zhixi Li","Jialun Pei","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2408.10538v4.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2404.14239v2","updated":"2024-12-17T04:47:44Z","published":"2024-04-22T14:47:54Z","title":"MultiBooth: Towards Generating All Your Concepts in an Image from Text","summary":"  This paper introduces MultiBooth, a novel and efficient technique for\nmulti-concept customization in image generation from text. Despite the\nsignificant advancements in customized generation methods, particularly with\nthe success of diffusion models, existing methods often struggle with\nmulti-concept scenarios due to low concept fidelity and high inference cost.\nMultiBooth addresses these issues by dividing the multi-concept generation\nprocess into two phases: a single-concept learning phase and a multi-concept\nintegration phase. During the single-concept learning phase, we employ a\nmulti-modal image encoder and an efficient concept encoding technique to learn\na concise and discriminative representation for each concept. In the\nmulti-concept integration phase, we use bounding boxes to define the generation\narea for each concept within the cross-attention map. This method enables the\ncreation of individual concepts within their specified regions, thereby\nfacilitating the formation of multi-concept images. This strategy not only\nimproves concept fidelity but also reduces additional inference cost.\nMultiBooth surpasses various baselines in both qualitative and quantitative\nevaluations, showcasing its superior performance and computational efficiency.\nProject Page: https://multibooth.github.io/\n","authors":["Chenyang Zhu","Kai Li","Yue Ma","Chunming He","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2404.14239v2.pdf","comment":"To be published in AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12532v1","updated":"2024-12-17T04:42:50Z","published":"2024-12-17T04:42:50Z","title":"Addressing Small and Imbalanced Medical Image Datasets Using Generative\n  Models: A Comparative Study of DDPM and PGGANs with Random and Greedy K\n  Sampling","summary":"  The development of accurate medical image classification models is often\nconstrained by privacy concerns and data scarcity for certain conditions,\nleading to small and imbalanced datasets. To address these limitations, this\nstudy explores the use of generative models, such as Denoising Diffusion\nProbabilistic Models (DDPM) and Progressive Growing Generative Adversarial\nNetworks (PGGANs), for dataset augmentation. The research introduces a\nframework to assess the impact of synthetic images generated by DDPM and PGGANs\non the performance of four models: a custom CNN, Untrained VGG16, Pretrained\nVGG16, and Pretrained ResNet50. Experiments were conducted using Random\nSampling and Greedy K Sampling to create small, imbalanced datasets. The\nsynthetic images were evaluated using Frechet Inception Distance (FID) and\ncompared to original datasets through classification metrics. The results show\nthat DDPM consistently generated more realistic images with lower FID scores\nand significantly outperformed PGGANs in improving classification metrics\nacross all models and datasets. Incorporating DDPM-generated images into the\noriginal datasets increased accuracy by up to 6%, enhancing model robustness\nand stability, particularly in imbalanced scenarios. Random Sampling\ndemonstrated superior stability, while Greedy K Sampling offered diversity at\nthe cost of higher FID scores. This study highlights the efficacy of DDPM in\naugmenting small, imbalanced medical image datasets, improving model\nperformance by balancing the dataset and expanding its size.\n","authors":["Iman Khazrak","Shakhnoza Takhirova","Mostafa M. Rezaee","Mehrdad Yadollahi","Robert C. Green II","Shuteng Niu"],"pdf_url":"https://arxiv.org/pdf/2412.12532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11892v2","updated":"2024-12-17T04:38:28Z","published":"2024-12-16T15:41:14Z","title":"From 2D CAD Drawings to 3D Parametric Models: A Vision-Language Approach","summary":"  In this paper, we present CAD2Program, a new method for reconstructing 3D\nparametric models from 2D CAD drawings. Our proposed method is inspired by\nrecent successes in vision-language models (VLMs), and departs from traditional\nmethods which rely on task-specific data representations and/or algorithms.\nSpecifically, on the input side, we simply treat the 2D CAD drawing as a raster\nimage, regardless of its original format, and encode the image with a standard\nViT model. We show that such an encoding scheme achieves competitive\nperformance against existing methods that operate on vector-graphics inputs,\nwhile imposing substantially fewer restrictions on the 2D drawings. On the\noutput side, our method auto-regressively predicts a general-purpose language\ndescribing 3D parametric models in text form. Compared to other sequence\nmodeling methods for CAD which use domain-specific sequence representations\nwith fixed-size slots, our text-based representation is more flexible, and can\nbe easily extended to arbitrary geometric entities and semantic or functional\nproperties. Experimental results on a large-scale dataset of cabinet models\ndemonstrate the effectiveness of our method.\n","authors":["Xilin Wang","Jia Zheng","Yuanchao Hu","Hao Zhu","Qian Yu","Zihan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.11892v2.pdf","comment":"To Appear in AAAI 2025. The project page is at\n  https://manycore-research.github.io/CAD2Program"},{"id":"http://arxiv.org/abs/2412.12525v1","updated":"2024-12-17T04:33:31Z","published":"2024-12-17T04:33:31Z","title":"CREST: An Efficient Conjointly-trained Spike-driven Framework for\n  Event-based Object Detection Exploiting Spatiotemporal Dynamics","summary":"  Event-based cameras feature high temporal resolution, wide dynamic range, and\nlow power consumption, which is ideal for high-speed and low-light object\ndetection. Spiking neural networks (SNNs) are promising for event-based object\nrecognition and detection due to their spiking nature but lack efficient\ntraining methods, leading to gradient vanishing and high computational\ncomplexity, especially in deep SNNs. Additionally, existing SNN frameworks\noften fail to effectively handle multi-scale spatiotemporal features, leading\nto increased data redundancy and reduced accuracy. To address these issues, we\npropose CREST, a novel conjointly-trained spike-driven framework to exploit\nspatiotemporal dynamics in event-based object detection. We introduce the\nconjoint learning rule to accelerate SNN learning and alleviate gradient\nvanishing. It also supports dual operation modes for efficient and flexible\nimplementation on different hardware types. Additionally, CREST features a\nfully spike-driven framework with a multi-scale spatiotemporal event integrator\n(MESTOR) and a spatiotemporal-IoU (ST-IoU) loss. Our approach achieves superior\nobject recognition & detection performance and up to 100X energy efficiency\ncompared with state-of-the-art SNN algorithms on three datasets, providing an\nefficient solution for event-based object detection algorithms suitable for SNN\nhardware implementation.\n","authors":["Ruixin Mao","Aoyu Shen","Lin Tang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12525v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2401.11791v3","updated":"2024-12-17T04:27:31Z","published":"2024-01-22T09:41:05Z","title":"Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation","summary":"  Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation\nmodels using image data with only image-level supervision. Since precise\npixel-level annotations are not accessible, existing methods typically focus on\nproducing pseudo masks for training segmentation models by refining CAM-like\nheatmaps. However, the produced heatmaps may capture only the discriminative\nimage regions of object categories or the associated co-occurring backgrounds.\nTo address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS)\nframework, which learns to effectively prompt the CLIP latent space to enhance\nthe semantic alignment between the segmented regions and the target object\ncategories. More specifically, we propose Contrastive Prompt Learning and\nPrompt-guided Semantic Refinement to learn the prompts that adequately describe\nand suppress the co-occurring backgrounds associated with each object category.\nIn this way, SemPLeS can perform better semantic alignment between object\nregions and class labels, resulting in desired pseudo masks for training\nsegmentation models. The proposed SemPLeS framework achieves competitive\nperformance on standard WSSS benchmarks, PASCAL VOC 2012 and MS COCO2014, and\nshows compatibility with other WSSS methods.\n","authors":["Ci-Siang Lin","Chien-Yi Wang","Yu-Chiang Frank Wang","Min-Hung Chen"],"pdf_url":"https://arxiv.org/pdf/2401.11791v3.pdf","comment":"WACV 2025. Project page: https://projectdisr.github.io/semples"},{"id":"http://arxiv.org/abs/2412.11620v2","updated":"2024-12-17T04:26:08Z","published":"2024-12-16T10:07:15Z","title":"Combating Semantic Contamination in Learning with Label Noise","summary":"  Noisy labels can negatively impact the performance of deep neural networks.\nOne common solution is label refurbishment, which involves reconstructing noisy\nlabels through predictions and distributions. However, these methods may\nintroduce problematic semantic associations, a phenomenon that we identify as\nSemantic Contamination. Through an analysis of Robust LR, a representative\nlabel refurbishment method, we found that utilizing the logits of views for\nrefurbishment does not adequately balance the semantic information of\nindividual classes. Conversely, using the logits of models fails to maintain\nconsistent semantic relationships across models, which explains why label\nrefurbishment methods frequently encounter issues related to Semantic\nContamination. To address this issue, we propose a novel method called\nCollaborative Cross Learning, which utilizes semi-supervised learning on\nrefurbished labels to extract appropriate semantic associations from embeddings\nacross views and models. Experimental results show that our method outperforms\nexisting approaches on both synthetic and real-world noisy datasets,\neffectively mitigating the impact of label noise and Semantic Contamination.\n","authors":["Wenxiao Fan","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2412.11620v2.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2310.19180v4","updated":"2024-12-17T04:08:33Z","published":"2023-10-29T22:51:49Z","title":"JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music\n  Generation","summary":"  With rapid advances in generative artificial intelligence, the text-to-music\nsynthesis task has emerged as a promising direction for music generation.\nNevertheless, achieving precise control over multi-track generation remains an\nopen challenge. While existing models excel in directly generating multi-track\nmix, their limitations become evident when it comes to composing individual\ntracks and integrating them in a controllable manner. This departure from the\ntypical workflows of professional composers hinders the ability to refine\ndetails in specific tracks. To address this gap, we propose JEN-1 Composer, a\nunified framework designed to efficiently model marginal, conditional, and\njoint distributions over multi-track music using a single model. Building upon\nan audio latent diffusion model, JEN-1 Composer extends the versatility of\nmulti-track music generation. We introduce a progressive curriculum training\nstrategy, which gradually escalates the difficulty of training tasks while\nensuring the model's generalization ability and facilitating smooth transitions\nbetween different scenarios. During inference, users can iteratively generate\nand select music tracks, thus incrementally composing entire musical pieces in\naccordance with the Human-AI co-composition workflow. Our approach demonstrates\nstate-of-the-art performance in controllable and high-fidelity multi-track\nmusic synthesis, marking a significant advancement in interactive AI-assisted\nmusic creation. Our demo pages are available at www.jenmusic.ai/research.\n","authors":["Yao Yao","Peike Li","Boyu Chen","Alex Wang"],"pdf_url":"https://arxiv.org/pdf/2310.19180v4.pdf","comment":"9 pages, 3 figures, 3 tables, accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.10443v2","updated":"2024-12-17T03:55:34Z","published":"2024-12-11T13:48:06Z","title":"SweetTokenizer: Semantic-Aware Spatial-Temporal Tokenizer for Compact\n  Visual Discretization","summary":"  This paper presents the \\textbf{S}emantic-a\\textbf{W}ar\\textbf{E}\nspatial-t\\textbf{E}mporal \\textbf{T}okenizer (SweetTokenizer), a compact yet\neffective discretization approach for vision data. Our goal is to boost\ntokenizers' compression ratio while maintaining reconstruction fidelity in the\nVQ-VAE paradigm. Firstly, to obtain compact latent representations, we decouple\nimages or videos into spatial-temporal dimensions, translating visual\ninformation into learnable querying spatial and temporal tokens through a\n\\textbf{C}ross-attention \\textbf{Q}uery \\textbf{A}uto\\textbf{E}ncoder (CQAE).\nSecondly, to complement visual information during compression, we quantize\nthese tokens via a specialized codebook derived from off-the-shelf LLM\nembeddings to leverage the rich semantics from language modality. Finally, to\nenhance training stability and convergence, we also introduce a curriculum\nlearning strategy, which proves critical for effective discrete visual\nrepresentation learning. SweetTokenizer achieves comparable video\nreconstruction fidelity with only \\textbf{25\\%} of the tokens used in previous\nstate-of-the-art video tokenizers, and boost video generation results by\n\\textbf{32.9\\%} w.r.t gFVD. When using the same token number, we significantly\nimproves video and image reconstruction results by \\textbf{57.1\\%} w.r.t rFVD\non UCF-101 and \\textbf{37.2\\%} w.r.t rFID on ImageNet-1K. Additionally, the\ncompressed tokens are imbued with semantic information, enabling few-shot\nrecognition capabilities powered by LLMs in downstream applications.\n","authors":["Zhentao Tan","Ben Xue","Jian Jia","Junhao Wang","Wencai Ye","Shaoyun Shi","Mingjie Sun","Wenjin Wu","Quan Chen","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.10443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12511v1","updated":"2024-12-17T03:50:13Z","published":"2024-12-17T03:50:13Z","title":"Invisible Watermarks: Attacks and Robustness","summary":"  As Generative AI continues to become more accessible, the case for robust\ndetection of generated images in order to combat misinformation is stronger\nthan ever. Invisible watermarking methods act as identifiers of generated\ncontent, embedding image- and latent-space messages that are robust to many\nforms of perturbations. The majority of current research investigates\nfull-image attacks against images with a single watermarking method applied. We\nintroduce novel improvements to watermarking robustness as well as minimizing\ndegradation on image quality during attack. Firstly, we examine the application\nof both image-space and latent-space watermarking methods on a single image,\nwhere we propose a custom watermark remover network which preserves one of the\nwatermarking modalities while completely removing the other during decoding.\nThen, we investigate localized blurring attacks (LBA) on watermarked images\nbased on the GradCAM heatmap acquired from the watermark decoder in order to\nreduce the amount of degradation to the target image. Our evaluation suggests\nthat 1) implementing the watermark remover model to preserve one of the\nwatermark modalities when decoding the other modality slightly improves on the\nbaseline performance, and that 2) LBA degrades the image significantly less\ncompared to uniform blurring of the entire image. Code is available at:\nhttps://github.com/tomputer-g/IDL_WAR\n","authors":["Dongjun Hwang","Sungwon Woo","Tom Gao","Raymond Luo","Sunghwan Baek"],"pdf_url":"https://arxiv.org/pdf/2412.12511v1.pdf","comment":"YouTube link for the presentation:\n  https://www.youtube.com/watch?v=0vwFG1HSrUE"},{"id":"http://arxiv.org/abs/2412.11409v2","updated":"2024-12-17T03:50:05Z","published":"2024-12-16T03:25:23Z","title":"Multi-modal and Multi-scale Spatial Environment Understanding for\n  Immersive Visual Text-to-Speech","summary":"  Visual Text-to-Speech (VTTS) aims to take the environmental image as the\nprompt to synthesize the reverberant speech for the spoken content. The\nchallenge of this task lies in understanding the spatial environment from the\nimage. Many attempts have been made to extract global spatial visual\ninformation from the RGB space of an spatial image. However, local and depth\nimage information are crucial for understanding the spatial environment, which\nprevious works have ignored. To address the issues, we propose a novel\nmulti-modal and multi-scale spatial environment understanding scheme to achieve\nimmersive VTTS, termed M2SE-VTTS. The multi-modal aims to take both the RGB and\nDepth spaces of the spatial image to learn more comprehensive spatial\ninformation, and the multi-scale seeks to model the local and global spatial\nknowledge simultaneously. Specifically, we first split the RGB and Depth images\ninto patches and adopt the Gemini-generated environment captions to guide the\nlocal spatial understanding. After that, the multi-modal and multi-scale\nfeatures are integrated by the local-aware global spatial understanding. In\nthis way, M2SE-VTTS effectively models the interactions between local and\nglobal spatial contexts in the multi-modal spatial environment. Objective and\nsubjective evaluations suggest that our model outperforms the advanced\nbaselines in environmental speech generation. The code and audio samples are\navailable at: https://github.com/AI-S2-Lab/M2SE-VTTS.\n","authors":["Rui Liu","Shuwei He","Yifan Hu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2412.11409v2.pdf","comment":"9 pages,2 figures, Accepted by AAAI'2025"},{"id":"http://arxiv.org/abs/2412.12507v1","updated":"2024-12-17T03:21:25Z","published":"2024-12-17T03:21:25Z","title":"3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian\n  Splatting","summary":"  3D Gaussian Splatting (3DGS) has shown great potential for efficient\nreconstruction and high-fidelity real-time rendering of complex scenes on\nconsumer hardware. However, due to its rasterization-based formulation, 3DGS is\nconstrained to ideal pinhole cameras and lacks support for secondary lighting\neffects. Recent methods address these limitations by tracing volumetric\nparticles instead, however, this comes at the cost of significantly slower\nrendering speeds. In this work, we propose 3D Gaussian Unscented Transform\n(3DGUT), replacing the EWA splatting formulation in 3DGS with the Unscented\nTransform that approximates the particles through sigma points, which can be\nprojected exactly under any nonlinear projection function. This modification\nenables trivial support of distorted cameras with time dependent effects such\nas rolling shutter, while retaining the efficiency of rasterization.\nAdditionally, we align our rendering formulation with that of tracing-based\nmethods, enabling secondary ray tracing required to represent phenomena such as\nreflections and refraction within the same 3D representation.\n","authors":["Qi Wu","Janick Martinez Esturo","Ashkan Mirzaei","Nicolas Moenne-Loccoz","Zan Gojcic"],"pdf_url":"https://arxiv.org/pdf/2412.12507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12503v1","updated":"2024-12-17T03:10:04Z","published":"2024-12-17T03:10:04Z","title":"Multi-Scale Cross-Fusion and Edge-Supervision Network for Image Splicing\n  Localization","summary":"  Image Splicing Localization (ISL) is a fundamental yet challenging task in\ndigital forensics. Although current approaches have achieved promising\nperformance, the edge information is insufficiently exploited, resulting in\npoor integrality and high false alarms. To tackle this problem, we propose a\nmulti-scale cross-fusion and edge-supervision network for ISL. Specifically,\nour framework consists of three key steps: multi-scale features cross-fusion,\nedge mask prediction and edge-supervision localization. Firstly, we input the\nRGB image and its noise image into a segmentation network to learn multi-scale\nfeatures, which are then aggregated via a cross-scale fusion followed by a\ncross-domain fusion to enhance feature representation. Secondly, we design an\nedge mask prediction module to effectively mine the reliable boundary\nartifacts. Finally, the cross-fused features and the reliable edge mask\ninformation are seamlessly integrated via an attention mechanism to\nincrementally supervise and facilitate model training. Extensive experiments on\npublicly available datasets demonstrate that our proposed method is superior to\nstate-of-the-art schemes.\n","authors":["Yakun Niu","Pei Chen","Lei Zhang","Hongjian Yin","Qi Chang"],"pdf_url":"https://arxiv.org/pdf/2412.12503v1.pdf","comment":"5 pages,3 figures"},{"id":"http://arxiv.org/abs/2412.12502v1","updated":"2024-12-17T03:06:12Z","published":"2024-12-17T03:06:12Z","title":"Track the Answer: Extending TextVQA from Image to Video with\n  Spatio-Temporal Clues","summary":"  Video text-based visual question answering (Video TextVQA) is a practical\ntask that aims to answer questions by jointly reasoning textual and visual\ninformation in a given video. Inspired by the development of TextVQA in image\ndomain, existing Video TextVQA approaches leverage a language model (e.g. T5)\nto process text-rich multiple frames and generate answers auto-regressively.\nNevertheless, the spatio-temporal relationships among visual entities\n(including scene text and objects) will be disrupted and models are susceptible\nto interference from unrelated information, resulting in irrational reasoning\nand inaccurate answering. To tackle these challenges, we propose the TEA\n(stands for ``\\textbf{T}rack th\\textbf{E} \\textbf{A}nswer'') method that better\nextends the generative TextVQA framework from image to video. TEA recovers the\nspatio-temporal relationships in a complementary way and incorporates OCR-aware\nclues to enhance the quality of reasoning questions. Extensive experiments on\nseveral public Video TextVQA datasets validate the effectiveness and\ngeneralization of our framework. TEA outperforms existing TextVQA methods,\nvideo-language pretraining methods and video large language models by great\nmargins.\n","authors":["Yan Zhang","Gangyan Zeng","Huawen Shen","Daiqing Wu","Yu Zhou","Can Ma"],"pdf_url":"https://arxiv.org/pdf/2412.12502v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12501v1","updated":"2024-12-17T03:05:27Z","published":"2024-12-17T03:05:27Z","title":"Unleashing the Potential of Model Bias for Generalized Category\n  Discovery","summary":"  Generalized Category Discovery is a significant and complex task that aims to\nidentify both known and undefined novel categories from a set of unlabeled\ndata, leveraging another labeled dataset containing only known categories. The\nprimary challenges stem from model bias induced by pre-training on only known\ncategories and the lack of precise supervision for novel ones, leading to\ncategory bias towards known categories and category confusion among different\nnovel categories, which hinders models' ability to identify novel categories\neffectively. To address these challenges, we propose a novel framework named\nSelf-Debiasing Calibration (SDC). Unlike prior methods that regard model bias\ntowards known categories as an obstacle to novel category identification, SDC\nprovides a novel insight into unleashing the potential of the bias to\nfacilitate novel category learning. Specifically, the output of the biased\nmodel serves two key purposes. First, it provides an accurate modeling of\ncategory bias, which can be utilized to measure the degree of bias and debias\nthe output of the current training model. Second, it offers valuable insights\nfor distinguishing different novel categories by transferring knowledge between\nsimilar categories. Based on these insights, SDC dynamically adjusts the output\nlogits of the current training model using the output of the biased model. This\napproach produces less biased logits to effectively address the issue of\ncategory bias towards known categories, and generates more accurate pseudo\nlabels for unlabeled data, thereby mitigating category confusion for novel\ncategories. Experiments on three benchmark datasets show that SDC outperforms\nSOTA methods, especially in the identification of novel categories. Our code\nand data are available at \\url{https://github.com/Lackel/SDC}.\n","authors":["Wenbin An","Haonan Lin","Jiahao Nie","Feng Tian","Wenkai Shi","Yaqiang Wu","Qianying Wang","Ping Chen"],"pdf_url":"https://arxiv.org/pdf/2412.12501v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2411.17214v2","updated":"2024-12-17T03:01:53Z","published":"2024-11-26T08:30:31Z","title":"MAT: Multi-Range Attention Transformer for Efficient Image\n  Super-Resolution","summary":"  Recent advances in image super-resolution (SR) have significantly benefited\nfrom the incorporation of Transformer architectures. However, conventional\ntechniques aimed at enlarging the self-attention window to capture broader\ncontexts come with inherent drawbacks, especially the significantly increased\ncomputational demands. Moreover, the feature perception within a fixed-size\nwindow of existing models restricts the effective receptive fields and the\nintermediate feature diversity. This study demonstrates that a flexible\nintegration of attention across diverse spatial extents can yield significant\nperformance enhancements. In line with this insight, we introduce Multi-Range\nAttention Transformer (MAT) tailored for SR tasks. MAT leverages the\ncomputational advantages inherent in dilation operation, in conjunction with\nself-attention mechanism, to facilitate both multi-range attention (MA) and\nsparse multi-range attention (SMA), enabling efficient capture of both regional\nand sparse global features. Further coupled with local feature extraction, MAT\nadeptly capture dependencies across various spatial ranges, improving the\ndiversity and efficacy of its feature representations. We also introduce the\nMSConvStar module, which augments the model's ability for multi-range\nrepresentation learning. Comprehensive experiments show that our MAT exhibits\nsuperior performance to existing state-of-the-art SR models with remarkable\nefficiency (~3.3 faster than SRFormer-light).\n","authors":["Chengxing Xie","Xiaoming Zhang","Linze Li","Yuqian Fu","Biao Gong","Tianrui Li","Kai Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.17214v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12496v1","updated":"2024-12-17T02:56:35Z","published":"2024-12-17T02:56:35Z","title":"Faster Vision Mamba is Rebuilt in Minutes via Merged Token Re-training","summary":"  Vision Mamba (e.g., Vim) has successfully been integrated into computer\nvision, and token reduction has yielded promising outcomes in Vision\nTransformers (ViTs). However, token reduction performs less effectively on\nVision Mamba compared to ViTs. Pruning informative tokens in Mamba leads to a\nhigh loss of key knowledge and bad performance. This makes it not a good\nsolution for enhancing efficiency in Mamba. Token merging, which preserves more\ntoken information than pruning, has demonstrated commendable performance in\nViTs. Nevertheless, vanilla merging performance decreases as the reduction\nratio increases either, failing to maintain the key knowledge in Mamba.\nRe-training the token-reduced model enhances the performance of Mamba, by\neffectively rebuilding the key knowledge. Empirically, pruned Vims only drop up\nto 0.9% accuracy on ImageNet-1K, recovered by our proposed framework R-MeeTo in\nour main evaluation. We show how simple and effective the fast recovery can be\nachieved at minute-level, in particular, a 35.9% accuracy spike over 3 epochs\nof training on Vim-Ti. Moreover, Vim-Ti/S/B are re-trained within 5/7/17\nminutes, and Vim-S only drop 1.3% with 1.2x (up to 1.5x) speed up in inference.\n","authors":["Mingjia Shi","Yuhao Zhou","Ruiji Yu","Zekai Li","Zhiyuan Liang","Xuanlei Zhao","Xiaojiang Peng","Tanmay Rajpurohit","Shanmukha Ramakrishna Vedantam","Wangbo Zhao","Kai Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2412.12496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12492v1","updated":"2024-12-17T02:47:00Z","published":"2024-12-17T02:47:00Z","title":"DuSSS: Dual Semantic Similarity-Supervised Vision-Language Model for\n  Semi-Supervised Medical Image Segmentation","summary":"  Semi-supervised medical image segmentation (SSMIS) uses consistency learning\nto regularize model training, which alleviates the burden of pixel-wise manual\nannotations. However, it often suffers from error supervision from low-quality\npseudo labels. Vision-Language Model (VLM) has great potential to enhance\npseudo labels by introducing text prompt guided multimodal supervision\ninformation. It nevertheless faces the cross-modal problem: the obtained\nmessages tend to correspond to multiple targets. To address aforementioned\nproblems, we propose a Dual Semantic Similarity-Supervised VLM (DuSSS) for\nSSMIS. Specifically, 1) a Dual Contrastive Learning (DCL) is designed to\nimprove cross-modal semantic consistency by capturing intrinsic representations\nwithin each modality and semantic correlations across modalities. 2) To\nencourage the learning of multiple semantic correspondences, a Semantic\nSimilarity-Supervision strategy (SSS) is proposed and injected into each\ncontrastive learning process in DCL, supervising semantic similarity via the\ndistribution-based uncertainty levels. Furthermore, a novel VLM-based SSMIS\nnetwork is designed to compensate for the quality deficiencies of\npseudo-labels. It utilizes the pretrained VLM to generate text prompt guided\nsupervision information, refining the pseudo label for better consistency\nregularization. Experimental results demonstrate that our DuSSS achieves\noutstanding performance with Dice of 82.52%, 74.61% and 78.03% on three public\ndatasets (QaTa-COV19, BM-Seg and MoNuSeg).\n","authors":["Qingtao Pan","Wenhao Qiao","Jingjiao Lou","Bing Ji","Shuo Li"],"pdf_url":"https://arxiv.org/pdf/2412.12492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10817v2","updated":"2024-12-17T02:35:10Z","published":"2024-12-14T12:58:15Z","title":"Enhance Vision-Language Alignment with Noise","summary":"  With the advancement of pre-trained vision-language (VL) models, enhancing\nthe alignment between visual and linguistic modalities in downstream tasks has\nemerged as a critical challenge. Different from existing fine-tuning methods\nthat add extra modules to these two modalities, we investigate whether the\nfrozen model can be fine-tuned by customized noise. Our approach is motivated\nby the scientific study of beneficial noise, namely Positive-incentive Noise\n(Pi-noise or $\\pi$-noise) , which quantitatively analyzes the impact of noise.\nIt therefore implies a new scheme to learn beneficial noise distribution that\ncan be employed to fine-tune VL models. Focusing on few-shot classification\ntasks based on CLIP, we reformulate the inference process of CLIP and apply\nvariational inference, demonstrating how to generate $\\pi$-noise towards visual\nand linguistic modalities. Then, we propose Positive-incentive Noise Injector\n(PiNI), which can fine-tune CLIP via injecting noise into both visual and text\nencoders. Since the proposed method can learn the distribution of beneficial\nnoise, we can obtain more diverse embeddings of vision and language to better\nalign these two modalities for specific downstream tasks within limited\ncomputational resources. We evaluate different noise incorporation approaches\nand network architectures of PiNI. The evaluation across 11 datasets\ndemonstrates its effectiveness.\n","authors":["Sida Huang","Hongyuan Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2412.10817v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2404.07846v3","updated":"2024-12-17T02:26:54Z","published":"2024-04-11T15:39:10Z","title":"Rethinking Transformer-Based Blind-Spot Network for Self-Supervised\n  Image Denoising","summary":"  Blind-spot networks (BSN) have been prevalent neural architectures in\nself-supervised image denoising (SSID). However, most existing BSNs are\nconducted with convolution layers. Although transformers have shown the\npotential to overcome the limitations of convolutions in many image restoration\ntasks, the attention mechanisms may violate the blind-spot requirement, thereby\nrestricting their applicability in BSN. To this end, we propose to analyze and\nredesign the channel and spatial attentions to meet the blind-spot requirement.\nSpecifically, channel self-attention may leak the blind-spot information in\nmulti-scale architectures, since the downsampling shuffles the spatial feature\ninto channel dimensions. To alleviate this problem, we divide the channel into\nseveral groups and perform channel attention separately. For spatial\nselfattention, we apply an elaborate mask to the attention matrix to restrict\nand mimic the receptive field of dilated convolution. Based on the redesigned\nchannel and window attentions, we build a Transformer-based Blind-Spot Network\n(TBSN), which shows strong local fitting and global perspective abilities.\nFurthermore, we introduce a knowledge distillation strategy that distills TBSN\ninto smaller denoisers to improve computational efficiency while maintaining\nperformance. Extensive experiments on real-world image denoising datasets show\nthat TBSN largely extends the receptive field and exhibits favorable\nperformance against state-of-theart SSID methods.\n","authors":["Junyi Li","Zhilu Zhang","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2404.07846v3.pdf","comment":"AAAI 2025 Camera Ready"},{"id":"http://arxiv.org/abs/2412.11067v2","updated":"2024-12-17T02:23:03Z","published":"2024-12-15T05:57:36Z","title":"CFSynthesis: Controllable and Free-view 3D Human Video Synthesis","summary":"  Human video synthesis aims to create lifelike characters in various\nenvironments, with wide applications in VR, storytelling, and content creation.\nWhile 2D diffusion-based methods have made significant progress, they struggle\nto generalize to complex 3D poses and varying scene backgrounds. To address\nthese limitations, we introduce CFSynthesis, a novel framework for generating\nhigh-quality human videos with customizable attributes, including identity,\nmotion, and scene configurations. Our method leverages a texture-SMPL-based\nrepresentation to ensure consistent and stable character appearances across\nfree viewpoints. Additionally, we introduce a novel foreground-background\nseparation strategy that effectively decomposes the scene as foreground and\nbackground, enabling seamless integration of user-defined backgrounds.\nExperimental results on multiple datasets show that CFSynthesis not only\nachieves state-of-the-art performance in complex human animations but also\nadapts effectively to 3D motions in free-view and user-specified scenarios.\n","authors":["Liyuan Cui","Xiaogang Xu","Wenqi Dong","Zesong Yang","Hujun Bao","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2412.11067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13213v3","updated":"2024-12-17T01:59:42Z","published":"2024-01-24T03:56:07Z","title":"Common-Sense Bias Discovery and Mitigation for Classification Tasks","summary":"  Machine learning model bias can arise from dataset composition: correlated\nsensitive features can disturb the downstream classification model's decision\nboundary and lead to performance differences along these features. Existing\nde-biasing works tackle most prominent bias features, like colors of digits or\nbackground of animals. However, a real-world dataset often includes a large\nnumber of feature correlations, that manifest intrinsically in the data as\ncommon sense information. Such spurious visual cues can further reduce model\nrobustness. Thus, practitioners desire the whole picture of correlations and\nflexibility to treat concerned bias for specific domain tasks. With this goal,\nwe propose a novel framework to extract comprehensive bias information in image\ndatasets based on textual descriptions, a common sense-rich modality.\nSpecifically, features are constructed by clustering noun phrase embeddings of\nsimilar semantics. Each feature's appearance across a dataset is inferred and\ntheir co-occurrence statistics are measured, with spurious correlations\noptionally examined by a human-in-the-loop interface. Downstream experiments\nshow that our method discovers novel model biases on multiple image benchmark\ndatasets. Furthermore, the discovered bias can be mitigated by a simple data\nre-weighting strategy that de-correlates the features, and outperforms\nstate-of-the-art unsupervised bias mitigation methods.\n","authors":["Miao Zhang","Zee fryer","Ben Colman","Ali Shahriyari","Gaurav Bharaj"],"pdf_url":"https://arxiv.org/pdf/2401.13213v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12463v1","updated":"2024-12-17T01:52:12Z","published":"2024-12-17T01:52:12Z","title":"Pattern Analogies: Learning to Perform Programmatic Image Edits by\n  Analogy","summary":"  Pattern images are everywhere in the digital and physical worlds, and tools\nto edit them are valuable. But editing pattern images is tricky: desired edits\nare often programmatic: structure-aware edits that alter the underlying program\nwhich generates the pattern. One could attempt to infer this underlying\nprogram, but current methods for doing so struggle with complex images and\nproduce unorganized programs that make editing tedious. In this work, we\nintroduce a novel approach to perform programmatic edits on pattern images. By\nusing a pattern analogy -- a pair of simple patterns to demonstrate the\nintended edit -- and a learning-based generative model to execute these edits,\nour method allows users to intuitively edit patterns. To enable this paradigm,\nwe introduce SplitWeave, a domain-specific language that, combined with a\nframework for sampling synthetic pattern analogies, enables the creation of a\nlarge, high-quality synthetic training dataset. We also present TriFuser, a\nLatent Diffusion Model (LDM) designed to overcome critical issues that arise\nwhen naively deploying LDMs to this task. Extensive experiments on real-world,\nartist-sourced patterns reveals that our method faithfully performs the\ndemonstrated edit while also generalizing to related pattern styles beyond its\ntraining distribution.\n","authors":["Aditya Ganeshan","Thibault Groueix","Paul Guerrero","Radomír Měch","Matthew Fisher","Daniel Ritchie"],"pdf_url":"https://arxiv.org/pdf/2412.12463v1.pdf","comment":"Website: https://bardofcodes.github.io/patterns/"},{"id":"http://arxiv.org/abs/2412.12460v1","updated":"2024-12-17T01:45:15Z","published":"2024-12-17T01:45:15Z","title":"PromptDet: A Lightweight 3D Object Detection Framework with LiDAR\n  Prompts","summary":"  Multi-camera 3D object detection aims to detect and localize objects in 3D\nspace using multiple cameras, which has attracted more attention due to its\ncost-effectiveness trade-off. However, these methods often struggle with the\nlack of accurate depth estimation caused by the natural weakness of the camera\nin ranging. Recently, multi-modal fusion and knowledge distillation methods for\n3D object detection have been proposed to solve this problem, which are\ntime-consuming during the training phase and not friendly to memory cost. In\nlight of this, we propose PromptDet, a lightweight yet effective 3D object\ndetection framework motivated by the success of prompt learning in 2D\nfoundation model. Our proposed framework, PromptDet, comprises two integral\ncomponents: a general camera-based detection module, exemplified by models like\nBEVDet and BEVDepth, and a LiDAR-assisted prompter. The LiDAR-assisted prompter\nleverages the LiDAR points as a complementary signal, enriched with a minimal\nset of additional trainable parameters. Notably, our framework is flexible due\nto our prompt-like design, which can not only be used as a lightweight\nmulti-modal fusion method but also as a camera-only method for 3D object\ndetection during the inference phase. Extensive experiments on nuScenes\nvalidate the effectiveness of the proposed PromptDet. As a multi-modal\ndetector, PromptDet improves the mAP and NDS by at most 22.8\\% and 21.1\\% with\nfewer than 2\\% extra parameters compared with the camera-only baseline. Without\nLiDAR points, PromptDet still achieves an improvement of at most 2.4\\% mAP and\n4.0\\% NDS with almost no impact on camera detection inference time.\n","authors":["Kun Guo","Qiang Ling"],"pdf_url":"https://arxiv.org/pdf/2412.12460v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.07375v2","updated":"2024-12-17T01:06:09Z","published":"2024-12-10T10:16:50Z","title":"StoryWeaver: A Unified World Model for Knowledge-Enhanced Story\n  Character Customization","summary":"  Story visualization has gained increasing attention in artificial\nintelligence. However, existing methods still struggle with maintaining a\nbalance between character identity preservation and text-semantics alignment,\nlargely due to a lack of detailed semantic modeling of the story scene. To\ntackle this challenge, we propose a novel knowledge graph, namely Character\nGraph (\\textbf{CG}), which comprehensively represents various story-related\nknowledge, including the characters, the attributes related to characters, and\nthe relationship between characters. We then introduce StoryWeaver, an image\ngenerator that achieve Customization via Character Graph (\\textbf{C-CG}),\ncapable of consistent story visualization with rich text semantics. To further\nimprove the multi-character generation performance, we incorporate\nknowledge-enhanced spatial guidance (\\textbf{KE-SG}) into StoryWeaver to\nprecisely inject character semantics into generation. To validate the\neffectiveness of our proposed method, extensive experiments are conducted using\na new benchmark called TBC-Bench. The experiments confirm that our StoryWeaver\nexcels not only in creating vivid visual story plots but also in accurately\nconveying character identities across various scenarios with considerable\nstorage efficiency, \\emph{e.g.}, achieving an average increase of +9.03\\%\nDINO-I and +13.44\\% CLIP-T. Furthermore, ablation experiments are conducted to\nverify the superiority of the proposed module. Codes and datasets are released\nat https://github.com/Aria-Zhangjl/StoryWeaver.\n","authors":["Jinlu Zhang","Jiji Tang","Rongsheng Zhang","Tangjie Lv","Xiaoshuai Sun"],"pdf_url":"https://arxiv.org/pdf/2412.07375v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12432v1","updated":"2024-12-17T00:49:12Z","published":"2024-12-17T00:49:12Z","title":"Three Things to Know about Deep Metric Learning","summary":"  This paper addresses supervised deep metric learning for open-set image\nretrieval, focusing on three key aspects: the loss function, mixup\nregularization, and model initialization. In deep metric learning, optimizing\nthe retrieval evaluation metric, recall@k, via gradient descent is desirable\nbut challenging due to its non-differentiable nature. To overcome this, we\npropose a differentiable surrogate loss that is computed on large batches,\nnearly equivalent to the entire training set. This computationally intensive\nprocess is made feasible through an implementation that bypasses the GPU memory\nlimitations. Additionally, we introduce an efficient mixup regularization\ntechnique that operates on pairwise scalar similarities, effectively increasing\nthe batch size even further. The training process is further enhanced by\ninitializing the vision encoder using foundational models, which are\npre-trained on large-scale datasets. Through a systematic study of these\ncomponents, we demonstrate that their synergy enables large models to nearly\nsolve popular benchmarks.\n","authors":["Yash Patel","Giorgos Tolias","Jiri Matas"],"pdf_url":"https://arxiv.org/pdf/2412.12432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13376v1","updated":"2024-12-17T23:23:25Z","published":"2024-12-17T23:23:25Z","title":"Targeted View-Invariant Adversarial Perturbations for 3D Object\n  Recognition","summary":"  Adversarial attacks pose significant challenges in 3D object recognition,\nespecially in scenarios involving multi-view analysis where objects can be\nobserved from varying angles. This paper introduces View-Invariant Adversarial\nPerturbations (VIAP), a novel method for crafting robust adversarial examples\nthat remain effective across multiple viewpoints. Unlike traditional methods,\nVIAP enables targeted attacks capable of manipulating recognition systems to\nclassify objects as specific, pre-determined labels, all while using a single\nuniversal perturbation. Leveraging a dataset of 1,210 images across 121 diverse\nrendered 3D objects, we demonstrate the effectiveness of VIAP in both targeted\nand untargeted settings. Our untargeted perturbations successfully generate a\nsingular adversarial noise robust to 3D transformations, while targeted attacks\nachieve exceptional results, with top-1 accuracies exceeding 95% across various\nepsilon values. These findings highlight VIAPs potential for real-world\napplications, such as testing the robustness of 3D recognition systems. The\nproposed method sets a new benchmark for view-invariant adversarial robustness,\nadvancing the field of adversarial machine learning for 3D object recognition.\n","authors":["Christian Green","Mehmet Ergezer","Abdurrahman Zeybey"],"pdf_url":"https://arxiv.org/pdf/2412.13376v1.pdf","comment":"Accepted to AAAI-25 Workshop on Artificial Intelligence for Cyber\n  Security (AICS): http://aics.site/AICS2025/index.html"}]},"2024-12-18T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.14173v1","updated":"2024-12-18T18:59:59Z","published":"2024-12-18T18:59:59Z","title":"AniDoc: Animation Creation Made Easier","summary":"  The production of 2D animation follows an industry-standard workflow,\nencompassing four essential stages: character design, keyframe animation,\nin-betweening, and coloring. Our research focuses on reducing the labor costs\nin the above process by harnessing the potential of increasingly powerful\ngenerative AI. Using video diffusion models as the foundation, AniDoc emerges\nas a video line art colorization tool, which automatically converts sketch\nsequences into colored animations following the reference character\nspecification. Our model exploits correspondence matching as an explicit\nguidance, yielding strong robustness to the variations (e.g., posture) between\nthe reference character and each line art frame. In addition, our model could\neven automate the in-betweening process, such that users can easily create a\ntemporally consistent animation by simply providing a character image as well\nas the start and end sketches. Our code is available at:\nhttps://yihao-meng.github.io/AniDoc_demo.\n","authors":["Yihao Meng","Hao Ouyang","Hanlin Wang","Qiuyu Wang","Wen Wang","Ka Leong Cheng","Zhiheng Liu","Yujun Shen","Huamin Qu"],"pdf_url":"https://arxiv.org/pdf/2412.14173v1.pdf","comment":"Project page and code: https://yihao-meng.github.io/AniDoc_demo"},{"id":"http://arxiv.org/abs/2412.14172v1","updated":"2024-12-18T18:59:56Z","published":"2024-12-18T18:59:56Z","title":"Learning from Massive Human Videos for Universal Humanoid Pose Control","summary":"  Scalable learning of humanoid robots is crucial for their deployment in\nreal-world applications. While traditional approaches primarily rely on\nreinforcement learning or teleoperation to achieve whole-body control, they are\noften limited by the diversity of simulated environments and the high costs of\ndemonstration collection. In contrast, human videos are ubiquitous and present\nan untapped source of semantic and motion information that could significantly\nenhance the generalization capabilities of humanoid robots. This paper\nintroduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot\nposes with corresponding text-based motion descriptions, designed to leverage\nthis abundant data. Humanoid-X is curated through a comprehensive pipeline:\ndata mining from the Internet, video caption generation, motion retargeting of\nhumans to humanoid robots, and policy learning for real-world deployment. With\nHumanoid-X, we further train a large humanoid model, UH-1, which takes text\ninstructions as input and outputs corresponding actions to control a humanoid\nrobot. Extensive simulated and real-world experiments validate that our\nscalable training approach leads to superior generalization in text-based\nhumanoid control, marking a significant step toward adaptable, real-world-ready\nhumanoid robots.\n","authors":["Jiageng Mao","Siheng Zhao","Siqi Song","Tianheng Shi","Junjie Ye","Mingtong Zhang","Haoran Geng","Jitendra Malik","Vitor Guizilini","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14171v1","updated":"2024-12-18T18:59:54Z","published":"2024-12-18T18:59:54Z","title":"Thinking in Space: How Multimodal Large Language Models See, Remember,\n  and Recall Spaces","summary":"  Humans possess the visual-spatial intelligence to remember spaces from\nsequential visual observations. However, can Multimodal Large Language Models\n(MLLMs) trained on million-scale video datasets also ``think in space'' from\nvideos? We present a novel video-based visual-spatial intelligence benchmark\n(VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit\ncompetitive - though subhuman - visual-spatial intelligence. We probe models to\nexpress how they think in space both linguistically and visually and find that\nwhile spatial reasoning capabilities remain the primary bottleneck for MLLMs to\nreach higher benchmark performance, local world models and spatial awareness do\nemerge within these models. Notably, prevailing linguistic reasoning techniques\n(e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve\nperformance, whereas explicitly generating cognitive maps during\nquestion-answering enhances MLLMs' spatial distance ability.\n","authors":["Jihan Yang","Shusheng Yang","Anjali W. Gupta","Rilyn Han","Li Fei-Fei","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2412.14171v1.pdf","comment":"Project page:\n  https://vision-x-nyu.github.io/thinking-in-space.github.io/"},{"id":"http://arxiv.org/abs/2412.14169v1","updated":"2024-12-18T18:59:53Z","published":"2024-12-18T18:59:53Z","title":"Autoregressive Video Generation without Vector Quantization","summary":"  This paper presents a novel approach that enables autoregressive video\ngeneration with high efficiency. We propose to reformulate the video generation\nproblem as a non-quantized autoregressive modeling of temporal frame-by-frame\nprediction and spatial set-by-set prediction. Unlike raster-scan prediction in\nprior autoregressive models or joint distribution modeling of fixed-length\ntokens in diffusion models, our approach maintains the causal property of\nGPT-style models for flexible in-context capabilities, while leveraging\nbidirectional modeling within individual frames for efficiency. With the\nproposed approach, we train a novel video autoregressive model without vector\nquantization, termed NOVA. Our results demonstrate that NOVA surpasses prior\nautoregressive video models in data efficiency, inference speed, visual\nfidelity, and video fluency, even with a much smaller model capacity, i.e.,\n0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models\nin text-to-image generation tasks, with a significantly lower training cost.\nAdditionally, NOVA generalizes well across extended video durations and enables\ndiverse zero-shot applications in one unified model. Code and models are\npublicly available at https://github.com/baaivision/NOVA.\n","authors":["Haoge Deng","Ting Pan","Haiwen Diao","Zhengxiong Luo","Yufeng Cui","Huchuan Lu","Shiguang Shan","Yonggang Qi","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14169v1.pdf","comment":"22 pages, 16 figures"},{"id":"http://arxiv.org/abs/2412.14170v1","updated":"2024-12-18T18:59:53Z","published":"2024-12-18T18:59:53Z","title":"E-CAR: Efficient Continuous Autoregressive Image Generation via\n  Multistage Modeling","summary":"  Recent advances in autoregressive (AR) models with continuous tokens for\nimage generation show promising results by eliminating the need for discrete\ntokenization. However, these models face efficiency challenges due to their\nsequential token generation nature and reliance on computationally intensive\ndiffusion-based sampling. We present ECAR (Efficient Continuous Auto-Regressive\nImage Generation via Multistage Modeling), an approach that addresses these\nlimitations through two intertwined innovations: (1) a stage-wise continuous\ntoken generation strategy that reduces computational complexity and provides\nprogressively refined token maps as hierarchical conditions, and (2) a\nmultistage flow-based distribution modeling method that transforms only\npartial-denoised distributions at each stage comparing to complete denoising in\nnormal diffusion models. Holistically, ECAR operates by generating tokens at\nincreasing resolutions while simultaneously denoising the image at each stage.\nThis design not only reduces token-to-image transformation cost by a factor of\nthe stage number but also enables parallel processing at the token level. Our\napproach not only enhances computational efficiency but also aligns naturally\nwith image generation principles by operating in continuous token space and\nfollowing a hierarchical generation process from coarse to fine details.\nExperimental results demonstrate that ECAR achieves comparable image quality to\nDiT Peebles & Xie [2023] while requiring 10$\\times$ FLOPs reduction and\n5$\\times$ speedup to generate a 256$\\times$256 image.\n","authors":["Zhihang Yuan","Yuzhang Shang","Hanling Zhang","Tongcheng Fang","Rui Xie","Bingxin Xu","Yan Yan","Shengen Yan","Guohao Dai","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14168v1","updated":"2024-12-18T18:59:50Z","published":"2024-12-18T18:59:50Z","title":"FashionComposer: Compositional Fashion Image Generation","summary":"  We present FashionComposer for compositional fashion image generation. Unlike\nprevious methods, FashionComposer is highly flexible. It takes multi-modal\ninput (i.e., text prompt, parametric human model, garment image, and face\nimage) and supports personalizing the appearance, pose, and figure of the human\nand assigning multiple garments in one pass. To achieve this, we first develop\na universal framework capable of handling diverse input modalities. We\nconstruct scaled training data to enhance the model's robust compositional\ncapabilities. To accommodate multiple reference images (garments and faces)\nseamlessly, we organize these references in a single image as an \"asset\nlibrary\" and employ a reference UNet to extract appearance features. To inject\nthe appearance features into the correct pixels in the generated result, we\npropose subject-binding attention. It binds the appearance features from\ndifferent \"assets\" with the corresponding text features. In this way, the model\ncould understand each asset according to their semantics, supporting arbitrary\nnumbers and types of reference images. As a comprehensive solution,\nFashionComposer also supports many other applications like human album\ngeneration, diverse virtual try-on tasks, etc.\n","authors":["Sihui Ji","Yiyang Wang","Xi Chen","Xiaogang Xu","Hao Luo","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.14168v1.pdf","comment":"https://sihuiji.github.io/FashionComposer-Page"},{"id":"http://arxiv.org/abs/2412.14167v1","updated":"2024-12-18T18:59:49Z","published":"2024-12-18T18:59:49Z","title":"VideoDPO: Omni-Preference Alignment for Video Diffusion Generation","summary":"  Recent progress in generative diffusion models has greatly advanced\ntext-to-video generation. While text-to-video models trained on large-scale,\ndiverse datasets can produce varied outputs, these generations often deviate\nfrom user preferences, highlighting the need for preference alignment on\npre-trained models. Although Direct Preference Optimization (DPO) has\ndemonstrated significant improvements in language and image generation, we\npioneer its adaptation to video diffusion models and propose a VideoDPO\npipeline by making several key adjustments. Unlike previous image alignment\nmethods that focus solely on either (i) visual quality or (ii) semantic\nalignment between text and videos, we comprehensively consider both dimensions\nand construct a preference score accordingly, which we term the OmniScore. We\ndesign a pipeline to automatically collect preference pair data based on the\nproposed OmniScore and discover that re-weighting these pairs based on the\nscore significantly impacts overall preference alignment. Our experiments\ndemonstrate substantial improvements in both visual quality and semantic\nalignment, ensuring that no preference aspect is neglected. Code and data will\nbe shared at https://videodpo.github.io/.\n","authors":["Runtao Liu","Haoyu Wu","Zheng Ziqiang","Chen Wei","Yingqing He","Renjie Pi","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14166v1","updated":"2024-12-18T18:59:38Z","published":"2024-12-18T18:59:38Z","title":"MegaSynth: Scaling Up 3D Scene Reconstruction with Synthesized Data","summary":"  We propose scaling up 3D scene reconstruction by training with synthesized\ndata. At the core of our work is MegaSynth, a procedurally generated 3D dataset\ncomprising 700K scenes - over 50 times larger than the prior real dataset DL3DV\n- dramatically scaling the training data. To enable scalable data generation,\nour key idea is eliminating semantic information, removing the need to model\ncomplex semantic priors such as object affordances and scene composition.\nInstead, we model scenes with basic spatial structures and geometry primitives,\noffering scalability. Besides, we control data complexity to facilitate\ntraining while loosely aligning it with real-world data distribution to benefit\nreal-world generalization. We explore training LRMs with both MegaSynth and\navailable real data. Experiment results show that joint training or\npre-training with MegaSynth improves reconstruction quality by 1.2 to 1.8 dB\nPSNR across diverse image domains. Moreover, models trained solely on MegaSynth\nperform comparably to those trained on real data, underscoring the low-level\nnature of 3D reconstruction. Additionally, we provide an in-depth analysis of\nMegaSynth's properties for enhancing model capability, training stability, and\ngeneralization.\n","authors":["Hanwen Jiang","Zexiang Xu","Desai Xie","Ziwen Chen","Haian Jin","Fujun Luan","Zhixin Shu","Kai Zhang","Sai Bi","Xin Sun","Jiuxiang Gu","Qixing Huang","Georgios Pavlakos","Hao Tan"],"pdf_url":"https://arxiv.org/pdf/2412.14166v1.pdf","comment":"Project page: https://hwjiang1510.github.io/MegaSynth/"},{"id":"http://arxiv.org/abs/2412.14164v1","updated":"2024-12-18T18:58:50Z","published":"2024-12-18T18:58:50Z","title":"MetaMorph: Multimodal Understanding and Generation via Instruction\n  Tuning","summary":"  In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a\nsimple and effective extension to visual instruction tuning that enables a\npretrained LLM to quickly morph into an unified autoregressive model capable of\ngenerating both text and visual tokens. VPiT teaches an LLM to predict discrete\ntext tokens and continuous visual tokens from any input sequence of image and\ntext data curated in an instruction-following format. Our empirical\ninvestigation reveals several intriguing properties of VPiT: (1) visual\ngeneration ability emerges as a natural byproduct of improved visual\nunderstanding, and can be unlocked efficiently with a small amount of\ngeneration data; (2) while we find understanding and generation to be mutually\nbeneficial, understanding data contributes to both capabilities more\neffectively than generation data. Building upon these findings, we train our\nMetaMorph model and achieve competitive performance on both visual\nunderstanding and generation. In visual generation, MetaMorph can leverage the\nworld knowledge and reasoning abilities gained from LLM pretraining, and\novercome common failure modes exhibited by other generation models. Our results\nsuggest that LLMs may have strong \"prior\" vision capabilities that can be\nefficiently adapted to both visual understanding and generation with a\nrelatively simple instruction tuning process.\n","authors":["Shengbang Tong","David Fan","Jiachen Zhu","Yunyang Xiong","Xinlei Chen","Koustuv Sinha","Michael Rabbat","Yann LeCun","Saining Xie","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14164v1.pdf","comment":"Project page at tsb0601.github.io/metamorph"},{"id":"http://arxiv.org/abs/2412.14158v1","updated":"2024-12-18T18:53:22Z","published":"2024-12-18T18:53:22Z","title":"AKiRa: Augmentation Kit on Rays for optical video generation","summary":"  Recent advances in text-conditioned video diffusion have greatly improved\nvideo quality. However, these methods offer limited or sometimes no control to\nusers on camera aspects, including dynamic camera motion, zoom, distorted lens\nand focus shifts. These motion and optical aspects are crucial for adding\ncontrollability and cinematic elements to generation frameworks, ultimately\nresulting in visual content that draws focus, enhances mood, and guides\nemotions according to filmmakers' controls. In this paper, we aim to close the\ngap between controllable video generation and camera optics. To achieve this,\nwe propose AKiRa (Augmentation Kit on Rays), a novel augmentation framework\nthat builds and trains a camera adapter with a complex camera model over an\nexisting video generation backbone. It enables fine-tuned control over camera\nmotion as well as complex optical parameters (focal length, distortion,\naperture) to achieve cinematic effects such as zoom, fisheye effect, and bokeh.\nExtensive experiments demonstrate AKiRa's effectiveness in combining and\ncomposing camera optics while outperforming all state-of-the-art methods. This\nwork sets a new landmark in controlled and optically enhanced video generation,\npaving the way for future optical video generation methods.\n","authors":["Xi Wang","Robin Courant","Marc Christie","Vicky Kalogeiton"],"pdf_url":"https://arxiv.org/pdf/2412.14158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14148v1","updated":"2024-12-18T18:45:35Z","published":"2024-12-18T18:45:35Z","title":"MCMat: Multiview-Consistent and Physically Accurate PBR Material\n  Generation","summary":"  Existing 2D methods utilize UNet-based diffusion models to generate\nmulti-view physically-based rendering (PBR) maps but struggle with multi-view\ninconsistency, while some 3D methods directly generate UV maps, encountering\ngeneralization issues due to the limited 3D data. To address these problems, we\npropose a two-stage approach, including multi-view generation and UV materials\nrefinement. In the generation stage, we adopt a Diffusion Transformer (DiT)\nmodel to generate PBR materials, where both the specially designed multi-branch\nDiT and reference-based DiT blocks adopt a global attention mechanism to\npromote feature interaction and fusion between different views, thereby\nimproving multi-view consistency. In addition, we adopt a PBR-based diffusion\nloss to ensure that the generated materials align with realistic physical\nprinciples. In the refinement stage, we propose a material-refined DiT that\nperforms inpainting in empty areas and enhances details in UV space. Except for\nthe normal condition, this refinement also takes the material map from the\ngeneration stage as an additional condition to reduce the learning difficulty\nand improve generalization. Extensive experiments show that our method achieves\nstate-of-the-art performance in texturing 3D objects with PBR materials and\nprovides significant advantages for graphics relighting applications. Project\nPage: https://lingtengqiu.github.io/2024/MCMat/\n","authors":["Shenhao Zhu","Lingteng Qiu","Xiaodong Gu","Zhengyi Zhao","Chao Xu","Yuxiao He","Zhe Li","Xiaoguang Han","Yao Yao","Xun Cao","Siyu Zhu","Weihao Yuan","Zilong Dong","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.14148v1.pdf","comment":"Project Page: https://lingtengqiu.github.io/2024/MCMat/"},{"id":"http://arxiv.org/abs/2412.14145v1","updated":"2024-12-18T18:43:21Z","published":"2024-12-18T18:43:21Z","title":"Incorporating Feature Pyramid Tokenization and Open Vocabulary Semantic\n  Segmentation","summary":"  The visual understanding are often approached from 3 granular levels: image,\npatch and pixel. Visual Tokenization, trained by self-supervised reconstructive\nlearning, compresses visual data by codebook in patch-level with marginal\ninformation loss, but the visual tokens does not have semantic meaning. Open\nVocabulary semantic segmentation benefits from the evolving Vision-Language\nmodels (VLMs) with strong image zero-shot capability, but transferring\nimage-level to pixel-level understanding remains an imminent challenge. In this\npaper, we treat segmentation as tokenizing pixels and study a united perceptual\nand semantic token compression for all granular understanding and consequently\nfacilitate open vocabulary semantic segmentation. Referring to the cognitive\nprocess of pretrained VLM where the low-level features are progressively\ncomposed to high-level semantics, we propose Feature Pyramid Tokenization (PAT)\nto cluster and represent multi-resolution feature by learnable codebooks and\nthen decode them by joint learning pixel reconstruction and semantic\nsegmentation. We design loosely coupled pixel and semantic learning branches.\nThe pixel branch simulates bottom-up composition and top-down visualization of\ncodebook tokens, while the semantic branch collectively fuse hierarchical\ncodebooks as auxiliary segmentation guidance. Our experiments show that PAT\nenhances the semantic intuition of VLM feature pyramid, improves performance\nover the baseline segmentation model and achieves competitive performance on\nopen vocabulary semantic segmentation benchmark. Our model is\nparameter-efficient for VLM integration and flexible for the independent\ntokenization. We hope to give inspiration not only on improving segmentation\nbut also on semantic visual token utilization.\n","authors":["Jianyu Zhang","Li Zhang","Shijian Li"],"pdf_url":"https://arxiv.org/pdf/2412.14145v1.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.14123v1","updated":"2024-12-18T18:11:53Z","published":"2024-12-18T18:11:53Z","title":"AnySat: An Earth Observation Model for Any Resolutions, Scales, and\n  Modalities","summary":"  Geospatial models must adapt to the diversity of Earth observation data in\nterms of resolutions, scales, and modalities. However, existing approaches\nexpect fixed input configurations, which limits their practical applicability.\nWe propose AnySat, a multimodal model based on joint embedding predictive\narchitecture (JEPA) and resolution-adaptive spatial encoders, allowing us to\ntrain a single model on highly heterogeneous data in a self-supervised manner.\nTo demonstrate the advantages of this unified approach, we compile GeoPlex, a\ncollection of $5$ multimodal datasets with varying characteristics and $11$\ndistinct sensors. We then train a single powerful model on these diverse\ndatasets simultaneously. Once fine-tuned, we achieve better or near\nstate-of-the-art results on the datasets of GeoPlex and $4$ additional ones for\n$5$ environment monitoring tasks: land cover mapping, tree species\nidentification, crop type classification, change detection, and flood\nsegmentation. The code and models are available at\nhttps://github.com/gastruc/AnySat.\n","authors":["Guillaume Astruc","Nicolas Gonthier","Clement Mallet","Loic Landrieu"],"pdf_url":"https://arxiv.org/pdf/2412.14123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10178v2","updated":"2024-12-18T18:05:43Z","published":"2024-12-13T14:50:26Z","title":"SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models","summary":"  Given an input video of a person and a new garment, the objective of this\npaper is to synthesize a new video where the person is wearing the specified\ngarment while maintaining spatiotemporal consistency. Although significant\nadvances have been made in image-based virtual try-on, extending these\nsuccesses to video often leads to frame-to-frame inconsistencies. Some\napproaches have attempted to address this by increasing the overlap of frames\nacross multiple video chunks, but this comes at a steep computational cost due\nto the repeated processing of the same frames, especially for long video\nsequences. To tackle these challenges, we reconceptualize video virtual try-on\nas a conditional video inpainting task, with garments serving as input\nconditions. Specifically, our approach enhances image diffusion models by\nincorporating temporal attention layers to improve temporal coherence. To\nreduce computational overhead, we propose ShiftCaching, a novel technique that\nmaintains temporal consistency while minimizing redundant computations.\nFurthermore, we introduce the TikTokDress dataset, a new video try-on dataset\nfeaturing more complex backgrounds, challenging movements, and higher\nresolution compared to existing public datasets. Extensive experiments\ndemonstrate that our approach outperforms current baselines, particularly in\nterms of video consistency and inference speed. The project page is available\nat https://swift-try.github.io/.\n","authors":["Hung Nguyen","Quang Qui-Vinh Nguyen","Khoi Nguyen","Rang Nguyen"],"pdf_url":"https://arxiv.org/pdf/2412.10178v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14118v1","updated":"2024-12-18T18:04:12Z","published":"2024-12-18T18:04:12Z","title":"GaraMoSt: Parallel Multi-Granularity Motion and Structural Modeling for\n  Efficient Multi-Frame Interpolation in DSA Images","summary":"  The rapid and accurate direct multi-frame interpolation method for Digital\nSubtraction Angiography (DSA) images is crucial for reducing radiation and\nproviding real-time assistance to physicians for precise diagnostics and\ntreatment. DSA images contain complex vascular structures and various motions.\nApplying natural scene Video Frame Interpolation (VFI) methods results in\nmotion artifacts, structural dissipation, and blurriness. Recently, MoSt-DSA\nhas specifically addressed these issues for the first time and achieved SOTA\nresults. However, MoSt-DSA's focus on real-time performance leads to\ninsufficient suppression of high-frequency noise and incomplete filtering of\nlow-frequency noise in the generated images. To address these issues within the\nsame computational time scale, we propose GaraMoSt. Specifically, we optimize\nthe network pipeline with a parallel design and propose a module named MG-MSFE.\nMG-MSFE extracts frame-relative motion and structural features at various\ngranularities in a fully convolutional parallel manner and supports\nindependent, flexible adjustment of context-aware granularity at different\nscales, thus enhancing computational efficiency and accuracy. Extensive\nexperiments demonstrate that GaraMoSt achieves the SOTA performance in\naccuracy, robustness, visual effects, and noise suppression, comprehensively\nsurpassing MoSt-DSA and other natural scene VFI methods. The code and models\nare available at https://github.com/ZyoungXu/GaraMoSt.\n","authors":["Ziyang Xu","Huangxuan Zhao","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14118v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14111v1","updated":"2024-12-18T17:58:16Z","published":"2024-12-18T17:58:16Z","title":"Event-based Photometric Bundle Adjustment","summary":"  We tackle the problem of bundle adjustment (i.e., simultaneous refinement of\ncamera poses and scene map) for a purely rotating event camera. Starting from\nfirst principles, we formulate the problem as a classical non-linear least\nsquares optimization. The photometric error is defined using the event\ngeneration model directly in the camera rotations and the semi-dense scene\nbrightness that triggers the events. We leverage the sparsity of event data to\ndesign a tractable Levenberg-Marquardt solver that handles the very large\nnumber of variables involved. To the best of our knowledge, our method, which\nwe call Event-based Photometric Bundle Adjustment (EPBA), is the first\nevent-only photometric bundle adjustment method that works on the brightness\nmap directly and exploits the space-time characteristics of event data, without\nhaving to convert events into image-like representations. Comprehensive\nexperiments on both synthetic and real-world datasets demonstrate EPBA's\neffectiveness in decreasing the photometric error (by up to 90%), yielding\nresults of unparalleled quality. The refined maps reveal details that were\nhidden using prior state-of-the-art rotation-only estimation methods. The\nexperiments on modern high-resolution event cameras show the applicability of\nEPBA to panoramic imaging in various scenarios (without map initialization, at\nmultiple resolutions, and in combination with other methods, such as IMU dead\nreckoning or previous event-based rotation estimation methods). We make the\nsource code publicly available. https://github.com/tub-rip/epba\n","authors":["Shuang Guo","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2412.14111v1.pdf","comment":"21 pages, 19 figures, 10 tables. Project page:\n  https://github.com/tub-rip/epba"},{"id":"http://arxiv.org/abs/2412.14103v1","updated":"2024-12-18T17:50:15Z","published":"2024-12-18T17:50:15Z","title":"Foundation Models Meet Low-Cost Sensors: Test-Time Adaptation for\n  Rescaling Disparity for Zero-Shot Metric Depth Estimation","summary":"  The recent development of foundation models for monocular depth estimation\nsuch as Depth Anything paved the way to zero-shot monocular depth estimation.\nSince it returns an affine-invariant disparity map, the favored technique to\nrecover the metric depth consists in fine-tuning the model. However, this stage\nis costly to perform because of the training but also due to the creation of\nthe dataset. It must contain images captured by the camera that will be used at\ntest time and the corresponding ground truth. Moreover, the fine-tuning may\nalso degrade the generalizing capacity of the original model. Instead, we\npropose in this paper a new method to rescale Depth Anything predictions using\n3D points provided by low-cost sensors or techniques such as low-resolution\nLiDAR, stereo camera, structure-from-motion where poses are given by an IMU.\nThus, this approach avoids fine-tuning and preserves the generalizing power of\nthe original depth estimation model while being robust to the noise of the\nsensor or of the depth model. Our experiments highlight improvements relative\nto other metric depth estimation methods and competitive results compared to\nfine-tuned approaches. Code available at\nhttps://gitlab.ensta.fr/ssh/monocular-depth-rescaling.\n","authors":["Rémi Marsal","Alexandre Chapoutot","Philippe Xu","David Filliat"],"pdf_url":"https://arxiv.org/pdf/2412.14103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10604v2","updated":"2024-12-18T17:49:32Z","published":"2024-12-13T23:15:35Z","title":"EvalGIM: A Library for Evaluating Generative Image Models","summary":"  As the use of text-to-image generative models increases, so does the adoption\nof automatic benchmarking methods used in their evaluation. However, while\nmetrics and datasets abound, there are few unified benchmarking libraries that\nprovide a framework for performing evaluations across many datasets and\nmetrics. Furthermore, the rapid introduction of increasingly robust\nbenchmarking methods requires that evaluation libraries remain flexible to new\ndatasets and metrics. Finally, there remains a gap in synthesizing evaluations\nin order to deliver actionable takeaways about model performance. To enable\nunified, flexible, and actionable evaluations, we introduce EvalGIM (pronounced\n''EvalGym''), a library for evaluating generative image models. EvalGIM\ncontains broad support for datasets and metrics used to measure quality,\ndiversity, and consistency of text-to-image generative models. In addition,\nEvalGIM is designed with flexibility for user customization as a top priority\nand contains a structure that allows plug-and-play additions of new datasets\nand metrics. To enable actionable evaluation insights, we introduce\n''Evaluation Exercises'' that highlight takeaways for specific evaluation\nquestions. The Evaluation Exercises contain easy-to-use and reproducible\nimplementations of two state-of-the-art evaluation methods of text-to-image\ngenerative models: consistency-diversity-realism Pareto Fronts and\ndisaggregated measurements of performance disparities across groups. EvalGIM\nalso contains Evaluation Exercises that introduce two new analysis methods for\ntext-to-image generative models: robustness analyses of model rankings and\nbalanced evaluations across different prompt styles. We encourage text-to-image\nmodel exploration with EvalGIM and invite contributions at\nhttps://github.com/facebookresearch/EvalGIM/.\n","authors":["Melissa Hall","Oscar Mañas","Reyhane Askari-Hemmat","Mark Ibrahim","Candace Ross","Pietro Astolfi","Tariq Berrada Ifriqi","Marton Havasi","Yohann Benchetrit","Karen Ullrich","Carolina Braga","Abhishek Charnalia","Maeve Ryan","Mike Rabbat","Michal Drozdzal","Jakob Verbeek","Adriana Romero-Soriano"],"pdf_url":"https://arxiv.org/pdf/2412.10604v2.pdf","comment":"For code, see https://github.com/facebookresearch/EvalGIM/tree/main"},{"id":"http://arxiv.org/abs/2412.14100v1","updated":"2024-12-18T17:48:32Z","published":"2024-12-18T17:48:32Z","title":"Parameter-efficient Fine-tuning for improved Convolutional Baseline for\n  Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset","summary":"  Automating brain tumor segmentation using deep learning methods is an ongoing\nchallenge in medical imaging. Multiple lingering issues exist including\ndomain-shift and applications in low-resource settings which brings a unique\nset of challenges including scarcity of data. As a step towards solving these\nspecific problems, we propose Convolutional adapter-inspired\nParameter-efficient Fine-tuning (PEFT) of MedNeXt architecture. To validate our\nidea, we show our method performs comparable to full fine-tuning with the added\nbenefit of reduced training compute using BraTS-2021 as pre-training dataset\nand BraTS-Africa as the fine-tuning dataset. BraTS-Africa consists of a small\ndataset (60 train / 35 validation) from the Sub-Saharan African population with\nmarked shift in the MRI quality compared to BraTS-2021 (1251 train samples). We\nfirst show that models trained on BraTS-2021 dataset do not generalize well to\nBraTS-Africa as shown by 20% reduction in mean dice on BraTS-Africa validation\nsamples. Then, we show that PEFT can leverage both the BraTS-2021 and\nBraTS-Africa dataset to obtain mean dice of 0.8 compared to 0.72 when trained\nonly on BraTS-Africa. Finally, We show that PEFT (0.80 mean dice) results in\ncomparable performance to full fine-tuning (0.77 mean dice) which may show PEFT\nto be better on average but the boxplots show that full finetuning results is\nmuch lesser variance in performance. Nevertheless, on disaggregation of the\ndice metrics, we find that the model has tendency to oversegment as shown by\nhigh specificity (0.99) compared to relatively low sensitivity(0.75). The\nsource code is available at\nhttps://github.com/CAMERA-MRI/SPARK2024/tree/main/PEFT_MedNeXt\n","authors":["Bijay Adhikari","Pratibha Kulung","Jakesh Bohaju","Laxmi Kanta Poudel","Confidence Raymond","Dong Zhang","Udunna C Anazodo","Bishesh Khanal","Mahesh Shakya"],"pdf_url":"https://arxiv.org/pdf/2412.14100v1.pdf","comment":"Accepted to \"The International Brain Tumor Segmentation (BraTS)\n  challenge organized at MICCAI 2024 conference\""},{"id":"http://arxiv.org/abs/2412.14097v1","updated":"2024-12-18T17:47:46Z","published":"2024-12-18T17:47:46Z","title":"Adaptive Concept Bottleneck for Foundation Models Under Distribution\n  Shifts","summary":"  Advancements in foundation models (FMs) have led to a paradigm shift in\nmachine learning. The rich, expressive feature representations from these\npre-trained, large-scale FMs are leveraged for multiple downstream tasks,\nusually via lightweight fine-tuning of a shallow fully-connected network\nfollowing the representation. However, the non-interpretable, black-box nature\nof this prediction pipeline can be a challenge, especially in critical domains\nsuch as healthcare, finance, and security. In this paper, we explore the\npotential of Concept Bottleneck Models (CBMs) for transforming complex,\nnon-interpretable foundation models into interpretable decision-making\npipelines using high-level concept vectors. Specifically, we focus on the\ntest-time deployment of such an interpretable CBM pipeline \"in the wild\", where\nthe input distribution often shifts from the original training distribution. We\nfirst identify the potential failure modes of such a pipeline under different\ntypes of distribution shifts. Then we propose an adaptive concept bottleneck\nframework to address these failure modes, that dynamically adapts the\nconcept-vector bank and the prediction layer based solely on unlabeled data\nfrom the target domain, without access to the source (training) dataset.\nEmpirical evaluations with various real-world distribution shifts show that our\nadaptation method produces concept-based interpretations better aligned with\nthe test data and boosts post-deployment accuracy by up to 28%, aligning the\nCBM performance with that of non-interpretable classification.\n","authors":["Jihye Choi","Jayaram Raghuram","Yixuan Li","Somesh Jha"],"pdf_url":"https://arxiv.org/pdf/2412.14097v1.pdf","comment":"The preliminary version of the work appeared in the ICML 2024\n  Workshop on Foundation Models in the Wild"},{"id":"http://arxiv.org/abs/2412.14088v1","updated":"2024-12-18T17:34:52Z","published":"2024-12-18T17:34:52Z","title":"Joint Perception and Prediction for Autonomous Driving: A Survey","summary":"  Perception and prediction modules are critical components of autonomous\ndriving systems, enabling vehicles to navigate safely through complex\nenvironments. The perception module is responsible for perceiving the\nenvironment, including static and dynamic objects, while the prediction module\nis responsible for predicting the future behavior of these objects. These\nmodules are typically divided into three tasks: object detection, object\ntracking, and motion prediction. Traditionally, these tasks are developed and\noptimized independently, with outputs passed sequentially from one to the next.\nHowever, this approach has significant limitations: computational resources are\nnot shared across tasks, the lack of joint optimization can amplify errors as\nthey propagate throughout the pipeline, and uncertainty is rarely propagated\nbetween modules, resulting in significant information loss. To address these\nchallenges, the joint perception and prediction paradigm has emerged,\nintegrating perception and prediction into a unified model through multi-task\nlearning. This strategy not only overcomes the limitations of previous methods,\nbut also enables the three tasks to have direct access to raw sensor data,\nallowing richer and more nuanced environmental interpretations. This paper\npresents the first comprehensive survey of joint perception and prediction for\nautonomous driving. We propose a taxonomy that categorizes approaches based on\ninput representation, scene context modeling, and output representation,\nhighlighting their contributions and limitations. Additionally, we present a\nqualitative analysis and quantitative comparison of existing methods. Finally,\nwe discuss future research directions based on identified gaps in the\nstate-of-the-art.\n","authors":["Lucas Dal'Col","Miguel Oliveira","Vítor Santos"],"pdf_url":"https://arxiv.org/pdf/2412.14088v1.pdf","comment":"24 pages, 5 sections, 7 figures, 7 tables. This work has been\n  submitted to the IEEE Transactions on Intelligent Transportation Systems for\n  possible publication"},{"id":"http://arxiv.org/abs/2412.14058v1","updated":"2024-12-18T17:07:20Z","published":"2024-12-18T17:07:20Z","title":"Towards Generalist Robot Policies: What Matters in Building\n  Vision-Language-Action Models","summary":"  Foundation Vision Language Models (VLMs) exhibit strong capabilities in\nmulti-modal representation learning, comprehension, and reasoning. By injecting\naction components into the VLMs, Vision-Language-Action Models (VLAs) can be\nnaturally formed and also show promising performance. Existing work has\ndemonstrated the effectiveness and generalization of VLAs in multiple scenarios\nand tasks. Nevertheless, the transfer from VLMs to VLAs is not trivial since\nexisting VLAs differ in their backbones, action-prediction formulations, data\ndistributions, and training recipes. This leads to a missing piece for a\nsystematic understanding of the design choices of VLAs. In this work, we\ndisclose the key factors that significantly influence the performance of VLA\nand focus on answering three essential design choices: which backbone to\nselect, how to formulate the VLA architectures, and when to add\ncross-embodiment data. The obtained results convince us firmly to explain why\nwe need VLA and develop a new family of VLAs, RoboVLMs, which require very few\nmanual designs and achieve a new state-of-the-art performance in three\nsimulation tasks and real-world experiments. Through our extensive experiments,\nwhich include over 8 VLM backbones, 4 policy architectures, and over 600\ndistinct designed experiments, we provide a detailed guidebook for the future\ndesign of VLAs. In addition to the study, the highly flexible RoboVLMs\nframework, which supports easy integrations of new VLMs and free combinations\nof various design choices, is made public to facilitate future research. We\nopen-source all details, including codes, models, datasets, and toolkits, along\nwith detailed training and evaluation recipes at: robovlms.github.io.\n","authors":["Xinghang Li","Peiyan Li","Minghuan Liu","Dong Wang","Jirong Liu","Bingyi Kang","Xiao Ma","Tao Kong","Hanbo Zhang","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14058v1.pdf","comment":"Project page: robovlms.github.io"},{"id":"http://arxiv.org/abs/2412.14056v1","updated":"2024-12-18T17:06:21Z","published":"2024-12-18T17:06:21Z","title":"A Review of Multimodal Explainable Artificial Intelligence: Past,\n  Present and Future","summary":"  Artificial intelligence (AI) has rapidly developed through advancements in\ncomputational power and the growth of massive datasets. However, this progress\nhas also heightened challenges in interpreting the \"black-box\" nature of AI\nmodels. To address these concerns, eXplainable AI (XAI) has emerged with a\nfocus on transparency and interpretability to enhance human understanding and\ntrust in AI decision-making processes. In the context of multimodal data fusion\nand complex reasoning scenarios, the proposal of Multimodal eXplainable AI\n(MXAI) integrates multiple modalities for prediction and explanation tasks.\nMeanwhile, the advent of Large Language Models (LLMs) has led to remarkable\nbreakthroughs in natural language processing, yet their complexity has further\nexacerbated the issue of MXAI. To gain key insights into the development of\nMXAI methods and provide crucial guidance for building more transparent, fair,\nand trustworthy AI systems, we review the MXAI methods from a historical\nperspective and categorize them across four eras: traditional machine learning,\ndeep learning, discriminative foundation models, and generative LLMs. We also\nreview evaluation metrics and datasets used in MXAI research, concluding with a\ndiscussion of future challenges and directions. A project related to this\nreview has been created at https://github.com/ShilinSun/mxai_review.\n","authors":["Shilin Sun","Wenbin An","Feng Tian","Fang Nan","Qidong Liu","Jun Liu","Nazaraf Shah","Ping Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14056v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2412.14042v1","updated":"2024-12-18T16:55:42Z","published":"2024-12-18T16:55:42Z","title":"CAD-Recode: Reverse Engineering CAD Code from Point Clouds","summary":"  Computer-Aided Design (CAD) models are typically constructed by sequentially\ndrawing parametric sketches and applying CAD operations to obtain a 3D model.\nThe problem of 3D CAD reverse engineering consists of reconstructing the sketch\nand CAD operation sequences from 3D representations such as point clouds. In\nthis paper, we address this challenge through novel contributions across three\nlevels: CAD sequence representation, network design, and dataset. In\nparticular, we represent CAD sketch-extrude sequences as Python code. The\nproposed CAD-Recode translates a point cloud into Python code that, when\nexecuted, reconstructs the CAD model. Taking advantage of the exposure of\npre-trained Large Language Models (LLMs) to Python code, we leverage a\nrelatively small LLM as a decoder for CAD-Recode and combine it with a\nlightweight point cloud projector. CAD-Recode is trained solely on a proposed\nsynthetic dataset of one million diverse CAD sequences. CAD-Recode\nsignificantly outperforms existing methods across three datasets while\nrequiring fewer input points. Notably, it achieves 10 times lower mean Chamfer\ndistance than state-of-the-art methods on DeepCAD and Fusion360 datasets.\nFurthermore, we show that our CAD Python code output is interpretable by\noff-the-shelf LLMs, enabling CAD editing and CAD-specific question answering\nfrom point clouds.\n","authors":["Danila Rukhovich","Elona Dupont","Dimitrios Mallis","Kseniya Cherenkova","Anis Kacem","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2412.14042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13372v2","updated":"2024-12-18T16:51:18Z","published":"2024-07-18T10:26:53Z","title":"Restore Anything Model via Efficient Degradation Adaptation","summary":"  With the proliferation of mobile devices, the need for an efficient model to\nrestore any degraded image has become increasingly significant and impactful.\nTraditional approaches typically involve training dedicated models for each\nspecific degradation, resulting in inefficiency and redundancy. More recent\nsolutions either introduce additional modules to learn visual prompts\nsignificantly increasing model size or incorporate cross-modal transfer from\nlarge language models trained on vast datasets, adding complexity to the system\narchitecture. In contrast, our approach, termed RAM, takes a unified path that\nleverages inherent similarities across various degradations to enable both\nefficient and comprehensive restoration through a joint embedding mechanism\nwithout scaling up the model or relying on large multimodal models.\nSpecifically, we examine the sub-latent space of each input, identifying key\ncomponents and reweighting them in a gated manner. This intrinsic degradation\nawareness is further combined with contextualized attention in an X-shaped\nframework, enhancing local-global interactions. Extensive benchmarking in an\nall-in-one restoration setting confirms RAM's SOTA performance, reducing model\ncomplexity by approximately 82% in trainable parameters and 85% in FLOPs. Our\ncode and models will be publicly available.\n","authors":["Bin Ren","Eduard Zamfir","Zongwei Wu","Yawei Li","Yidi Li","Danda Pani Paudel","Radu Timofte","Ming-Hsuan Yang","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2407.13372v2.pdf","comment":"Efficient Any Image Restoration"},{"id":"http://arxiv.org/abs/2412.14018v1","updated":"2024-12-18T16:34:51Z","published":"2024-12-18T16:34:51Z","title":"SurgSora: Decoupled RGBD-Flow Diffusion Model for Controllable Surgical\n  Video Generation","summary":"  Medical video generation has transformative potential for enhancing surgical\nunderstanding and pathology insights through precise and controllable visual\nrepresentations. However, current models face limitations in controllability\nand authenticity. To bridge this gap, we propose SurgSora, a\nmotion-controllable surgical video generation framework that uses a single\ninput frame and user-controllable motion cues. SurgSora consists of three key\nmodules: the Dual Semantic Injector (DSI), which extracts object-relevant RGB\nand depth features from the input frame and integrates them with segmentation\ncues to capture detailed spatial features of complex anatomical structures; the\nDecoupled Flow Mapper (DFM), which fuses optical flow with semantic-RGB-D\nfeatures at multiple scales to enhance temporal understanding and object\nspatial dynamics; and the Trajectory Controller (TC), which allows users to\nspecify motion directions and estimates sparse optical flow, guiding the video\ngeneration process. The fused features are used as conditions for a frozen\nStable Diffusion model to produce realistic, temporally coherent surgical\nvideos. Extensive evaluations demonstrate that SurgSora outperforms\nstate-of-the-art methods in controllability and authenticity, showing its\npotential to advance surgical video generation for medical education, training,\nand research.\n","authors":["Tong Chen","Shuya Yang","Junyi Wang","Long Bai","Hongliang Ren","Luping Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14015v1","updated":"2024-12-18T16:32:12Z","published":"2024-12-18T16:32:12Z","title":"Prompting Depth Anything for 4K Resolution Accurate Metric Depth\n  Estimation","summary":"  Prompts play a critical role in unleashing the power of language and vision\nfoundation models for specific tasks. For the first time, we introduce\nprompting into depth foundation models, creating a new paradigm for metric\ndepth estimation termed Prompt Depth Anything. Specifically, we use a low-cost\nLiDAR as the prompt to guide the Depth Anything model for accurate metric depth\noutput, achieving up to 4K resolution. Our approach centers on a concise prompt\nfusion design that integrates the LiDAR at multiple scales within the depth\ndecoder. To address training challenges posed by limited datasets containing\nboth LiDAR depth and precise GT depth, we propose a scalable data pipeline that\nincludes synthetic data LiDAR simulation and real data pseudo GT depth\ngeneration. Our approach sets new state-of-the-arts on the ARKitScenes and\nScanNet++ datasets and benefits downstream applications, including 3D\nreconstruction and generalized robotic grasping.\n","authors":["Haotong Lin","Sida Peng","Jingxiao Chen","Songyou Peng","Jiaming Sun","Minghuan Liu","Hujun Bao","Jiashi Feng","Xiaowei Zhou","Bingyi Kang"],"pdf_url":"https://arxiv.org/pdf/2412.14015v1.pdf","comment":"Project page: https://PromptDA.github.io/"},{"id":"http://arxiv.org/abs/2412.14006v1","updated":"2024-12-18T16:20:40Z","published":"2024-12-18T16:20:40Z","title":"InstructSeg: Unifying Instructed Visual Segmentation with Multi-modal\n  Large Language Models","summary":"  Boosted by Multi-modal Large Language Models (MLLMs), text-guided universal\nsegmentation models for the image and video domains have made rapid progress\nrecently. However, these methods are often developed separately for specific\ndomains, overlooking the similarities in task settings and solutions across\nthese two areas. In this paper, we define the union of referring segmentation\nand reasoning segmentation at both the image and video levels as Instructed\nVisual Segmentation (IVS). Correspondingly, we propose InstructSeg, an\nend-to-end segmentation pipeline equipped with MLLMs for IVS. Specifically, we\nemploy an object-aware video perceiver to extract temporal and object\ninformation from reference frames, facilitating comprehensive video\nunderstanding. Additionally, we introduce vision-guided multi-granularity text\nfusion to better integrate global and detailed text information with\nfine-grained visual guidance. By leveraging multi-task and end-to-end training,\nInstructSeg demonstrates superior performance across diverse image and video\nsegmentation tasks, surpassing both segmentation specialists and MLLM-based\nmethods with a single model. Our code is available at\nhttps://github.com/congvvc/InstructSeg.\n","authors":["Cong Wei","Yujie Zhong","Haoxian Tan","Yingsen Zeng","Yong Liu","Zheng Zhao","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2412.14006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14005v1","updated":"2024-12-18T16:20:21Z","published":"2024-12-18T16:20:21Z","title":"Real-Time Position-Aware View Synthesis from Single-View Input","summary":"  Recent advancements in view synthesis have significantly enhanced immersive\nexperiences across various computer graphics and multimedia applications,\nincluding telepresence, and entertainment. By enabling the generation of new\nperspectives from a single input view, view synthesis allows users to better\nperceive and interact with their environment. However, many state-of-the-art\nmethods, while achieving high visual quality, face limitations in real-time\nperformance, which makes them less suitable for live applications where low\nlatency is critical. In this paper, we present a lightweight, position-aware\nnetwork designed for real-time view synthesis from a single input image and a\ntarget camera pose. The proposed framework consists of a Position Aware\nEmbedding, modeled with a multi-layer perceptron, which efficiently maps\npositional information from the target pose to generate high dimensional\nfeature maps. These feature maps, along with the input image, are fed into a\nRendering Network that merges features from dual encoder branches to resolve\nboth high level semantics and low level details, producing a realistic new view\nof the scene. Experimental results demonstrate that our method achieves\nsuperior efficiency and visual quality compared to existing approaches,\nparticularly in handling complex translational movements without explicit\ngeometric operations like warping. This work marks a step toward enabling\nreal-time view synthesis from a single image for live and interactive\napplications.\n","authors":["Manu Gond","Emin Zerman","Sebastian Knorr","Mårten Sjöström"],"pdf_url":"https://arxiv.org/pdf/2412.14005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13983v1","updated":"2024-12-18T16:05:40Z","published":"2024-12-18T16:05:40Z","title":"GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians","summary":"  Rendering photorealistic head avatars from arbitrary viewpoints is crucial\nfor various applications like virtual reality. Although previous methods based\non Neural Radiance Fields (NeRF) can achieve impressive results, they lack\nfidelity and efficiency. Recent methods using 3D Gaussian Splatting (3DGS) have\nimproved rendering quality and real-time performance but still require\nsignificant storage overhead. In this paper, we introduce a method called\nGraphAvatar that utilizes Graph Neural Networks (GNN) to generate 3D Gaussians\nfor the head avatar. Specifically, GraphAvatar trains a geometric GNN and an\nappearance GNN to generate the attributes of the 3D Gaussians from the tracked\nmesh. Therefore, our method can store the GNN models instead of the 3D\nGaussians, significantly reducing the storage overhead to just 10MB. To reduce\nthe impact of face-tracking errors, we also present a novel graph-guided\noptimization module to refine face-tracking parameters during training.\nFinally, we introduce a 3D-aware enhancer for post-processing to enhance the\nrendering quality. We conduct comprehensive experiments to demonstrate the\nadvantages of GraphAvatar, surpassing existing methods in visual fidelity and\nstorage consumption. The ablation study sheds light on the trade-offs between\nrendering quality and model size. The code will be released at:\nhttps://github.com/ucwxb/GraphAvatar\n","authors":["Xiaobao Wei","Peng Chen","Ming Lu","Hui Chen","Feng Tian"],"pdf_url":"https://arxiv.org/pdf/2412.13983v1.pdf","comment":"accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2410.13016v3","updated":"2024-12-18T16:01:44Z","published":"2024-10-16T20:18:21Z","title":"Interpreting and Analysing CLIP's Zero-Shot Image Classification via\n  Mutual Knowledge","summary":"  Contrastive Language-Image Pretraining (CLIP) performs zero-shot image\nclassification by mapping images and textual class representation into a shared\nembedding space, then retrieving the class closest to the image. This work\nprovides a new approach for interpreting CLIP models for image classification\nfrom the lens of mutual knowledge between the two modalities. Specifically, we\nask: what concepts do both vision and language CLIP encoders learn in common\nthat influence the joint embedding space, causing points to be closer or\nfurther apart? We answer this question via an approach of textual concept-based\nexplanations, showing their effectiveness, and perform an analysis encompassing\na pool of 13 CLIP models varying in architecture, size and pretraining\ndatasets. We explore those different aspects in relation to mutual knowledge,\nand analyze zero-shot predictions. Our approach demonstrates an effective and\nhuman-friendly way of understanding zero-shot classification decisions with\nCLIP.\n","authors":["Fawaz Sammani","Nikos Deligiannis"],"pdf_url":"https://arxiv.org/pdf/2410.13016v3.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.11657v2","updated":"2024-12-18T15:56:51Z","published":"2024-12-16T11:00:02Z","title":"CNNtention: Can CNNs do better with Attention?","summary":"  Convolutional Neural Networks (CNNs) have been the standard for image\nclassification tasks for a long time, but more recently attention-based\nmechanisms have gained traction. This project aims to compare traditional CNNs\nwith attention-augmented CNNs across an image classification task. By\nevaluating and comparing their performance, accuracy and computational\nefficiency, the project will highlight benefits and trade-off of the localized\nfeature extraction of traditional CNNs and the global context capture in\nattention-augmented CNNs. By doing this, we can reveal further insights into\ntheir respective strengths and weaknesses, guide the selection of models based\non specific application needs and ultimately, enhance understanding of these\narchitectures in the deep learning community.\n  This was our final project for CS7643 Deep Learning course at Georgia Tech.\n","authors":["Julian Glattki","Nikhil Kapila","Tejas Rathi"],"pdf_url":"https://arxiv.org/pdf/2412.11657v2.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.03401v2","updated":"2024-12-18T15:47:57Z","published":"2024-12-04T15:32:37Z","title":"Benchmarking Pretrained Attention-based Models for Real-Time Recognition\n  in Robot-Assisted Esophagectomy","summary":"  Esophageal cancer is among the most common types of cancer worldwide. It is\ntraditionally treated using open esophagectomy, but in recent years,\nrobot-assisted minimally invasive esophagectomy (RAMIE) has emerged as a\npromising alternative. However, robot-assisted surgery can be challenging for\nnovice surgeons, as they often suffer from a loss of spatial orientation.\nComputer-aided anatomy recognition holds promise for improving surgical\nnavigation, but research in this area remains limited. In this study, we\ndeveloped a comprehensive dataset for semantic segmentation in RAMIE, featuring\nthe largest collection of vital anatomical structures and surgical instruments\nto date. Handling this diverse set of classes presents challenges, including\nclass imbalance and the recognition of complex structures such as nerves. This\nstudy aims to understand the challenges and limitations of current\nstate-of-the-art algorithms on this novel dataset and problem. Therefore, we\nbenchmarked eight real-time deep learning models using two pretraining\ndatasets. We assessed both traditional and attention-based networks,\nhypothesizing that attention-based networks better capture global patterns and\naddress challenges such as occlusion caused by blood or other tissues. The\nbenchmark includes our RAMIE dataset and the publicly available CholecSeg8k\ndataset, enabling a thorough assessment of surgical segmentation tasks. Our\nfindings indicate that pretraining on ADE20k, a dataset for semantic\nsegmentation, is more effective than pretraining on ImageNet. Furthermore,\nattention-based models outperform traditional convolutional neural networks,\nwith SegNeXt and Mask2Former achieving higher Dice scores, and Mask2Former\nadditionally excelling in average symmetric surface distance.\n","authors":["Ronald L. P. D. de Jong","Yasmina al Khalil","Tim J. M. Jaspers","Romy C. van Jaarsveld","Gino M. Kuiper","Yiping Li","Richard van Hillegersberg","Jelle P. Ruurda","Marcel Breeuwer","Fons van der Sommen"],"pdf_url":"https://arxiv.org/pdf/2412.03401v2.pdf","comment":"Accepted for presentation at the SPIE Medical Imaging Conference,\n  2025"},{"id":"http://arxiv.org/abs/2412.13949v1","updated":"2024-12-18T15:29:30Z","published":"2024-12-18T15:29:30Z","title":"Cracking the Code of Hallucination in LVLMs with Vision-aware Head\n  Divergence","summary":"  Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead.\n","authors":["Jinghan He","Kuan Zhu","Haiyun Guo","Junfeng Fang","Zhenglin Hua","Yuheng Jia","Ming Tang","Tat-Seng Chua","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13947v1","updated":"2024-12-18T15:28:08Z","published":"2024-12-18T15:28:08Z","title":"Real Classification by Description: Extending CLIP's Limits of Part\n  Attributes Recognition","summary":"  In this study, we define and tackle zero shot \"real\" classification by\ndescription, a novel task that evaluates the ability of Vision-Language Models\n(VLMs) like CLIP to classify objects based solely on descriptive attributes,\nexcluding object class names. This approach highlights the current limitations\nof VLMs in understanding intricate object descriptions, pushing these models\nbeyond mere object recognition. To facilitate this exploration, we introduce a\nnew challenge and release description data for six popular fine-grained\nbenchmarks, which omit object names to encourage genuine zero-shot learning\nwithin the research community. Additionally, we propose a method to enhance\nCLIP's attribute detection capabilities through targeted training using\nImageNet21k's diverse object categories, paired with rich attribute\ndescriptions generated by large language models. Furthermore, we introduce a\nmodified CLIP architecture that leverages multiple resolutions to improve the\ndetection of fine-grained part attributes. Through these efforts, we broaden\nthe understanding of part-attribute recognition in CLIP, improving its\nperformance in fine-grained classification tasks across six popular benchmarks,\nas well as in the PACO dataset, a widely used benchmark for object-attribute\nrecognition. Code is available at:\nhttps://github.com/ethanbar11/grounding_ge_public.\n","authors":["Ethan Baron","Idan Tankel","Peter Tu","Guy Ben-Yosef"],"pdf_url":"https://arxiv.org/pdf/2412.13947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13943v1","updated":"2024-12-18T15:25:36Z","published":"2024-12-18T15:25:36Z","title":"On Explaining Knowledge Distillation: Measuring and Visualising the\n  Knowledge Transfer Process","summary":"  Knowledge distillation (KD) remains challenging due to the opaque nature of\nthe knowledge transfer process from a Teacher to a Student, making it difficult\nto address certain issues related to KD. To address this, we proposed UniCAM, a\nnovel gradient-based visual explanation method, which effectively interprets\nthe knowledge learned during KD. Our experimental results demonstrate that with\nthe guidance of the Teacher's knowledge, the Student model becomes more\nefficient, learning more relevant features while discarding those that are not\nrelevant. We refer to the features learned with the Teacher's guidance as\ndistilled features and the features irrelevant to the task and ignored by the\nStudent as residual features. Distilled features focus on key aspects of the\ninput, such as textures and parts of objects. In contrast, residual features\ndemonstrate more diffused attention, often targeting irrelevant areas,\nincluding the backgrounds of the target objects. In addition, we proposed two\nnovel metrics: the feature similarity score (FSS) and the relevance score (RS),\nwhich quantify the relevance of the distilled knowledge. Experiments on the\nCIFAR10, ASIRRA, and Plant Disease datasets demonstrate that UniCAM and the two\nmetrics offer valuable insights to explain the KD process.\n","authors":["Gereziher Adhane","Mohammad Mahdi Dehshibi","Dennis Vetter","David Masip","Gemma Roig"],"pdf_url":"https://arxiv.org/pdf/2412.13943v1.pdf","comment":"Accepted to 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV'25). Includes 5 pages of supplementary material"},{"id":"http://arxiv.org/abs/2412.13187v2","updated":"2024-12-18T15:19:55Z","published":"2024-12-17T18:58:33Z","title":"HandsOnVLM: Vision-Language Models for Hand-Object Interaction\n  Prediction","summary":"  How can we predict future interaction trajectories of human hands in a scene\ngiven high-level colloquial task specifications in the form of natural\nlanguage? In this paper, we extend the classic hand trajectory prediction task\nto two tasks involving explicit or implicit language queries. Our proposed\ntasks require extensive understanding of human daily activities and reasoning\nabilities about what should be happening next given cues from the current\nscene. We also develop new benchmarks to evaluate the proposed two tasks,\nVanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). We\nenable solving these tasks by integrating high-level world knowledge and\nreasoning capabilities of Vision-Language Models (VLMs) with the\nauto-regressive nature of low-level ego-centric hand trajectories. Our model,\nHandsOnVLM is a novel VLM that can generate textual responses and produce\nfuture hand trajectories through natural-language conversations. Our\nexperiments show that HandsOnVLM outperforms existing task-specific methods and\nother VLM baselines on proposed tasks, and demonstrates its ability to\neffectively utilize world knowledge for reasoning about low-level human hand\ntrajectories based on the provided context. Our website contains code and\ndetailed video results https://www.chenbao.tech/handsonvlm/\n","authors":["Chen Bao","Jiarui Xu","Xiaolong Wang","Abhinav Gupta","Homanga Bharadhwaj"],"pdf_url":"https://arxiv.org/pdf/2412.13187v2.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2403.13352v4","updated":"2024-12-18T15:14:48Z","published":"2024-03-20T07:31:07Z","title":"AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in\n  Text-to-Image Generation","summary":"  Text-to-Image (T2I) diffusion models have achieved remarkable success in\nimage generation. Despite their progress, challenges remain in both\nprompt-following ability, image quality and lack of high-quality datasets,\nwhich are essential for refining these models. As acquiring labeled data is\ncostly, we introduce AGFSync, a framework that enhances T2I diffusion models\nthrough Direct Preference Optimization (DPO) in a fully AI-driven approach.\nAGFSync utilizes Vision-Language Models (VLM) to assess image quality across\nstyle, coherence, and aesthetics, generating feedback data within an AI-driven\nloop. By applying AGFSync to leading T2I models such as SD v1.4, v1.5, and\nSDXL-base, our extensive experiments on the TIFA dataset demonstrate notable\nimprovements in VQA scores, aesthetic evaluations, and performance on the HPSv2\nbenchmark, consistently outperforming the base models. AGFSync's method of\nrefining T2I diffusion models paves the way for scalable alignment techniques.\nOur code and dataset are publicly available at\nhttps://anjingkun.github.io/AGFSync.\n","authors":["Jingkun An","Yinghao Zhu","Zongjian Li","Enshen Zhou","Haoran Feng","Xijie Huang","Bohua Chen","Yemin Shi","Chengwei Pan"],"pdf_url":"https://arxiv.org/pdf/2403.13352v4.pdf","comment":"Accepted by AAAI-2025"},{"id":"http://arxiv.org/abs/2307.16879v2","updated":"2024-12-18T15:07:39Z","published":"2023-07-31T17:45:16Z","title":"Image Synthesis under Limited Data: A Survey and Taxonomy","summary":"  Deep generative models, which target reproducing the given data distribution\nto produce novel samples, have made unprecedented advancements in recent years.\nTheir technical breakthroughs have enabled unparalleled quality in the\nsynthesis of visual content. However, one critical prerequisite for their\ntremendous success is the availability of a sufficient number of training\nsamples, which requires massive computation resources. When trained on limited\ndata, generative models tend to suffer from severe performance deterioration\ndue to overfitting and memorization. Accordingly, researchers have devoted\nconsiderable attention to develop novel models that are capable of generating\nplausible and diverse images from limited training data recently. Despite\nnumerous efforts to enhance training stability and synthesis quality in the\nlimited data scenarios, there is a lack of a systematic survey that provides 1)\na clear problem definition, critical challenges, and taxonomy of various tasks;\n2) an in-depth analysis on the pros, cons, and remain limitations of existing\nliterature; as well as 3) a thorough discussion on the potential applications\nand future directions in the field of image synthesis under limited data. In\norder to fill this gap and provide a informative introduction to researchers\nwho are new to this topic, this survey offers a comprehensive review and a\nnovel taxonomy on the development of image synthesis under limited data. In\nparticular, it covers the problem definition, requirements, main solutions,\npopular benchmarks, and remain challenges in a comprehensive and all-around\nmanner.\n","authors":["Mengping Yang","Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2307.16879v2.pdf","comment":"230 references, 25 pages. GitHub:\n  https://github.com/kobeshegu/awesome-few-shot-generation"},{"id":"http://arxiv.org/abs/2412.13916v1","updated":"2024-12-18T14:56:03Z","published":"2024-12-18T14:56:03Z","title":"Retrieval Augmented Image Harmonization","summary":"  When embedding objects (foreground) into images (background), considering the\ninfluence of photography conditions like illumination, it is usually necessary\nto perform image harmonization to make the foreground object coordinate with\nthe background image in terms of brightness, color, and etc. Although existing\nimage harmonization methods have made continuous efforts toward visually\npleasing results, they are still plagued by two main issues. Firstly, the image\nharmonization becomes highly ill-posed when there are no contents similar to\nthe foreground object in the background, making the harmonization results\nunreliable. Secondly, even when similar contents are available, the\nharmonization process is often interfered with by irrelevant areas, mainly\nattributed to an insufficient understanding of image contents and inaccurate\nattention. As a remedy, we present a retrieval-augmented image harmonization\n(Raiha) framework, which seeks proper reference images to reduce the\nill-posedness and restricts the attention to better utilize the useful\ninformation. Specifically, an efficient retrieval method is designed to find\nreference images that contain similar objects as the foreground while the\nillumination is consistent with the background. For training the Raiha\nframework to effectively utilize the reference information, a data augmentation\nstrategy is delicately designed by leveraging existing non-reference image\nharmonization datasets. Besides, the image content priors are introduced to\nensure reasonable attention. With the presented Raiha framework, the image\nharmonization performance is greatly boosted under both non-reference and\nretrieval-augmented settings. The source code and pre-trained models will be\npublicly available.\n","authors":["Haolin Wang","Ming Liu","Zifei Yan","Chao Zhou","Longan Xiao","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2412.13916v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2412.13913v1","updated":"2024-12-18T14:53:38Z","published":"2024-12-18T14:53:38Z","title":"A Black-Box Evaluation Framework for Semantic Robustness in Bird's Eye\n  View Detection","summary":"  Camera-based Bird's Eye View (BEV) perception models receive increasing\nattention for their crucial role in autonomous driving, a domain where concerns\nabout the robustness and reliability of deep learning have been raised. While\nonly a few works have investigated the effects of randomly generated semantic\nperturbations, aka natural corruptions, on the multi-view BEV detection task,\nwe develop a black-box robustness evaluation framework that adversarially\noptimises three common semantic perturbations: geometric transformation, colour\nshifting, and motion blur, to deceive BEV models, serving as the first approach\nin this emerging field. To address the challenge posed by optimising the\nsemantic perturbation, we design a smoothed, distance-based surrogate function\nto replace the mAP metric and introduce SimpleDIRECT, a deterministic\noptimisation algorithm that utilises observed slopes to guide the optimisation\nprocess. By comparing with randomised perturbation and two optimisation\nbaselines, we demonstrate the effectiveness of the proposed framework.\nAdditionally, we provide a benchmark on the semantic robustness of ten recent\nBEV models. The results reveal that PolarFormer, which emphasises geometric\ninformation from multi-view images, exhibits the highest robustness, whereas\nBEVDet is fully compromised, with its precision reduced to zero.\n","authors":["Fu Wang","Yanghao Zhang","Xiangyu Yin","Guangliang Cheng","Zeyu Fu","Xiaowei Huang","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2412.13913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13908v1","updated":"2024-12-18T14:51:25Z","published":"2024-12-18T14:51:25Z","title":"Memorizing SAM: 3D Medical Segment Anything Model with Memorizing\n  Transformer","summary":"  Segment Anything Models (SAMs) have gained increasing attention in medical\nimage analysis due to their zero-shot generalization capability in segmenting\nobjects of unseen classes and domains when provided with appropriate user\nprompts. Addressing this performance gap is important to fully leverage the\npre-trained weights of SAMs, particularly in the domain of volumetric medical\nimage segmentation, where accuracy is important but well-annotated 3D medical\ndata for fine-tuning is limited. In this work, we investigate whether\nintroducing the memory mechanism as a plug-in, specifically the ability to\nmemorize and recall internal representations of past inputs, can improve the\nperformance of SAM with limited computation cost. To this end, we propose\nMemorizing SAM, a novel 3D SAM architecture incorporating a memory Transformer\nas a plug-in. Unlike conventional memorizing Transformers that save the\ninternal representation during training or inference, our Memorizing SAM\nutilizes existing highly accurate internal representation as the memory source\nto ensure the quality of memory. We evaluate the performance of Memorizing SAM\nin 33 categories from the TotalSegmentator dataset, which indicates that\nMemorizing SAM can outperform state-of-the-art 3D SAM variant i.e., FastSAM3D\nwith an average Dice increase of 11.36% at the cost of only 4.38 millisecond\nincrease in inference time. The source code is publicly available at\nhttps://github.com/swedfr/memorizingSAM\n","authors":["Xinyuan Shao","Yiqing Shen","Mathias Unberath"],"pdf_url":"https://arxiv.org/pdf/2412.13908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.06740v2","updated":"2024-12-18T14:41:53Z","published":"2022-05-13T16:19:21Z","title":"Towards Deployable OCR models for Indic languages","summary":"  Recognition of text on word or line images, without the need for sub-word\nsegmentation has become the mainstream of research and development of text\nrecognition for Indian languages. Modelling unsegmented sequences using\nConnectionist Temporal Classification (CTC) is the most commonly used approach\nfor segmentation-free OCR. In this work we present a comprehensive empirical\nstudy of various neural network models that uses CTC for transcribing step-wise\npredictions in the neural network output to a Unicode sequence. The study is\nconducted for 13 Indian languages, using an internal dataset that has around\n1000 pages per language. We study the choice of line vs word as the recognition\nunit, and use of synthetic data to train the models. We compare our models with\npopular publicly available OCR tools for end-to-end document image recognition.\nOur end-to-end pipeline that employ our recognition models and existing text\nsegmentation tools outperform these public OCR tools for 8 out of the 13\nlanguages. We also introduce a new public dataset called Mozhi for word and\nline recognition in Indian language. The dataset contains more than 1.2 million\nannotated word images (120 thousand text lines) across 13 Indian languages. Our\ncode, trained models and the Mozhi dataset will be made available at\nhttp://cvit.iiit.ac.in/research/projects/cvit-projects/\n","authors":["Minesh Mathew","Ajoy Mondal","CV Jawahar"],"pdf_url":"https://arxiv.org/pdf/2205.06740v2.pdf","comment":"presented at ICPR 2024;\n  https://link.springer.com/chapter/10.1007/978-3-031-78495-8_11"},{"id":"http://arxiv.org/abs/2412.13897v1","updated":"2024-12-18T14:39:43Z","published":"2024-12-18T14:39:43Z","title":"Data-Efficient Inference of Neural Fluid Fields via SciML Foundation\n  Model","summary":"  Recent developments in 3D vision have enabled successful progress in\ninferring neural fluid fields and realistic rendering of fluid dynamics.\nHowever, these methods require real-world flow captures, which demand dense\nvideo sequences and specialized lab setups, making the process costly and\nchallenging. Scientific machine learning (SciML) foundation models, which are\npretrained on extensive simulations of partial differential equations (PDEs),\nencode rich multiphysics knowledge and thus provide promising sources of domain\npriors for inferring fluid fields. Nevertheless, their potential to advance\nreal-world vision problems remains largely underexplored, raising questions\nabout the transferability and practical utility of these foundation models. In\nthis work, we demonstrate that SciML foundation model can significantly improve\nthe data efficiency of inferring real-world 3D fluid dynamics with improved\ngeneralization. At the core of our method is leveraging the strong forecasting\ncapabilities and meaningful representations of SciML foundation models. We\nequip neural fluid fields with a novel collaborative training approach that\nutilizes augmented views and fluid features extracted by our foundation model.\nOur method demonstrates significant improvements in both quantitative metrics\nand visual quality, showcasing the practical applicability of SciML foundation\nmodels in real-world fluid dynamics.\n","authors":["Yuqiu Liu","Jingxuan Xu","Mauricio Soroco","Yunchao Wei","Wuyang Chen"],"pdf_url":"https://arxiv.org/pdf/2412.13897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13884v1","updated":"2024-12-18T14:23:54Z","published":"2024-12-18T14:23:54Z","title":"Navigating limitations with precision: A fine-grained ensemble approach\n  to wrist pathology recognition on a limited x-ray dataset","summary":"  The exploration of automated wrist fracture recognition has gained\nconsiderable research attention in recent years. In practical medical\nscenarios, physicians and surgeons may lack the specialized expertise required\nfor accurate X-ray interpretation, highlighting the need for machine vision to\nenhance diagnostic accuracy. However, conventional recognition techniques face\nchallenges in discerning subtle differences in X-rays when classifying wrist\npathologies, as many of these pathologies, such as fractures, can be small and\nhard to distinguish. This study tackles wrist pathology recognition as a\nfine-grained visual recognition (FGVR) problem, utilizing a limited,\ncustom-curated dataset that mirrors real-world medical constraints, relying\nsolely on image-level annotations. We introduce a specialized FGVR-based\nensemble approach to identify discriminative regions within X-rays. We employ\nan Explainable AI (XAI) technique called Grad-CAM to pinpoint these regions.\nOur ensemble approach outperformed many conventional SOTA and FGVR techniques,\nunderscoring the effectiveness of our strategy in enhancing accuracy in wrist\npathology recognition.\n","authors":["Ammar Ahmed","Ali Shariq Imran","Mohib Ullah","Zenun Kastrati","Sher Muhammad Daudpota"],"pdf_url":"https://arxiv.org/pdf/2412.13884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13875v1","updated":"2024-12-18T14:16:40Z","published":"2024-12-18T14:16:40Z","title":"Denoising Nearest Neighbor Graph via Continuous CRF for Visual\n  Re-ranking without Fine-tuning","summary":"  Visual re-ranking using Nearest Neighbor graph~(NN graph) has been adapted to\nyield high retrieval accuracy, since it is beneficial to exploring an\nhigh-dimensional manifold and applicable without additional fine-tuning. The\nquality of visual re-ranking using NN graph, however, is limited to that of\nconnectivity, i.e., edges of the NN graph. Some edges can be misconnected with\nnegative images. This is known as a noisy edge problem, resulting in a\ndegradation of the retrieval quality. To address this, we propose a\ncomplementary denoising method based on Continuous Conditional Random Field\n(C-CRF) that uses a statistical distance of our similarity-based distribution.\nThis method employs the concept of cliques to make the process computationally\nfeasible. We demonstrate the complementarity of our method through its\napplication to three visual re-ranking methods, observing quality boosts in\nlandmark retrieval and person re-identification (re-ID).\n","authors":["Jaeyoon Kim","Yoonki Cho","Taeyong Kim","Sung-Eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2412.13875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13871v1","updated":"2024-12-18T14:07:46Z","published":"2024-12-18T14:07:46Z","title":"LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via\n  Hierarchical Window Transformer","summary":"  In multimodal large language models (MLLMs), vision transformers (ViTs) are\nwidely employed for visual encoding. However, their performance in solving\nuniversal MLLM tasks is not satisfactory. We attribute it to a lack of\ninformation from diverse visual levels, impeding alignment with the various\nsemantic granularity required for language generation. To address this issue,\nwe present LLaVA-UHD v2, an advanced MLLM centered around a Hierarchical window\ntransformer that enables capturing diverse visual granularity by constructing\nand integrating a high-resolution feature pyramid. As a vision-language\nprojector, Hiwin transformer comprises two primary modules: (i) an inverse\nfeature pyramid, constructed by a ViT-derived feature up-sampling process\nutilizing high-frequency details from an image pyramid, and (ii) hierarchical\nwindow attention, focusing on a set of key sampling features within cross-scale\nwindows to condense multi-level feature maps. Extensive experiments demonstrate\nthat LLaVA-UHD v2 achieves superior performance over existing MLLMs on popular\nbenchmarks. Notably, our design brings an average boost of 3.7% across 14\nbenchmarks compared with the baseline method, 9.3% on DocVQA for instance. We\nmake all the data, model checkpoint, and code publicly available to facilitate\nfuture research.\n","authors":["Yipeng Zhang","Yifan Liu","Zonghao Guo","Yidan Zhang","Xuesong Yang","Chi Chen","Jun Song","Bo Zheng","Yuan Yao","Zhiyuan Liu","Tat-Seng Chua","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2412.13871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20008v2","updated":"2024-12-18T13:54:02Z","published":"2024-05-30T12:45:34Z","title":"Sharing Key Semantics in Transformer Makes Efficient Image Restoration","summary":"  Image Restoration (IR), a classic low-level vision task, has witnessed\nsignificant advancements through deep models that effectively model global\ninformation. Notably, the emergence of Vision Transformers (ViTs) has further\npropelled these advancements. When computing, the self-attention mechanism, a\ncornerstone of ViTs, tends to encompass all global cues, even those from\nsemantically unrelated objects or regions. This inclusivity introduces\ncomputational inefficiencies, particularly noticeable with high input\nresolution, as it requires processing irrelevant information, thereby impeding\nefficiency. Additionally, for IR, it is commonly noted that small segments of a\ndegraded image, particularly those closely aligned semantically, provide\nparticularly relevant information to aid in the restoration process, as they\ncontribute essential contextual cues crucial for accurate reconstruction. To\naddress these challenges, we propose boosting IR's performance by sharing the\nkey semantics via Transformer for IR (\\ie, SemanIR) in this paper.\nSpecifically, SemanIR initially constructs a sparse yet comprehensive\nkey-semantic dictionary within each transformer stage by establishing essential\nsemantic connections for every degraded patch. Subsequently, this dictionary is\nshared across all subsequent transformer blocks within the same stage. This\nstrategy optimizes attention calculation within each block by focusing\nexclusively on semantically related components stored in the key-semantic\ndictionary. As a result, attention calculation achieves linear computational\ncomplexity within each window. Extensive experiments across 6 IR tasks confirm\nthe proposed SemanIR's state-of-the-art performance, quantitatively and\nqualitatively showcasing advancements. The visual results, code, and trained\nmodels are available at https://github.com/Amazingren/SemanIR.\n","authors":["Bin Ren","Yawei Li","Jingyun Liang","Rakesh Ranjan","Mengyuan Liu","Rita Cucchiara","Luc Van Gool","Ming-Hsuan Yang","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2405.20008v2.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2412.13859v1","updated":"2024-12-18T13:53:16Z","published":"2024-12-18T13:53:16Z","title":"Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image\n  Classification Using Large Language Models","summary":"  Classifying scanned documents is a challenging problem that involves image,\nlayout, and text analysis for document understanding. Nevertheless, for certain\nbenchmark datasets, notably RVL-CDIP, the state of the art is closing in to\nnear-perfect performance when considering hundreds of thousands of training\nsamples. With the advent of large language models (LLMs), which are excellent\nfew-shot learners, the question arises to what extent the document\nclassification problem can be addressed with only a few training samples, or\neven none at all. In this paper, we investigate this question in the context of\nzero-shot prompting and few-shot model fine-tuning, with the aim of reducing\nthe need for human-annotated training samples as much as possible.\n","authors":["Anna Scius-Bertrand","Michael Jungo","Lars Vögtlin","Jean-Marc Spat","Andreas Fischer"],"pdf_url":"https://arxiv.org/pdf/2412.13859v1.pdf","comment":"ICPR 2024"},{"id":"http://arxiv.org/abs/2304.06376v2","updated":"2024-12-18T13:53:04Z","published":"2023-04-13T10:01:29Z","title":"Signal Reconstruction from Samples at Unknown Locations with Application\n  to 2D Unknown View Tomography","summary":"  It is well known that a band-limited signal can be reconstructed from its\nuniformly spaced samples if the sampling rate is sufficiently high. More\nrecently, it has been proved that one can reconstruct a 1D band-limited signal\neven if the exact sample locations are unknown, but given a uniform\ndistribution of the sample locations and their ordering in 1D. In this work, we\nextend the analytical error bounds in such scenarios for quasi-bandlimited\n(QBL) signals, and for the case of arbitrary but known sampling distributions.\nWe also prove that such reconstruction methods are resilient to a certain\nproportion of errors in the specification of the sample location ordering. We\nthen express the problem of tomographic reconstruction of 2D images from 1D\nRadon projections under unknown angles (2D UVT) with known angle distribution,\nas a special case for reconstruction of QBL signals from samples at unknown\nlocations with known distribution. Building upon our theoretical background, we\npresent asymptotic bounds for 2D QBL image reconstruction from 1D Radon\nprojections in the unknown angles setting, and present an extensive set of\nsimulations to verify these bounds in varied parameter regimes. To the best of\nour knowledge, this is the first piece of work to perform such an analysis for\n2D UVT and explicitly relate it to advances in sampling theory, even though the\nassociated reconstruction algorithms have been known for a long time.\n","authors":["Sheel Shah","Kaishva Shah","Karthik S. Gurumoorthy","Ajit Rajwade"],"pdf_url":"https://arxiv.org/pdf/2304.06376v2.pdf","comment":"This is a preprint of a paper accepted to Signal Processing\n  (Elsevier)"},{"id":"http://arxiv.org/abs/2412.13857v1","updated":"2024-12-18T13:52:42Z","published":"2024-12-18T13:52:42Z","title":"Diagnosising Helicobacter pylori using AutoEncoders and Limited\n  Annotations through Anomalous Staining Patterns in IHC Whole Slide Images","summary":"  Purpose: This work addresses the detection of Helicobacter pylori (H. pylori)\nin histological images with immunohistochemical staining. This analysis is a\ntime demanding task, currently done by an expert pathologist that visually\ninspects the samples. Given the effort required to localise the pathogen in\nimages, a limited number of annotations might be available in an initial\nsetting. Our goal is to design an approach that, using a limited set of\nannotations, is capable of obtaining results good enough to be used as a\nsupport tool. Methods: We propose to use autoencoders to learn the latent\npatterns of healthy patches and formulate a specific measure of the\nreconstruction error of the image in HSV space. ROC analysis is used to set the\noptimal threshold of this measure and the percentage of positive patches in a\nsample that determines the presence of H. pylori. Results: Our method has been\ntested on an own database of 245 Whole Slide Images (WSI) having 117 cases\nwithout H. pylori and different density of the bacteria in the remaining ones.\nThe database has 1211 annotated patches, with only 163 positive patches. This\ndataset of positive annotations was used to train a baseline thresholding and\nan SVM using the features of a pre-trained RedNet18 and ViT models. A 10-fold\ncross-validation shows that our method has better performance with 91%\naccuracy, 86% sensitivity, 96% specificity and 0.97 AUC in the diagnosis of H.\npylori. Conclusion: Unlike classification approaches, our shallow autoencoder\nwith threshold adaptation for the detection of anomalous staining is able to\nachieve competitive results with a limited set of annotated data. This initial\napproach is good enough to be used as a guide for fast annotation of infected\npatches.\n","authors":["Pau Cano","Eva Musulen","Debora Gil"],"pdf_url":"https://arxiv.org/pdf/2412.13857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13856v1","updated":"2024-12-18T13:52:20Z","published":"2024-12-18T13:52:20Z","title":"A Systematic Analysis of Input Modalities for Fracture Classification of\n  the Paediatric Wrist","summary":"  Fractures, particularly in the distal forearm, are among the most common\ninjuries in children and adolescents, with approximately 800 000 cases treated\nannually in Germany. The AO/OTA system provides a structured fracture type\nclassification, which serves as the foundation for treatment decisions.\nAlthough accurately classifying fractures can be challenging, current deep\nlearning models have demonstrated performance comparable to that of experienced\nradiologists. While most existing approaches rely solely on radiographs, the\npotential impact of incorporating other additional modalities, such as\nautomatic bone segmentation, fracture location, and radiology reports, remains\nunderexplored. In this work, we systematically analyse the contribution of\nthese three additional information types, finding that combining them with\nradiographs increases the AUROC from 91.71 to 93.25. Our code is available on\nGitHub.\n","authors":["Ron Keuth","Maren Balks","Sebastian Tschauner","Ludger Tüshaus","Mattias Heinrich"],"pdf_url":"https://arxiv.org/pdf/2412.13856v1.pdf","comment":"Code available on\n  https://github.com/multimodallearning/AO_Classification"},{"id":"http://arxiv.org/abs/2404.09507v2","updated":"2024-12-18T13:50:13Z","published":"2024-04-15T06:58:09Z","title":"Clothes-Changing Person Re-Identification with Feasibility-Aware\n  Intermediary Matching","summary":"  Current clothes-changing person re-identification (re-id) approaches usually\nperform retrieval based on clothes-irrelevant features, while neglecting the\npotential of clothes-relevant features. However, we observe that relying solely\non clothes-irrelevant features for clothes-changing re-id is limited, since\nthey often lack adequate identity information and suffer from large intra-class\nvariations. On the contrary, clothes-relevant features can be used to discover\nsame-clothes intermediaries that possess informative identity clues. Based on\nthis observation, we propose a Feasibility-Aware Intermediary Matching (FAIM)\nframework to additionally utilize clothes-relevant features for retrieval.\nFirstly, an Intermediary Matching (IM) module is designed to perform an\nintermediary-assisted matching process. This process involves using\nclothes-relevant features to find informative intermediates, and then using\nclothes-irrelevant features of these intermediates to complete the matching.\nSecondly, in order to reduce the negative effect of low-quality intermediaries,\nan Intermediary-Based Feasibility Weighting (IBFW) module is designed to\nevaluate the feasibility of intermediary matching process by assessing the\nquality of intermediaries. Extensive experiments demonstrate that our method\noutperforms state-of-the-art methods on several widely-used clothes-changing\nre-id benchmarks.\n","authors":["Jiahe Zhao","Ruibing Hou","Hong Chang","Xinqian Gu","Bingpeng Ma","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2404.09507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01476v3","updated":"2024-12-18T13:47:38Z","published":"2024-06-03T16:05:25Z","title":"DreamPhysics: Learning Physics-Based 3D Dynamics with Video Diffusion\n  Priors","summary":"  Dynamic 3D interaction has been attracting a lot of attention recently.\nHowever, creating such 4D content remains challenging. One solution is to\nanimate 3D scenes with physics-based simulation, which requires manually\nassigning precise physical properties to the object or the simulated results\nwould become unnatural. Another solution is to learn the deformation of 3D\nobjects with the distillation of video generative models, which, however, tends\nto produce 3D videos with small and discontinuous motions due to the\ninappropriate extraction and application of physics priors. In this work, to\ncombine the strengths and complementing shortcomings of the above two\nsolutions, we propose to learn the physical properties of a material field with\nvideo diffusion priors, and then utilize a physics-based Material-Point-Method\n(MPM) simulator to generate 4D content with realistic motions. In particular,\nwe propose motion distillation sampling to emphasize video motion information\nduring distillation. In addition, to facilitate the optimization, we further\npropose a KAN-based material field with frame boosting. Experimental results\ndemonstrate that our method enjoys more realistic motions than\nstate-of-the-arts do.\n","authors":["Tianyu Huang","Haoze Zhang","Yihan Zeng","Zhilu Zhang","Hui Li","Wangmeng Zuo","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2406.01476v3.pdf","comment":"Accepted by AAAI 2025. Codes are released at:\n  https://github.com/tyhuang0428/DreamPhysics"},{"id":"http://arxiv.org/abs/2412.13848v1","updated":"2024-12-18T13:42:06Z","published":"2024-12-18T13:42:06Z","title":"MobiFuse: A High-Precision On-device Depth Perception System with\n  Multi-Data Fusion","summary":"  We present MobiFuse, a high-precision depth perception system on mobile\ndevices that combines dual RGB and Time-of-Flight (ToF) cameras. To achieve\nthis, we leverage physical principles from various environmental factors to\npropose the Depth Error Indication (DEI) modality, characterizing the depth\nerror of ToF and stereo-matching. Furthermore, we employ a progressive fusion\nstrategy, merging geometric features from ToF and stereo depth maps with depth\nerror features from the DEI modality to create precise depth maps.\nAdditionally, we create a new ToF-Stereo depth dataset, RealToF, to train and\nvalidate our model. Our experiments demonstrate that MobiFuse excels over\nbaselines by significantly reducing depth measurement errors by up to 77.7%. It\nalso showcases strong generalization across diverse datasets and proves\neffectiveness in two downstream tasks: 3D reconstruction and 3D segmentation.\nThe demo video of MobiFuse in real-life scenarios is available at the\nde-identified YouTube link(https://youtu.be/jy-Sp7T1LVs).\n","authors":["Jinrui Zhang","Deyu Zhang","Tingting Long","Wenxin Chen","Ju Ren","Yunxin Liu","Yudong Zhao","Yaoxue Zhang","Youngki Lee"],"pdf_url":"https://arxiv.org/pdf/2412.13848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13845v1","updated":"2024-12-18T13:38:06Z","published":"2024-12-18T13:38:06Z","title":"Do Language Models Understand Time?","summary":"  Large language models (LLMs) have revolutionized video-based computer vision\napplications, including action recognition, anomaly detection, and video\nsummarization. Videos inherently pose unique challenges, combining spatial\ncomplexity with temporal dynamics that are absent in static images or textual\ndata. Current approaches to video understanding with LLMs often rely on\npretrained video encoders to extract spatiotemporal features and text encoders\nto capture semantic meaning. These representations are integrated within LLM\nframeworks, enabling multimodal reasoning across diverse video tasks. However,\nthe critical question persists: Can LLMs truly understand the concept of time,\nand how effectively can they reason about temporal relationships in videos?\nThis work critically examines the role of LLMs in video processing, with a\nspecific focus on their temporal reasoning capabilities. We identify key\nlimitations in the interaction between LLMs and pretrained encoders, revealing\ngaps in their ability to model long-term dependencies and abstract temporal\nconcepts such as causality and event progression. Furthermore, we analyze\nchallenges posed by existing video datasets, including biases, lack of temporal\nannotations, and domain-specific limitations that constrain the temporal\nunderstanding of LLMs. To address these gaps, we explore promising future\ndirections, including the co-evolution of LLMs and encoders, the development of\nenriched datasets with explicit temporal labels, and innovative architectures\nfor integrating spatial, temporal, and semantic reasoning. By addressing these\nchallenges, we aim to advance the temporal comprehension of LLMs, unlocking\ntheir full potential in video analysis and beyond.\n","authors":["Xi Ding","Lei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13845v1.pdf","comment":"Research report"},{"id":"http://arxiv.org/abs/2412.12525v2","updated":"2024-12-18T13:37:48Z","published":"2024-12-17T04:33:31Z","title":"CREST: An Efficient Conjointly-trained Spike-driven Framework for\n  Event-based Object Detection Exploiting Spatiotemporal Dynamics","summary":"  Event-based cameras feature high temporal resolution, wide dynamic range, and\nlow power consumption, which is ideal for high-speed and low-light object\ndetection. Spiking neural networks (SNNs) are promising for event-based object\nrecognition and detection due to their spiking nature but lack efficient\ntraining methods, leading to gradient vanishing and high computational\ncomplexity, especially in deep SNNs. Additionally, existing SNN frameworks\noften fail to effectively handle multi-scale spatiotemporal features, leading\nto increased data redundancy and reduced accuracy. To address these issues, we\npropose CREST, a novel conjointly-trained spike-driven framework to exploit\nspatiotemporal dynamics in event-based object detection. We introduce the\nconjoint learning rule to accelerate SNN learning and alleviate gradient\nvanishing. It also supports dual operation modes for efficient and flexible\nimplementation on different hardware types. Additionally, CREST features a\nfully spike-driven framework with a multi-scale spatiotemporal event integrator\n(MESTOR) and a spatiotemporal-IoU (ST-IoU) loss. Our approach achieves superior\nobject recognition & detection performance and up to 100X energy efficiency\ncompared with state-of-the-art SNN algorithms on three datasets, providing an\nefficient solution for event-based object detection algorithms suitable for SNN\nhardware implementation.\n","authors":["Ruixin Mao","Aoyu Shen","Lin Tang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12525v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2407.21534v4","updated":"2024-12-18T13:12:29Z","published":"2024-07-31T11:40:29Z","title":"ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large\n  Language Models","summary":"  In this work, we propose a training-free method to inject visual referring\ninto Multimodal Large Language Models (MLLMs) through learnable visual token\noptimization. We observe the relationship between text prompt tokens and visual\ntokens in MLLMs, where attention layers model the connection between them. Our\napproach involves adjusting visual tokens from the MLP output during inference,\ncontrolling which text prompt tokens attend to which visual tokens. We optimize\na learnable visual token based on an energy function, enhancing the strength of\nreferential regions in the attention map. This enables detailed region\ndescription and reasoning without the need for substantial training costs or\nmodel retraining. Our method offers a promising direction for integrating\nreferential abilities into MLLMs. Our method support referring with box, mask,\nscribble and point. The results demonstrate that our method exhibits\ncontrollability and interpretability.\n","authors":["Mingrui Wu","Xinyue Cai","Jiayi Ji","Jiale Li","Oucheng Huang","Gen Luo","Hao Fei","Guannan Jiang","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2407.21534v4.pdf","comment":"Accepted to NeurIPS 2024;\n  Code:https://github.com/mrwu-mac/ControlMLLM"},{"id":"http://arxiv.org/abs/2412.13823v1","updated":"2024-12-18T13:11:58Z","published":"2024-12-18T13:11:58Z","title":"Prompt Categories Cluster for Weakly Supervised Semantic Segmentation","summary":"  Weakly Supervised Semantic Segmentation (WSSS), which leverages image-level\nlabels, has garnered significant attention due to its cost-effectiveness. The\nprevious methods mainly strengthen the inter-class differences to avoid class\nsemantic ambiguity which may lead to erroneous activation. However, they\noverlook the positive function of some shared information between similar\nclasses. Categories within the same cluster share some similar features.\nAllowing the model to recognize these features can further relieve the semantic\nambiguity between these classes. To effectively identify and utilize this\nshared information, in this paper, we introduce a novel WSSS framework called\nPrompt Categories Clustering (PCC). Specifically, we explore the ability of\nLarge Language Models (LLMs) to derive category clusters through prompts. These\nclusters effectively represent the intrinsic relationships between categories.\nBy integrating this relational information into the training network, our model\nis able to better learn the hidden connections between categories. Experimental\nresults demonstrate the effectiveness of our approach, showing its ability to\nenhance performance on the PASCAL VOC 2012 dataset and surpass existing\nstate-of-the-art methods in WSSS.\n","authors":["Wangyu Wu","Xianglin Qiu","Siqi Song","Xiaowei Huang","Fei Ma","Jimin Xiao"],"pdf_url":"https://arxiv.org/pdf/2412.13823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17686v2","updated":"2024-12-18T13:09:20Z","published":"2024-09-26T09:51:11Z","title":"MoGenTS: Motion Generation based on Spatial-Temporal Joint Modeling","summary":"  Motion generation from discrete quantization offers many advantages over\ncontinuous regression, but at the cost of inevitable approximation errors.\nPrevious methods usually quantize the entire body pose into one code, which not\nonly faces the difficulty in encoding all joints within one vector but also\nloses the spatial relationship between different joints. Differently, in this\nwork we quantize each individual joint into one vector, which i) simplifies the\nquantization process as the complexity associated with a single joint is\nmarkedly lower than that of the entire pose; ii) maintains a spatial-temporal\nstructure that preserves both the spatial relationships among joints and the\ntemporal movement patterns; iii) yields a 2D token map, which enables the\napplication of various 2D operations widely used in 2D images. Grounded in the\n2D motion quantization, we build a spatial-temporal modeling framework, where\n2D joint VQVAE, temporal-spatial 2D masking technique, and spatial-temporal 2D\nattention are proposed to take advantage of spatial-temporal signals among the\n2D tokens. Extensive experiments demonstrate that our method significantly\noutperforms previous methods across different datasets, with a 26.6% decrease\nof FID on HumanML3D and a 29.9% decrease on KIT-ML. Project page:\nhttps://aigc3d.github.io/mogents.\n","authors":["Weihao Yuan","Weichao Shen","Yisheng He","Yuan Dong","Xiaodong Gu","Zilong Dong","Liefeng Bo","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2409.17686v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.15105v2","updated":"2024-12-18T13:08:47Z","published":"2024-10-19T13:37:24Z","title":"Standardizing Generative Face Video Compression using Supplemental\n  Enhancement Information","summary":"  This paper proposes a Generative Face Video Compression (GFVC) approach using\nSupplemental Enhancement Information (SEI), where a series of compact spatial\nand temporal representations of a face video signal (i.e., 2D/3D key-points,\nfacial semantics and compact features) can be coded using SEI message and\ninserted into the coded video bitstream. At the time of writing, the proposed\nGFVC approach using SEI messages has been adopted into the official working\ndraft of Versatile Supplemental Enhancement Information (VSEI) standard by the\nJoint Video Experts Team (JVET) of ISO/IEC JTC 1/SC 29 and ITU-T SG16, which\nwill be standardized as a new version for \"ITU-T H.274 | ISO/IEC 23002-7\". To\nthe best of the authors' knowledge, the JVET work on the proposed SEI-based\nGFVC approach is the first standardization activity for generative video\ncompression. The proposed SEI approach has not only advanced the reconstruction\nquality of early-day Model-Based Coding (MBC) via the state-of-the-art\ngenerative technique, but also established a new SEI definition for future GFVC\napplications and deployment. Experimental results illustrate that the proposed\nSEI-based GFVC approach can achieve remarkable rate-distortion performance\ncompared with the latest Versatile Video Coding (VVC) standard, whilst also\npotentially enabling a wide variety of functionalities including user-specified\nanimation/filtering and metaverse-related applications.\n","authors":["Bolin Chen","Yan Ye","Jie Chen","Ru-Ling Liao","Shanzhi Yin","Shiqi Wang","Kaifa Yang","Yue Li","Yiling Xu","Ye-Kui Wang","Shiv Gehlot","Guan-Ming Su","Peng Yin","Sean McCarthy","Gary J. Sullivan"],"pdf_url":"https://arxiv.org/pdf/2410.15105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13817v1","updated":"2024-12-18T13:04:30Z","published":"2024-12-18T13:04:30Z","title":"Nullu: Mitigating Object Hallucinations in Large Vision-Language Models\n  via HalluSpace Projection","summary":"  Recent studies have shown that large vision-language models (LVLMs) often\nsuffer from the issue of object hallucinations (OH). To mitigate this issue, we\nintroduce an efficient method that edits the model weights based on an unsafe\nsubspace, which we call HalluSpace in this paper. With truthful and\nhallucinated text prompts accompanying the visual content as inputs, the\nHalluSpace can be identified by extracting the hallucinated embedding features\nand removing the truthful representations in LVLMs. By orthogonalizing the\nmodel weights, input features will be projected into the Null space of the\nHalluSpace to reduce OH, based on which we name our method Nullu. We reveal\nthat HalluSpaces generally contain statistical bias and unimodal priors of the\nlarge language models (LLMs) applied to build LVLMs, which have been shown as\nessential causes of OH in previous studies. Therefore, null space projection\nsuppresses the LLMs' priors to filter out the hallucinated features, resulting\nin contextually accurate outputs. Experiments show that our method can\neffectively mitigate OH across different LVLM families without extra inference\ncosts and also show strong performance in general LVLM benchmarks. Code is\nreleased at \\url{https://github.com/Ziwei-Zheng/Nullu}.\n","authors":["Le Yang","Ziwei Zheng","Boxu Chen","Zhengyu Zhao","Chenhao Lin","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2412.13817v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2412.13815v1","updated":"2024-12-18T13:03:00Z","published":"2024-12-18T13:03:00Z","title":"Object Style Diffusion for Generalized Object Detection in Urban Scene","summary":"  Object detection is a critical task in computer vision, with applications in\nvarious domains such as autonomous driving and urban scene monitoring. However,\ndeep learning-based approaches often demand large volumes of annotated data,\nwhich are costly and difficult to acquire, particularly in complex and\nunpredictable real-world environments. This dependency significantly hampers\nthe generalization capability of existing object detection techniques. To\naddress this issue, we introduce a novel single-domain object detection\ngeneralization method, named GoDiff, which leverages a pre-trained model to\nenhance generalization in unseen domains. Central to our approach is the Pseudo\nTarget Data Generation (PTDG) module, which employs a latent diffusion model to\ngenerate pseudo-target domain data that preserves source domain characteristics\nwhile introducing stylistic variations. By integrating this pseudo data with\nsource domain data, we diversify the training dataset. Furthermore, we\nintroduce a cross-style instance normalization technique to blend style\nfeatures from different domains generated by the PTDG module, thereby\nincreasing the detector's robustness. Experimental results demonstrate that our\nmethod not only enhances the generalization ability of existing detectors but\nalso functions as a plug-and-play enhancement for other single-domain\ngeneralization methods, achieving state-of-the-art performance in autonomous\ndriving scenarios.\n","authors":["Hao Li","Xiangyuan Yang","Mengzhu Wang","Long Lan","Ke Liang","Xinwang Liu","Kenli Li"],"pdf_url":"https://arxiv.org/pdf/2412.13815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12888v2","updated":"2024-12-18T13:01:11Z","published":"2024-12-17T13:12:31Z","title":"ArtAug: Enhancing Text-to-Image Generation through\n  Synthesis-Understanding Interaction","summary":"  The emergence of diffusion models has significantly advanced image synthesis.\nThe recent studies of model interaction and self-corrective reasoning approach\nin large language models offer new insights for enhancing text-to-image models.\nInspired by these studies, we propose a novel method called ArtAug for\nenhancing text-to-image models in this paper. To the best of our knowledge,\nArtAug is the first one that improves image synthesis models via model\ninteractions with understanding models. In the interactions, we leverage human\npreferences implicitly learned by image understanding models to provide\nfine-grained suggestions for image synthesis models. The interactions can\nmodify the image content to make it aesthetically pleasing, such as adjusting\nexposure, changing shooting angles, and adding atmospheric effects. The\nenhancements brought by the interaction are iteratively fused into the\nsynthesis model itself through an additional enhancement module. This enables\nthe synthesis model to directly produce aesthetically pleasing images without\nany extra computational cost. In the experiments, we train the ArtAug\nenhancement module on existing text-to-image models. Various evaluation metrics\nconsistently demonstrate that ArtAug enhances the generative capabilities of\ntext-to-image models without incurring additional computational costs. The\nsource code and models will be released publicly.\n","authors":["Zhongjie Duan","Qianyi Zhao","Cen Chen","Daoyuan Chen","Wenmeng Zhou","Yaliang Li","Yingda Chen"],"pdf_url":"https://arxiv.org/pdf/2412.12888v2.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.13811v1","updated":"2024-12-18T12:58:38Z","published":"2024-12-18T12:58:38Z","title":"Spatial Brain Tumor Concentration Estimation for Individualized\n  Radiotherapy Planning","summary":"  Biophysical modeling of brain tumors has emerged as a promising strategy for\npersonalizing radiotherapy planning by estimating the otherwise hidden\ndistribution of tumor cells within the brain. However, many existing\nstate-of-the-art methods are computationally intensive, limiting their\nwidespread translation into clinical practice. In this work, we propose an\nefficient and direct method that utilizes soft physical constraints to estimate\nthe tumor cell concentration from preoperative MRI of brain tumor patients. Our\napproach optimizes a 3D tumor concentration field by simultaneously minimizing\nthe difference between the observed MRI and a physically informed loss\nfunction. Compared to existing state-of-the-art techniques, our method\nsignificantly improves predicting tumor recurrence on two public datasets with\na total of 192 patients while maintaining a clinically viable runtime of under\none minute - a substantial reduction from the 30 minutes required by the\ncurrent best approach. Furthermore, we showcase the generalizability of our\nframework by incorporating additional imaging information and physical\nconstraints, highlighting its potential to translate to various medical\ndiffusion phenomena with imperfect data.\n","authors":["Jonas Weidner","Michal Balcerak","Ivan Ezhov","André Datchev","Laurin Lux","Lucas Zimmerand Daniel Rueckert","Björn Menze","Benedikt Wiestler"],"pdf_url":"https://arxiv.org/pdf/2412.13811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13810v1","updated":"2024-12-18T12:57:56Z","published":"2024-12-18T12:57:56Z","title":"CAD-Assistant: Tool-Augmented VLLMs as Generic CAD Task Solvers?","summary":"  We propose CAD-Assistant, a general-purpose CAD agent for AI-assisted design.\nOur approach is based on a powerful Vision and Large Language Model (VLLM) as a\nplanner and a tool-augmentation paradigm using CAD-specific modules.\nCAD-Assistant addresses multimodal user queries by generating actions that are\niteratively executed on a Python interpreter equipped with the FreeCAD\nsoftware, accessed via its Python API. Our framework is able to assess the\nimpact of generated CAD commands on geometry and adapts subsequent actions\nbased on the evolving state of the CAD design. We consider a wide range of\nCAD-specific tools including Python libraries, modules of the FreeCAD Python\nAPI, helpful routines, rendering functions and other specialized modules. We\nevaluate our method on multiple CAD benchmarks and qualitatively demonstrate\nthe potential of tool-augmented VLLMs as generic CAD task solvers across\ndiverse CAD workflows.\n","authors":["Dimitrios Mallis","Ahmet Serdar Karadeniz","Sebastian Cavada","Danila Rukhovich","Niki Foteinopoulou","Kseniya Cherenkova","Anis Kacem","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2412.13810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04377v2","updated":"2024-12-18T12:55:49Z","published":"2024-12-05T17:52:35Z","title":"A Hitchhiker's Guide to Understanding Performances of Two-Class\n  Classifiers","summary":"  Properly understanding the performances of classifiers is essential in\nvarious scenarios. However, the literature often relies only on one or two\nstandard scores to compare classifiers, which fails to capture the nuances of\napplication-specific requirements, potentially leading to suboptimal classifier\nselection. Recently, a paper on the foundations of the theory of\nperformance-based ranking introduced a tool, called the Tile, that organizes an\ninfinity of ranking scores into a 2D map. Thanks to the Tile, it is now\npossible to evaluate and compare classifiers efficiently, displaying all\npossible application-specific preferences instead of having to rely on a pair\nof scores. In this paper, we provide a first hitchhiker's guide for\nunderstanding the performances of two-class classifiers by presenting four\nscenarios, each showcasing a different user profile: a theoretical analyst, a\nmethod designer, a benchmarker, and an application developer. Particularly, we\nshow that we can provide different interpretative flavors that are adapted to\nthe user's needs by mapping different values on the Tile. As an illustration,\nwe leverage the newly introduced Tile tool and the different flavors to rank\nand analyze the performances of 74 state-of-the-art semantic segmentation\nmodels in two-class classification through the eyes of the four user profiles.\nThrough these user profiles, we demonstrate that the Tile effectively captures\nthe behavior of classifiers in a single visualization, while accommodating an\ninfinite number of ranking scores.\n","authors":["Anaïs Halin","Sébastien Piérard","Anthony Cioppa","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2412.04377v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04309v2","updated":"2024-12-18T12:50:29Z","published":"2024-12-05T16:27:59Z","title":"The Tile: A 2D Map of Ranking Scores for Two-Class Classification","summary":"  In the computer vision and machine learning communities, as well as in many\nother research domains, rigorous evaluation of any new method, including\nclassifiers, is essential. One key component of the evaluation process is the\nability to compare and rank methods. However, ranking classifiers and\naccurately comparing their performances, especially when taking\napplication-specific preferences into account, remains challenging. For\ninstance, commonly used evaluation tools like Receiver Operating Characteristic\n(ROC) and Precision/Recall (PR) spaces display performances based on two\nscores. Hence, they are inherently limited in their ability to compare\nclassifiers across a broader range of scores and lack the capability to\nestablish a clear ranking among classifiers. In this paper, we present a novel\nversatile tool, named the Tile, that organizes an infinity of ranking scores in\na single 2D map for two-class classifiers, including common evaluation scores\nsuch as the accuracy, the true positive rate, the positive predictive value,\nJaccard's coefficient, and all F-beta scores. Furthermore, we study the\nproperties of the underlying ranking scores, such as the influence of the\npriors or the correspondences with the ROC space, and depict how to\ncharacterize any other score by comparing them to the Tile. Overall, we\ndemonstrate that the Tile is a powerful tool that effectively captures all the\nrankings in a single visualization and allows interpreting them.\n","authors":["Sébastien Piérard","Anaïs Halin","Anthony Cioppa","Adrien Deliège","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2412.04309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13803v1","updated":"2024-12-18T12:50:11Z","published":"2024-12-18T12:50:11Z","title":"M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object\n  Segmentation","summary":"  Intelligent robots need to interact with diverse objects across various\nenvironments. The appearance and state of objects frequently undergo complex\ntransformations depending on the object properties, e.g., phase transitions.\nHowever, in the vision community, segmenting dynamic objects with phase\ntransitions is overlooked. In light of this, we introduce the concept of phase\nin segmentation, which categorizes real-world objects based on their visual\ncharacteristics and potential morphological and appearance changes. Then, we\npresent a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video\nObject Segmentation (M3-VOS), to verify the ability of models to understand\nobject phases, which consists of 479 high-resolution videos spanning over 10\ndistinct everyday scenarios. It provides dense instance mask annotations that\ncapture both object phases and their transitions. We evaluate state-of-the-art\nmethods on M3-VOS, yielding several key insights. Notably, current appearance\nbased approaches show significant room for improvement when handling objects\nwith phase transitions. The inherent changes in disorder suggest that the\npredictive performance of the forward entropy-increasing process can be\nimproved through a reverse entropy-reducing process. These findings lead us to\npropose ReVOS, a new plug-and-play model that improves its performance by\nreversal refinement. Our data and code will be publicly available\n","authors":["Zixuan Chen","Jiaxin Li","Liming Tan","Yejie Guo","Junxuan Liang","Cewu Lu","Yonglu Li"],"pdf_url":"https://arxiv.org/pdf/2412.13803v1.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.04227v2","updated":"2024-12-18T12:45:58Z","published":"2024-12-05T15:05:25Z","title":"Foundations of the Theory of Performance-Based Ranking","summary":"  Ranking entities such as algorithms, devices, methods, or models based on\ntheir performances, while accounting for application-specific preferences, is a\nchallenge. To address this challenge, we establish the foundations of a\nuniversal theory for performance-based ranking. First, we introduce a rigorous\nframework built on top of both the probability and order theories. Our new\nframework encompasses the elements necessary to (1) manipulate performances as\nmathematical objects, (2) express which performances are worse than or\nequivalent to others, (3) model tasks through a variable called satisfaction,\n(4) consider properties of the evaluation, (5) define scores, and (6) specify\napplication-specific preferences through a variable called importance. On top\nof this framework, we propose the first axiomatic definition of performance\norderings and performance-based rankings. Then, we introduce a universal\nparametric family of scores, called ranking scores, that can be used to\nestablish rankings satisfying our axioms, while considering\napplication-specific preferences. Finally, we show, in the case of two-class\nclassification, that the family of ranking scores encompasses well-known\nperformance scores, including the accuracy, the true positive rate (recall,\nsensitivity), the true negative rate (specificity), the positive predictive\nvalue (precision), and F1. However, we also show that some other scores\ncommonly used to compare classifiers are unsuitable to derive performance\norderings satisfying the axioms. Therefore, this paper provides the computer\nvision and machine learning communities with a rigorous framework for\nevaluating and ranking entities.\n","authors":["Sébastien Piérard","Anaïs Halin","Anthony Cioppa","Adrien Deliège","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2412.04227v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00705v2","updated":"2024-12-18T12:26:03Z","published":"2024-12-01T07:02:36Z","title":"Photoacoustic Iterative Optimization Algorithm with Shape Prior\n  Regularization","summary":"  Photoacoustic imaging (PAI) suffers from inherent limitations that can\ndegrade the quality of reconstructed results, such as noise, artifacts and\nincomplete data acquisition caused by sparse sampling or partial array\ndetection. In this study, we proposed a new optimization method for both\ntwo-dimensional (2D) and three-dimensional (3D) PAI reconstruction results,\ncalled the regularized iteration method with shape prior. The shape prior is a\nprobability matrix derived from the reconstruction results of multiple sets of\nrandom partial array signals in a computational imaging system using any\nreconstruction algorithm, such as Delay-and-Sum (DAS) and Back-Projection (BP).\nIn the probability matrix, high-probability locations indicate high consistency\namong multiple reconstruction results at those positions, suggesting a high\nlikelihood of representing the true imaging results. In contrast,\nlow-probability locations indicate higher randomness, leaning more towards\nnoise or artifacts. As a shape prior, this probability matrix guides the\niteration and regularization of the entire array signal reconstruction results\nusing the original reconstruction algorithm (the same algorithm for processing\nrandom partial array signals). The method takes advantage of the property that\nthe similarity of the object to be imitated is higher than that of noise or\nartifact in the results reconstructed by multiple sets of random partial array\nsignals of the entire imaging system. The probability matrix is taken as a\nprerequisite for improving the original reconstruction results, and the\noptimizer is used to further iterate the imaging results to remove noise and\nartifacts and improve the imaging fidelity. Especially in the case involving\nsparse view which brings more artifacts, the effect is remarkable. Simulation\nand real experiments have both demonstrated the superiority of this method.\n","authors":["Yu Zhang","Shuang Li","Yibing Wang","Yu Sun","Wenyi Xiang"],"pdf_url":"https://arxiv.org/pdf/2412.00705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13772v1","updated":"2024-12-18T12:10:33Z","published":"2024-12-18T12:10:33Z","title":"An Efficient Occupancy World Model via Decoupled Dynamic Flow and\n  Image-assisted Training","summary":"  The field of autonomous driving is experiencing a surge of interest in world\nmodels, which aim to predict potential future scenarios based on historical\nobservations. In this paper, we introduce DFIT-OccWorld, an efficient 3D\noccupancy world model that leverages decoupled dynamic flow and image-assisted\ntraining strategy, substantially improving 4D scene forecasting performance. To\nsimplify the training process, we discard the previous two-stage training\nstrategy and innovatively reformulate the occupancy forecasting problem as a\ndecoupled voxels warping process. Our model forecasts future dynamic voxels by\nwarping existing observations using voxel flow, whereas static voxels are\neasily obtained through pose transformation. Moreover, our method incorporates\nan image-assisted training paradigm to enhance prediction reliability.\nSpecifically, differentiable volume rendering is adopted to generate rendered\ndepth maps through predicted future volumes, which are adopted in render-based\nphotometric consistency. Experiments demonstrate the effectiveness of our\napproach, showcasing its state-of-the-art performance on the nuScenes and\nOpenScene benchmarks for 4D occupancy forecasting, end-to-end motion planning\nand point cloud forecasting. Concretely, it achieves state-of-the-art\nperformances compared to existing 3D world models while incurring substantially\nlower computational costs.\n","authors":["Haiming Zhang","Ying Xue","Xu Yan","Jiacheng Zhang","Weichao Qiu","Dongfeng Bai","Bingbing Liu","Shuguang Cui","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2412.13772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09954v2","updated":"2024-12-18T11:51:45Z","published":"2024-12-13T08:24:12Z","title":"A2RNet: Adversarial Attack Resilient Network for Robust Infrared and\n  Visible Image Fusion","summary":"  Infrared and visible image fusion (IVIF) is a crucial technique for enhancing\nvisual performance by integrating unique information from different modalities\ninto one fused image. Exiting methods pay more attention to conducting fusion\nwith undisturbed data, while overlooking the impact of deliberate interference\non the effectiveness of fusion results. To investigate the robustness of fusion\nmodels, in this paper, we propose a novel adversarial attack resilient network,\ncalled $\\textrm{A}^{\\textrm{2}}$RNet. Specifically, we develop an adversarial\nparadigm with an anti-attack loss function to implement adversarial attacks and\ntraining. It is constructed based on the intrinsic nature of IVIF and provide a\nrobust foundation for future research advancements. We adopt a Unet as the\npipeline with a transformer-based defensive refinement module (DRM) under this\nparadigm, which guarantees fused image quality in a robust coarse-to-fine\nmanner. Compared to previous works, our method mitigates the adverse effects of\nadversarial perturbations, consistently maintaining high-fidelity fusion\nresults. Furthermore, the performance of downstream tasks can also be well\nmaintained under adversarial attacks. Code is available at\nhttps://github.com/lok-18/A2RNet.\n","authors":["Jiawei Li","Hongwei Yu","Jiansheng Chen","Xinlong Ding","Jinlong Wang","Jinyuan Liu","Bochao Zou","Huimin Ma"],"pdf_url":"https://arxiv.org/pdf/2412.09954v2.pdf","comment":"9 pages, 8 figures, The 39th Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2412.13753v1","updated":"2024-12-18T11:43:41Z","published":"2024-12-18T11:43:41Z","title":"Mesoscopic Insights: Orchestrating Multi-scale & Hybrid Architecture for\n  Image Manipulation Localization","summary":"  The mesoscopic level serves as a bridge between the macroscopic and\nmicroscopic worlds, addressing gaps overlooked by both. Image manipulation\nlocalization (IML), a crucial technique to pursue truth from fake images, has\nlong relied on low-level (microscopic-level) traces. However, in practice, most\ntampering aims to deceive the audience by altering image semantics. As a\nresult, manipulation commonly occurs at the object level (macroscopic level),\nwhich is equally important as microscopic traces. Therefore, integrating these\ntwo levels into the mesoscopic level presents a new perspective for IML\nresearch. Inspired by this, our paper explores how to simultaneously construct\nmesoscopic representations of micro and macro information for IML and\nintroduces the Mesorch architecture to orchestrate both. Specifically, this\narchitecture i) combines Transformers and CNNs in parallel, with Transformers\nextracting macro information and CNNs capturing micro details, and ii) explores\nacross different scales, assessing micro and macro information seamlessly.\nAdditionally, based on the Mesorch architecture, the paper introduces two\nbaseline models aimed at solving IML tasks through mesoscopic representation.\nExtensive experiments across four datasets have demonstrated that our models\nsurpass the current state-of-the-art in terms of performance, computational\ncomplexity, and robustness.\n","authors":["Xuekang Zhu","Xiaochen Ma","Lei Su","Zhuohang Jiang","Bo Du","Xiwen Wang","Zeyu Lei","Wentao Feng","Chi-Man Pun","Jizhe Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.13753v1.pdf","comment":"AAAI 2025. Code:\n  $\\href{https://github.com/scu-zjz/Mesorch}{this~url}$"},{"id":"http://arxiv.org/abs/2412.13749v1","updated":"2024-12-18T11:33:16Z","published":"2024-12-18T11:33:16Z","title":"Multi-Exposure Image Fusion via Distilled 3D LUT Grid with Editable Mode","summary":"  With the rising imaging resolution of handheld devices, existing\nmulti-exposure image fusion algorithms struggle to generate a high dynamic\nrange image with ultra-high resolution in real-time. Apart from that, there is\na trend to design a manageable and editable algorithm as the different needs of\nreal application scenarios. To tackle these issues, we introduce 3D LUT\ntechnology, which can enhance images with ultra-high-definition (UHD)\nresolution in real time on resource-constrained devices. However, since the\nfusion of information from multiple images with different exposure rates is\nuncertain, and this uncertainty significantly trials the generalization power\nof the 3D LUT grid. To address this issue and ensure a robust learning space\nfor the model, we propose using a teacher-student network to model the\nuncertainty on the 3D LUT grid.Furthermore, we provide an editable mode for the\nmulti-exposure image fusion algorithm by using the implicit representation\nfunction to match the requirements in different scenarios. Extensive\nexperiments demonstrate that our proposed method is highly competitive in\nefficiency and accuracy.\n","authors":["Xin Su","Zhuoran Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.13749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13742v1","updated":"2024-12-18T11:19:23Z","published":"2024-12-18T11:19:23Z","title":"Learnable Prompting SAM-induced Knowledge Distillation for\n  Semi-supervised Medical Image Segmentation","summary":"  The limited availability of labeled data has driven advancements in\nsemi-supervised learning for medical image segmentation. Modern large-scale\nmodels tailored for general segmentation, such as the Segment Anything Model\n(SAM), have revealed robust generalization capabilities. However, applying\nthese models directly to medical image segmentation still exposes performance\ndegradation. In this paper, we propose a learnable prompting SAM-induced\nKnowledge distillation framework (KnowSAM) for semi-supervised medical image\nsegmentation. Firstly, we propose a Multi-view Co-training (MC) strategy that\nemploys two distinct sub-networks to employ a co-teaching paradigm, resulting\nin more robust outcomes. Secondly, we present a Learnable Prompt Strategy (LPS)\nto dynamically produce dense prompts and integrate an adapter to fine-tune SAM\nspecifically for medical image segmentation tasks. Moreover, we propose\nSAM-induced Knowledge Distillation (SKD) to transfer useful knowledge from SAM\nto two sub-networks, enabling them to learn from SAM's predictions and\nalleviate the effects of incorrect pseudo-labels during training. Notably, the\npredictions generated by our subnets are used to produce mask prompts for SAM,\nfacilitating effective inter-module information exchange. Extensive\nexperimental results on various medical segmentation tasks demonstrate that our\nmodel outperforms the state-of-the-art semi-supervised segmentation approaches.\nCrucially, our SAM distillation framework can be seamlessly integrated into\nother semi-supervised segmentation methods to enhance performance. The code\nwill be released upon acceptance of this manuscript at:\nhttps://github.com/taozh2017/KnowSAM\n","authors":["Kaiwen Huang","Tao Zhou","Huazhu Fu","Yizhe Zhang","Yi Zhou","Chen Gong","Dong Liang"],"pdf_url":"https://arxiv.org/pdf/2412.13742v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.13736v1","updated":"2024-12-18T11:14:02Z","published":"2024-12-18T11:14:02Z","title":"MedCoT: Medical Chain of Thought via Hierarchical Expert","summary":"  Artificial intelligence has advanced in Medical Visual Question Answering\n(Med-VQA), but prevalent research tends to focus on the accuracy of the\nanswers, often overlooking the reasoning paths and interpretability, which are\ncrucial in clinical settings. Besides, current Med-VQA algorithms, typically\nreliant on singular models, lack the robustness needed for real-world medical\ndiagnostics which usually require collaborative expert evaluation. To address\nthese shortcomings, this paper presents MedCoT, a novel hierarchical expert\nverification reasoning chain method designed to enhance interpretability and\naccuracy in biomedical imaging inquiries. MedCoT is predicated on two\nprinciples: The necessity for explicit reasoning paths in Med-VQA and the\nrequirement for multi-expert review to formulate accurate conclusions. The\nmethodology involves an Initial Specialist proposing diagnostic rationales,\nfollowed by a Follow-up Specialist who validates these rationales, and finally,\na consensus is reached through a vote among a sparse Mixture of Experts within\nthe locally deployed Diagnostic Specialist, which then provides the definitive\ndiagnosis. Experimental evaluations on four standard Med-VQA datasets\ndemonstrate that MedCoT surpasses existing state-of-the-art approaches,\nproviding significant improvements in performance and interpretability.\n","authors":["Jiaxiang Liu","Yuan Wang","Jiawei Du","Joey Tianyi Zhou","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13735v1","updated":"2024-12-18T11:14:01Z","published":"2024-12-18T11:14:01Z","title":"3D Registration in 30 Years: A Survey","summary":"  3D point cloud registration is a fundamental problem in computer vision,\ncomputer graphics, robotics, remote sensing, and etc. Over the last thirty\nyears, we have witnessed the amazing advancement in this area with numerous\nkinds of solutions. Although a handful of relevant surveys have been conducted,\ntheir coverage is still limited. In this work, we present a comprehensive\nsurvey on 3D point cloud registration, covering a set of sub-areas such as\npairwise coarse registration, pairwise fine registration, multi-view\nregistration, cross-scale registration, and multi-instance registration. The\ndatasets, evaluation metrics, method taxonomy, discussions of the merits and\ndemerits, insightful thoughts of future directions are comprehensively\npresented in this survey. The regularly updated project page of the survey is\navailable at https://github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.\n","authors":["Jiaqi Yang","Chu'ai Zhang","Zhengbao Wang","Xinyue Cao","Xuan Ouyang","Xiyu Zhang","Zhenxuan Zeng","Zhao Zeng","Borui Lu","Zhiyi Xia","Qian Zhang","Yulan Guo","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09892v2","updated":"2024-12-18T11:12:49Z","published":"2024-12-13T06:14:57Z","title":"VQTalker: Towards Multilingual Talking Avatars through Facial Motion\n  Tokenization","summary":"  We present VQTalker, a Vector Quantization-based framework for multilingual\ntalking head generation that addresses the challenges of lip synchronization\nand natural motion across diverse languages. Our approach is grounded in the\nphonetic principle that human speech comprises a finite set of distinct sound\nunits (phonemes) and corresponding visual articulations (visemes), which often\nshare commonalities across languages. We introduce a facial motion tokenizer\nbased on Group Residual Finite Scalar Quantization (GRFSQ), which creates a\ndiscretized representation of facial features. This method enables\ncomprehensive capture of facial movements while improving generalization to\nmultiple languages, even with limited training data. Building on this quantized\nrepresentation, we implement a coarse-to-fine motion generation process that\nprogressively refines facial animations. Extensive experiments demonstrate that\nVQTalker achieves state-of-the-art performance in both video-driven and\nspeech-driven scenarios, particularly in multilingual settings. Notably, our\nmethod achieves high-quality results at a resolution of 512*512 pixels while\nmaintaining a lower bitrate of approximately 11 kbps. Our work opens new\npossibilities for cross-lingual talking face generation. Synthetic results can\nbe viewed at https://x-lance.github.io/VQTalker.\n","authors":["Tao Liu","Ziyang Ma","Qi Chen","Feilong Chen","Shuai Fan","Xie Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2412.09892v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2412.13734v1","updated":"2024-12-18T11:12:10Z","published":"2024-12-18T11:12:10Z","title":"Text2Relight: Creative Portrait Relighting with Text Guidance","summary":"  We present a lighting-aware image editing pipeline that, given a portrait\nimage and a text prompt, performs single image relighting. Our model modifies\nthe lighting and color of both the foreground and background to align with the\nprovided text description. The unbounded nature in creativeness of a text\nallows us to describe the lighting of a scene with any sensory features\nincluding temperature, emotion, smell, time, and so on. However, the modeling\nof such mapping between the unbounded text and lighting is extremely\nchallenging due to the lack of dataset where there exists no scalable data that\nprovides large pairs of text and relighting, and therefore, current text-driven\nimage editing models does not generalize to lighting-specific use cases. We\novercome this problem by introducing a novel data synthesis pipeline: First,\ndiverse and creative text prompts that describe the scenes with various\nlighting are automatically generated under a crafted hierarchy using a large\nlanguage model (*e.g.,* ChatGPT). A text-guided image generation model creates\na lighting image that best matches the text. As a condition of the lighting\nimages, we perform image-based relighting for both foreground and background\nusing a single portrait image or a set of OLAT (One-Light-at-A-Time) images\ncaptured from lightstage system. Particularly for the background relighting, we\nrepresent the lighting image as a set of point lights and transfer them to\nother background images. A generative diffusion model learns the synthesized\nlarge-scale data with auxiliary task augmentation (*e.g.,* portrait delighting\nand light positioning) to correlate the latent text and lighting distribution\nfor text-guided portrait relighting.\n","authors":["Junuk Cha","Mengwei Ren","Krishna Kumar Singh","He Zhang","Yannick Hold-Geoffroy","Seunghyun Yoon","HyunJoon Jung","Jae Shin Yoon","Seungryul Baek"],"pdf_url":"https://arxiv.org/pdf/2412.13734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13732v1","updated":"2024-12-18T11:10:18Z","published":"2024-12-18T11:10:18Z","title":"Modelling Multi-modal Cross-interaction for ML-FSIC Based on Local\n  Feature Selection","summary":"  The aim of multi-label few-shot image classification (ML-FSIC) is to assign\nsemantic labels to images, in settings where only a small number of training\nexamples are available for each label. A key feature of the multi-label setting\nis that images often have several labels, which typically refer to objects\nappearing in different regions of the image. When estimating label prototypes,\nin a metric-based setting, it is thus important to determine which regions are\nrelevant for which labels, but the limited amount of training data and the\nnoisy nature of local features make this highly challenging. As a solution, we\npropose a strategy in which label prototypes are gradually refined. First, we\ninitialize the prototypes using word embeddings, which allows us to leverage\nprior knowledge about the meaning of the labels. Second, taking advantage of\nthese initial prototypes, we then use a Loss Change Measurement~(LCM) strategy\nto select the local features from the training images (i.e.\\ the support set)\nthat are most likely to be representative of a given label. Third, we construct\nthe final prototype of the label by aggregating these representative local\nfeatures using a multi-modal cross-interaction mechanism, which again relies on\nthe initial word embedding-based prototypes. Experiments on COCO, PASCAL VOC,\nNUS-WIDE, and iMaterialist show that our model substantially improves the\ncurrent state-of-the-art.\n","authors":["Kun Yan","Zied Bouraoui","Fangyun Wei","Chang Xu","Ping Wang","Shoaib Jameel","Steven Schockaert"],"pdf_url":"https://arxiv.org/pdf/2412.13732v1.pdf","comment":"Accepted in Transactions on Multimedia Computing Communications and\n  Applications"},{"id":"http://arxiv.org/abs/2412.13726v1","updated":"2024-12-18T11:05:56Z","published":"2024-12-18T11:05:56Z","title":"Unified Understanding of Environment, Task, and Human for Human-Robot\n  Interaction in Real-World Environments","summary":"  To facilitate human--robot interaction (HRI) tasks in real-world scenarios,\nservice robots must adapt to dynamic environments and understand the required\ntasks while effectively communicating with humans. To accomplish HRI in\npractice, we propose a novel indoor dynamic map, task understanding system, and\nresponse generation system. The indoor dynamic map optimizes robot behavior by\nmanaging an occupancy grid map and dynamic information, such as furniture and\nhumans, in separate layers. The task understanding system targets tasks that\nrequire multiple actions, such as serving ordered items. Task representations\nthat predefine the flow of necessary actions are applied to achieve highly\naccurate understanding. The response generation system is executed in parallel\nwith task understanding to facilitate smooth HRI by informing humans of the\nsubsequent actions of the robot. In this study, we focused on waiter duties in\na restaurant setting as a representative application of HRI in a dynamic\nenvironment. We developed an HRI system that could perform tasks such as\nserving food and cleaning up while communicating with customers. In experiments\nconducted in a simulated restaurant environment, the proposed HRI system\nsuccessfully communicated with customers and served ordered food with 90\\%\naccuracy. In a questionnaire administered after the experiment, the HRI system\nof the robot received 4.2 points out of 5. These outcomes indicated the\neffectiveness of the proposed method and HRI system in executing waiter tasks\nin real-world environments.\n","authors":["Yuga Yano","Akinobu Mizutani","Yukiya Fukuda","Daiju Kanaoka","Tomohiro Ono","Hakaru Tamukoh"],"pdf_url":"https://arxiv.org/pdf/2412.13726v1.pdf","comment":"2024 33rd IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN)"},{"id":"http://arxiv.org/abs/2408.10360v2","updated":"2024-12-18T11:02:07Z","published":"2024-08-19T18:56:24Z","title":"HaSPeR: An Image Repository for Hand Shadow Puppet Recognition","summary":"  Hand shadow puppetry, also known as shadowgraphy or ombromanie, is a form of\ntheatrical art and storytelling where hand shadows are projected onto flat\nsurfaces to create illusions of living creatures. The skilled performers create\nthese silhouettes by hand positioning, finger movements, and dexterous gestures\nto resemble shadows of animals and objects. Due to the lack of practitioners\nand a seismic shift in people's entertainment standards, this art form is on\nthe verge of extinction. To facilitate its preservation and proliferate it to a\nwider audience, we introduce ${\\rm H{\\small A}SP{\\small E}R}$, a novel dataset\nconsisting of 15,000 images of hand shadow puppets across 15 classes extracted\nfrom both professional and amateur hand shadow puppeteer clips. We provide a\ndetailed statistical analysis of the dataset and employ a range of pretrained\nimage classification models to establish baselines. Our findings show a\nsubstantial performance superiority of skip-connected convolutional models over\nattention-based transformer architectures. We also find that lightweight\nmodels, such as MobileNetV2, suited for mobile applications and embedded\ndevices, perform comparatively well. We surmise that such low-latency\narchitectures can be useful in developing ombromanie teaching tools, and we\ncreate a prototype application to explore this surmission. Keeping the\nbest-performing model ResNet34 under the limelight, we conduct comprehensive\nfeature-spatial, explainability, and error analyses to gain insights into its\ndecision-making process. To the best of our knowledge, this is the first\ndocumented dataset and research endeavor to preserve this dying art for future\ngenerations, with computer vision approaches. Our code and data will be\npublicly available.\n","authors":["Syed Rifat Raiyan","Zibran Zarif Amio","Sabbir Ahmed"],"pdf_url":"https://arxiv.org/pdf/2408.10360v2.pdf","comment":"Submitted to IEEE Transactions on Artificial Intelligence (IEEE TAI),\n  13 pages, 105 figures, 2 tables"},{"id":"http://arxiv.org/abs/2412.13717v1","updated":"2024-12-18T10:55:58Z","published":"2024-12-18T10:55:58Z","title":"Towards Automatic Evaluation for Image Transcreation","summary":"  Beyond conventional paradigms of translating speech and text, recently, there\nhas been interest in automated transcreation of images to facilitate\nlocalization of visual content across different cultures. Attempts to define\nthis as a formal Machine Learning (ML) problem have been impeded by the lack of\nautomatic evaluation mechanisms, with previous work relying solely on human\nevaluation. In this paper, we seek to close this gap by proposing a suite of\nautomatic evaluation metrics inspired by machine translation (MT) metrics,\ncategorized into: a) Object-based, b) Embedding-based, and c) VLM-based.\nDrawing on theories from translation studies and real-world transcreation\npractices, we identify three critical dimensions of image transcreation:\ncultural relevance, semantic equivalence and visual similarity, and design our\nmetrics to evaluate systems along these axes. Our results show that proprietary\nVLMs best identify cultural relevance and semantic equivalence, while\nvision-encoder representations are adept at measuring visual similarity.\nMeta-evaluation across 7 countries shows our metrics agree strongly with human\nratings, with average segment-level correlations ranging from 0.55-0.87.\nFinally, through a discussion of the merits and demerits of each metric, we\noffer a robust framework for automated image transcreation evaluation, grounded\nin both theoretical foundations and practical application. Our code can be\nfound here: https://github.com/simran-khanuja/automatic-eval-transcreation\n","authors":["Simran Khanuja","Vivek Iyer","Claire He","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2412.13717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13709v1","updated":"2024-12-18T10:51:59Z","published":"2024-12-18T10:51:59Z","title":"Physics-Based Adversarial Attack on Near-Infrared Human Detector for\n  Nighttime Surveillance Camera Systems","summary":"  Many surveillance cameras switch between daytime and nighttime modes based on\nilluminance levels. During the day, the camera records ordinary RGB images\nthrough an enabled IR-cut filter. At night, the filter is disabled to capture\nnear-infrared (NIR) light emitted from NIR LEDs typically mounted around the\nlens. While RGB-based AI algorithm vulnerabilities have been widely reported,\nthe vulnerabilities of NIR-based AI have rarely been investigated. In this\npaper, we identify fundamental vulnerabilities in NIR-based image understanding\ncaused by color and texture loss due to the intrinsic characteristics of\nclothes' reflectance and cameras' spectral sensitivity in the NIR range. We\nfurther show that the nearly co-located configuration of illuminants and\ncameras in existing surveillance systems facilitates concealing and fully\npassive attacks in the physical world. Specifically, we demonstrate how\nretro-reflective and insulation plastic tapes can manipulate the intensity\ndistribution of NIR images. We showcase an attack on the YOLO-based human\ndetector using binary patterns designed in the digital space (via black-box\nquery and searching) and then physically realized using tapes pasted onto\nclothes. Our attack highlights significant reliability concerns for nighttime\nsurveillance systems, which are intended to enhance security. Codes Available:\nhttps://github.com/MyNiuuu/AdvNIR\n","authors":["Muyao Niu","Zhuoxiao Li","Yifan Zhan","Huy H. Nguyen","Isao Echizen","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.13709v1.pdf","comment":"Appeared in ACM MM 2023"},{"id":"http://arxiv.org/abs/2412.13708v1","updated":"2024-12-18T10:51:31Z","published":"2024-12-18T10:51:31Z","title":"JoVALE: Detecting Human Actions in Video Using Audiovisual and Language\n  Contexts","summary":"  Video Action Detection (VAD) involves localizing and categorizing action\ninstances in videos. Videos inherently contain various information sources,\nincluding audio, visual cues, and surrounding scene contexts. Effectively\nleveraging this multi-modal information for VAD is challenging, as the model\nmust accurately focus on action-relevant cues. In this study, we introduce a\nnovel multi-modal VAD architecture called the Joint Actor-centric Visual,\nAudio, Language Encoder (JoVALE). JoVALE is the first VAD method to integrate\naudio and visual features with scene descriptive context derived from large\nimage captioning models. The core principle of JoVALE is the actor-centric\naggregation of audio, visual, and scene descriptive contexts, where\naction-related cues from each modality are identified and adaptively combined.\nWe propose a specialized module called the Actor-centric Multi-modal Fusion\nNetwork, designed to capture the joint interactions among actors and\nmulti-modal contexts through Transformer architecture. Our evaluation conducted\non three popular VAD benchmarks, AVA, UCF101-24, and JHMDB51-21, demonstrates\nthat incorporating multi-modal information leads to significant performance\ngains. JoVALE achieves state-of-the-art performances. The code will be\navailable at \\texttt{https://github.com/taeiin/AAAI2025-JoVALE}.\n","authors":["Taein Son","Soo Won Seo","Jisong Kim","Seok Hwan Lee","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2412.13708v1.pdf","comment":"Accepted to AAAI Conference on Artificial Intelligence 2025, 9 pages,\n  5 figures"},{"id":"http://arxiv.org/abs/2412.13705v1","updated":"2024-12-18T10:49:41Z","published":"2024-12-18T10:49:41Z","title":"Mitigating Adversarial Attacks in LLMs through Defensive Suffix\n  Generation","summary":"  Large language models (LLMs) have exhibited outstanding performance in\nnatural language processing tasks. However, these models remain susceptible to\nadversarial attacks in which slight input perturbations can lead to harmful or\nmisleading outputs. A gradient-based defensive suffix generation algorithm is\ndesigned to bolster the robustness of LLMs. By appending carefully optimized\ndefensive suffixes to input prompts, the algorithm mitigates adversarial\ninfluences while preserving the models' utility. To enhance adversarial\nunderstanding, a novel total loss function ($L_{\\text{total}}$) combining\ndefensive loss ($L_{\\text{def}}$) and adversarial loss ($L_{\\text{adv}}$)\ngenerates defensive suffixes more effectively. Experimental evaluations\nconducted on open-source LLMs such as Gemma-7B, mistral-7B, Llama2-7B, and\nLlama2-13B show that the proposed method reduces attack success rates (ASR) by\nan average of 11\\% compared to models without defensive suffixes. Additionally,\nthe perplexity score of Gemma-7B decreased from 6.57 to 3.93 when applying the\ndefensive suffix generated by openELM-270M. Furthermore, TruthfulQA evaluations\ndemonstrate consistent improvements with Truthfulness scores increasing by up\nto 10\\% across tested configurations. This approach significantly enhances the\nsecurity of LLMs in critical applications without requiring extensive\nretraining.\n","authors":["Minkyoung Kim","Yunha Kim","Hyeram Seo","Heejung Choi","Jiye Han","Gaeun Kee","Soyoung Ko","HyoJe Jung","Byeolhee Kim","Young-Hak Kim","Sanghyun Park","Tae Joon Jun"],"pdf_url":"https://arxiv.org/pdf/2412.13705v1.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2412.13703v1","updated":"2024-12-18T10:46:04Z","published":"2024-12-18T10:46:04Z","title":"MBInception: A new Multi-Block Inception Model for Enhancing Image\n  Processing Efficiency","summary":"  Deep learning models, specifically convolutional neural networks, have\ntransformed the landscape of image classification by autonomously extracting\nfeatures directly from raw pixel data. This article introduces an innovative\nimage classification model that employs three consecutive inception blocks\nwithin a convolutional neural networks framework, providing a comprehensive\ncomparative analysis with well-established architectures such as Visual\nGeometry Group, Residual Network, and MobileNet. Through the utilization of\nbenchmark datasets, including Canadian Institute for Advanced Researc, Modified\nNational Institute of Standards and Technology database, and Fashion Modified\nNational Institute of Standards and Technology database, we assess the\nperformance of our proposed model in comparison to these benchmarks. The\noutcomes reveal that our novel model consistently outperforms its counterparts\nacross diverse datasets, underscoring its effectiveness and potential for\nadvancing the current state-of-the-art in image classification. Evaluation\nmetrics further emphasize that the proposed model surpasses the other compared\narchitectures, thereby enhancing the efficiency of image classification on\nstandard datasets.\n","authors":["Fatemeh Froughirad","Reza Bakhoda Eshtivani","Hamed Khajavi","Amir Rastgoo"],"pdf_url":"https://arxiv.org/pdf/2412.13703v1.pdf","comment":"26 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.02347v3","updated":"2024-12-18T10:45:06Z","published":"2024-06-04T14:23:27Z","title":"Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few\n  Steps Image Generation","summary":"  In this paper, we propose an efficient, fast, and versatile distillation\nmethod to accelerate the generation of pre-trained diffusion models: Flash\nDiffusion. The method reaches state-of-the-art performances in terms of FID and\nCLIP-Score for few steps image generation on the COCO2014 and COCO2017\ndatasets, while requiring only several GPU hours of training and fewer\ntrainable parameters than existing methods. In addition to its efficiency, the\nversatility of the method is also exposed across several tasks such as\ntext-to-image, inpainting, face-swapping, super-resolution and using different\nbackbones such as UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-$\\alpha$),\nas well as adapters. In all cases, the method allowed to reduce drastically the\nnumber of sampling steps while maintaining very high-quality image generation.\nThe official implementation is available at\nhttps://github.com/gojasper/flash-diffusion.\n","authors":["Clément Chadebec","Onur Tasar","Eyal Benaroche","Benjamin Aubin"],"pdf_url":"https://arxiv.org/pdf/2406.02347v3.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13695v1","updated":"2024-12-18T10:36:46Z","published":"2024-12-18T10:36:46Z","title":"Optical aberrations in autonomous driving: Physics-informed\n  parameterized temperature scaling for neural network uncertainty calibration","summary":"  'A trustworthy representation of uncertainty is desirable and should be\nconsidered as a key feature of any machine learning method' (Huellermeier and\nWaegeman, 2021). This conclusion of Huellermeier et al. underpins the\nimportance of calibrated uncertainties. Since AI-based algorithms are heavily\nimpacted by dataset shifts, the automotive industry needs to safeguard its\nsystem against all possible contingencies. One important but often neglected\ndataset shift is caused by optical aberrations induced by the windshield. For\nthe verification of the perception system performance, requirements on the AI\nperformance need to be translated into optical metrics by a bijective mapping\n(Braun, 2023). Given this bijective mapping it is evident that the optical\nsystem characteristics add additional information about the magnitude of the\ndataset shift. As a consequence, we propose to incorporate a physical inductive\nbias into the neural network calibration architecture to enhance the robustness\nand the trustworthiness of the AI target application, which we demonstrate by\nusing a semantic segmentation task as an example. By utilizing the Zernike\ncoefficient vector of the optical system as a physical prior we can\nsignificantly reduce the mean expected calibration error in case of optical\naberrations. As a result, we pave the way for a trustworthy uncertainty\nrepresentation and for a holistic verification strategy of the perception\nchain.\n","authors":["Dominik Werner Wolf","Alexander Braun","Markus Ulrich"],"pdf_url":"https://arxiv.org/pdf/2412.13695v1.pdf","comment":"Under review at the International Journal of Computer Vision (IJCV)"},{"id":"http://arxiv.org/abs/2410.23318v2","updated":"2024-12-18T10:32:31Z","published":"2024-10-29T21:38:54Z","title":"Denoising Diffusion Probabilistic Models for Magnetic Resonance\n  Fingerprinting","summary":"  Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to\nquantitative MRI, enabling the mapping of multiple tissue properties from a\nsingle, accelerated scan. However, achieving accurate reconstructions remains\nchallenging, particularly in highly accelerated and undersampled acquisitions,\nwhich are crucial for reducing scan times. While deep learning techniques have\nadvanced image reconstruction, the recent introduction of diffusion models\noffers new possibilities for imaging tasks, though their application in the\nmedical field is still emerging. Notably, diffusion models have not yet been\nexplored for the MRF problem. In this work, we propose for the first time a\nconditional diffusion probabilistic model for MRF image reconstruction.\nQualitative and quantitative comparisons on in-vivo brain scan data demonstrate\nthat the proposed approach can outperform established deep learning and\ncompressed sensing algorithms for MRF reconstruction. Extensive ablation\nstudies also explore strategies to improve computational efficiency of our\napproach.\n","authors":["Perla Mayo","Carolin M. Pirkl","Alin Achim","Bjoern H. Menze","Mohammad Golbabaee"],"pdf_url":"https://arxiv.org/pdf/2410.23318v2.pdf","comment":"13 pages, 5 figures, 3 tables, 2 algorithms"},{"id":"http://arxiv.org/abs/2412.13684v1","updated":"2024-12-18T10:19:12Z","published":"2024-12-18T10:19:12Z","title":"MMO-IG: Multi-Class and Multi-Scale Object Image Generation for Remote\n  Sensing","summary":"  The rapid advancement of deep generative models (DGMs) has significantly\nadvanced research in computer vision, providing a cost-effective alternative to\nacquiring vast quantities of expensive imagery. However, existing methods\npredominantly focus on synthesizing remote sensing (RS) images aligned with\nreal images in a global layout view, which limits their applicability in RS\nimage object detection (RSIOD) research. To address these challenges, we\npropose a multi-class and multi-scale object image generator based on DGMs,\ntermed MMO-IG, designed to generate RS images with supervised object labels\nfrom global and local aspects simultaneously. Specifically, from the local\nview, MMO-IG encodes various RS instances using an iso-spacing instance map\n(ISIM). During the generation process, it decodes each instance region with\niso-spacing value in ISIM-corresponding to both background and foreground\ninstances-to produce RS images through the denoising process of diffusion\nmodels. Considering the complex interdependencies among MMOs, we construct a\nspatial-cross dependency knowledge graph (SCDKG). This ensures a realistic and\nreliable multidirectional distribution among MMOs for region embedding, thereby\nreducing the discrepancy between source and target domains. Besides, we propose\na structured object distribution instruction (SODI) to guide the generation of\nsynthesized RS image content from a global aspect with SCDKG-based ISIM\ntogether. Extensive experimental results demonstrate that our MMO-IG exhibits\nsuperior generation capabilities for RS images with dense MMO-supervised\nlabels, and RS detectors pre-trained with MMO-IG show excellent performance on\nreal-world datasets.\n","authors":["Chuang Yang","Bingxuan Zhao","Qing Zhou","Qi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.14164v4","updated":"2024-12-18T10:16:59Z","published":"2022-10-19T21:52:01Z","title":"Understanding Key Point Cloud Features for Development Three-dimensional\n  Adversarial Attacks","summary":"  Adversarial attacks pose serious challenges for deep neural network\n(DNN)-based analysis of various input signals. In the case of three-dimensional\npoint clouds, methods have been developed to identify points that play a key\nrole in network decision, and these become crucial in generating existing\nadversarial attacks. For example, a saliency map approach is a popular method\nfor identifying adversarial drop points, whose removal would significantly\nimpact the network decision. This paper seeks to enhance the understanding of\nthree-dimensional adversarial attacks by exploring which point cloud features\nare most important for predicting adversarial points. Specifically, Fourteen\nkey point cloud features such as edge intensity and distance from the centroid\nare defined, and multiple linear regression is employed to assess their\npredictive power for adversarial points. Based on critical feature selection\ninsights, a new attack method has been developed to evaluate whether the\nselected features can generate an attack successfully. Unlike traditional\nattack methods that rely on model-specific vulnerabilities, this approach\nfocuses on the intrinsic characteristics of the point clouds themselves. It is\ndemonstrated that these features can predict adversarial points across four\ndifferent DNN architectures, Point Network (PointNet), PointNet++, Dynamic\nGraph Convolutional Neural Networks (DGCNN), and Point Convolutional Network\n(PointConv) outperforming random guessing and achieving results comparable to\nsaliency map-based attacks. This study has important engineering applications,\nsuch as enhancing the security and robustness of three-dimensional point\ncloud-based systems in fields like robotics and autonomous driving.\n","authors":["Hanieh Naderi","Chinthaka Dinesh","Ivan V. Bajic","Shohreh Kasaei"],"pdf_url":"https://arxiv.org/pdf/2210.14164v4.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.07616v2","updated":"2024-12-18T10:02:35Z","published":"2024-12-10T15:54:53Z","title":"PVP: Polar Representation Boost for 3D Semantic Occupancy Prediction","summary":"  Recently, polar coordinate-based representations have shown promise for 3D\nperceptual tasks. Compared to Cartesian methods, polar grids provide a viable\nalternative, offering better detail preservation in nearby spaces while\ncovering larger areas. However, they face feature distortion due to non-uniform\ndivision. To address these issues, we introduce the Polar Voxel Occupancy\nPredictor (PVP), a novel 3D multi-modal predictor that operates in polar\ncoordinates. PVP features two key design elements to overcome distortion: a\nGlobal Represent Propagation (GRP) module that integrates global spatial data\ninto 3D volumes, and a Plane Decomposed Convolution (PD-Conv) that simplifies\n3D distortions into 2D convolutions. These innovations enable PVP to outperform\nexisting methods, achieving significant improvements in mIoU and IoU metrics on\nthe OpenOccupancy dataset.\n","authors":["Yujing Xue","Jiaxiang Liu","Jiawei Du","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.07616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12628v2","updated":"2024-12-18T09:58:32Z","published":"2024-12-17T07:43:36Z","title":"Dense Audio-Visual Event Localization under Cross-Modal Consistency and\n  Multi-Temporal Granularity Collaboration","summary":"  In the field of audio-visual learning, most research tasks focus exclusively\non short videos. This paper focuses on the more practical Dense Audio-Visual\nEvent Localization (DAVEL) task, advancing audio-visual scene understanding for\nlonger, untrimmed videos. This task seeks to identify and temporally pinpoint\nall events simultaneously occurring in both audio and visual streams.\nTypically, each video encompasses dense events of multiple classes, which may\noverlap on the timeline, each exhibiting varied durations. Given these\nchallenges, effectively exploiting the audio-visual relations and the temporal\nfeatures encoded at various granularities becomes crucial. To address these\nchallenges, we introduce a novel CCNet, comprising two core modules: the\nCross-Modal Consistency Collaboration (CMCC) and the Multi-Temporal Granularity\nCollaboration (MTGC). Specifically, the CMCC module contains two branches: a\ncross-modal interaction branch and a temporal consistency-gated branch. The\nformer branch facilitates the aggregation of consistent event semantics across\nmodalities through the encoding of audio-visual relations, while the latter\nbranch guides one modality's focus to pivotal event-relevant temporal areas as\ndiscerned in the other modality. The MTGC module includes a coarse-to-fine\ncollaboration block and a fine-to-coarse collaboration block, providing\nbidirectional support among coarse- and fine-grained temporal features.\nExtensive experiments on the UnAV-100 dataset validate our module design,\nresulting in a new state-of-the-art performance in dense audio-visual event\nlocalization. The code is available at\nhttps://github.com/zzhhfut/CCNet-AAAI2025.\n","authors":["Ziheng Zhou","Jinxing Zhou","Wei Qian","Shengeng Tang","Xiaojun Chang","Dan Guo"],"pdf_url":"https://arxiv.org/pdf/2412.12628v2.pdf","comment":"Accepted by AAAI 2025. Project page:\n  https://github.com/zzhhfut/CCNet-AAAI2025. Jinxing Zhou and Dan Guo are the\n  corresponding authors"},{"id":"http://arxiv.org/abs/2408.11481v2","updated":"2024-12-18T09:55:40Z","published":"2024-08-21T09:49:32Z","title":"VE-Bench: Subjective-Aligned Benchmark Suite for Text-Driven Video\n  Editing Quality Assessment","summary":"  Text-driven video editing has recently experienced rapid development. Despite\nthis, evaluating edited videos remains a considerable challenge. Current\nmetrics tend to fail to align with human perceptions, and effective\nquantitative metrics for video editing are still notably absent. To address\nthis, we introduce VE-Bench, a benchmark suite tailored to the assessment of\ntext-driven video editing. This suite includes VE-Bench DB, a video quality\nassessment (VQA) database for video editing. VE-Bench DB encompasses a diverse\nset of source videos featuring various motions and subjects, along with\nmultiple distinct editing prompts, editing results from 8 different models, and\nthe corresponding Mean Opinion Scores (MOS) from 24 human annotators. Based on\nVE-Bench DB, we further propose VE-Bench QA, a quantitative human-aligned\nmeasurement for the text-driven video editing task. In addition to the\naesthetic, distortion, and other visual quality indicators that traditional VQA\nmethods emphasize, VE-Bench QA focuses on the text-video alignment and the\nrelevance modeling between source and edited videos. It proposes a new\nassessment network for video editing that attains superior performance in\nalignment with human preferences. To the best of our knowledge, VE-Bench\nintroduces the first quality assessment dataset for video editing and an\neffective subjective-aligned quantitative metric for this domain. All data and\ncode will be publicly available at https://github.com/littlespray/VE-Bench.\n","authors":["Shangkun Sun","Xiaoyu Liang","Songlin Fan","Wenxu Gao","Wei Gao"],"pdf_url":"https://arxiv.org/pdf/2408.11481v2.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2409.17920v2","updated":"2024-12-18T09:55:15Z","published":"2024-09-26T15:04:13Z","title":"Resolving Multi-Condition Confusion for Finetuning-Free Personalized\n  Image Generation","summary":"  Personalized text-to-image generation methods can generate customized images\nbased on the reference images, which have garnered wide research interest.\nRecent methods propose a finetuning-free approach with a decoupled\ncross-attention mechanism to generate personalized images requiring no\ntest-time finetuning. However, when multiple reference images are provided, the\ncurrent decoupled cross-attention mechanism encounters the object confusion\nproblem and fails to map each reference image to its corresponding object,\nthereby seriously limiting its scope of application. To address the object\nconfusion problem, in this work we investigate the relevance of different\npositions of the latent image features to the target object in diffusion model,\nand accordingly propose a weighted-merge method to merge multiple reference\nimage features into the corresponding objects. Next, we integrate this\nweighted-merge method into existing pre-trained models and continue to train\nthe model on a multi-object dataset constructed from the open-sourced SA-1B\ndataset. To mitigate object confusion and reduce training costs, we propose an\nobject quality score to estimate the image quality for the selection of\nhigh-quality training samples. Furthermore, our weighted-merge training\nframework can be employed on single-object generation when a single object has\nmultiple reference images. The experiments verify that our method achieves\nsuperior performance to the state-of-the-arts on the Concept101 dataset and\nDreamBooth dataset of multi-object personalized image generation, and\nremarkably improves the performance on single-object personalized image\ngeneration. Our code is available at https://github.com/hqhQAQ/MIP-Adapter.\n","authors":["Qihan Huang","Siming Fu","Jinlong Liu","Hao Jiang","Yipeng Yu","Jie Song"],"pdf_url":"https://arxiv.org/pdf/2409.17920v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15024v2","updated":"2024-12-18T09:47:25Z","published":"2024-11-22T15:55:19Z","title":"DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models","summary":"  Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.\n","authors":["Keda Tao","Can Qin","Haoxuan You","Yang Sui","Huan Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15024v2.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.13168v2","updated":"2024-12-18T09:47:15Z","published":"2024-12-17T18:45:53Z","title":"Lifting Scheme-Based Implicit Disentanglement of Emotion-Related Facial\n  Dynamics in the Wild","summary":"  In-the-wild dynamic facial expression recognition (DFER) encounters a\nsignificant challenge in recognizing emotion-related expressions, which are\noften temporally and spatially diluted by emotion-irrelevant expressions and\nglobal context. Most prior DFER methods directly utilize coupled spatiotemporal\nrepresentations that may incorporate weakly relevant features with\nemotion-irrelevant context bias. Several DFER methods highlight dynamic\ninformation for DFER, but following explicit guidance that may be vulnerable to\nirrelevant motion. In this paper, we propose a novel Implicit Facial Dynamics\nDisentanglement framework (IFDD). Through expanding wavelet lifting scheme to\nfully learnable framework, IFDD disentangles emotion-related dynamic\ninformation from emotion-irrelevant global context in an implicit manner, i.e.,\nwithout exploit operations and external guidance. The disentanglement process\ncontains two stages. The first is Inter-frame Static-dynamic Splitting Module\n(ISSM) for rough disentanglement estimation, which explores inter-frame\ncorrelation to generate content-aware splitting indexes on-the-fly. We utilize\nthese indexes to split frame features into two groups, one with greater global\nsimilarity, and the other with more unique dynamic features. The second stage\nis Lifting-based Aggregation-Disentanglement Module (LADM) for further\nrefinement. LADM first aggregates two groups of features from ISSM to obtain\nfine-grained global context features by an updater, and then disentangles\nemotion-related facial dynamic features from the global context by a predictor.\nExtensive experiments on in-the-wild datasets have demonstrated that IFDD\noutperforms prior supervised DFER methods with higher recognition accuracy and\ncomparable efficiency. Code is available at\nhttps://github.com/CyberPegasus/IFDD.\n","authors":["Xingjian Wang","Li Chai"],"pdf_url":"https://arxiv.org/pdf/2412.13168v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2211.05781v3","updated":"2024-12-18T09:45:17Z","published":"2022-11-10T18:59:43Z","title":"Demystify Transformers & Convolutions in Modern Image Deep Networks","summary":"  Vision transformers have gained popularity recently, leading to the\ndevelopment of new vision backbones with improved features and consistent\nperformance gains. However, these advancements are not solely attributable to\nnovel feature transformation designs; certain benefits also arise from advanced\nnetwork-level and block-level architectures. This paper aims to identify the\nreal gains of popular convolution and attention operators through a detailed\nstudy. We find that the key difference among these feature transformation\nmodules, such as attention or convolution, lies in their spatial feature\naggregation approach, known as the \"spatial token mixer\" (STM). To facilitate\nan impartial comparison, we introduce a unified architecture to neutralize the\nimpact of divergent network-level and block-level designs. Subsequently,\nvarious STMs are integrated into this unified framework for comprehensive\ncomparative analysis. Our experiments on various tasks and an analysis of\ninductive bias show a significant performance boost due to advanced\nnetwork-level and block-level designs, but performance differences persist\namong different STMs. Our detailed analysis also reveals various findings about\ndifferent STMs, including effective receptive fields, invariance, and\nadversarial robustness tests.\n","authors":["Xiaowei Hu","Min Shi","Weiyun Wang","Sitong Wu","Linjie Xing","Wenhai Wang","Xizhou Zhu","Lewei Lu","Jie Zhou","Xiaogang Wang","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2211.05781v3.pdf","comment":"This paper was accepted to IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (IEEE TPAMI). All models and codes used in this study\n  are publicly available at https://github.com/OpenGVLab/STM-Evaluation"},{"id":"http://arxiv.org/abs/2412.13662v1","updated":"2024-12-18T09:39:12Z","published":"2024-12-18T09:39:12Z","title":"When Should We Prefer State-to-Visual DAgger Over Visual Reinforcement\n  Learning?","summary":"  Learning policies from high-dimensional visual inputs, such as pixels and\npoint clouds, is crucial in various applications. Visual reinforcement learning\nis a promising approach that directly trains policies from visual observations,\nalthough it faces challenges in sample efficiency and computational costs. This\nstudy conducts an empirical comparison of State-to-Visual DAgger, a two-stage\nframework that initially trains a state policy before adopting online imitation\nto learn a visual policy, and Visual RL across a diverse set of tasks. We\nevaluate both methods across 16 tasks from three benchmarks, focusing on their\nasymptotic performance, sample efficiency, and computational costs.\nSurprisingly, our findings reveal that State-to-Visual DAgger does not\nuniversally outperform Visual RL but shows significant advantages in\nchallenging tasks, offering more consistent performance. In contrast, its\nbenefits in sample efficiency are less pronounced, although it often reduces\nthe overall wall-clock time required for training. Based on our findings, we\nprovide recommendations for practitioners and hope that our results contribute\nvaluable perspectives for future research in visual policy learning.\n","authors":["Tongzhou Mu","Zhaoyang Li","Stanisław Wiktor Strzelecki","Xiu Yuan","Yunchao Yao","Litian Liang","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2412.13662v1.pdf","comment":"Accepted by The 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2412.08200v2","updated":"2024-12-18T09:36:47Z","published":"2024-12-11T08:43:52Z","title":"GN-FR:Generalizable Neural Radiance Fields for Flare Removal","summary":"  Flare, an optical phenomenon resulting from unwanted scattering and\nreflections within a lens system, presents a significant challenge in imaging.\nThe diverse patterns of flares, such as halos, streaks, color bleeding, and\nhaze, complicate the flare removal process. Existing traditional and\nlearning-based methods have exhibited limited efficacy due to their reliance on\nsingle-image approaches, where flare removal is highly ill-posed. We address\nthis by framing flare removal as a multi-view image problem, taking advantage\nof the view-dependent nature of flare artifacts. This approach leverages\ninformation from neighboring views to recover details obscured by flare in\nindividual images. Our proposed framework, GN-FR (Generalizable Neural Radiance\nFields for Flare Removal), can render flare-free views from a sparse set of\ninput images affected by lens flare and generalizes across different scenes in\nan unsupervised manner. GN-FR incorporates several modules within the\nGeneralizable NeRF Transformer (GNT) framework: Flare-occupancy Mask Generation\n(FMG), View Sampler (VS), and Point Sampler (PS). To overcome the\nimpracticality of capturing both flare-corrupted and flare-free data, we\nintroduce a masking loss function that utilizes mask information in an\nunsupervised setting. Additionally, we present a 3D multi-view flare dataset,\ncomprising 17 real flare scenes with 782 images, 80 real flare patterns, and\ntheir corresponding annotated flare-occupancy masks. To our knowledge, this is\nthe first work to address flare removal within a Neural Radiance Fields (NeRF)\nframework.\n","authors":["Gopi Raju Matta","Rahul Siddartha","Rongali Simhachala Venkata Girish","Sumit Sharma","Kaushik Mitra"],"pdf_url":"https://arxiv.org/pdf/2412.08200v2.pdf","comment":"Accepted for publication at BMVC-24"},{"id":"http://arxiv.org/abs/2412.13656v1","updated":"2024-12-18T09:34:59Z","published":"2024-12-18T09:34:59Z","title":"GLCF: A Global-Local Multimodal Coherence Analysis Framework for Talking\n  Face Generation Detection","summary":"  Talking face generation (TFG) allows for producing lifelike talking videos of\nany character using only facial images and accompanying text. Abuse of this\ntechnology could pose significant risks to society, creating the urgent need\nfor research into corresponding detection methods. However, research in this\nfield has been hindered by the lack of public datasets. In this paper, we\nconstruct the first large-scale multi-scenario talking face dataset (MSTF),\nwhich contains 22 audio and video forgery techniques, filling the gap of\ndatasets in this field. The dataset covers 11 generation scenarios and more\nthan 20 semantic scenarios, closer to the practical application scenario of\nTFG. Besides, we also propose a TFG detection framework, which leverages the\nanalysis of both global and local coherence in the multimodal content of TFG\nvideos. Therefore, a region-focused smoothness detection module (RSFDM) and a\ndiscrepancy capture-time frame aggregation module (DCTAM) are introduced to\nevaluate the global temporal coherence of TFG videos, aggregating multi-grained\nspatial information. Additionally, a visual-audio fusion module (V-AFM) is\ndesigned to evaluate audiovisual coherence within a localized temporal\nperspective. Comprehensive experiments demonstrate the reasonableness and\nchallenges of our datasets, while also indicating the superiority of our\nproposed method compared to the state-of-the-art deepfake detection approaches.\n","authors":["Xiaocan Chen","Qilin Yin","Jiarui Liu","Wei Lu","Xiangyang Luo","Jiantao Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.13656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13655v1","updated":"2024-12-18T09:34:32Z","published":"2024-12-18T09:34:32Z","title":"VIIS: Visible and Infrared Information Synthesis for Severe Low-light\n  Image Enhancement","summary":"  Images captured in severe low-light circumstances often suffer from\nsignificant information absence. Existing singular modality image enhancement\nmethods struggle to restore image regions lacking valid information. By\nleveraging light-impervious infrared images, visible and infrared image fusion\nmethods have the potential to reveal information hidden in darkness. However,\nthey primarily emphasize inter-modal complementation but neglect intra-modal\nenhancement, limiting the perceptual quality of output images. To address these\nlimitations, we propose a novel task, dubbed visible and infrared information\nsynthesis (VIIS), which aims to achieve both information enhancement and fusion\nof the two modalities. Given the difficulty in obtaining ground truth in the\nVIIS task, we design an information synthesis pretext task (ISPT) based on\nimage augmentation. We employ a diffusion model as the framework and design a\nsparse attention-based dual-modalities residual (SADMR) conditioning mechanism\nto enhance information interaction between the two modalities. This mechanism\nenables features with prior knowledge from both modalities to adaptively and\niteratively attend to each modality's information during the denoising process.\nOur extensive experiments demonstrate that our model qualitatively and\nquantitatively outperforms not only the state-of-the-art methods in relevant\nfields but also the newly designed baselines capable of both information\nenhancement and fusion. The code is available at\nhttps://github.com/Chenz418/VIIS.\n","authors":["Chen Zhao","Mengyuan Yu","Fan Yang","Peiguang Jing"],"pdf_url":"https://arxiv.org/pdf/2412.13655v1.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2306.09801v3","updated":"2024-12-18T09:34:18Z","published":"2023-06-16T12:22:19Z","title":"Semantics-Aware Next-best-view Planning for Efficient Search and\n  Detection of Task-relevant Plant Parts","summary":"  Searching and detecting the task-relevant parts of plants is important to\nautomate harvesting and de-leafing of tomato plants using robots. This is\nchallenging due to high levels of occlusion in tomato plants. Active vision is\na promising approach in which the robot strategically plans its camera\nviewpoints to overcome occlusion and improve perception accuracy. However,\ncurrent active-vision algorithms cannot differentiate between relevant and\nirrelevant plant parts and spend time on perceiving irrelevant plant parts.\nThis work proposed a semantics-aware active-vision strategy that uses semantic\ninformation to identify the relevant plant parts and prioritise them during\nview planning. The proposed strategy was evaluated on the task of searching and\ndetecting the relevant plant parts using simulation and real-world experiments.\nIn simulation experiments, the semantics-aware strategy proposed could search\nand detect 81.8% of the relevant plant parts using nine viewpoints. It was\nsignificantly faster and detected more plant parts than predefined, random, and\nvolumetric active-vision strategies that do not use semantic information. The\nstrategy proposed was also robust to uncertainty in plant and plant-part\npositions, plant complexity, and different viewpoint-sampling strategies. In\nreal-world experiments, the semantics-aware strategy could search and detect\n82.7% of the relevant plant parts using seven viewpoints, under complex\ngreenhouse conditions with natural variation and occlusion, natural\nillumination, sensor noise, and uncertainty in camera poses. The results of\nthis work clearly indicate the advantage of using semantics-aware active vision\nfor targeted perception of plant parts and its applicability in the real world.\nIt can significantly improve the efficiency of automated harvesting and\nde-leafing in tomato crop production.\n","authors":["Akshay K. Burusa","Joost Scholten","David Rapado Rincon","Xin Wang","Eldert J. van Henten","Gert Kootstra"],"pdf_url":"https://arxiv.org/pdf/2306.09801v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13654v1","updated":"2024-12-18T09:33:20Z","published":"2024-12-18T09:33:20Z","title":"GAGS: Granularity-Aware Feature Distillation for Language Gaussian\n  Splatting","summary":"  3D open-vocabulary scene understanding, which accurately perceives complex\nsemantic properties of objects in space, has gained significant attention in\nrecent years. In this paper, we propose GAGS, a framework that distills 2D CLIP\nfeatures into 3D Gaussian splatting, enabling open-vocabulary queries for\nrenderings on arbitrary viewpoints. The main challenge of distilling 2D\nfeatures for 3D fields lies in the multiview inconsistency of extracted 2D\nfeatures, which provides unstable supervision for the 3D feature field. GAGS\naddresses this challenge with two novel strategies. First, GAGS associates the\nprompt point density of SAM with the camera distances, which significantly\nimproves the multiview consistency of segmentation results. Second, GAGS\nfurther decodes a granularity factor to guide the distillation process and this\ngranularity factor can be learned in a unsupervised manner to only select the\nmultiview consistent 2D features in the distillation process. Experimental\nresults on two datasets demonstrate significant performance and stability\nimprovements of GAGS in visual grounding and semantic segmentation, with an\ninference speed 2$\\times$ faster than baseline methods. The code and additional\nresults are available at https://pz0826.github.io/GAGS-Webpage/ .\n","authors":["Yuning Peng","Haiping Wang","Yuan Liu","Chenglu Wen","Zhen Dong","Bisheng Yang"],"pdf_url":"https://arxiv.org/pdf/2412.13654v1.pdf","comment":"Project page: https://pz0826.github.io/GAGS-Webpage/"},{"id":"http://arxiv.org/abs/2412.13652v1","updated":"2024-12-18T09:31:06Z","published":"2024-12-18T09:31:06Z","title":"RelationField: Relate Anything in Radiance Fields","summary":"  Neural radiance fields are an emerging 3D scene representation and recently\neven been extended to learn features for scene understanding by distilling\nopen-vocabulary features from vision-language models. However, current method\nprimarily focus on object-centric representations, supporting object\nsegmentation or detection, while understanding semantic relationships between\nobjects remains largely unexplored. To address this gap, we propose\nRelationField, the first method to extract inter-object relationships directly\nfrom neural radiance fields. RelationField represents relationships between\nobjects as pairs of rays within a neural radiance field, effectively extending\nits formulation to include implicit relationship queries. To teach\nRelationField complex, open-vocabulary relationships, relationship knowledge is\ndistilled from multi-modal LLMs. To evaluate RelationField, we solve\nopen-vocabulary 3D scene graph generation tasks and relationship-guided\ninstance segmentation, achieving state-of-the-art performance in both tasks.\nSee the project website at https://relationfield.github.io.\n","authors":["Sebastian Koch","Johanna Wald","Mirco Colosi","Narunas Vaskevicius","Pedro Hermosilla","Federico Tombari","Timo Ropinski"],"pdf_url":"https://arxiv.org/pdf/2412.13652v1.pdf","comment":"Project page: https://relationfield.github.io"},{"id":"http://arxiv.org/abs/2407.19323v4","updated":"2024-12-18T09:28:37Z","published":"2024-07-27T19:00:44Z","title":"MSP-MVS: Multi-Granularity Segmentation Prior Guided Multi-View Stereo","summary":"  Recently, patch deformation-based methods have demonstrated significant\nstrength in multi-view stereo by adaptively expanding the reception field of\npatches to help reconstruct textureless areas. However, such methods mainly\nconcentrate on searching for pixels without matching ambiguity (i.e., reliable\npixels) when constructing deformed patches, while neglecting the deformation\ninstability caused by unexpected edge-skipping, resulting in potential matching\ndistortions. Addressing this, we propose MSP-MVS, a method introducing\nmulti-granularity segmentation prior for edge-confined patch deformation.\nSpecifically, to avoid unexpected edge-skipping, we first aggregate and further\nrefine multi-granularity depth edges gained from Semantic-SAM as prior to guide\npatch deformation within depth-continuous (i.e., homogeneous) areas. Moreover,\nto address attention imbalance caused by edge-confined patch deformation, we\nimplement adaptive equidistribution and disassemble-clustering of correlative\nreliable pixels (i.e., anchors), thereby promoting attention-consistent patch\ndeformation. Finally, to prevent deformed patches from falling into\nlocal-minimum matching costs caused by the fixed sampling pattern, we introduce\ndisparity-sampling synergistic 3D optimization to help identify global-minimum\nmatching costs. Evaluations on ETH3D and Tanks & Temples benchmarks prove our\nmethod obtains state-of-the-art performance with remarkable generalization.\n","authors":["Zhenlong Yuan","Cong Liu","Fei Shen","Zhaoxin Li","Jinguo Luo","Tianlu Mao","Zhaoqi Wang"],"pdf_url":"https://arxiv.org/pdf/2407.19323v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13647v1","updated":"2024-12-18T09:23:12Z","published":"2024-12-18T09:23:12Z","title":"G-VEval: A Versatile Metric for Evaluating Image and Video Captions\n  Using GPT-4o","summary":"  Evaluation metric of visual captioning is important yet not thoroughly\nexplored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss\nsemantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are\nlimited in zero-shot scenarios. Advanced Language Model-based metrics also\nstruggle with aligning to nuanced human preferences. To address these issues,\nwe introduce G-VEval, a novel metric inspired by G-Eval and powered by the new\nGPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and\nsupports three modes: reference-free, reference-only, and combined,\naccommodating both video and image inputs. We also propose MSVD-Eval, a new\ndataset for video captioning evaluation, to establish a more transparent and\nconsistent framework for both human experts and evaluation metrics. It is\ndesigned to address the lack of clear criteria in existing datasets by\nintroducing distinct dimensions of Accuracy, Completeness, Conciseness, and\nRelevance (ACCR). Extensive results show that G-VEval outperforms existing\nmethods in correlation with human annotations, as measured by Kendall tau-b and\nKendall tau-c. This provides a flexible solution for diverse captioning tasks\nand suggests a straightforward yet effective approach for large language models\nto understand video content, paving the way for advancements in automated\ncaptioning. Codes are available at https://github.com/ztangaj/gveval\n","authors":["Tony Cheng Tong","Sirui He","Zhiwen Shao","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2412.13647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09365v3","updated":"2024-12-18T09:11:06Z","published":"2024-05-15T14:17:44Z","title":"SARATR-X: Towards Building A Foundation Model for SAR Target Recognition","summary":"  Despite the remarkable progress in synthetic aperture radar automatic target\nrecognition (SAR ATR), recent efforts have concentrated on detecting and\nclassifying a specific category, e.g., vehicles, ships, airplanes, or\nbuildings. One of the fundamental limitations of the top-performing SAR ATR\nmethods is that the learning paradigm is supervised, task-specific,\nlimited-category, closed-world learning, which depends on massive amounts of\naccurately annotated samples that are expensively labeled by expert SAR\nanalysts and have limited generalization capability and scalability. In this\nwork, we make the first attempt towards building a foundation model for SAR\nATR, termed SARATR-X. SARATR-X learns generalizable representations via\nself-supervised learning (SSL) and provides a cornerstone for label-efficient\nmodel adaptation to generic SAR target detection and classification tasks.\nSpecifically, SARATR-X is trained on 0.18 M unlabelled SAR target samples,\nwhich are curated by combining contemporary benchmarks and constitute the\nlargest publicly available dataset till now. Considering the characteristics of\nSAR images, a backbone tailored for SAR ATR is carefully designed, and a\ntwo-step SSL method endowed with multi-scale gradient features was applied to\nensure the feature diversity and model scalability of SARATR-X. The\ncapabilities of SARATR-X are evaluated on classification under few-shot and\nrobustness settings and detection across various categories and scenes, and\nimpressive performance is achieved, often competitive with or even superior to\nprior fully supervised, semi-supervised, or self-supervised algorithms. Our\nSARATR-X and the curated dataset are released at\nhttps://github.com/waterdisappear/SARATR-X to foster research into foundation\nmodels for SAR image interpretation.\n","authors":["Weijie Li","Wei Yang","Yuenan Hou","Li Liu","Yongxiang Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2405.09365v3.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.13636v1","updated":"2024-12-18T09:09:41Z","published":"2024-12-18T09:09:41Z","title":"Consistency of Compositional Generalization across Multiple Levels","summary":"  Compositional generalization is the capability of a model to understand novel\ncompositions composed of seen concepts. There are multiple levels of novel\ncompositions including phrase-phrase level, phrase-word level, and word-word\nlevel. Existing methods achieve promising compositional generalization, but the\nconsistency of compositional generalization across multiple levels of novel\ncompositions remains unexplored. The consistency refers to that a model should\ngeneralize to a phrase-phrase level novel composition, and\nphrase-word/word-word level novel compositions that can be derived from it\nsimultaneously. In this paper, we propose a meta-learning based framework, for\nachieving consistent compositional generalization across multiple levels. The\nbasic idea is to progressively learn compositions from simple to complex for\nconsistency. Specifically, we divide the original training set into multiple\nvalidation sets based on compositional complexity, and introduce multiple\nmeta-weight-nets to generate sample weights for samples in different validation\nsets. To fit the validation sets in order of increasing compositional\ncomplexity, we optimize the parameters of each meta-weight-net independently\nand sequentially in a multilevel optimization manner. We build a GQA-CCG\ndataset to quantitatively evaluate the consistency. Experimental results on\nvisual question answering and temporal video grounding, demonstrate the\neffectiveness of the proposed framework. We release GQA-CCG at\nhttps://github.com/NeverMoreLCH/CCG.\n","authors":["Chuanhao Li","Zhen Li","Chenchen Jing","Xiaomeng Fan","Wenbo Ye","Yuwei Wu","Yunde Jia"],"pdf_url":"https://arxiv.org/pdf/2412.13636v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13635v1","updated":"2024-12-18T09:09:39Z","published":"2024-12-18T09:09:39Z","title":"Self-control: A Better Conditional Mechanism for Masked Autoregressive\n  Model","summary":"  Autoregressive conditional image generation algorithms are capable of\ngenerating photorealistic images that are consistent with given textual or\nimage conditions, and have great potential for a wide range of applications.\nNevertheless, the majority of popular autoregressive image generation methods\nrely heavily on vector quantization, and the inherent discrete characteristic\nof codebook presents a considerable challenge to achieving high-quality image\ngeneration. To address this limitation, this paper introduces a novel\nconditional introduction network for continuous masked autoregressive models.\nThe proposed self-control network serves to mitigate the negative impact of\nvector quantization on the quality of the generated images, while\nsimultaneously enhancing the conditional control during the generation process.\nIn particular, the self-control network is constructed upon a continuous mask\nautoregressive generative model, which incorporates multimodal conditional\ninformation, including text and images, into a unified autoregressive sequence\nin a serial manner. Through a self-attention mechanism, the network is capable\nof generating images that are controllable based on specific conditions. The\nself-control network discards the conventional cross-attention-based\nconditional fusion mechanism and effectively unifies the conditional and\ngenerative information within the same space, thereby facilitating more\nseamless learning and fusion of multimodal features.\n","authors":["Qiaoying Qu","Shiyu Shen"],"pdf_url":"https://arxiv.org/pdf/2412.13635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10843v3","updated":"2024-12-18T08:54:03Z","published":"2024-08-19T11:42:54Z","title":"Detecting Wildfires on UAVs with Real-time Segmentation Trained by\n  Larger Teacher Models","summary":"  Early detection of wildfires is essential to prevent large-scale fires\nresulting in extensive environmental, structural, and societal damage. Uncrewed\naerial vehicles (UAVs) can cover large remote areas effectively with quick\ndeployment requiring minimal infrastructure and equipping them with small\ncameras and computers enables autonomous real-time detection. In remote areas,\nhowever, detection methods are limited to onboard computation due to the lack\nof high-bandwidth mobile networks. For accurate camera-based localisation,\nsegmentation of the detected smoke is essential but training data for deep\nlearning-based wildfire smoke segmentation is limited. This study shows how\nsmall specialised segmentation models can be trained using only bounding box\nlabels, leveraging zero-shot foundation model supervision. The method offers\nthe advantages of needing only fairly easily obtainable bounding box labels and\nrequiring training solely for the smaller student network. The proposed method\nachieved 63.3% mIoU on a manually annotated and diverse wildfire dataset. The\nused model can perform in real-time at ~25 fps with a UAV-carried NVIDIA Jetson\nOrin NX computer while reliably recognising smoke, as demonstrated at\nreal-world forest burning events. Code is available at:\nhttps://gitlab.com/fgi_nls/public/wildfire-real-time-segmentation\n","authors":["Julius Pesonen","Teemu Hakala","Väinö Karjalainen","Niko Koivumäki","Lauri Markelin","Anna-Maria Raita-Hakola","Juha Suomalainen","Ilkka Pölönen","Eija Honkavaara"],"pdf_url":"https://arxiv.org/pdf/2408.10843v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13615v1","updated":"2024-12-18T08:53:52Z","published":"2024-12-18T08:53:52Z","title":"MambaLCT: Boosting Tracking via Long-term Context State Space Model","summary":"  Effectively constructing context information with long-term dependencies from\nvideo sequences is crucial for object tracking. However, the context length\nconstructed by existing work is limited, only considering object information\nfrom adjacent frames or video clips, leading to insufficient utilization of\ncontextual information. To address this issue, we propose MambaLCT, which\nconstructs and utilizes target variation cues from the first frame to the\ncurrent frame for robust tracking. First, a novel unidirectional Context Mamba\nmodule is designed to scan frame features along the temporal dimension,\ngathering target change cues throughout the entire sequence. Specifically,\ntarget-related information in frame features is compressed into a hidden state\nspace through selective scanning mechanism. The target information across the\nentire video is continuously aggregated into target variation cues. Next, we\ninject the target change cues into the attention mechanism, providing temporal\ninformation for modeling the relationship between the template and search\nframes. The advantage of MambaLCT is its ability to continuously extend the\nlength of the context, capturing complete target change cues, which enhances\nthe stability and robustness of the tracker. Extensive experiments show that\nlong-term context information enhances the model's ability to perceive targets\nin complex scenarios. MambaLCT achieves new SOTA performance on six benchmarks\nwhile maintaining real-time running speeds.\n","authors":["Xiaohai Li","Bineng Zhong","Qihua Liang","Guorong Li","Zhiyi Mo","Shuxiang Song"],"pdf_url":"https://arxiv.org/pdf/2412.13615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01988v2","updated":"2024-12-18T08:51:39Z","published":"2024-11-04T11:20:17Z","title":"QCS:Feature Refining from Quadruplet Cross Similarity for Facial\n  Expression Recognition","summary":"  Facial expression recognition faces challenges where labeled significant\nfeatures in datasets are mixed with unlabeled redundant ones. In this paper, we\nintroduce Cross Similarity Attention (CSA) to mine richer intrinsic information\nfrom image pairs, overcoming a limitation when the Scaled Dot-Product Attention\nof ViT is directly applied to calculate the similarity between two different\nimages. Based on CSA, we simultaneously minimize intra-class differences and\nmaximize inter-class differences at the fine-grained feature level through\ninteractions among multiple branches. Contrastive residual distillation is\nutilized to transfer the information learned in the cross module back to the\nbase network. We ingeniously design a four-branch centrally symmetric network,\nnamed Quadruplet Cross Similarity (QCS), which alleviates gradient conflicts\narising from the cross module and achieves balanced and stable training. It can\nadaptively extract discriminative features while isolating redundant ones. The\ncross-attention modules exist during training, and only one base branch is\nretained during inference, resulting in no increase in inference time. Our\nproposed method achieves state-of-the-art performance on several FER datasets.\n","authors":["Chengpeng Wang","Li Chen","Lili Wang","Zhaofan Li","Xuebin Lv"],"pdf_url":"https://arxiv.org/pdf/2411.01988v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11223v2","updated":"2024-12-18T08:49:16Z","published":"2024-11-18T01:25:58Z","title":"Efficient Transfer Learning for Video-language Foundation Models","summary":"  Pre-trained vision-language models provide a robust foundation for efficient\ntransfer learning across various downstream tasks. In the field of video action\nrecognition, mainstream approaches often introduce additional parameter modules\nto capture temporal information. While the increased model capacity brought by\nthese additional parameters helps better fit the video-specific inductive\nbiases, existing methods require learning a large number of parameters and are\nprone to catastrophic forgetting of the original generalizable knowledge. In\nthis paper, we propose a simple yet effective Multi-modal Spatio-Temporal\nAdapter (MSTA) to improve the alignment between representations in the text and\nvision branches, achieving a balance between general knowledge and\ntask-specific knowledge. Furthermore, to mitigate over-fitting and enhance\ngeneralizability, we introduce a spatio-temporal description-guided consistency\nconstraint. This constraint involves feeding template inputs (i.e., ``a video\nof $\\{\\textbf{cls}\\}$'') into the trainable language branch, while\nLLM-generated spatio-temporal descriptions are input into the pre-trained\nlanguage branch, enforcing consistency between the outputs of the two branches.\nThis mechanism prevents over-fitting to downstream tasks and improves the\ndistinguishability of the trainable branch within the spatio-temporal semantic\nspace. We evaluate the effectiveness of our approach across four tasks:\nzero-shot transfer, few-shot learning, base-to-novel generalization, and\nfully-supervised learning. Compared to many state-of-the-art methods, our MSTA\nachieves outstanding performance across all evaluations, while using only 2-7\\%\nof the trainable parameters in the original model. Code will be avaliable at\nhttps://github.com/chenhaoxing/ETL4Video.\n","authors":["Haoxing Chen","Zizheng Huang","Yan Hong","Yanshuo Wang","Zhongcai Lyu","Zhuoer Xu","Jun Lan","Zhangxuan Gu"],"pdf_url":"https://arxiv.org/pdf/2411.11223v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13614v1","updated":"2024-12-18T08:49:01Z","published":"2024-12-18T08:49:01Z","title":"Reverse Region-to-Entity Annotation for Pixel-Level Visual Entity\n  Linking","summary":"  Visual Entity Linking (VEL) is a crucial task for achieving fine-grained\nvisual understanding, matching objects within images (visual mentions) to\nentities in a knowledge base. Previous VEL tasks rely on textual inputs, but\nwriting queries for complex scenes can be challenging. Visual inputs like\nclicks or bounding boxes offer a more convenient alternative. Therefore, we\npropose a new task, Pixel-Level Visual Entity Linking (PL-VEL), which uses\npixel masks from visual inputs to refer to objects, supplementing reference\nmethods for VEL. To facilitate research on this task, we have constructed the\nMaskOVEN-Wiki dataset through an entirely automatic reverse region-entity\nannotation framework. This dataset contains over 5 million annotations aligning\npixel-level regions with entity-level labels, which will advance visual\nunderstanding towards fine-grained. Moreover, as pixel masks correspond to\nsemantic regions in an image, we enhance previous patch-interacted attention\nwith region-interacted attention by a visual semantic tokenization approach.\nManual evaluation results indicate that the reverse annotation framework\nachieved a 94.8% annotation success rate. Experimental results show that models\ntrained on this dataset improved accuracy by 18 points compared to zero-shot\nmodels. Additionally, the semantic tokenization method achieved a 5-point\naccuracy improvement over the trained baseline.\n","authors":["Zhengfei Xu","Sijia Zhao","Yanchao Hao","Xiaolong Liu","Lili Li","Yuyang Yin","Bo Li","Xi Chen","Xin Xin"],"pdf_url":"https://arxiv.org/pdf/2412.13614v1.pdf","comment":"AAAI 2025;Dataset are released at\n  https://github.com/NP-NET-research/PL-VEL"},{"id":"http://arxiv.org/abs/2405.16751v2","updated":"2024-12-18T08:38:06Z","published":"2024-05-27T01:47:14Z","title":"REVECA: Adaptive Planning and Trajectory-based Validation in Cooperative\n  Language Agents using Information Relevance and Relative Proximity","summary":"  We address the challenge of multi-agent cooperation, where agents achieve a\ncommon goal by cooperating with decentralized agents under complex partial\nobservations. Existing cooperative agent systems often struggle with\nefficiently processing continuously accumulating information, managing globally\nsuboptimal planning due to lack of consideration of collaborators, and\naddressing false planning caused by environmental changes introduced by other\ncollaborators. To overcome these challenges, we propose the RElevance,\nProximity, and Validation-Enhanced Cooperative Language Agent (REVECA), a novel\ncognitive architecture powered by GPT-4o-mini. REVECA enables efficient memory\nmanagement, optimal planning, and cost-effective prevention of false planning\nby leveraging Relevance Estimation, Adaptive Planning, and Trajectory-based\nValidation. Extensive experimental results demonstrate REVECA's superiority\nover existing methods across various benchmarks, while a user study reveals its\npotential for achieving trustworthy human-AI cooperation.\n","authors":["SeungWon Seo","SeongRae Noh","Junhyeok Lee","SooBin Lim","Won Hee Lee","HyeongYeop Kang"],"pdf_url":"https://arxiv.org/pdf/2405.16751v2.pdf","comment":"v2 is the AAAI'25 camera-ready version, including the appendix, which\n  has been enhanced based on the reviewers' comments"},{"id":"http://arxiv.org/abs/2412.13611v1","updated":"2024-12-18T08:37:22Z","published":"2024-12-18T08:37:22Z","title":"Robust Tracking via Mamba-based Context-aware Token Learning","summary":"  How to make a good trade-off between performance and computational cost is\ncrucial for a tracker. However, current famous methods typically focus on\ncomplicated and time-consuming learning that combining temporal and appearance\ninformation by input more and more images (or features). Consequently, these\nmethods not only increase the model's computational source and learning burden\nbut also introduce much useless and potentially interfering information. To\nalleviate the above issues, we propose a simple yet robust tracker that\nseparates temporal information learning from appearance modeling and extracts\ntemporal relations from a set of representative tokens rather than several\nimages (or features). Specifically, we introduce one track token for each frame\nto collect the target's appearance information in the backbone. Then, we design\na mamba-based Temporal Module for track tokens to be aware of context by\ninteracting with other track tokens within a sliding window. This module\nconsists of a mamba layer with autoregressive characteristic and a\ncross-attention layer with strong global perception ability, ensuring\nsufficient interaction for track tokens to perceive the appearance changes and\nmovement trends of the target. Finally, track tokens serve as a guidance to\nadjust the appearance feature for the final prediction in the head. Experiments\nshow our method is effective and achieves competitive performance on multiple\nbenchmarks at a real-time speed. Code and trained models will be available at\nhttps://github.com/GXNU-ZhongLab/TemTrack.\n","authors":["Jinxia Xie","Bineng Zhong","Qihua Liang","Ning Li","Zhiyi Mo","Shuxiang Song"],"pdf_url":"https://arxiv.org/pdf/2412.13611v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.13610v1","updated":"2024-12-18T08:37:13Z","published":"2024-12-18T08:37:13Z","title":"Faster and Stronger: When ANN-SNN Conversion Meets Parallel Spiking\n  Calculation","summary":"  Spiking Neural Network (SNN), as a brain-inspired and energy-efficient\nnetwork, is currently facing the pivotal challenge of exploring a suitable and\nefficient learning framework. The predominant training methodologies, namely\nSpatial-Temporal Back-propagation (STBP) and ANN-SNN Conversion, are encumbered\nby substantial training overhead or pronounced inference latency, which impedes\nthe advancement of SNNs in scaling to larger networks and navigating intricate\napplication domains. In this work, we propose a novel parallel conversion\nlearning framework, which establishes a mathematical mapping relationship\nbetween each time-step of the parallel spiking neurons and the cumulative spike\nfiring rate. We theoretically validate the lossless and sorting properties of\nthe conversion process, as well as pointing out the optimal shifting distance\nfor each step. Furthermore, by integrating the above framework with the\ndistribution-aware error calibration technique, we can achieve efficient\nconversion towards more general activation functions or training-free\ncircumstance. Extensive experiments have confirmed the significant performance\nadvantages of our method for various conversion cases under ultra-low time\nlatency. To our best knowledge, this is the first work which jointly utilizes\nparallel spiking calculation and ANN-SNN Conversion, providing a highly\npromising approach for SNN supervised training.\n","authors":["Zecheng Hao","Zhaofei Yu","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2412.13610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13609v1","updated":"2024-12-18T08:36:35Z","published":"2024-12-18T08:36:35Z","title":"Sign-IDD: Iconicity Disentangled Diffusion for Sign Language Production","summary":"  Sign Language Production (SLP) aims to generate semantically consistent sign\nvideos from textual statements, where the conversion from textual glosses to\nsign poses (G2P) is a crucial step. Existing G2P methods typically treat sign\nposes as discrete three-dimensional coordinates and directly fit them, which\noverlooks the relative positional relationships among joints. To this end, we\nprovide a new perspective, constraining joint associations and gesture details\nby modeling the limb bones to improve the accuracy and naturalness of the\ngenerated poses. In this work, we propose a pioneering iconicity disentangled\ndiffusion framework, termed Sign-IDD, specifically designed for SLP. Sign-IDD\nincorporates a novel Iconicity Disentanglement (ID) module to bridge the gap\nbetween relative positions among joints. The ID module disentangles the\nconventional 3D joint representation into a 4D bone representation, comprising\nthe 3D spatial direction vector and 1D spatial distance vector between adjacent\njoints. Additionally, an Attribute Controllable Diffusion (ACD) module is\nintroduced to further constrain joint associations, in which the attribute\nseparation layer aims to separate the bone direction and length attributes, and\nthe attribute control layer is designed to guide the pose generation by\nleveraging the above attributes. The ACD module utilizes the gloss embeddings\nas semantic conditions and finally generates sign poses from noise embeddings.\nExtensive experiments on PHOENIX14T and USTC-CSL datasets validate the\neffectiveness of our method. The code is available at:\nhttps://github.com/NaVi-start/Sign-IDD.\n","authors":["Shengeng Tang","Jiayi He","Dan Guo","Yanyan Wei","Feng Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2412.13609v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.13601v1","updated":"2024-12-18T08:31:34Z","published":"2024-12-18T08:31:34Z","title":"Hybrid CNN-LSTM based Indoor Pedestrian Localization with CSI\n  Fingerprint Maps","summary":"  The paper presents a novel Wi-Fi fingerprinting system that uses Channel\nState Information (CSI) data for fine-grained pedestrian localization. The\nproposed system exploits the frequency diversity and spatial diversity of the\nfeatures extracted from CSI data to generate a 2D+channel image termed as a CSI\nFingerprint Map. We then use this CSI Fingerprint Map representation of CSI\ndata to generate a pedestrian trajectory hypothesis using a hybrid architecture\nthat combines a Convolutional Neural Network and a Long Short-Term Memory\nRecurrent Neural Network model. The proposed architecture exploits the temporal\nand spatial relationship information among the CSI data observations gathered\nat neighboring locations. A particle filter is then employed to separate out\nthe most likely hypothesis matching a human walk model. The experimental\nperformance of our method is compared to existing deep learning localization\nmethods such ConFi, DeepFi and to a self-developed temporal-feature based LSTM\nbased location classifier. The experimental results show marked improvement\nwith an average RMSE of 0.36 m in a moderately dynamic and 0.17 m in a static\nenvironment. Our method is essentially a proof of concept that with (1) sparse\navailability of observations, (2) limited infrastructure requirements, (3)\nmoderate level of short-term and long-term noise in the training and testing\nenvironment, reliable fine-grained Wi-Fi based pedestrian localization is a\npotential option.\n","authors":["Muhammad Emad-ud-din"],"pdf_url":"https://arxiv.org/pdf/2412.13601v1.pdf","comment":"12 pages, 14 figures and 3 tables"},{"id":"http://arxiv.org/abs/2412.13599v1","updated":"2024-12-18T08:31:26Z","published":"2024-12-18T08:31:26Z","title":"Unlocking the Potential of Weakly Labeled Data: A Co-Evolutionary\n  Learning Framework for Abnormality Detection and Report Generation","summary":"  Anatomical abnormality detection and report generation of chest X-ray (CXR)\nare two essential tasks in clinical practice. The former aims at localizing and\ncharacterizing cardiopulmonary radiological findings in CXRs, while the latter\nsummarizes the findings in a detailed report for further diagnosis and\ntreatment. Existing methods often focused on either task separately, ignoring\ntheir correlation. This work proposes a co-evolutionary abnormality detection\nand report generation (CoE-DG) framework. The framework utilizes both fully\nlabeled (with bounding box annotations and clinical reports) and weakly labeled\n(with reports only) data to achieve mutual promotion between the abnormality\ndetection and report generation tasks. Specifically, we introduce a\nbi-directional information interaction strategy with generator-guided\ninformation propagation (GIP) and detector-guided information propagation\n(DIP). For semi-supervised abnormality detection, GIP takes the informative\nfeature extracted by the generator as an auxiliary input to the detector and\nuses the generator's prediction to refine the detector's pseudo labels. We\nfurther propose an intra-image-modal self-adaptive non-maximum suppression\nmodule (SA-NMS). This module dynamically rectifies pseudo detection labels\ngenerated by the teacher detection model with high-confidence predictions by\nthe student.Inversely, for report generation, DIP takes the abnormalities'\ncategories and locations predicted by the detector as input and guidance for\nthe generator to improve the generated reports.\n","authors":["Jinghan Sun","Dong Wei","Zhe Xu","Donghuan Lu","Hong Liu","Hong Wang","Sotirios A. Tsaftaris","Steven McDonagh","Yefeng Zheng","Liansheng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04363v2","updated":"2024-12-18T08:30:59Z","published":"2024-04-05T19:16:30Z","title":"Idea23D: Collaborative LMM Agents Enable 3D Model Generation from\n  Interleaved Multimodal Inputs","summary":"  With the success of 2D diffusion models, 2D AIGC content has already\ntransformed our lives. Recently, this success has been extended to 3D AIGC,\nwith state-of-the-art methods generating textured 3D models from single images\nor text. However, we argue that current 3D AIGC methods still do not fully\nunleash human creativity. We often imagine 3D content made from multimodal\ninputs, such as what it would look like if my pet bunny were eating a doughnut\non the table. In this paper, we explore a novel 3D AIGC approach: generating 3D\ncontent from IDEAs. An IDEA is a multimodal input composed of text, image, and\n3D models. To our knowledge, this challenging and exciting 3D AIGC setting has\nnot been studied before. We propose the new framework Idea23D, which combines\nthree agents based on large multimodal models (LMMs) and existing algorithmic\ntools. These three LMM-based agents are tasked with prompt generation, model\nselection, and feedback reflection. They collaborate and critique each other in\na fully automated loop, without human intervention. The framework then\ngenerates a text prompt to create 3D models that align closely with the input\nIDEAs. We demonstrate impressive 3D AIGC results that surpass previous methods.\nTo comprehensively assess the 3D AIGC capabilities of Idea23D, we introduce the\nEval3DAIGC-198 dataset, containing 198 multimodal inputs for 3D generation\ntasks. This dataset evaluates the alignment between generated 3D content and\ninput IDEAs. Our user study and quantitative results show that Idea23D\nsignificantly improves the success rate and accuracy of 3D generation, with\nexcellent compatibility across various LMM, Text-to-Image, and Image-to-3D\nmodels. Code and dataset are available at \\url{https://idea23d.github.io/}.\n","authors":["Junhao Chen","Xiang Li","Xiaojun Ye","Chao Li","Zhaoxin Fan","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2404.04363v2.pdf","comment":"Accepted by COLING 2025 (The 31st International Conference on\n  Computational Linguistics) Project Page: https://idea23d.github.io/ Code:\n  https://github.com/yisuanwang/Idea23D"},{"id":"http://arxiv.org/abs/2412.10824v2","updated":"2024-12-18T08:25:55Z","published":"2024-12-14T13:05:05Z","title":"Diffusion Model from Scratch","summary":"  Diffusion generative models are currently the most popular generative models.\nHowever, their underlying modeling process is quite complex, and starting\ndirectly with the seminal paper Denoising Diffusion Probability Model (DDPM)\ncan be challenging. This paper aims to assist readers in building a\nfoundational understanding of generative models by tracing the evolution from\nVAEs to DDPM through detailed mathematical derivations and a problem-oriented\nanalytical approach. It also explores the core ideas and improvement strategies\nof current mainstream methodologies, providing guidance for undergraduate and\ngraduate students interested in learning about diffusion models.\n","authors":["Wang Zhen","Dong Yunyun"],"pdf_url":"https://arxiv.org/pdf/2412.10824v2.pdf","comment":"There were problems with the typography of our illustrations, and\n  there were problems with the derivation of the 200-step formula"},{"id":"http://arxiv.org/abs/2412.13594v1","updated":"2024-12-18T08:18:03Z","published":"2024-12-18T08:18:03Z","title":"Generalizable Sensor-Based Activity Recognition via Categorical Concept\n  Invariant Learning","summary":"  Human Activity Recognition (HAR) aims to recognize activities by training\nmodels on massive sensor data. In real-world deployment, a crucial aspect of\nHAR that has been largely overlooked is that the test sets may have different\ndistributions from training sets due to inter-subject variability including\nage, gender, behavioral habits, etc., which leads to poor generalization\nperformance. One promising solution is to learn domain-invariant\nrepresentations to enable a model to generalize on an unseen distribution.\nHowever, most existing methods only consider the feature-invariance of the\npenultimate layer for domain-invariant learning, which leads to suboptimal\nresults. In this paper, we propose a Categorical Concept Invariant Learning\n(CCIL) framework for generalizable activity recognition, which introduces a\nconcept matrix to regularize the model in the training stage by simultaneously\nconcentrating on feature-invariance and logit-invariance. Our key idea is that\nthe concept matrix for samples belonging to the same activity category should\nbe similar. Extensive experiments on four public HAR benchmarks demonstrate\nthat our CCIL substantially outperforms the state-of-the-art approaches under\ncross-person, cross-dataset, cross-position, and one-person-to-another\nsettings.\n","authors":["Di Xiong","Shuoyuan Wang","Lei Zhang","Wenbo Huang","Chaolei Han"],"pdf_url":"https://arxiv.org/pdf/2412.13594v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11077v2","updated":"2024-12-18T07:59:03Z","published":"2024-12-15T06:22:20Z","title":"Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for\n  Training-Free Zero-Shot Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) aims to retrieve target images that closely\nresemble a reference image while integrating user-specified textual\nmodifications, thereby capturing user intent more precisely. Existing\ntraining-free zero-shot CIR (ZS-CIR) methods often employ a two-stage process:\nthey first generate a caption for the reference image and then use Large\nLanguage Models for reasoning to obtain a target description. However, these\nmethods suffer from missing critical visual details and limited reasoning\ncapabilities, leading to suboptimal retrieval performance. To address these\nchallenges, we propose a novel, training-free one-stage method, One-Stage\nReflective Chain-of-Thought Reasoning for ZS-CIR (OSrCIR), which employs\nMultimodal Large Language Models to retain essential visual information in a\nsingle-stage reasoning process, eliminating the information loss seen in\ntwo-stage methods. Our Reflective Chain-of-Thought framework further improves\ninterpretative accuracy by aligning manipulation intent with contextual cues\nfrom reference images. OSrCIR achieves performance gains of 1.80% to 6.44% over\nexisting training-free methods across multiple tasks, setting new\nstate-of-the-art results in ZS-CIR and enhancing its utility in vision-language\napplications. Our code will be available at\nhttps://github.com/Pter61/osrcir2024/.\n","authors":["Yuanmin Tang","Xiaoting Qin","Jue Zhang","Jing Yu","Gaopeng Gou","Gang Xiong","Qingwei Ling","Saravan Rajmohan","Dongmei Zhang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2412.11077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12974v2","updated":"2024-12-18T07:52:14Z","published":"2024-12-17T14:56:59Z","title":"Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential\n  via Self-Attention Redirection Guidance","summary":"  Recently, diffusion models have emerged as promising newcomers in the field\nof generative models, shining brightly in image generation. However, when\nemployed for object removal tasks, they still encounter issues such as\ngenerating random artifacts and the incapacity to repaint foreground object\nareas with appropriate content after removal. To tackle these problems, we\npropose Attentive Eraser, a tuning-free method to empower pre-trained diffusion\nmodels for stable and effective object removal. Firstly, in light of the\nobservation that the self-attention maps influence the structure and shape\ndetails of the generated images, we propose Attention Activation and\nSuppression (ASS), which re-engineers the self-attention mechanism within the\npre-trained diffusion models based on the given mask, thereby prioritizing the\nbackground over the foreground object during the reverse generation process.\nMoreover, we introduce Self-Attention Redirection Guidance (SARG), which\nutilizes the self-attention redirected by ASS to guide the generation process,\neffectively removing foreground objects within the mask while simultaneously\ngenerating content that is both plausible and coherent. Experiments demonstrate\nthe stability and effectiveness of Attentive Eraser in object removal across a\nvariety of pre-trained diffusion models, outperforming even training-based\nmethods. Furthermore, Attentive Eraser can be implemented in various diffusion\nmodel architectures and checkpoints, enabling excellent scalability. Code is\navailable at https://github.com/Anonym0u3/AttentiveEraser.\n","authors":["Wenhao Sun","Benlei Cui","Xue-Mei Dong","Jingqun Tang"],"pdf_url":"https://arxiv.org/pdf/2412.12974v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13577v1","updated":"2024-12-18T07:51:35Z","published":"2024-12-18T07:51:35Z","title":"Bridge then Begin Anew: Generating Target-relevant Intermediate Model\n  for Source-free Visual Emotion Adaptation","summary":"  Visual emotion recognition (VER), which aims at understanding humans'\nemotional reactions toward different visual stimuli, has attracted increasing\nattention. Given the subjective and ambiguous characteristics of emotion,\nannotating a reliable large-scale dataset is hard. For reducing reliance on\ndata labeling, domain adaptation offers an alternative solution by adapting\nmodels trained on labeled source data to unlabeled target data. Conventional\ndomain adaptation methods require access to source data. However, due to\nprivacy concerns, source emotional data may be inaccessible. To address this\nissue, we propose an unexplored task: source-free domain adaptation (SFDA) for\nVER, which does not have access to source data during the adaptation process.\nTo achieve this, we propose a novel framework termed Bridge then Begin Anew\n(BBA), which consists of two steps: domain-bridged model generation (DMG) and\ntarget-related model adaptation (TMA). First, the DMG bridges cross-domain gaps\nby generating an intermediate model, avoiding direct alignment between two VER\ndatasets with significant differences. Then, the TMA begins training the target\nmodel anew to fit the target structure, avoiding the influence of\nsource-specific knowledge. Extensive experiments are conducted on six SFDA\nsettings for VER. The results demonstrate the effectiveness of BBA, which\nachieves remarkable performance gains compared with state-of-the-art SFDA\nmethods and outperforms representative unsupervised domain adaptation\napproaches.\n","authors":["Jiankun Zhu","Sicheng Zhao","Jing Jiang","Wenbo Tang","Zhaopan Xu","Tingting Han","Pengfei Xu","Hongxun Yao"],"pdf_url":"https://arxiv.org/pdf/2412.13577v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.13573v1","updated":"2024-12-18T07:45:30Z","published":"2024-12-18T07:45:30Z","title":"Seeking Consistent Flat Minima for Better Domain Generalization via\n  Refining Loss Landscapes","summary":"  Domain generalization aims to learn a model from multiple training domains\nand generalize it to unseen test domains. Recent theory has shown that seeking\nthe deep models, whose parameters lie in the flat minima of the loss landscape,\ncan significantly reduce the out-of-domain generalization error. However,\nexisting methods often neglect the consistency of loss landscapes in different\ndomains, resulting in models that are not simultaneously in the optimal flat\nminima in all domains, which limits their generalization ability. To address\nthis issue, this paper proposes an iterative Self-Feedback Training (SFT)\nframework to seek consistent flat minima that are shared across different\ndomains by progressively refining loss landscapes during training. It\nalternatively generates a feedback signal by measuring the inconsistency of\nloss landscapes in different domains and refines these loss landscapes for\ngreater consistency using this feedback signal. Benefiting from the consistency\nof the flat minima within these refined loss landscapes, our SFT helps achieve\nbetter out-of-domain generalization. Extensive experiments on DomainBed\ndemonstrate superior performances of SFT when compared to state-of-the-art\nsharpness-aware methods and other prevalent DG baselines. On average across\nfive DG benchmarks, SFT surpasses the sharpness-aware minimization by 2.6% with\nResNet-50 and 1.5% with ViT-B/16, respectively. The code will be available\nsoon.\n","authors":["Aodi Li","Liansheng Zhuang","Xiao Long","Minghong Yao","Shafei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08584v2","updated":"2024-12-18T07:45:11Z","published":"2024-10-11T07:24:21Z","title":"ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification","summary":"  The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.\n","authors":["Yefei He","Feng Chen","Jing Liu","Wenqi Shao","Hong Zhou","Kaipeng Zhang","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2410.08584v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2411.18290v2","updated":"2024-12-18T07:40:45Z","published":"2024-11-27T12:28:46Z","title":"Leveraging Semantic Asymmetry for Precise Gross Tumor Volume\n  Segmentation of Nasopharyngeal Carcinoma in Planning CT","summary":"  In the radiation therapy of nasopharyngeal carcinoma (NPC), clinicians\ntypically delineate the gross tumor volume (GTV) using non-contrast planning\ncomputed tomography to ensure accurate radiation dose delivery. However, the\nlow contrast between tumors and adjacent normal tissues necessitates that\nradiation oncologists manually delineate the tumors, often relying on\ndiagnostic MRI for guidance. % In this study, we propose a novel approach to\ndirectly segment NPC gross tumors on non-contrast planning CT images,\ncircumventing potential registration errors when aligning MRI or MRI-derived\ntumor masks to planning CT. To address the low contrast issues between tumors\nand adjacent normal structures in planning CT, we introduce a 3D Semantic\nAsymmetry Tumor segmentation (SATs) method. Specifically, we posit that a\nhealthy nasopharyngeal region is characteristically bilaterally symmetric,\nwhereas the emergence of nasopharyngeal carcinoma disrupts this symmetry. Then,\nwe propose a Siamese contrastive learning segmentation framework that minimizes\nthe voxel-wise distance between original and flipped areas without tumor and\nencourages a larger distance between original and flipped areas with tumor.\nThus, our approach enhances the sensitivity of features to semantic\nasymmetries. % Extensive experiments demonstrate that the proposed SATs\nachieves the leading NPC GTV segmentation performance in both internal and\nexternal testing, \\emph{e.g.}, with at least 2\\% absolute Dice score\nimprovement and 12\\% average distance error reduction when compared to other\nstate-of-the-art methods in the external testing.\n","authors":["Zi Li","Ying Chen","Zeli Chen","Yanzhou Su","Tai Ma","Tony C. W. Mok","Yan-Jie Zhou","Yunhai Bai","Zhinlin Zheng","Le Lu","Yirui Wang","Jia Ge","Xianghua Ye","Senxiang Yan","Dakai Jin"],"pdf_url":"https://arxiv.org/pdf/2411.18290v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13569v1","updated":"2024-12-18T07:35:42Z","published":"2024-12-18T07:35:42Z","title":"Multi-View Pedestrian Occupancy Prediction with a Novel Synthetic\n  Dataset","summary":"  We address an advanced challenge of predicting pedestrian occupancy as an\nextension of multi-view pedestrian detection in urban traffic. To support this,\nwe have created a new synthetic dataset called MVP-Occ, designed for dense\npedestrian scenarios in large-scale scenes. Our dataset provides detailed\nrepresentations of pedestrians using voxel structures, accompanied by rich\nsemantic scene understanding labels, facilitating visual navigation and\ninsights into pedestrian spatial information. Furthermore, we present a robust\nbaseline model, termed OmniOcc, capable of predicting both the voxel occupancy\nstate and panoptic labels for the entire scene from multi-view images. Through\nin-depth analysis, we identify and evaluate the key elements of our proposed\nmodel, highlighting their specific contributions and importance.\n","authors":["Sithu Aung","Min-Cheol Sagong","Junghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2412.13569v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13565v1","updated":"2024-12-18T07:33:22Z","published":"2024-12-18T07:33:22Z","title":"CA-Edit: Causality-Aware Condition Adapter for High-Fidelity Local\n  Facial Attribute Editing","summary":"  For efficient and high-fidelity local facial attribute editing, most existing\nediting methods either require additional fine-tuning for different editing\neffects or tend to affect beyond the editing regions. Alternatively, inpainting\nmethods can edit the target image region while preserving external areas.\nHowever, current inpainting methods still suffer from the generation\nmisalignment with facial attributes description and the loss of facial skin\ndetails. To address these challenges, (i) a novel data utilization strategy is\nintroduced to construct datasets consisting of attribute-text-image triples\nfrom a data-driven perspective, (ii) a Causality-Aware Condition Adapter is\nproposed to enhance the contextual causality modeling of specific details,\nwhich encodes the skin details from the original image while preventing\nconflicts between these cues and textual conditions. In addition, a Skin\nTransition Frequency Guidance technique is introduced for the local modeling of\ncontextual causality via sampling guidance driven by low-frequency alignment.\nExtensive quantitative and qualitative experiments demonstrate the\neffectiveness of our method in boosting both fidelity and editability for\nlocalized attribute editing. The code is available at\nhttps://github.com/connorxian/CA-Edit.\n","authors":["Xiaole Xian","Xilin He","Zenghao Niu","Junliang Zhang","Weicheng Xie","Siyang Song","Zitong Yu","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2412.13565v1.pdf","comment":"accepted by aaai"},{"id":"http://arxiv.org/abs/2412.04062v2","updated":"2024-12-18T07:28:52Z","published":"2024-12-05T10:57:08Z","title":"ZipAR: Accelerating Auto-regressive Image Generation through Spatial\n  Locality","summary":"  In this paper, we propose ZipAR, a training-free, plug-and-play parallel\ndecoding framework for accelerating auto-regressive (AR) visual generation. The\nmotivation stems from the observation that images exhibit local structures, and\nspatially distant regions tend to have minimal interdependence. Given a\npartially decoded set of visual tokens, in addition to the original next-token\nprediction scheme in the row dimension, the tokens corresponding to spatially\nadjacent regions in the column dimension can be decoded in parallel, enabling\nthe ``next-set prediction'' paradigm. By decoding multiple tokens\nsimultaneously in a single forward pass, the number of forward passes required\nto generate an image is significantly reduced, resulting in a substantial\nimprovement in generation efficiency. Experiments demonstrate that ZipAR can\nreduce the number of model forward passes by up to 91% on the Emu3-Gen model\nwithout requiring any additional retraining. Code is available here:\nhttps://github.com/ThisisBillhe/ZipAR.\n","authors":["Yefei He","Feng Chen","Yuanyu He","Shaoxuan He","Hong Zhou","Kaipeng Zhang","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2412.04062v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2412.08941v2","updated":"2024-12-18T07:26:20Z","published":"2024-12-12T05:08:05Z","title":"Optimized Gradient Clipping for Noisy Label Learning","summary":"  Previous research has shown that constraining the gradient of loss function\nwith respect to model-predicted probabilities can enhance the model robustness\nagainst noisy labels. These methods typically specify a fixed optimal threshold\nfor gradient clipping through validation data to obtain the desired robustness\nagainst noise. However, this common practice overlooks the dynamic distribution\nof gradients from both clean and noisy-labeled samples at different stages of\ntraining, significantly limiting the model capability to adapt to the variable\nnature of gradients throughout the training process. To address this issue, we\npropose a simple yet effective approach called Optimized Gradient Clipping\n(OGC), which dynamically adjusts the clipping threshold based on the ratio of\nnoise gradients to clean gradients after clipping, estimated by modeling the\ndistributions of clean and noisy samples. This approach allows us to modify the\nclipping threshold at each training step, effectively controlling the influence\nof noise gradients. Additionally, we provide statistical analysis to certify\nthe noise-tolerance ability of OGC. Our extensive experiments across various\ntypes of label noise, including symmetric, asymmetric, instance-dependent, and\nreal-world noise, demonstrate the effectiveness of our approach.\n","authors":["Xichen Ye","Yifan Wu","Weizhong Zhang","Xiaoqiang Li","Yifan Chen","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2412.08941v2.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2406.16993v2","updated":"2024-12-18T07:26:10Z","published":"2024-06-24T08:01:05Z","title":"Are Vision xLSTM Embedded UNet More Reliable in Medical 3D Image\n  Segmentation?","summary":"  The development of efficient segmentation strategies for medical images has\nevolved from its initial dependence on Convolutional Neural Networks (CNNs) to\nthe current investigation of hybrid models that combine CNNs with Vision\nTransformers. There is an increasing focus on creating architectures that are\nboth high-performance and computationally efficient, able to be deployed on\nremote systems with limited resources. Although transformers can capture global\ndependencies in the input space, they face challenges from the corresponding\nhigh computational and storage expenses involved. This paper investigates the\nintegration of CNNs with Vision Extended Long Short-Term Memory (Vision-xLSTM)s\nby introducing the novel {\\it \\textbf{U-VixLSTM}}.\n  The Vision-xLSTM blocks capture temporal and global relationships within the\npatches, as extracted from the CNN feature maps. The convolutional feature\nreconstruction path upsamples the output volume from the Vision-xLSTM blocks,\nto produce the segmentation output. Our primary objective is to propose that\nVision-xLSTM forms an appropriate backbone for medical image segmentation,\noffering excellent performance with reduced computational costs. The U-VixLSTM\nexhibits superior performance, compared to the state-of-the-art networks in the\npublicly available Synapse, ISIC and ACDC datasets. Code provided:\nhttps://github.com/duttapallabi2907/U-VixLSTM\n","authors":["Pallabi Dutta","Soham Bose","Swalpa Kumar Roy","Sushmita Mitra"],"pdf_url":"https://arxiv.org/pdf/2406.16993v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13558v1","updated":"2024-12-18T07:19:48Z","published":"2024-12-18T07:19:48Z","title":"Read Like a Radiologist: Efficient Vision-Language Model for 3D Medical\n  Imaging Interpretation","summary":"  Recent medical vision-language models (VLMs) have shown promise in 2D medical\nimage interpretation. However extending them to 3D medical imaging has been\nchallenging due to computational complexities and data scarcity. Although a few\nrecent VLMs specified for 3D medical imaging have emerged, all are limited to\nlearning volumetric representation of a 3D medical image as a set of\nsub-volumetric features. Such process introduces overly correlated\nrepresentations along the z-axis that neglect slice-specific clinical details,\nparticularly for 3D medical images where adjacent slices have low redundancy.\nTo address this limitation, we introduce MS-VLM that mimic radiologists'\nworkflow in 3D medical image interpretation. Specifically, radiologists analyze\n3D medical images by examining individual slices sequentially and synthesizing\ninformation across slices and views. Likewise, MS-VLM leverages self-supervised\n2D transformer encoders to learn a volumetric representation that capture\ninter-slice dependencies from a sequence of slice-specific features. Unbound by\nsub-volumetric patchification, MS-VLM is capable of obtaining useful volumetric\nrepresentations from 3D medical images with any slice length and from multiple\nimages acquired from different planes and phases. We evaluate MS-VLM on\npublicly available chest CT dataset CT-RATE and in-house rectal MRI dataset. In\nboth scenarios, MS-VLM surpasses existing methods in radiology report\ngeneration, producing more coherent and clinically relevant reports. These\nfindings highlight the potential of MS-VLM to advance 3D medical image\ninterpretation and improve the robustness of medical VLMs.\n","authors":["Changsun Lee","Sangjoon Park","Cheong-Il Shin","Woo Hee Choi","Hyun Jeong Park","Jeong Eun Lee","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2412.13558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10050v2","updated":"2024-12-18T07:08:26Z","published":"2024-12-13T11:22:01Z","title":"ManipGPT: Is Affordance Segmentation by Large Vision Models Enough for\n  Articulated Object Manipulation?","summary":"  Visual actionable affordance has emerged as a transformative approach in\nrobotics, focusing on perceiving interaction areas prior to manipulation.\nTraditional methods rely on pixel sampling to identify successful interaction\nsamples or processing pointclouds for affordance mapping. However, these\napproaches are computationally intensive and struggle to adapt to diverse and\ndynamic environments. This paper introduces ManipGPT, a framework designed to\npredict optimal interaction areas for articulated objects using a large\npre-trained vision transformer (ViT). We created a dataset of 9.9k simulated\nand real images to bridge the sim-to-real gap and enhance real-world\napplicability. By fine-tuning the vision transformer on this small dataset, we\nsignificantly improved part-level affordance segmentation, adapting the model's\nin-context segmentation capabilities to robot manipulation scenarios. This\nenables effective manipulation across simulated and real-world environments by\ngenerating part-level affordance masks, paired with an impedance adaptation\npolicy, sufficiently eliminating the need for complex datasets or perception\nsystems.\n","authors":["Taewhan Kim","Hojin Bae","Zeming Li","Xiaoqi Li","Iaroslav Ponomarenko","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2412.10050v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.06268v2","updated":"2024-12-18T07:08:01Z","published":"2024-12-09T07:39:39Z","title":"Open-Vocabulary High-Resolution 3D (OVHR3D) Data Segmentation and\n  Annotation Framework","summary":"  In the domain of the U.S. Army modeling and simulation, the availability of\nhigh quality annotated 3D data is pivotal to creating virtual environments for\ntraining and simulations. Traditional methodologies for 3D semantic and\ninstance segmentation, such as KpConv, RandLA, Mask3D, etc., are designed to\ntrain on extensive labeled datasets to obtain satisfactory performance in\npractical tasks. This requirement presents a significant challenge, given the\ninherent scarcity of manually annotated 3D datasets, particularly for the\nmilitary use cases. Recognizing this gap, our previous research leverages the\nOne World Terrain data repository manually annotated databases, as showcased at\nIITSEC 2019 and 2021, to enrich the training dataset for deep learning models.\nHowever, collecting and annotating large scale 3D data for specific tasks\nremains costly and inefficient. To this end, the objective of this research is\nto design and develop a comprehensive and efficient framework for 3D\nsegmentation tasks to assist in 3D data annotation. This framework integrates\nGrounding DINO and Segment anything Model, augmented by an enhancement in 2D\nimage rendering via 3D mesh. Furthermore, the authors have also developed a\nuser friendly interface that facilitates the 3D annotation process, offering\nintuitive visualization of rendered images and the 3D point cloud.\n","authors":["Jiuyi Xu","Meida Chen","Andrew Feng","Zifan Yu","Yangming Shi"],"pdf_url":"https://arxiv.org/pdf/2412.06268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13552v1","updated":"2024-12-18T07:02:01Z","published":"2024-12-18T07:02:01Z","title":"DragScene: Interactive 3D Scene Editing with Single-view Drag\n  Instructions","summary":"  3D editing has shown remarkable capability in editing scenes based on various\ninstructions. However, existing methods struggle with achieving intuitive,\nlocalized editing, such as selectively making flowers blossom. Drag-style\nediting has shown exceptional capability to edit images with direct\nmanipulation instead of ambiguous text commands. Nevertheless, extending\ndrag-based editing to 3D scenes presents substantial challenges due to\nmulti-view inconsistency. To this end, we introduce DragScene, a framework that\nintegrates drag-style editing with diverse 3D representations. First, latent\noptimization is performed on a reference view to generate 2D edits based on\nuser instructions. Subsequently, coarse 3D clues are reconstructed from the\nreference view using a point-based representation to capture the geometric\ndetails of the edits. The latent representation of the edited view is then\nmapped to these 3D clues, guiding the latent optimization of other views. This\nprocess ensures that edits are propagated seamlessly across multiple views,\nmaintaining multi-view consistency. Finally, the target 3D scene is\nreconstructed from the edited multi-view images. Extensive experiments\ndemonstrate that DragScene facilitates precise and flexible drag-style editing\nof 3D scenes, supporting broad applicability across diverse 3D representations.\n","authors":["Chenghao Gu","Zhenzhe Li","Zhengqi Zhang","Yunpeng Bai","Shuzhao Xie","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18853v2","updated":"2024-12-18T06:55:31Z","published":"2024-02-29T05:00:30Z","title":"Rethinking Multi-domain Generalization with A General Learning Objective","summary":"  Multi-domain generalization (mDG) is universally aimed to minimize the\ndiscrepancy between training and testing distributions to enhance\nmarginal-to-label distribution mapping. However, existing mDG literature lacks\na general learning objective paradigm and often imposes constraints on static\ntarget marginal distributions. In this paper, we propose to leverage a\n$Y$-mapping to relax the constraint. We rethink the learning objective for mDG\nand design a new \\textbf{general learning objective} to interpret and analyze\nmost existing mDG wisdom. This general objective is bifurcated into two\nsynergistic amis: learning domain-independent conditional features and\nmaximizing a posterior. Explorations also extend to two effective\nregularization terms that incorporate prior information and suppress invalid\ncausality, alleviating the issues that come with relaxed constraints. We\ntheoretically contribute an upper bound for the domain alignment of\ndomain-independent conditional features, disclosing that many previous mDG\nendeavors actually \\textbf{optimize partially the objective} and thus lead to\nlimited performance. As such, our study distills a general learning objective\ninto four practical components, providing a general, robust, and flexible\nmechanism to handle complex domain shifts. Extensive empirical results indicate\nthat the proposed objective with $Y$-mapping leads to substantially better mDG\nperformance in various downstream tasks, including regression, segmentation,\nand classification.\n","authors":["Zhaorui Tan","Xi Yang","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2402.18853v2.pdf","comment":"Accepted by CVPR24"},{"id":"http://arxiv.org/abs/2412.13547v1","updated":"2024-12-18T06:46:40Z","published":"2024-12-18T06:46:40Z","title":"Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance\n  Fields","summary":"  Novel-view synthesis is an important problem in computer vision with\napplications in 3D reconstruction, mixed reality, and robotics. Recent methods\nlike 3D Gaussian Splatting (3DGS) have become the preferred method for this\ntask, providing high-quality novel views in real time. However, the training\ntime of a 3DGS model is slow, often taking 30 minutes for a scene with 200\nviews. In contrast, our goal is to reduce the optimization time by training for\nfewer steps while maintaining high rendering quality. Specifically, we combine\nthe guidance from both the position error and the appearance error to achieve a\nmore effective densification. To balance the rate between adding new Gaussians\nand fitting old Gaussians, we develop a convergence-aware budget control\nmechanism. Moreover, to make the densification process more reliable, we\nselectively add new Gaussians from mostly visited regions. With these designs,\nwe reduce the Gaussian optimization steps to one-third of the previous approach\nwhile achieving a comparable or even better novel view rendering quality. To\nfurther facilitate the rapid fitting of 4K resolution images, we introduce a\ndilation-based rendering technique. Our method, Turbo-GS, speeds up\noptimization for typical scenes and scales well to high-resolution (4K)\nscenarios on standard datasets. Through extensive experiments, we show that our\nmethod is significantly faster in optimization than other methods while\nretaining quality. Project page: https://ivl.cs.brown.edu/research/turbo-gs.\n","authors":["Tao Lu","Ankit Dhiman","R Srinath","Emre Arslan","Angela Xing","Yuanbo Xiangli","R Venkatesh Babu","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2412.13547v1.pdf","comment":"Project page: https://ivl.cs.brown.edu/research/turbo-gs"},{"id":"http://arxiv.org/abs/2404.17805v2","updated":"2024-12-18T06:46:25Z","published":"2024-04-27T07:05:41Z","title":"From Optimization to Generalization: Fair Federated Learning against\n  Quality Shift via Inter-Client Sharpness Matching","summary":"  Due to escalating privacy concerns, federated learning has been recognized as\na vital approach for training deep neural networks with decentralized medical\ndata. In practice, it is challenging to ensure consistent imaging quality\nacross various institutions, often attributed to equipment malfunctions\naffecting a minority of clients. This imbalance in image quality can cause the\nfederated model to develop an inherent bias towards higher-quality images, thus\nposing a severe fairness issue. In this study, we pioneer the identification\nand formulation of this new fairness challenge within the context of the\nimaging quality shift. Traditional methods for promoting fairness in federated\nlearning predominantly focus on balancing empirical risks across diverse client\ndistributions. This strategy primarily facilitates fair optimization across\ndifferent training data distributions, yet neglects the crucial aspect of\ngeneralization. To address this, we introduce a solution termed Federated\nlearning with Inter-client Sharpness Matching (FedISM). FedISM enhances both\nlocal training and global aggregation by incorporating sharpness-awareness,\naiming to harmonize the sharpness levels across clients for fair\ngeneralization. Our empirical evaluations, conducted using the widely-used ICH\nand ISIC 2019 datasets, establish FedISM's superiority over current\nstate-of-the-art federated learning methods in promoting fairness. Code is\navailable at https://github.com/wnn2000/FFL4MIA.\n","authors":["Nannan Wu","Zhuo Kuang","Zengqiang Yan","Li Yu"],"pdf_url":"https://arxiv.org/pdf/2404.17805v2.pdf","comment":"This paper is accepted at IJCAI'24 (Main Track)"},{"id":"http://arxiv.org/abs/2412.13543v1","updated":"2024-12-18T06:43:06Z","published":"2024-12-18T06:43:06Z","title":"Query-centric Audio-Visual Cognition Network for Moment Retrieval,\n  Segmentation and Step-Captioning","summary":"  Video has emerged as a favored multimedia format on the internet. To better\ngain video contents, a new topic HIREST is presented, including video\nretrieval, moment retrieval, moment segmentation, and step-captioning. The\npioneering work chooses the pre-trained CLIP-based model for video retrieval,\nand leverages it as a feature extractor for other three challenging tasks\nsolved in a multi-task learning paradigm. Nevertheless, this work struggles to\nlearn the comprehensive cognition of user-preferred content, due to\ndisregarding the hierarchies and association relations across modalities. In\nthis paper, guided by the shallow-to-deep principle, we propose a query-centric\naudio-visual cognition (QUAG) network to construct a reliable multi-modal\nrepresentation for moment retrieval, segmentation and step-captioning.\nSpecifically, we first design the modality-synergistic perception to obtain\nrich audio-visual content, by modeling global contrastive alignment and local\nfine-grained interaction between visual and audio modalities. Then, we devise\nthe query-centric cognition that uses the deep-level query to perform the\ntemporal-channel filtration on the shallow-level audio-visual representation.\nThis can cognize user-preferred content and thus attain a query-centric\naudio-visual representation for three tasks. Extensive experiments show QUAG\nachieves the SOTA results on HIREST. Further, we test QUAG on the query-based\nvideo summarization task and verify its good generalization.\n","authors":["Yunbin Tu","Liang Li","Li Su","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2412.13543v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13541v1","updated":"2024-12-18T06:40:53Z","published":"2024-12-18T06:40:53Z","title":"Spatio-Temporal Fuzzy-oriented Multi-Modal Meta-Learning for\n  Fine-grained Emotion Recognition","summary":"  Fine-grained emotion recognition (FER) plays a vital role in various fields,\nsuch as disease diagnosis, personalized recommendations, and multimedia mining.\nHowever, existing FER methods face three key challenges in real-world\napplications: (i) they rely on large amounts of continuously annotated data to\nensure accuracy since emotions are complex and ambiguous in reality, which is\ncostly and time-consuming; (ii) they cannot capture the temporal heterogeneity\ncaused by changing emotion patterns, because they usually assume that the\ntemporal correlation within sampling periods is the same; (iii) they do not\nconsider the spatial heterogeneity of different FER scenarios, that is, the\ndistribution of emotion information in different data may have bias or\ninterference. To address these challenges, we propose a Spatio-Temporal\nFuzzy-oriented Multi-modal Meta-learning framework (ST-F2M). Specifically,\nST-F2M first divides the multi-modal videos into multiple views, and each view\ncorresponds to one modality of one emotion. Multiple randomly selected views\nfor the same emotion form a meta-training task. Next, ST-F2M uses an integrated\nmodule with spatial and temporal convolutions to encode the data of each task,\nreflecting the spatial and temporal heterogeneity. Then it adds fuzzy semantic\ninformation to each task based on generalized fuzzy rules, which helps handle\nthe complexity and ambiguity of emotions. Finally, ST-F2M learns\nemotion-related general meta-knowledge through meta-recurrent neural networks\nto achieve fast and robust fine-grained emotion recognition. Extensive\nexperiments show that ST-F2M outperforms various state-of-the-art methods in\nterms of accuracy and model efficiency. In addition, we construct ablation\nstudies and further analysis to explore why ST-F2M performs well.\n","authors":["Jingyao Wang","Yuxuan Yang","Wenwen Qiang","Changwen Zheng","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2412.13541v1.pdf","comment":"13 pages, Submitted to TMM in 30-May-2024"},{"id":"http://arxiv.org/abs/2412.13540v1","updated":"2024-12-18T06:35:18Z","published":"2024-12-18T06:35:18Z","title":"Benchmarking and Improving Large Vision-Language Models for Fundamental\n  Visual Graph Understanding and Reasoning","summary":"  Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross diverse tasks. Despite great success, recent studies show that LVLMs\nencounter substantial limitations when engaging with visual graphs. To study\nthe reason behind these limitations, we propose VGCure, a comprehensive\nbenchmark covering 22 tasks for examining the fundamental graph understanding\nand reasoning capacities of LVLMs. Extensive evaluations conducted on 14 LVLMs\nreveal that LVLMs are weak in basic graph understanding and reasoning tasks,\nparticularly those concerning relational or structurally complex information.\nBased on this observation, we propose a structure-aware fine-tuning framework\nto enhance LVLMs with structure learning abilities through 3 self-supervised\nlearning tasks. Experiments validate the effectiveness of our method in\nimproving LVLMs' zero-shot performance on fundamental graph learning tasks, as\nwell as enhancing the robustness of LVLMs against complex visual graphs.\n","authors":["Yingjie Zhu","Xuefeng Bai","Kehai Chen","Yang Xiang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06742v3","updated":"2024-12-18T06:33:44Z","published":"2024-08-13T09:03:00Z","title":"Long-Tailed Out-of-Distribution Detection: Prioritizing Attention to\n  Tail","summary":"  Current out-of-distribution (OOD) detection methods typically assume balanced\nin-distribution (ID) data, while most real-world data follow a long-tailed\ndistribution. Previous approaches to long-tailed OOD detection often involve\nbalancing the ID data by reducing the semantics of head classes. However, this\nreduction can severely affect the classification accuracy of ID data. The main\nchallenge of this task lies in the severe lack of features for tail classes,\nleading to confusion with OOD data. To tackle this issue, we introduce a novel\nPrioritizing Attention to Tail (PATT) method using augmentation instead of\nreduction. Our main intuition involves using a mixture of von Mises-Fisher\n(vMF) distributions to model the ID data and a temperature scaling module to\nboost the confidence of ID data. This enables us to generate infinite\ncontrastive pairs, implicitly enhancing the semantics of ID classes while\npromoting differentiation between ID and OOD data. To further strengthen the\ndetection of OOD data without compromising the classification performance of ID\ndata, we propose feature calibration during the inference phase. By extracting\nan attention weight from the training set that prioritizes the tail classes and\nreduces the confidence in OOD data, we improve the OOD detection capability.\nExtensive experiments verified that our method outperforms the current\nstate-of-the-art methods on various benchmarks.\n","authors":["Yina He","Lei Peng","Yongcun Zhang","Juanjuan Weng","Zhiming Luo","Shaozi Li"],"pdf_url":"https://arxiv.org/pdf/2408.06742v3.pdf","comment":"Accepted by AAAI'25. Extended version with full appendix, 13 pages"},{"id":"http://arxiv.org/abs/2412.13533v1","updated":"2024-12-18T06:19:03Z","published":"2024-12-18T06:19:03Z","title":"Language-guided Medical Image Segmentation with Target-informed\n  Multi-level Contrastive Alignments","summary":"  Medical image segmentation is crucial in modern medical image analysis, which\ncan aid into diagnosis of various disease conditions. Recently, language-guided\nsegmentation methods have shown promising results in automating image\nsegmentation where text reports are incorporated as guidance. These text\nreports, containing image impressions and insights given by clinicians,\nprovides auxiliary guidance. However, these methods neglect the inherent\npattern gaps between the two distinct modalities, which leads to sub-optimal\nimage-text feature fusion without proper cross-modality feature alignments.\nContrastive alignments are widely used to associate image-text semantics in\nrepresentation learning; however, it has not been exploited to bridge the\npattern gaps in language-guided segmentation that relies on subtle low level\nimage details to represent diseases. Existing contrastive alignment methods\ntypically algin high-level global image semantics without involving low-level,\nlocalized target information, and therefore fails to explore fine-grained text\nguidance for language-guided segmentation. In this study, we propose a\nlanguage-guided segmentation network with Target-informed Multi-level\nContrastive Alignments (TMCA). TMCA enables target-informed cross-modality\nalignments and fine-grained text guidance to bridge the pattern gaps in\nlanguage-guided segmentation. Specifically, we introduce: 1) a target-sensitive\nsemantic distance module that enables granular image-text alignment modelling,\nand 2) a multi-level alignment strategy that directs text guidance on low-level\nimage features. In addition, a language-guided target enhancement module is\nproposed to leverage the aligned text to redirect attention to focus on\ncritical localized image features. Extensive experiments on 4 image-text\ndatasets, involving 3 medical imaging modalities, demonstrated that our TMCA\nachieved superior performances.\n","authors":["Mingjian Li","Mingyuan Meng","Shuchang Ye","David Dagan Feng","Lei Bi","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2412.13533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13525v1","updated":"2024-12-18T05:52:16Z","published":"2024-12-18T05:52:16Z","title":"Hybrid Data-Free Knowledge Distillation","summary":"  Data-free knowledge distillation aims to learn a compact student network from\na pre-trained large teacher network without using the original training data of\nthe teacher network. Existing collection-based and generation-based methods\ntrain student networks by collecting massive real examples and generating\nsynthetic examples, respectively. However, they inevitably become weak in\npractical scenarios due to the difficulties in gathering or emulating\nsufficient real-world data. To solve this problem, we propose a novel method\ncalled \\textbf{H}ybr\\textbf{i}d \\textbf{D}ata-\\textbf{F}ree\n\\textbf{D}istillation (HiDFD), which leverages only a small amount of collected\ndata as well as generates sufficient examples for training student networks.\nOur HiDFD comprises two primary modules, \\textit{i.e.}, the teacher-guided\ngeneration and student distillation. The teacher-guided generation module\nguides a Generative Adversarial Network (GAN) by the teacher network to produce\nhigh-quality synthetic examples from very few real-world collected examples.\nSpecifically, we design a feature integration mechanism to prevent the GAN from\noverfitting and facilitate the reliable representation learning from the\nteacher network. Meanwhile, we drive a category frequency smoothing technique\nvia the teacher network to balance the generative training of each category. In\nthe student distillation module, we explore a data inflation strategy to\nproperly utilize a blend of real and synthetic data to train the student\nnetwork via a classifier-sharing-based feature alignment technique. Intensive\nexperiments across multiple benchmarks demonstrate that our HiDFD can achieve\nstate-of-the-art performance using 120 times less collected data than existing\nmethods. Code is available at https://github.com/tangjialiang97/HiDFD.\n","authors":["Jialiang Tang","Shuo Chen","Chen Gong"],"pdf_url":"https://arxiv.org/pdf/2412.13525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13161v2","updated":"2024-12-18T05:51:58Z","published":"2024-12-17T18:39:10Z","title":"BanglishRev: A Large-Scale Bangla-English and Code-mixed Dataset of\n  Product Reviews in E-Commerce","summary":"  This work presents the BanglishRev Dataset, the largest e-commerce product\nreview dataset to date for reviews written in Bengali, English, a mixture of\nboth and Banglish, Bengali words written with English alphabets. The dataset\ncomprises of 1.74 million written reviews from 3.2 million ratings information\ncollected from a total of 128k products being sold in online e-commerce\nplatforms targeting the Bengali population. It includes an extensive array of\nrelated metadata for each of the reviews including the rating given by the\nreviewer, date the review was posted and date of purchase, number of likes,\ndislikes, response from the seller, images associated with the review etc. With\nsentiment analysis being the most prominent usage of review datasets,\nexperimentation with a binary sentiment analysis model with the review rating\nserving as an indicator of positive or negative sentiment was conducted to\nevaluate the effectiveness of the large amount of data presented in BanglishRev\nfor sentiment analysis tasks. A BanglishBERT model is trained on the data from\nBanglishRev with reviews being considered labeled positive if the rating is\ngreater than 3 and negative if the rating is less than or equal to 3. The model\nis evaluated by being testing against a previously published manually annotated\ndataset for e-commerce reviews written in a mixture of Bangla, English and\nBanglish. The experimental model achieved an exceptional accuracy of 94\\% and\nF1 score of 0.94, demonstrating the dataset's efficacy for sentiment analysis.\nSome of the intriguing patterns and observations seen within the dataset and\nfuture research directions where the dataset can be utilized is also discussed\nand explored. The dataset can be accessed through\nhttps://huggingface.co/datasets/BanglishRev/bangla-english-and-code-mixed-ecommerce-review-dataset.\n","authors":["Mohammad Nazmush Shamael","Sabila Nawshin","Swakkhar Shatabda","Salekul Islam"],"pdf_url":"https://arxiv.org/pdf/2412.13161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08475v2","updated":"2024-12-18T05:47:35Z","published":"2024-09-13T02:02:07Z","title":"RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense\n  Positive Supervision","summary":"  RT-DETR is the first real-time end-to-end transformer-based object detector.\nIts efficiency comes from the framework design and the Hungarian matching.\nHowever, compared to dense supervision detectors like the YOLO series, the\nHungarian matching provides much sparser supervision, leading to insufficient\nmodel training and difficult to achieve optimal results. To address these\nissues, we proposed a hierarchical dense positive supervision method based on\nRT-DETR, named RT-DETRv3. Firstly, we introduce a CNN-based auxiliary branch\nthat provides dense supervision that collaborates with the original decoder to\nenhance the encoder feature representation. Secondly, to address insufficient\ndecoder training, we propose a novel learning strategy involving self-attention\nperturbation. This strategy diversifies label assignment for positive samples\nacross multiple query groups, thereby enriching positive supervisions.\nAdditionally, we introduce a shared-weight decoder branch for dense positive\nsupervision to ensure more high-quality queries matching each ground truth.\nNotably, all aforementioned modules are training-only. We conduct extensive\nexperiments to demonstrate the effectiveness of our approach on COCO val2017.\nRT-DETRv3 significantly outperforms existing real-time detectors, including the\nRT-DETR series and the YOLO series. For example, RT-DETRv3-R18 achieves 48.1%\nAP (+1.6%/+1.4%) compared to RT-DETR-R18/RT-DETRv2-R18, while maintaining the\nsame latency. Furthermore, RT-DETRv3-R101 can attain an impressive 54.6% AP\noutperforming YOLOv10-X. The code will be released at\nhttps://github.com/clxia12/RT-DETRv3.\n","authors":["Shuo Wang","Chunlong Xia","Feng Lv","Yifeng Shi"],"pdf_url":"https://arxiv.org/pdf/2409.08475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13510v1","updated":"2024-12-18T05:19:09Z","published":"2024-12-18T05:19:09Z","title":"Dynamic Adapter with Semantics Disentangling for Cross-lingual\n  Cross-modal Retrieval","summary":"  Existing cross-modal retrieval methods typically rely on large-scale\nvision-language pair data. This makes it challenging to efficiently develop a\ncross-modal retrieval model for under-resourced languages of interest.\nTherefore, Cross-lingual Cross-modal Retrieval (CCR), which aims to align\nvision and the low-resource language (the target language) without using any\nhuman-labeled target-language data, has gained increasing attention. As a\ngeneral parameter-efficient way, a common solution is to utilize adapter\nmodules to transfer the vision-language alignment ability of Vision-Language\nPretraining (VLP) models from a source language to a target language. However,\nthese adapters are usually static once learned, making it difficult to adapt to\ntarget-language captions with varied expressions. To alleviate it, we propose\nDynamic Adapter with Semantics Disentangling (DASD), whose parameters are\ndynamically generated conditioned on the characteristics of the input captions.\nConsidering that the semantics and expression styles of the input caption\nlargely influence how to encode it, we propose a semantic disentangling module\nto extract the semantic-related and semantic-agnostic features from the input,\nensuring that generated adapters are well-suited to the characteristics of\ninput caption. Extensive experiments on two image-text datasets and one\nvideo-text dataset demonstrate the effectiveness of our model for cross-lingual\ncross-modal retrieval, as well as its good compatibility with various VLP\nmodels.\n","authors":["Rui Cai","Zhiyu Dong","Jianfeng Dong","Xun Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13510v1.pdf","comment":"Accepted by the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.12772v2","updated":"2024-12-18T05:17:53Z","published":"2024-12-17T10:33:36Z","title":"Optimize the Unseen -- Fast NeRF Cleanup with Free Space Prior","summary":"  Neural Radiance Fields (NeRF) have advanced photorealistic novel view\nsynthesis, but their reliance on photometric reconstruction introduces\nartifacts, commonly known as \"floaters\". These artifacts degrade novel view\nquality, especially in areas unseen by the training cameras. We present a fast,\npost-hoc NeRF cleanup method that eliminates such artifacts by enforcing our\nFree Space Prior, effectively minimizing floaters without disrupting the NeRF's\nrepresentation of observed regions. Unlike existing approaches that rely on\neither Maximum Likelihood (ML) estimation to fit the data or a complex, local\ndata-driven prior, our method adopts a Maximum-a-Posteriori (MAP) approach,\nselecting the optimal model parameters under a simple global prior assumption\nthat unseen regions should remain empty. This enables our method to clean\nartifacts in both seen and unseen areas, enhancing novel view quality even in\nchallenging scene regions. Our method is comparable with existing NeRF cleanup\nmodels while being 2.5x faster in inference time, requires no additional memory\nbeyond the original NeRF, and achieves cleanup training in less than 30\nseconds. Our code will be made publically available.\n","authors":["Leo Segre","Shai Avidan"],"pdf_url":"https://arxiv.org/pdf/2412.12772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13508v1","updated":"2024-12-18T05:14:13Z","published":"2024-12-18T05:14:13Z","title":"Plug-and-Play Tri-Branch Invertible Block for Image Rescaling","summary":"  High-resolution (HR) images are commonly downscaled to low-resolution (LR) to\nreduce bandwidth, followed by upscaling to restore their original details.\nRecent advancements in image rescaling algorithms have employed invertible\nneural networks (INNs) to create a unified framework for downscaling and\nupscaling, ensuring a one-to-one mapping between LR and HR images. Traditional\nmethods, utilizing dual-branch based vanilla invertible blocks, process\nhigh-frequency and low-frequency information separately, often relying on\nspecific distributions to model high-frequency components. However, processing\nthe low-frequency component directly in the RGB domain introduces channel\nredundancy, limiting the efficiency of image reconstruction. To address these\nchallenges, we propose a plug-and-play tri-branch invertible block\n(T-InvBlocks) that decomposes the low-frequency branch into luminance (Y) and\nchrominance (CbCr) components, reducing redundancy and enhancing feature\nprocessing. Additionally, we adopt an all-zero mapping strategy for\nhigh-frequency components during upscaling, focusing essential rescaling\ninformation within the LR image. Our T-InvBlocks can be seamlessly integrated\ninto existing rescaling models, improving performance in both general rescaling\ntasks and scenarios involving lossy compression. Extensive experiments confirm\nthat our method advances the state of the art in HR image reconstruction.\n","authors":["Jingwei Bao","Jinhua Hao","Pengcheng Xu","Ming Sun","Chao Zhou","Shuyuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.13508v1.pdf","comment":"Accepted by AAAI 2025. Code is available at\n  https://github.com/Jingwei-Bao/T-InvBlocks"},{"id":"http://arxiv.org/abs/2410.15584v2","updated":"2024-12-18T05:13:44Z","published":"2024-10-21T02:10:49Z","title":"Deep Learning and Machine Learning -- Object Detection and Semantic\n  Segmentation: From Theory to Applications","summary":"  An in-depth exploration of object detection and semantic segmentation is\nprovided, combining theoretical foundations with practical applications.\nState-of-the-art advancements in machine learning and deep learning are\nreviewed, focusing on convolutional neural networks (CNNs), YOLO architectures,\nand transformer-based approaches such as DETR. The integration of artificial\nintelligence (AI) techniques and large language models for enhancing object\ndetection in complex environments is examined. Additionally, a comprehensive\nanalysis of big data processing is presented, with emphasis on model\noptimization and performance evaluation metrics. By bridging the gap between\ntraditional methods and modern deep learning frameworks, valuable insights are\noffered for researchers, data scientists, and engineers aiming to apply\nAI-driven methodologies to large-scale object detection tasks.\n","authors":["Jintao Ren","Ziqian Bi","Qian Niu","Junyu Liu","Benji Peng","Sen Zhang","Xuanhe Pan","Jinlang Wang","Keyu Chen","Caitlyn Heqi Yin","Pohsun Feng","Yizhu Wen","Tianyang Wang","Silin Chen","Ming Li","Jiawei Xu","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2410.15584v2.pdf","comment":"167 pages"},{"id":"http://arxiv.org/abs/2412.13507v1","updated":"2024-12-18T05:03:18Z","published":"2024-12-18T05:03:18Z","title":"Novel AI Camera Camouflage: Face Cloaking Without Full Disguise","summary":"  This study demonstrates a novel approach to facial camouflage that combines\ntargeted cosmetic perturbations and alpha transparency layer manipulation to\nevade modern facial recognition systems. Unlike previous methods -- such as CV\ndazzle, adversarial patches, and theatrical disguises -- this work achieves\neffective obfuscation through subtle modifications to key-point regions,\nparticularly the brow, nose bridge, and jawline. Empirical testing with Haar\ncascade classifiers and commercial systems like BetaFaceAPI and Microsoft Bing\nVisual Search reveals that vertical perturbations near dense facial key points\nsignificantly disrupt detection without relying on overt disguises.\nAdditionally, leveraging alpha transparency attacks in PNG images creates a\ndual-layer effect: faces remain visible to human observers but disappear in\nmachine-readable RGB layers, rendering them unidentifiable during reverse image\nsearches. The results highlight the potential for creating scalable,\nlow-visibility facial obfuscation strategies that balance effectiveness and\nsubtlety, opening pathways for defeating surveillance while maintaining\nplausible anonymity.\n","authors":["David Noever","Forrest McKee"],"pdf_url":"https://arxiv.org/pdf/2412.13507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13504v1","updated":"2024-12-18T04:56:29Z","published":"2024-12-18T04:56:29Z","title":"Urban Air Temperature Prediction using Conditional Diffusion Models","summary":"  Urbanization as a global trend has led to many environmental challenges,\nincluding the urban heat island (UHI) effect. The increase in temperature has a\nsignificant impact on the well-being of urban residents. Air temperature\n($T_a$) at 2m above the surface is a key indicator of the UHI effect. How land\nuse land cover (LULC) affects $T_a$ is a critical research question which\nrequires high-resolution (HR) $T_a$ data at neighborhood scale. However,\nweather stations providing $T_a$ measurements are sparsely distributed e.g.\nmore than 10km apart; and numerical models are impractically slow and\ncomputationally expensive. In this work, we propose a novel method to predict\nHR $T_a$ at 100m ground separation distance (gsd) using land surface\ntemperature (LST) and other LULC related features which can be easily obtained\nfrom satellite imagery. Our method leverages diffusion models for the first\ntime to generate accurate and visually realistic HR $T_a$ maps, which\noutperforms prior methods. We pave the way for meteorological research using\ncomputer vision techniques by providing a dataset of an extended spatial and\ntemporal coverage, and a high spatial resolution as a benchmark for future\nresearch. Furthermore, we show that our model can be applied to urban planning\nby simulating the impact of different urban designs on $T_a$.\n","authors":["Siyang Dai","Jun Liu","Ngai-Man Cheung"],"pdf_url":"https://arxiv.org/pdf/2412.13504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13502v1","updated":"2024-12-18T04:50:19Z","published":"2024-12-18T04:50:19Z","title":"Level-Set Parameters: Novel Representation for 3D Shape Analysis","summary":"  3D shape analysis has been largely focused on traditional 3D representations\nof point clouds and meshes, but the discrete nature of these data makes the\nanalysis susceptible to variations in input resolutions. Recent development of\nneural fields brings in level-set parameters from signed distance functions as\na novel, continuous, and numerical representation of 3D shapes, where the shape\nsurfaces are defined as zero-level-sets of those functions. This motivates us\nto extend shape analysis from the traditional 3D data to these novel parameter\ndata. Since the level-set parameters are not Euclidean like point clouds, we\nestablish correlations across different shapes by formulating them as a\npseudo-normal distribution, and learn the distribution prior from the\nrespective dataset. To further explore the level-set parameters with shape\ntransformations, we propose to condition a subset of these parameters on\nrotations and translations, and generate them with a hypernetwork. This\nsimplifies the pose-related shape analysis compared to using traditional data.\nWe demonstrate the promise of the novel representations through applications in\nshape classification (arbitrary poses), retrieval, and 6D object pose\nestimation. Code and data in this research are provided at\nhttps://github.com/EnyaHermite/LevelSetParamData.\n","authors":["Huan Lei","Hongdong Li","Andreas Geiger","Anthony Dick"],"pdf_url":"https://arxiv.org/pdf/2412.13502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18868v4","updated":"2024-12-18T04:43:59Z","published":"2024-06-27T03:48:57Z","title":"Advancing Cross-domain Discriminability in Continual Learning of\n  Vision-Language Models","summary":"  Continual learning (CL) with Vision-Language Models (VLMs) has overcome the\nconstraints of traditional CL, which only focuses on previously encountered\nclasses. During the CL of VLMs, we need not only to prevent the catastrophic\nforgetting on incrementally learned knowledge but also to preserve the\nzero-shot ability of VLMs. However, existing methods require additional\nreference datasets to maintain such zero-shot ability and rely on\ndomain-identity hints to classify images across different domains. In this\nstudy, we propose Regression-based Analytic Incremental Learning (RAIL), which\nutilizes a recursive ridge regression-based adapter to learn from a sequence of\ndomains in a non-forgetting manner and decouple the cross-domain correlations\nby projecting features to a higher-dimensional space. Cooperating with a\ntraining-free fusion module, RAIL absolutely preserves the VLM's zero-shot\nability on unseen domains without any reference data. Additionally, we\nintroduce Cross-domain Task-Agnostic Incremental Learning (X-TAIL) setting. In\nthis setting, a CL learner is required to incrementally learn from multiple\ndomains and classify test images from both seen and unseen domains without any\ndomain-identity hint. We theoretically prove RAIL's absolute memorization on\nincrementally learned domains. Experiment results affirm RAIL's\nstate-of-the-art performance in both X-TAIL and existing Multi-domain\nTask-Incremental Learning settings. The code is released at\nhttps://github.com/linghan1997/Regression-based-Analytic-Incremental-Learning.\n","authors":["Yicheng Xu","Yuxin Chen","Jiahao Nie","Yusong Wang","Huiping Zhuang","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2406.18868v4.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.12716v2","updated":"2024-12-18T04:42:07Z","published":"2024-12-17T09:30:31Z","title":"Unsupervised UAV 3D Trajectories Estimation with Sparse Point Clouds","summary":"  Compact UAV systems, while advancing delivery and surveillance, pose\nsignificant security challenges due to their small size, which hinders\ndetection by traditional methods. This paper presents a cost-effective,\nunsupervised UAV detection method using spatial-temporal sequence processing to\nfuse multiple LiDAR scans for accurate UAV tracking in real-world scenarios.\nOur approach segments point clouds into foreground and background, analyzes\nspatial-temporal data, and employs a scoring mechanism to enhance detection\naccuracy. Tested on a public dataset, our solution placed 4th in the CVPR 2024\nUG2+ Challenge, demonstrating its practical effectiveness. We plan to\nopen-source all designs, code, and sample data for the research community\ngithub.com/lianghanfang/UnLiDAR-UAV-Est.\n","authors":["Hanfang Liang","Yizhuo Yang","Jinming Hu","Jianfei Yang","Fen Liu","Shenghai Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.12716v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17166v3","updated":"2024-12-18T04:38:38Z","published":"2023-09-29T11:59:57Z","title":"Advances in Kidney Biopsy Lesion Assessment through Dense Instance\n  Segmentation","summary":"  Renal biopsies are the gold standard for the diagnosis of kidney diseases.\nLesion scores made by renal pathologists are semi-quantitative and exhibit high\ninter-observer variability. Automating lesion classification within segmented\nanatomical structures can provide decision support in quantification analysis,\nthereby reducing inter-observer variability. Nevertheless, classifying lesions\nin regions-of-interest (ROIs) is clinically challenging due to (a) a large\namount of densely packed anatomical objects, (b) class imbalance across\ndifferent compartments (at least 3), (c) significant variation in size and\nshape of anatomical objects and (d) the presence of multi-label lesions per\nanatomical structure. Existing models cannot address these complexities in an\nefficient and generic manner. This paper presents an analysis for a\n\\textbf{generalized solution} to datasets from various sources (pathology\ndepartments) with different types of lesions. Our approach utilizes two\nsub-networks: dense instance segmentation and lesion classification. We\nintroduce \\textbf{DiffRegFormer}, an end-to-end dense instance segmentation\nsub-network designed for multi-class, multi-scale objects within ROIs.\nCombining diffusion models, transformers, and RCNNs, DiffRegFormer {is a\ncomputational-friendly framework that can efficiently recognize over 500\nobjects across three anatomical classes, i.e., glomeruli, tubuli, and arteries,\nwithin ROIs.} In a dataset of 303 ROIs from 148 Jones' silver-stained renal\nWhole Slide Images (WSIs), our approach outperforms previous methods, achieving\nan Average Precision of 52.1\\% (detection) and 46.8\\% (segmentation). Moreover,\nour lesion classification sub-network achieves 89.2\\% precision and 64.6\\%\nrecall on 21889 object patches out of the 303 ROIs. Lastly, our model\ndemonstrates direct domain transfer to PAS-stained renal WSIs without\nfine-tuning.\n","authors":["Zhan Xiong","Junling He","Pieter Valkema","Tri Q. Nguyen","Maarten Naesens","Jesper Kers","Fons J. Verbeek"],"pdf_url":"https://arxiv.org/pdf/2309.17166v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13496v1","updated":"2024-12-18T04:34:46Z","published":"2024-12-18T04:34:46Z","title":"QueryCDR: Query-based Controllable Distortion Rectification Network for\n  Fisheye Images","summary":"  Fisheye image rectification aims to correct distortions in images taken with\nfisheye cameras. Although current models show promising results on images with\na similar degree of distortion as the training data, they will produce\nsub-optimal results when the degree of distortion changes and without\nretraining. The lack of generalization ability for dealing with varying degrees\nof distortion limits their practical application. In this paper, we take one\nstep further to enable effective distortion rectification for images with\nvarying degrees of distortion without retraining. We propose a novel\nQuery-based Controllable Distortion Rectification network for fisheye images\n(QueryCDR). In particular, we first present the Distortion-aware Learnable\nQuery Mechanism (DLQM), which defines the latent spatial relationships for\ndifferent distortion degrees as a series of learnable queries. Each query can\nbe learned to obtain position-dependent rectification control conditions,\nproviding control over the rectification process. Then, we propose two kinds of\ncontrollable modulating blocks to enable the control conditions to guide the\nmodulation of the distortion features better. These core components cooperate\nwith each other to effectively boost the generalization ability of the model at\nvarying degrees of distortion. Extensive experiments on fisheye image datasets\nwith different distortion degrees demonstrate our approach achieves\nhigh-quality and controllable distortion rectification.\n","authors":["Pengbo Guo","Chengxu Liu","Xingsong Hou","Xueming Qian"],"pdf_url":"https://arxiv.org/pdf/2412.13496v1.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2404.11064v3","updated":"2024-12-18T04:23:29Z","published":"2024-04-17T04:46:27Z","title":"Rethinking 3D Dense Caption and Visual Grounding in A Unified Framework\n  through Prompt-based Localization","summary":"  3D Visual Grounding (3DVG) and 3D Dense Captioning (3DDC) are two crucial\ntasks in various 3D applications, which require both shared and complementary\ninformation in localization and visual-language relationships. Therefore,\nexisting approaches adopt the two-stage \"detect-then-describe/discriminate\"\npipeline, which relies heavily on the performance of the detector, resulting in\nsuboptimal performance. Inspired by DETR, we propose a unified framework,\n3DGCTR, to jointly solve these two distinct but closely related tasks in an\nend-to-end fashion. The key idea is to reconsider the prompt-based localization\nability of the 3DVG model. In this way, the 3DVG model with a well-designed\nprompt as input can assist the 3DDC task by extracting localization information\nfrom the prompt. In terms of implementation, we integrate a Lightweight Caption\nHead into the existing 3DVG network with a Caption Text Prompt as a connection,\neffectively harnessing the existing 3DVG model's inherent localization\ncapacity, thereby boosting 3DDC capability. This integration facilitates\nsimultaneous multi-task training on both tasks, mutually enhancing their\nperformance. Extensive experimental results demonstrate the effectiveness of\nthis approach. Specifically, on the ScanRefer dataset, 3DGCTR surpasses the\nstate-of-the-art 3DDC method by 4.3% in CIDEr@0.5IoU in MLE training and\nimproves upon the SOTA 3DVG method by 3.16% in Acc@0.25IoU. The codes are at\nhttps://github.com/Leon1207/3DGCTR.\n","authors":["Yongdong Luo","Haojia Lin","Xiawu Zheng","Yigeng Jiang","Fei Chao","Jie Hu","Guannan Jiang","Songan Zhang","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2404.11064v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13490v1","updated":"2024-12-18T04:15:32Z","published":"2024-12-18T04:15:32Z","title":"Comparative Analysis of YOLOv9, YOLOv10 and RT-DETR for Real-Time Weed\n  Detection","summary":"  This paper presents a comprehensive evaluation of state-of-the-art object\ndetection models, including YOLOv9, YOLOv10, and RT-DETR, for the task of weed\ndetection in smart-spraying applications focusing on three classes: Sugarbeet,\nMonocot, and Dicot. The performance of these models is compared based on mean\nAverage Precision (mAP) scores and inference times on different GPU devices. We\nconsider various model variations, such as nano, small, medium, large alongside\ndifferent image resolutions (320px, 480px, 640px, 800px, 960px). The results\nhighlight the trade-offs between inference time and detection accuracy,\nproviding valuable insights for selecting the most suitable model for real-time\nweed detection. This study aims to guide the development of efficient and\neffective smart spraying systems, enhancing agricultural productivity through\nprecise weed management.\n","authors":["Ahmet Oğuz Saltık","Alicia Allmendinger","Anthony Stein"],"pdf_url":"https://arxiv.org/pdf/2412.13490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04766v2","updated":"2024-12-18T04:14:40Z","published":"2024-09-07T08:53:17Z","title":"Cross-Dataset Gaze Estimation by Evidential Inter-intra Fusion","summary":"  Achieving accurate and reliable gaze predictions in complex and diverse\nenvironments remains challenging. Fortunately, it is straightforward to access\ndiverse gaze datasets in real-world applications. We discover that training\nthese datasets jointly can significantly improve the generalization of gaze\nestimation, which is overlooked in previous works. However, due to the inherent\ndistribution shift across different datasets, simply mixing multiple dataset\ndecreases the performance in the original domain despite gaining better\ngeneralization abilities. To address the problem of ``cross-dataset gaze\nestimation'', we propose a novel Evidential Inter-intra Fusion EIF framework,\nfor training a cross-dataset model that performs well across all source and\nunseen domains. Specifically, we build independent single-dataset branches for\nvarious datasets where the data space is partitioned into overlapping subspaces\nwithin each dataset for local regression, and further create a cross-dataset\nbranch to integrate the generalizable features from single-dataset branches.\nFurthermore, evidential regressors based on the Normal and Inverse-Gamma (NIG)\ndistribution are designed to additionally provide uncertainty estimation apart\nfrom predicting gaze. Building upon this foundation, our proposed framework\nachieves both intra-evidential fusion among multiple local regressors within\neach dataset and inter-evidential fusion among multiple branches by Mixture\n\\textbfof Normal Inverse-Gamma (MoNIG distribution. Experiments demonstrate\nthat our method consistently achieves notable improvements in both source\ndomains and unseen domains.\n","authors":["Shijing Wang","Yaping Huang","Jun Xie","Yi Tian","Feng Chen","Zhepeng Wang"],"pdf_url":"https://arxiv.org/pdf/2409.04766v2.pdf","comment":"This paper was previously submitted to ACM MM 2024"},{"id":"http://arxiv.org/abs/2412.13486v1","updated":"2024-12-18T04:01:32Z","published":"2024-12-18T04:01:32Z","title":"T$^3$-S2S: Training-free Triplet Tuning for Sketch to Scene Generation","summary":"  Scene generation is crucial to many computer graphics applications. Recent\nadvances in generative AI have streamlined sketch-to-image workflows, easing\nthe workload for artists and designers in creating scene concept art. However,\nthese methods often struggle for complex scenes with multiple detailed objects,\nsometimes missing small or uncommon instances. In this paper, we propose a\nTraining-free Triplet Tuning for Sketch-to-Scene (T3-S2S) generation after\nreviewing the entire cross-attention mechanism. This scheme revitalizes the\nexisting ControlNet model, enabling effective handling of multi-instance\ngenerations, involving prompt balance, characteristics prominence, and dense\ntuning. Specifically, this approach enhances keyword representation via the\nprompt balance module, reducing the risk of missing critical instances. It also\nincludes a characteristics prominence module that highlights TopK indices in\neach channel, ensuring essential features are better represented based on token\nsketches. Additionally, it employs dense tuning to refine contour details in\nthe attention map, compensating for instance-related regions. Experiments\nvalidate that our triplet tuning approach substantially improves the\nperformance of existing sketch-to-image models. It consistently generates\ndetailed, multi-instance 2D images, closely adhering to the input prompts and\nenhancing visual quality in complex multi-instance scenes. Code is available at\nhttps://github.com/chaos-sun/t3s2s.git.\n","authors":["Zhenhong Sun","Yifu Wang","Yonhon Ng","Yunfei Duan","Daoyi Dong","Hongdong Li","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2412.13486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04456v2","updated":"2024-12-18T03:54:25Z","published":"2024-12-05T18:59:00Z","title":"HeatFormer: A Neural Optimizer for Multiview Human Mesh Recovery","summary":"  We introduce a novel method for human shape and pose recovery that can fully\nleverage multiple static views. We target fixed-multiview people monitoring,\nincluding elderly care and safety monitoring, in which calibrated cameras can\nbe installed at the corners of a room or an open space but whose configuration\nmay vary depending on the environment. Our key idea is to formulate it as\nneural optimization. We achieve this with HeatFormer, a neural optimizer that\niteratively refines the SMPL parameters given multiview images, which is\nfundamentally agonistic to the configuration of views. HeatFormer realizes this\nSMPL parameter estimation as heat map generation and alignment with a novel\ntransformer encoder and decoder. We demonstrate the effectiveness of HeatFormer\nincluding its accuracy, robustness to occlusion, and generalizability through\nan extensive set of experiments. We believe HeatFormer can serve a key role in\npassive human behavior modeling.\n","authors":["Yuto Matsubara","Ko Nishino"],"pdf_url":"https://arxiv.org/pdf/2412.04456v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09599v2","updated":"2024-12-18T03:49:22Z","published":"2024-12-12T18:59:00Z","title":"RatBodyFormer: Rodent Body Surface from Keypoints","summary":"  Rat behavior modeling goes to the heart of many scientific studies, yet the\ntextureless body surface evades automatic analysis as it literally has no\nkeypoints that detectors can find. The movement of the body surface, however,\nis a rich source of information for deciphering the rat behavior. We introduce\ntwo key contributions to automatically recover densely 3D sampled rat body\nsurface points, passively. The first is RatDome, a novel multi-camera system\nfor rat behavior capture, and a large-scale dataset captured with it that\nconsists of pairs of 3D keypoints and 3D body surface points. The second is\nRatBodyFormer, a novel network to transform detected keypoints to 3D body\nsurface points. RatBodyFormer is agnostic to the exact locations of the 3D body\nsurface points in the training data and is trained with masked-learning. We\nexperimentally validate our framework with a number of real-world experiments.\nOur results collectively serve as a novel foundation for automated rat behavior\nanalysis and will likely have far-reaching implications for biomedical and\nneuroscientific research.\n","authors":["Ayaka Higami","Karin Oshima","Tomoyo Isoguchi Shiramatsu","Hirokazu Takahashi","Shohei Nobuhara","Ko Nishino"],"pdf_url":"https://arxiv.org/pdf/2412.09599v2.pdf","comment":"https://vision.ist.i.kyoto-u.ac.jp/research/ratbodyformer/"},{"id":"http://arxiv.org/abs/2412.13479v1","updated":"2024-12-18T03:42:42Z","published":"2024-12-18T03:42:42Z","title":"Real-time One-Step Diffusion-based Expressive Portrait Videos Generation","summary":"  Latent diffusion models have made great strides in generating expressive\nportrait videos with accurate lip-sync and natural motion from a single\nreference image and audio input. However, these models are far from real-time,\noften requiring many sampling steps that take minutes to generate even one\nsecond of video-significantly limiting practical use. We introduce OSA-LCM\n(One-Step Avatar Latent Consistency Model), paving the way for real-time\ndiffusion-based avatars. Our method achieves comparable video quality to\nexisting methods but requires only one sampling step, making it more than 10x\nfaster. To accomplish this, we propose a novel avatar discriminator design that\nguides lip-audio consistency and motion expressiveness to enhance video quality\nin limited sampling steps. Additionally, we employ a second-stage training\narchitecture using an editing fine-tuned method (EFT), transforming video\ngeneration into an editing task during training to effectively address the\ntemporal gap challenge in single-step generation. Experiments demonstrate that\nOSA-LCM outperforms existing open-source portrait video generation models while\noperating more efficiently with a single sampling step.\n","authors":["Hanzhong Guo","Hongwei Yi","Daquan Zhou","Alexander William Bergman","Michael Lingelbach","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2412.13479v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2412.13477v1","updated":"2024-12-18T03:41:34Z","published":"2024-12-18T03:41:34Z","title":"Generating Unseen Nonlinear Evolution in Sea Surface Temperature Using a\n  Deep Learning-Based Latent Space Data Assimilation Framework","summary":"  Advances in data assimilation (DA) methods have greatly improved the accuracy\nof Earth system predictions. To fuse multi-source data and reconstruct the\nnonlinear evolution missing from observations, geoscientists are developing\nfuture-oriented DA methods. In this paper, we redesign a purely data-driven\nlatent space DA framework (DeepDA) that employs a generative artificial\nintelligence model to capture the nonlinear evolution in sea surface\ntemperature. Under variational constraints, DeepDA embedded with nonlinear\nfeatures can effectively fuse heterogeneous data. The results show that DeepDA\nremains highly stable in capturing and generating nonlinear evolutions even\nwhen a large amount of observational information is missing. It can be found\nthat when only 10% of the observation information is available, the error\nincrease of DeepDA does not exceed 40%. Furthermore, DeepDA has been shown to\nbe robust in the fusion of real observations and ensemble simulations. In\nparticular, this paper provides a mechanism analysis of the nonlinear evolution\ngenerated by DeepDA from the perspective of physical patterns, which reveals\nthe inherent explainability of our DL model in capturing multi-scale ocean\nsignals.\n","authors":["Qingyu Zheng","Guijun Han","Wei Li","Lige Cao","Gongfu Zhou","Haowen Wu","Qi Shao","Ru Wang","Xiaobo Wu","Xudong Cui","Hong Li","Xuan Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13477v1.pdf","comment":"31 pages, 14 figures"},{"id":"http://arxiv.org/abs/2409.04851v2","updated":"2024-12-18T03:40:35Z","published":"2024-09-07T15:06:30Z","title":"Towards Weather-Robust 3D Human Body Reconstruction: Millimeter-Wave\n  Radar-Based Dataset, Benchmark, and Multi-Modal Fusion","summary":"  3D human reconstruction from RGB images achieves decent results in good\nweather conditions but degrades dramatically in rough weather. Complementarily,\nmmWave radars have been employed to reconstruct 3D human joints and meshes in\nrough weather. However, combining RGB and mmWave signals for weather-robust 3D\nhuman reconstruction is still an open challenge, given the sparse nature of\nmmWave and the vulnerability of RGB images. The limited research about the\nimpact of missing points and sparsity features of mmWave data on reconstruction\nperformance, as well as the lack of available datasets for paired mmWave-RGB\ndata, further complicates the process of fusing the two modalities. To fill\nthese gaps, we build up an automatic 3D body annotation system with multiple\nsensors to collect a large-scale mmWave dataset. The dataset consists of\nsynchronized and calibrated mmWave radar point clouds and RGB(D) images under\ndifferent weather conditions and skeleton/mesh annotations for humans in these\nscenes. With this dataset, we conduct a comprehensive analysis about the\nlimitations of single-modality reconstruction and the impact of missing points\nand sparsity on the reconstruction performance. Based on the guidance of this\nanalysis, we design ImmFusion, the first mmWave-RGB fusion solution to robustly\nreconstruct 3D human bodies in various weather conditions. Specifically, our\nImmFusion consists of image and point backbones for token feature extraction\nand a Transformer module for token fusion. The image and point backbones refine\nglobal and local features from original data, and the Fusion Transformer Module\naims for effective information fusion of two modalities by dynamically\nselecting informative tokens. Extensive experiments demonstrate that ImmFusion\ncan efficiently utilize the information of two modalities to achieve robust 3D\nhuman body reconstruction in various weather environments.\n","authors":["Anjun Chen","Xiangyu Wang","Kun Shi","Yuchi Huo","Jiming Chen","Qi Ye"],"pdf_url":"https://arxiv.org/pdf/2409.04851v2.pdf","comment":"TCSVT 2024, Project Page:\n  https://chen3110.github.io/mmbody/index.html"},{"id":"http://arxiv.org/abs/2412.13469v1","updated":"2024-12-18T03:35:55Z","published":"2024-12-18T03:35:55Z","title":"Enabling Region-Specific Control via Lassos in Point-Based Colorization","summary":"  Point-based interactive colorization techniques allow users to effortlessly\ncolorize grayscale images using user-provided color hints. However, point-based\nmethods often face challenges when different colors are given to semantically\nsimilar areas, leading to color intermingling and unsatisfactory results-an\nissue we refer to as color collapse. The fundamental cause of color collapse is\nthe inadequacy of points for defining the boundaries for each color. To\nmitigate color collapse, we introduce a lasso tool that can control the scope\nof each color hint. Additionally, we design a framework that leverages the\nuser-provided lassos to localize the attention masks. The experimental results\nshow that using a single lasso is as effective as applying 4.18 individual\ncolor hints and can achieve the desired outcomes in 30% less time than using\npoints alone.\n","authors":["Sanghyeon Lee","Jooyeol Yun","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2412.13469v1.pdf","comment":"Accepted to AAAI2025"},{"id":"http://arxiv.org/abs/2412.09982v2","updated":"2024-12-18T03:25:50Z","published":"2024-12-13T09:09:14Z","title":"SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D\n  Gaussians from Monocular Video","summary":"  Synthesizing novel views from in-the-wild monocular videos is challenging due\nto scene dynamics and the lack of multi-view cues. To address this, we propose\nSplineGS, a COLMAP-free dynamic 3D Gaussian Splatting (3DGS) framework for\nhigh-quality reconstruction and fast rendering from monocular videos. At its\ncore is a novel Motion-Adaptive Spline (MAS) method, which represents\ncontinuous dynamic 3D Gaussian trajectories using cubic Hermite splines with a\nsmall number of control points. For MAS, we introduce a Motion-Adaptive Control\npoints Pruning (MACP) method to model the deformation of each dynamic 3D\nGaussian across varying motions, progressively pruning control points while\nmaintaining dynamic modeling integrity. Additionally, we present a joint\noptimization strategy for camera parameter estimation and 3D Gaussian\nattributes, leveraging photometric and geometric consistency. This eliminates\nthe need for Structure-from-Motion preprocessing and enhances SplineGS's\nrobustness in real-world conditions. Experiments show that SplineGS\nsignificantly outperforms state-of-the-art methods in novel view synthesis\nquality for dynamic scenes from monocular videos, achieving thousands times\nfaster rendering speed.\n","authors":["Jongmin Park","Minh-Quan Viet Bui","Juan Luis Gonzalez Bello","Jaeho Moon","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2412.09982v2.pdf","comment":"The first two authors contributed equally to this work (equal\n  contribution). The last two authors advised equally to this work. Please\n  visit our project page at this https://kaist-viclab.github.io/splinegs-site/"},{"id":"http://arxiv.org/abs/2311.12059v2","updated":"2024-12-18T03:23:19Z","published":"2023-11-18T16:14:08Z","title":"Mesh Watermark Removal Attack and Mitigation: A Novel Perspective of\n  Function Space","summary":"  Mesh watermark embeds secret messages in 3D meshes and decodes the message\nfrom watermarked meshes for ownership verification. Current watermarking\nmethods directly hide secret messages in vertex and face sets of meshes.\nHowever, mesh is a discrete representation that uses vertex and face sets to\ndescribe a continuous signal, which can be discretized in other discrete\nrepresentations with different vertex and face sets. This raises the question\nof whether the watermark can still be verified on the different discrete\nrepresentations of the watermarked mesh. We conduct this research in an\nattack-then-defense manner by proposing a novel function space mesh watermark\nremoval attack FuncEvade and then mitigating it through function space mesh\nwatermarking FuncMark. In detail, FuncEvade generates a different discrete\nrepresentation of a watermarked mesh by extracting it from the signed distance\nfunction of the watermarked mesh. We observe that the generated mesh can evade\nALL previous watermarking methods. FuncMark mitigates FuncEvade by watermarking\nsigned distance function through message-guided deformation. Such deformation\ncan survive isosurfacing and thus be inherited by the extracted meshes for\nfurther watermark decoding. Extensive experiments demonstrate that FuncEvade\nachieves 100% evasion rate among all previous watermarking methods while\nachieving only 0.3% evasion rate on FuncMark. Besides, our FuncMark performs\nsimilarly on other metrics compared to state-of-the-art mesh watermarking\nmethods.\n","authors":["Xingyu Zhu","Guanhui Ye","Chengdong Dong","Xiapu Luo","Xuetao Wei"],"pdf_url":"https://arxiv.org/pdf/2311.12059v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13463v1","updated":"2024-12-18T03:18:11Z","published":"2024-12-18T03:18:11Z","title":"FlexPose: Pose Distribution Adaptation with Limited Guidance","summary":"  Numerous well-annotated human key-point datasets are publicly available to\ndate. However, annotating human poses for newly collected images is still a\ncostly and time-consuming progress. Pose distributions from different datasets\nshare similar pose hinge-structure priors with different geometric\ntransformations, such as pivot orientation, joint rotation, and bone length\nratio. The difference between Pose distributions is essentially the difference\nbetween the transformation distributions. Inspired by this fact, we propose a\nmethod to calibrate a pre-trained pose generator in which the pose prior has\nalready been learned to an adapted one following a new pose distribution. We\ntreat the representation of human pose joint coordinates as skeleton image and\ntransfer a pre-trained pose annotation generator with only a few annotation\nguidance. By fine-tuning a limited number of linear layers that closely related\nto the pose transformation, the adapted generator is able to produce any number\nof pose annotations that are similar to the target poses. We evaluate our\nproposed method, FlexPose, on several cross-dataset settings both qualitatively\nand quantitatively, which demonstrates that our approach achieves\nstate-of-the-art performance compared to the existing generative-model-based\ntransfer learning methods when given limited annotation guidance.\n","authors":["Zixiao Wang","Junwu Weng","Mengyuan Liu","Bei Yu"],"pdf_url":"https://arxiv.org/pdf/2412.13463v1.pdf","comment":"Accepted by AAAI25, 12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2409.05166v5","updated":"2024-12-18T03:14:55Z","published":"2024-09-08T17:35:48Z","title":"CD-NGP: A Fast Scalable Continual Representation for Dynamic Scenes","summary":"  Current methods for novel view synthesis (NVS) in dynamic scenes encounter\nsignificant challenges in managing memory consumption, model complexity,\ntraining efficiency, and rendering fidelity. Existing offline techniques, while\ndelivering high-quality results, face challenges from substantial memory\ndemands and limited scalability. Conversely, online methods struggle to balance\nrapid convergence with model compactness. To address these issues, we propose\ncontinual dynamic neural graphics primitives (CD-NGP). Our approach leverages a\ncontinual learning framework to reduce memory overhead, and it also integrates\nfeatures from distinct temporal and spatial hash encodings for high rendering\nquality. Meanwhile, our method employs parameter reuse to achieve high\nscalability. Additionally, we introduce a novel dataset featuring multi-view,\nexceptionally long video sequences with substantial rigid and non-rigid motion,\nwhich is seldom possessed by existing datasets. We evaluate the reconstruction\nquality, speed and scalability of our method on both the established public\ndatasets and our exceptionally long video dataset. Notably, our method achieves\nan $85\\%$ reduction in training memory consumption (less than 14GB) compared to\noffline techniques and significantly lowers streaming bandwidth requirements\n(less than 0.4MB/frame) relative to other online alternatives. The experimental\nresults on our long video sequences dataset show the superior scalability and\nreconstruction quality compared to existing state-of-the-art approaches.\n","authors":["Zhenhuan Liu","Shuai Liu","Zhiwei Ning","Jie Yang","Yifan Zuo","Yuming Fang","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2409.05166v5.pdf","comment":"12+1 pages"},{"id":"http://arxiv.org/abs/2412.13461v1","updated":"2024-12-18T03:14:11Z","published":"2024-12-18T03:14:11Z","title":"Look Inside for More: Internal Spatial Modality Perception for 3D\n  Anomaly Detection","summary":"  3D anomaly detection has recently become a significant focus in computer\nvision. Several advanced methods have achieved satisfying anomaly detection\nperformance. However, they typically concentrate on the external structure of\n3D samples and struggle to leverage the internal information embedded within\nsamples. Inspired by the basic intuition of why not look inside for more, we\nintroduce a straightforward method named Internal Spatial Modality Perception\n(ISMP) to explore the feature representation from internal views fully.\nSpecifically, our proposed ISMP consists of a critical perception module,\nSpatial Insight Engine (SIE), which abstracts complex internal information of\npoint clouds into essential global features. Besides, to better align\nstructural information with point data, we propose an enhanced key point\nfeature extraction module for amplifying spatial structure feature\nrepresentation. Simultaneously, a novel feature filtering module is\nincorporated to reduce noise and redundant features for further aligning\nprecise spatial structure. Extensive experiments validate the effectiveness of\nour proposed method, achieving object-level and pixel-level AUROC improvements\nof 4.2% and 13.1%, respectively, on the Real3D-AD benchmarks. Note that the\nstrong generalization ability of SIE has been theoretically proven and is\nverified in both classification and segmentation tasks.\n","authors":["Hanzhe Liang","Guoyang Xie","Chengbin Hou","Bingshu Wang","Can Gao","Jinbao Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13461v1.pdf","comment":"AAAI2025 Accepted"},{"id":"http://arxiv.org/abs/2412.13026v2","updated":"2024-12-18T03:05:45Z","published":"2024-12-17T15:48:25Z","title":"NAVCON: A Cognitively Inspired and Linguistically Grounded Corpus for\n  Vision and Language Navigation","summary":"  We present NAVCON, a large-scale annotated Vision-Language Navigation (VLN)\ncorpus built on top of two popular datasets (R2R and RxR). The paper introduces\nfour core, cognitively motivated and linguistically grounded, navigation\nconcepts and an algorithm for generating large-scale silver annotations of\nnaturally occurring linguistic realizations of these concepts in navigation\ninstructions. We pair the annotated instructions with video clips of an agent\nacting on these instructions. NAVCON contains 236, 316 concept annotations for\napproximately 30, 0000 instructions and 2.7 million aligned images (from\napproximately 19, 000 instructions) showing what the agent sees when executing\nan instruction. To our knowledge, this is the first comprehensive resource of\nnavigation concepts. We evaluated the quality of the silver annotations by\nconducting human evaluation studies on NAVCON samples. As further validation of\nthe quality and usefulness of the resource, we trained a model for detecting\nnavigation concepts and their linguistic realizations in unseen instructions.\nAdditionally, we show that few-shot learning with GPT-4o performs well on this\ntask using large-scale silver annotations of NAVCON.\n","authors":["Karan Wanchoo","Xiaoye Zuo","Hannah Gonzalez","Soham Dan","Georgios Georgakis","Dan Roth","Kostas Daniilidis","Eleni Miltsakaki"],"pdf_url":"https://arxiv.org/pdf/2412.13026v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11525v2","updated":"2024-12-18T03:00:13Z","published":"2024-12-16T08:00:50Z","title":"Sequence Matters: Harnessing Video Models in 3D Super-Resolution","summary":"  3D super-resolution aims to reconstruct high-fidelity 3D models from\nlow-resolution (LR) multi-view images. Early studies primarily focused on\nsingle-image super-resolution (SISR) models to upsample LR images into\nhigh-resolution images. However, these methods often lack view consistency\nbecause they operate independently on each image. Although various\npost-processing techniques have been extensively explored to mitigate these\ninconsistencies, they have yet to fully resolve the issues. In this paper, we\nperform a comprehensive study of 3D super-resolution by leveraging video\nsuper-resolution (VSR) models. By utilizing VSR models, we ensure a higher\ndegree of spatial consistency and can reference surrounding spatial\ninformation, leading to more accurate and detailed reconstructions. Our\nfindings reveal that VSR models can perform remarkably well even on sequences\nthat lack precise spatial alignment. Given this observation, we propose a\nsimple yet practical approach to align LR images without involving fine-tuning\nor generating 'smooth' trajectory from the trained 3D models over LR images.\nThe experimental results show that the surprisingly simple algorithms can\nachieve the state-of-the-art results of 3D super-resolution tasks on standard\nbenchmark datasets, such as the NeRF-synthetic and MipNeRF-360 datasets.\nProject page: https://ko-lani.github.io/Sequence-Matters\n","authors":["Hyun-kyu Ko","Dongheok Park","Youngin Park","Byeonghyeon Lee","Juhee Han","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2412.11525v2.pdf","comment":"Project page: https://ko-lani.github.io/Sequence-Matters"},{"id":"http://arxiv.org/abs/2412.13454v1","updated":"2024-12-18T02:54:30Z","published":"2024-12-18T02:54:30Z","title":"Pre-training a Density-Aware Pose Transformer for Robust LiDAR-based 3D\n  Human Pose Estimation","summary":"  With the rapid development of autonomous driving, LiDAR-based 3D Human Pose\nEstimation (3D HPE) is becoming a research focus. However, due to the noise and\nsparsity of LiDAR-captured point clouds, robust human pose estimation remains\nchallenging. Most of the existing methods use temporal information, multi-modal\nfusion, or SMPL optimization to correct biased results. In this work, we try to\nobtain sufficient information for 3D HPE only by modeling the intrinsic\nproperties of low-quality point clouds. Hence, a simple yet powerful method is\nproposed, which provides insights both on modeling and augmentation of point\nclouds. Specifically, we first propose a concise and effective density-aware\npose transformer (DAPT) to get stable keypoint representations. By using a set\nof joint anchors and a carefully designed exchange module, valid information is\nextracted from point clouds with different densities. Then 1D heatmaps are\nutilized to represent the precise locations of the keypoints. Secondly, a\ncomprehensive LiDAR human synthesis and augmentation method is proposed to\npre-train the model, enabling it to acquire a better human body prior. We\nincrease the diversity of point clouds by randomly sampling human positions and\norientations and by simulating occlusions through the addition of laser-level\nmasks. Extensive experiments have been conducted on multiple datasets,\nincluding IMU-annotated LidarHuman26M, SLOPER4D, and manually annotated Waymo\nOpen Dataset v2.0 (Waymo), HumanM3. Our method demonstrates SOTA performance in\nall scenarios. In particular, compared with LPFormer on Waymo, we reduce the\naverage MPJPE by $10.0mm$. Compared with PRN on SLOPER4D, we notably reduce the\naverage MPJPE by $20.7mm$.\n","authors":["Xiaoqi An","Lin Zhao","Chen Gong","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2412.13454v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2403.15063v2","updated":"2024-12-18T02:51:53Z","published":"2024-03-22T09:40:52Z","title":"Towards a Comprehensive, Efficient and Promptable Anatomic Structure\n  Segmentation Model using 3D Whole-body CT Scans","summary":"  Segment anything model (SAM) demonstrates strong generalization ability on\nnatural image segmentation. However, its direct adaptation in medical image\nsegmentation tasks shows significant performance drops. It also requires an\nexcessive number of prompt points to obtain a reasonable accuracy. Although\nquite a few studies explore adapting SAM into medical image volumes, the\nefficiency of 2D adaptation methods is unsatisfactory and 3D adaptation methods\nare only capable of segmenting specific organs/tumors. In this work, we propose\na comprehensive and scalable 3D SAM model for whole-body CT segmentation, named\nCT-SAM3D. Instead of adapting SAM, we propose a 3D promptable segmentation\nmodel using a (nearly) fully labeled CT dataset. To train CT-SAM3D effectively,\nensuring the model's accurate responses to higher-dimensional spatial prompts\nis crucial, and 3D patch-wise training is required due to GPU memory\nconstraints. Therefore, we propose two key technical developments: 1) a\nprogressively and spatially aligned prompt encoding method to effectively\nencode click prompts in local 3D space; and 2) a cross-patch prompt scheme to\ncapture more 3D spatial context, which is beneficial for reducing the editing\nworkloads when interactively prompting on large organs. CT-SAM3D is trained\nusing a curated dataset of 1204 CT scans containing 107 whole-body anatomies\nand extensively validated using five datasets, achieving significantly better\nresults against all previous SAM-derived models. Code, data, and our 3D\ninteractive segmentation tool with quasi-real-time responses are available at\nhttps://github.com/alibaba-damo-academy/ct-sam3d.\n","authors":["Heng Guo","Jianfeng Zhang","Jiaxing Huang","Tony C. W. Mok","Dazhou Guo","Ke Yan","Le Lu","Dakai Jin","Minfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.15063v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13452v1","updated":"2024-12-18T02:49:20Z","published":"2024-12-18T02:49:20Z","title":"ConDo: Continual Domain Expansion for Absolute Pose Regression","summary":"  Visual localization is a fundamental machine learning problem. Absolute Pose\nRegression (APR) trains a scene-dependent model to efficiently map an input\nimage to the camera pose in a pre-defined scene. However, many applications\nhave continually changing environments, where inference data at novel poses or\nscene conditions (weather, geometry) appear after deployment. Training APR on a\nfixed dataset leads to overfitting, making it fail catastrophically on\nchallenging novel data. This work proposes Continual Domain Expansion (ConDo),\nwhich continually collects unlabeled inference data to update the deployed APR.\nInstead of applying standard unsupervised domain adaptation methods which are\nineffective for APR, ConDo effectively learns from unlabeled data by distilling\nknowledge from scene-agnostic localization methods. By sampling data uniformly\nfrom historical and newly collected data, ConDo can effectively expand the\ngeneralization domain of APR. Large-scale benchmarks with various scene types\nare constructed to evaluate models under practical (long-term) data changes.\nConDo consistently and significantly outperforms baselines across\narchitectures, scene types, and data changes. On challenging scenes (Fig.1), it\nreduces the localization error by >7x (14.8m vs 1.7m). Analysis shows the\nrobustness of ConDo against compute budgets, replay buffer sizes and teacher\nprediction noise. Comparing to model re-training, ConDo achieves similar\nperformance up to 25x faster.\n","authors":["Zijun Li","Zhipeng Cai","Bochun Yang","Xuelun Shen","Siqi Shen","Xiaoliang Fan","Michael Paulitsch","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13452v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.06286v2","updated":"2024-12-18T02:44:01Z","published":"2024-12-09T08:16:24Z","title":"No Annotations for Object Detection in Art through Stable Diffusion","summary":"  Object detection in art is a valuable tool for the digital humanities, as it\nallows for faster identification of objects in artistic and historical images\ncompared to humans. However, annotating such images poses significant\nchallenges due to the need for specialized domain expertise. We present NADA\n(no annotations for detection in art), a pipeline that leverages diffusion\nmodels' art-related knowledge for object detection in paintings without the\nneed for full bounding box supervision. Our method, which supports both\nweakly-supervised and zero-shot scenarios and does not require any fine-tuning\nof its pretrained components, consists of a class proposer based on large\nvision-language models and a class-conditioned detector based on Stable\nDiffusion. NADA is evaluated on two artwork datasets, ArtDL 2.0 and IconArt,\noutperforming prior work in weakly-supervised detection, while being the first\nwork for zero-shot object detection in art. Code is available at\nhttps://github.com/patrick-john-ramos/nada\n","authors":["Patrick Ramos","Nicolas Gonthier","Selina Khan","Yuta Nakashima","Noa Garcia"],"pdf_url":"https://arxiv.org/pdf/2412.06286v2.pdf","comment":"8 pages, 6 figures, to be published in WACV 2025"},{"id":"http://arxiv.org/abs/2401.13270v2","updated":"2024-12-18T02:43:40Z","published":"2024-01-24T07:22:05Z","title":"Audio-Infused Automatic Image Colorization by Exploiting Audio Scene\n  Semantics","summary":"  Automatic image colorization is inherently an ill-posed problem with\nuncertainty, which requires an accurate semantic understanding of scenes to\nestimate reasonable colors for grayscale images. Although recent\ninteraction-based methods have achieved impressive performance, it is still a\nvery difficult task to infer realistic and accurate colors for automatic\ncolorization. To reduce the difficulty of semantic understanding of grayscale\nscenes, this paper tries to utilize corresponding audio, which naturally\ncontains extra semantic information about the same scene. Specifically, a novel\nand pluggable audio-infused automatic image colorization (AIAIC) method is\nproposed, which consists of three stages. First, we take color image semantics\nas a bridge and pretrain a colorization network guided by color image\nsemantics. Second, the natural co-occurrence of audio and video is utilized to\nlearn the color semantic correlations between audio and visual scenes. Third,\nthe implicit audio semantic representation is fed into the pretrained network\nto finally realize the audio-guided colorization. The whole process is trained\nin a self-supervised manner without human annotation. Experiments demonstrate\nthat audio guidance can effectively improve the performance of automatic\ncolorization, especially for some scenes that are difficult to understand only\nfrom visual modality.\n","authors":["Pengcheng Zhao","Yanxiang Chen","Yang Zhao","Zhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.13270v2.pdf","comment":"Accepted by ICONIP-2024"},{"id":"http://arxiv.org/abs/2412.13443v1","updated":"2024-12-18T02:31:37Z","published":"2024-12-18T02:31:37Z","title":"DarkIR: Robust Low-Light Image Restoration","summary":"  Photography during night or in dark conditions typically suffers from noise,\nlow light and blurring issues due to the dim environment and the common use of\nlong exposure. Although Deblurring and Low-light Image Enhancement (LLIE) are\nrelated under these conditions, most approaches in image restoration solve\nthese tasks separately. In this paper, we present an efficient and robust\nneural network for multi-task low-light image restoration. Instead of following\nthe current tendency of Transformer-based models, we propose new attention\nmechanisms to enhance the receptive field of efficient CNNs. Our method reduces\nthe computational costs in terms of parameters and MAC operations compared to\nprevious methods. Our model, DarkIR, achieves new state-of-the-art results on\nthe popular LOLBlur, LOLv2 and Real-LOLBlur datasets, being able to generalize\non real-world night and dark images. Code and models at\nhttps://github.com/cidautai/DarkIR\n","authors":["Daniel Feijoo","Juan C. Benito","Alvaro Garcia","Marcos V. Conde"],"pdf_url":"https://arxiv.org/pdf/2412.13443v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.10967v2","updated":"2024-12-18T02:26:46Z","published":"2024-12-14T20:55:31Z","title":"Biological and Radiological Dictionary of Radiomics Features: Addressing\n  Understandable AI Issues in Personalized Prostate Cancer; Dictionary Version\n  PM1.0","summary":"  We investigate the connection between visual semantic features defined in\nPI-RADS and associated risk factors, moving beyond abnormal imaging findings,\nestablishing a shared framework between medical and AI professionals by\ncreating a standardized dictionary of biological/radiological RFs.\nSubsequently, 6 interpretable and seven complex classifiers, linked with nine\ninterpretable feature selection algorithms (FSA) applied to risk factors, were\nextracted from segmented lesions in T2-weighted imaging (T2WI),\ndiffusion-weighted imaging (DWI), and apparent diffusion coefficient (ADC)\nmultiparametric-prostate MRI sequences to predict the UCLA scores. We then\nutilized the created dictionary to interpret the best-predictive models.\nCombining T2WI, DWI, and ADC with FSAs including ANOVA F-test, Correlation\nCoefficient, and Fisher Score, and utilizing logistic regression, identified\nkey features: The 90th percentile from T2WI, which captures hypo-intensity\nrelated to prostate cancer risk; Variance from T2WI, indicating lesion\nheterogeneity; shape metrics including Least Axis Length and Surface Area to\nVolume ratio from ADC, describing lesion shape and compactness; and Run Entropy\nfrom ADC, reflecting texture consistency. This approach achieved the highest\naverage accuracy of 0.78, significantly outperforming single-sequence methods\n(p-value<0.05). The developed dictionary for Prostate-MRI (PM1.0) serves as a\ncommon language, fosters collaboration between clinical professionals and AI\ndevelopers to advance trustworthy AI solutions that support\nreliable/interpretable clinical decisions.\n","authors":["Mohammad R. Salmanpour","Sajad Amiri","Sara Gharibi","Ahmad Shariftabrizi","Yixi Xu","William B Weeks","Arman Rahmim","Ilker Hacihaliloglu"],"pdf_url":"https://arxiv.org/pdf/2412.10967v2.pdf","comment":"24 pages, 3 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2412.13441v1","updated":"2024-12-18T02:23:33Z","published":"2024-12-18T02:23:33Z","title":"FlashVTG: Feature Layering and Adaptive Score Handling Network for Video\n  Temporal Grounding","summary":"  Text-guided Video Temporal Grounding (VTG) aims to localize relevant segments\nin untrimmed videos based on textual descriptions, encompassing two subtasks:\nMoment Retrieval (MR) and Highlight Detection (HD). Although previous typical\nmethods have achieved commendable results, it is still challenging to retrieve\nshort video moments. This is primarily due to the reliance on sparse and\nlimited decoder queries, which significantly constrain the accuracy of\npredictions. Furthermore, suboptimal outcomes often arise because previous\nmethods rank predictions based on isolated predictions, neglecting the broader\nvideo context. To tackle these issues, we introduce FlashVTG, a framework\nfeaturing a Temporal Feature Layering (TFL) module and an Adaptive Score\nRefinement (ASR) module. The TFL module replaces the traditional decoder\nstructure to capture nuanced video content variations across multiple temporal\nscales, while the ASR module improves prediction ranking by integrating context\nfrom adjacent moments and multi-temporal-scale features. Extensive experiments\ndemonstrate that FlashVTG achieves state-of-the-art performance on four widely\nadopted datasets in both MR and HD. Specifically, on the QVHighlights dataset,\nit boosts mAP by 5.8% for MR and 3.3% for HD. For short-moment retrieval,\nFlashVTG increases mAP to 125% of previous SOTA performance. All these\nimprovements are made without adding training burdens, underscoring its\neffectiveness. Our code is available at https://github.com/Zhuo-Cao/FlashVTG.\n","authors":["Zhuo Cao","Bingqing Zhang","Heming Du","Xin Yu","Xue Li","Sen Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13441v1.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2407.19708v3","updated":"2024-12-18T02:10:48Z","published":"2024-07-29T05:19:23Z","title":"ALEN: A Dual-Approach for Uniform and Non-Uniform Low-Light Image\n  Enhancement","summary":"  Low-light image enhancement is an important task in computer vision,\nessential for improving the visibility and quality of images captured in\nnon-optimal lighting conditions. Inadequate illumination can lead to\nsignificant information loss and poor image quality, impacting various\napplications such as surveillance. photography, or even autonomous driving. In\nthis regard, automated methods have been developed to automatically adjust\nillumination in the image for a better visual perception. Current enhancement\ntechniques often use specific datasets to enhance low-light images, but still\npresent challenges when adapting to diverse real-world conditions, where\nillumination degradation may be localized to specific regions. To address this\nchallenge, the Adaptive Light Enhancement Network (ALEN) is introduced, whose\nmain approach is the use of a classification mechanism to determine whether\nlocal or global illumination enhancement is required. Subsequently, estimator\nnetworks adjust illumination based on this classification and simultaneously\nenhance color fidelity. ALEN integrates the Light Classification Network\n(LCNet) for illuminance categorization, complemented by the Single-Channel\nNetwork (SCNet), and Multi-Channel Network (MCNet) for precise estimation of\nillumination and color, respectively. Extensive experiments on publicly\navailable datasets for low-light conditions were carried out to underscore\nALEN's robust generalization capabilities, demonstrating superior performance\nin both quantitative metrics and qualitative assessments when compared to\nrecent state-of-the-art methods. The ALEN not only enhances image quality in\nterms of visual perception but also represents an advancement in high-level\nvision tasks, such as semantic segmentation, as presented in this work. The\ncode of this method is available at https://github.com/xingyumex/ALEN\n","authors":["Ezequiel Perez-Zarate","Oscar Ramos-Soto","Chunxiao Liu","Diego Oliva","Marco Perez-Cisneros"],"pdf_url":"https://arxiv.org/pdf/2407.19708v3.pdf","comment":"Minor updates and corrections"},{"id":"http://arxiv.org/abs/2304.12921v2","updated":"2024-12-18T02:08:15Z","published":"2023-04-24T03:09:25Z","title":"AwesomeMeta+: Bridging the Technical Barriers to Meta-Learning via A\n  Prototyping and Learning System","summary":"  Meta-learning, also known as \"learning to learn\", enables models to acquire\ngreat generalization abilities by learning from various tasks. Recent\nadvancements have made these models applicable across various fields without\ndata constraints, offering new opportunities for general artificial\nintelligence. However, applying these models can be challenging due to their\noften task-specific, standalone nature and the technical barriers involved. To\naddress this challenge, we develop AwesomeMeta+, a prototyping and learning\nsystem that standardizes different components of meta-learning and uses a\nbuilding block metaphor to assist in model construction. AwesomeMeta+ allows\nusers to assemble compatible algorithm modules to meet the application needs in\npractice. To optimize AwesomeMeta+, we provide the interface to 50 researchers\nand refine the design based on their feedback. Through machine-based testing\nand user studies, we demonstrate that AwesomeMeta+ enhances users'\nunderstanding of the related technologies and accelerates their engineering\nprocesses by offering guidance for meta-learning deployments.\n","authors":["Jingyao Wang","Yuxuan Yang","Wenwen Qiang","Changwen Zheng"],"pdf_url":"https://arxiv.org/pdf/2304.12921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01284v2","updated":"2024-12-18T01:56:53Z","published":"2024-12-02T08:56:13Z","title":"MFTF: Mask-free Training-free Object Level Layout Control Diffusion\n  Model","summary":"  Text-to-image generation models have revolutionized content creation, but\ndiffusion-based vision-language models still face challenges in precisely\ncontrolling the shape, appearance, and positional placement of objects in\ngenerated images using text guidance alone. Existing global image editing\nmodels rely on additional masks or images as guidance to achieve layout\ncontrol, often requiring retraining of the model. While local object-editing\nmodels allow modifications to object shapes, they lack the capability to\ncontrol object positions. To address these limitations, we propose the\nMask-free Training-free Object-Level Layout Control Diffusion Model (MFTF),\nwhich provides precise control over object positions without requiring\nadditional masks or images. The MFTF model supports both single-object and\nmulti-object positional adjustments, such as translation and rotation, while\nenabling simultaneous layout control and object semantic editing. The MFTF\nmodel employs a parallel denoising process for both the source and target\ndiffusion models. During this process, attention masks are dynamically\ngenerated from the cross-attention layers of the source diffusion model and\napplied to queries from the self-attention layers to isolate objects. These\nqueries, generated in the source diffusion model, are then adjusted according\nto the layout control parameters and re-injected into the self-attention layers\nof the target diffusion model. This approach ensures accurate and precise\npositional control of objects. Project source code available at\nhttps://github.com/syang-genai/MFTF.\n","authors":["Shan Yang"],"pdf_url":"https://arxiv.org/pdf/2412.01284v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.11067v3","updated":"2024-12-18T01:55:12Z","published":"2024-12-15T05:57:36Z","title":"CFSynthesis: Controllable and Free-view 3D Human Video Synthesis","summary":"  Human video synthesis aims to create lifelike characters in various\nenvironments, with wide applications in VR, storytelling, and content creation.\nWhile 2D diffusion-based methods have made significant progress, they struggle\nto generalize to complex 3D poses and varying scene backgrounds. To address\nthese limitations, we introduce CFSynthesis, a novel framework for generating\nhigh-quality human videos with customizable attributes, including identity,\nmotion, and scene configurations. Our method leverages a texture-SMPL-based\nrepresentation to ensure consistent and stable character appearances across\nfree viewpoints. Additionally, we introduce a novel foreground-background\nseparation strategy that effectively decomposes the scene as foreground and\nbackground, enabling seamless integration of user-defined backgrounds.\nExperimental results on multiple datasets show that CFSynthesis not only\nachieves state-of-the-art performance in complex human animations but also\nadapts effectively to 3D motions in free-view and user-specified scenarios.\n","authors":["Liyuan Cui","Xiaogang Xu","Wenqi Dong","Zesong Yang","Hujun Bao","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2412.11067v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13419v1","updated":"2024-12-18T01:31:08Z","published":"2024-12-18T01:31:08Z","title":"Exploring Transformer-Augmented LSTM for Temporal and Spatial Feature\n  Learning in Trajectory Prediction","summary":"  Accurate vehicle trajectory prediction is crucial for ensuring safe and\nefficient autonomous driving. This work explores the integration of Transformer\nbased model with Long Short-Term Memory (LSTM) based technique to enhance\nspatial and temporal feature learning in vehicle trajectory prediction. Here, a\nhybrid model that combines LSTMs for temporal encoding with a Transformer\nencoder for capturing complex interactions between vehicles is proposed.\nSpatial trajectory features of the neighboring vehicles are processed and goes\nthrough a masked scatter mechanism in a grid based environment, which is then\ncombined with temporal trajectory of the vehicles. This combined trajectory\ndata are learned by sequential LSTM encoding and Transformer based attention\nlayers. The proposed model is benchmarked against predecessor LSTM based\nmethods, including STA-LSTM, SA-LSTM, CS-LSTM, and NaiveLSTM. Our results,\nwhile not outperforming it's predecessor, demonstrate the potential of\nintegrating Transformers with LSTM based technique to build interpretable\ntrajectory prediction model. Future work will explore alternative architectures\nusing Transformer applications to further enhance performance. This study\nprovides a promising direction for improving trajectory prediction models by\nleveraging transformer based architectures, paving the way for more robust and\ninterpretable vehicle trajectory prediction system.\n","authors":["Chandra Raskoti","Weizi Li"],"pdf_url":"https://arxiv.org/pdf/2412.13419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11917v2","updated":"2024-12-18T01:15:42Z","published":"2024-12-16T16:01:18Z","title":"Does VLM Classification Benefit from LLM Description Semantics?","summary":"  Accurately describing images with text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect, where multiple modified text prompts act as a noisy test-time\naugmentation for the original one. We propose an alternative evaluation\nscenario to decide if a performance boost of LLM-generated descriptions is\ncaused by such a noise augmentation effect or rather by genuine description\nsemantics. The proposed scenario avoids noisy test-time augmentation and\nensures that genuine, distinctive descriptions cause the performance boost.\nFurthermore, we propose a training-free method for selecting discriminative\ndescriptions that work independently of classname-ensembling effects. Our\napproach identifies descriptions that effectively differentiate classes within\na local CLIP label neighborhood, improving classification accuracy across seven\ndatasets. Additionally, we provide insights into the explainability of\ndescription-based image classification with VLMs.\n","authors":["Pingchuan Ma","Lennart Rietdorf","Dmytro Kotovenko","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2412.11917v2.pdf","comment":"AAAI-25, Code: https://github.com/CompVis/DisCLIP"},{"id":"http://arxiv.org/abs/2412.13401v1","updated":"2024-12-18T00:31:18Z","published":"2024-12-18T00:31:18Z","title":"Zero-Shot Low Light Image Enhancement with Diffusion Prior","summary":"  Balancing aesthetic quality with fidelity when enhancing images from\nchallenging, degraded sources is a core objective in computational photography.\nIn this paper, we address low light image enhancement (LLIE), a task in which\ndark images often contain limited visible information. Diffusion models, known\nfor their powerful image enhancement capacities, are a natural choice for this\nproblem. However, their deep generative priors can also lead to hallucinations,\nintroducing non-existent elements or substantially altering the visual\nsemantics of the original scene. In this work, we introduce a novel zero-shot\nmethod for controlling and refining the generative behavior of diffusion models\nfor dark-to-light image conversion tasks. Our method demonstrates superior\nperformance over existing state-of-the-art methods in the task of low-light\nimage enhancement, as evidenced by both quantitative metrics and qualitative\nanalysis.\n","authors":["Joshua Cho","Sara Aghajanzadeh","Zhen Zhu","D. A. Forsyth"],"pdf_url":"https://arxiv.org/pdf/2412.13401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01521v2","updated":"2024-12-18T00:26:39Z","published":"2024-07-01T17:59:23Z","title":"Improving Diffusion Inverse Problem Solving with Decoupled Noise\n  Annealing","summary":"  Diffusion models have recently achieved success in solving Bayesian inverse\nproblems with learned data priors. Current methods build on top of the\ndiffusion sampling process, where each denoising step makes small modifications\nto samples from the previous step. However, this process struggles to correct\nerrors from earlier sampling steps, leading to worse performance in complicated\nnonlinear inverse problems, such as phase retrieval. To address this challenge,\nwe propose a new method called Decoupled Annealing Posterior Sampling (DAPS)\nthat relies on a novel noise annealing process. Specifically, we decouple\nconsecutive steps in a diffusion sampling trajectory, allowing them to vary\nconsiderably from one another while ensuring their time-marginals anneal to the\ntrue posterior as we reduce noise levels. This approach enables the exploration\nof a larger solution space, improving the success rate for accurate\nreconstructions. We demonstrate that DAPS significantly improves sample quality\nand stability across multiple image restoration tasks, particularly in\ncomplicated nonlinear inverse problems.\n","authors":["Bingliang Zhang","Wenda Chu","Julius Berner","Chenlin Meng","Anima Anandkumar","Yang Song"],"pdf_url":"https://arxiv.org/pdf/2407.01521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13394v1","updated":"2024-12-18T00:10:44Z","published":"2024-12-18T00:10:44Z","title":"Distribution Shifts at Scale: Out-of-distribution Detection in Earth\n  Observation","summary":"  Training robust deep learning models is critical in Earth Observation, where\nglobally deployed models often face distribution shifts that degrade\nperformance, especially in low-data regions. Out-of-distribution (OOD)\ndetection addresses this challenge by identifying inputs that differ from\nin-distribution (ID) data. However, existing methods either assume access to\nOOD data or compromise primary task performance, making them unsuitable for\nreal-world deployment. We propose TARDIS, a post-hoc OOD detection method for\nscalable geospatial deployments. The core novelty lies in generating surrogate\nlabels by integrating information from ID data and unknown distributions,\nenabling OOD detection at scale. Our method takes a pre-trained model, ID data,\nand WILD samples, disentangling the latter into surrogate ID and surrogate OOD\nlabels based on internal activations, and fits a binary classifier as an OOD\ndetector. We validate TARDIS on EuroSAT and xBD datasets, across 17\nexperimental setups covering covariate and semantic shifts, showing that it\nperforms close to the theoretical upper bound in assigning surrogate ID and OOD\nsamples in 13 cases. To demonstrate scalability, we deploy TARDIS on the Fields\nof the World dataset, offering actionable insights into pre-trained model\nbehavior for large-scale deployments. The code is publicly available at\nhttps://github.com/microsoft/geospatial-ood-detection.\n","authors":["Burak Ekim","Girmaw Abebe Tadesse","Caleb Robinson","Gilles Hacheme","Michael Schmitt","Rahul Dodhia","Juan M. Lavista Ferres"],"pdf_url":"https://arxiv.org/pdf/2412.13394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13393v1","updated":"2024-12-18T00:10:00Z","published":"2024-12-18T00:10:00Z","title":"MMHMR: Generative Masked Modeling for Hand Mesh Recovery","summary":"  Reconstructing a 3D hand mesh from a single RGB image is challenging due to\ncomplex articulations, self-occlusions, and depth ambiguities. Traditional\ndiscriminative methods, which learn a deterministic mapping from a 2D image to\na single 3D mesh, often struggle with the inherent ambiguities in 2D-to-3D\nmapping. To address this challenge, we propose MMHMR, a novel generative masked\nmodel for hand mesh recovery that synthesizes plausible 3D hand meshes by\nlearning and sampling from the probabilistic distribution of the ambiguous\n2D-to-3D mapping process. MMHMR consists of two key components: (1) a VQ-MANO,\nwhich encodes 3D hand articulations as discrete pose tokens in a latent space,\nand (2) a Context-Guided Masked Transformer that randomly masks out pose tokens\nand learns their joint distribution, conditioned on corrupted token sequences,\nimage context, and 2D pose cues. This learned distribution facilitates\nconfidence-guided sampling during inference, producing mesh reconstructions\nwith low uncertainty and high precision. Extensive evaluations on benchmark and\nreal-world datasets demonstrate that MMHMR achieves state-of-the-art accuracy,\nrobustness, and realism in 3D hand mesh reconstruction. Project website:\nhttps://m-usamasaleem.github.io/publication/MMHMR/mmhmr.html\n","authors":["Muhammad Usama Saleem","Ekkasit Pinyoanuntapong","Mayur Jagdishbhai Patel","Hongfei Xue","Ahmed Helmy","Srijan Das","Pu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13389v1","updated":"2024-12-18T00:06:41Z","published":"2024-12-18T00:06:41Z","title":"Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion","summary":"  Depth completion upgrades sparse depth measurements into dense depth maps\nguided by a conventional image. Existing methods for this highly ill-posed task\noperate in tightly constrained settings and tend to struggle when applied to\nimages outside the training domain or when the available depth measurements are\nsparse, irregularly distributed, or of varying density. Inspired by recent\nadvances in monocular depth estimation, we reframe depth completion as an\nimage-conditional depth map generation guided by sparse measurements. Our\nmethod, Marigold-DC, builds on a pretrained latent diffusion model for\nmonocular depth estimation and injects the depth observations as test-time\nguidance via an optimization scheme that runs in tandem with the iterative\ninference of denoising diffusion. The method exhibits excellent zero-shot\ngeneralization across a diverse range of environments and handles even\nextremely sparse guidance effectively. Our results suggest that contemporary\nmonocular depth priors greatly robustify depth completion: it may be better to\nview the task as recovering dense depth from (dense) image pixels, guided by\nsparse depth; rather than as inpainting (sparse) depth, guided by an image.\nProject website: https://MarigoldDepthCompletion.github.io/\n","authors":["Massimiliano Viola","Kevin Qu","Nando Metzger","Bingxin Ke","Alexander Becker","Konrad Schindler","Anton Obukhov"],"pdf_url":"https://arxiv.org/pdf/2412.13389v1.pdf","comment":null}]},"2024-12-19T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.15216v1","updated":"2024-12-19T18:59:58Z","published":"2024-12-19T18:59:58Z","title":"UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit\n  Consistency","summary":"  We propose an unsupervised model for instruction-based image editing that\neliminates the need for ground-truth edited images during training. Existing\nsupervised methods depend on datasets containing triplets of input image,\nedited image, and edit instruction. These are generated by either existing\nediting methods or human-annotations, which introduce biases and limit their\ngeneralization ability. Our method addresses these challenges by introducing a\nnovel editing mechanism called Cycle Edit Consistency (CEC), which applies\nforward and backward edits in one training step and enforces consistency in\nimage and attention spaces. This allows us to bypass the need for ground-truth\nedited images and unlock training for the first time on datasets comprising\neither real image-caption pairs or image-caption-edit triplets. We empirically\nshow that our unsupervised technique performs better across a broader range of\nedits with high fidelity and precision. By eliminating the need for\npre-existing datasets of triplets, reducing biases associated with supervised\nmethods, and proposing CEC, our work represents a significant advancement in\nunblocking scaling of instruction-based image editing.\n","authors":["Enis Simsar","Alessio Tonioni","Yongqin Xian","Thomas Hofmann","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2412.15216v1.pdf","comment":"Project page: https://enis.dev/uip2p/"},{"id":"http://arxiv.org/abs/2412.15215v1","updated":"2024-12-19T18:59:57Z","published":"2024-12-19T18:59:57Z","title":"EnvGS: Modeling View-Dependent Appearance with Environment Gaussian","summary":"  Reconstructing complex reflections in real-world scenes from 2D images is\nessential for achieving photorealistic novel view synthesis. Existing methods\nthat utilize environment maps to model reflections from distant lighting often\nstruggle with high-frequency reflection details and fail to account for\nnear-field reflections. In this work, we introduce EnvGS, a novel approach that\nemploys a set of Gaussian primitives as an explicit 3D representation for\ncapturing reflections of environments. These environment Gaussian primitives\nare incorporated with base Gaussian primitives to model the appearance of the\nwhole scene. To efficiently render these environment Gaussian primitives, we\ndeveloped a ray-tracing-based renderer that leverages the GPU's RT core for\nfast rendering. This allows us to jointly optimize our model for high-quality\nreconstruction while maintaining real-time rendering speeds. Results from\nmultiple real-world and synthetic datasets demonstrate that our method produces\nsignificantly more detailed reflections, achieving the best rendering quality\nin real-time novel view synthesis.\n","authors":["Tao Xie","Xi Chen","Zhen Xu","Yiman Xie","Yudong Jin","Yujun Shen","Sida Peng","Hujun Bao","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.15215v1.pdf","comment":"Project page: https://zju3dv.github.io/envgs/"},{"id":"http://arxiv.org/abs/2412.15213v1","updated":"2024-12-19T18:59:56Z","published":"2024-12-19T18:59:56Z","title":"Flowing from Words to Pixels: A Framework for Cross-Modality Evolution","summary":"  Diffusion models, and their generalization, flow matching, have had a\nremarkable impact on the field of media generation. Here, the conventional\napproach is to learn the complex mapping from a simple source distribution of\nGaussian noise to the target media distribution. For cross-modal tasks such as\ntext-to-image generation, this same mapping from noise to image is learnt\nwhilst including a conditioning mechanism in the model. One key and thus far\nrelatively unexplored feature of flow matching is that, unlike Diffusion\nmodels, they are not constrained for the source distribution to be noise.\nHence, in this paper, we propose a paradigm shift, and ask the question of\nwhether we can instead train flow matching models to learn a direct mapping\nfrom the distribution of one modality to the distribution of another, thus\nobviating the need for both the noise distribution and conditioning mechanism.\nWe present a general and simple framework, CrossFlow, for cross-modal flow\nmatching. We show the importance of applying Variational Encoders to the input\ndata, and introduce a method to enable Classifier-free guidance. Surprisingly,\nfor text-to-image, CrossFlow with a vanilla transformer without cross attention\nslightly outperforms standard flow matching, and we show that it scales better\nwith training steps and model size, while also allowing for interesting latent\narithmetic which results in semantically meaningful edits in the output space.\nTo demonstrate the generalizability of our approach, we also show that\nCrossFlow is on par with or outperforms the state-of-the-art for various\ncross-modal / intra-modal mapping tasks, viz. image captioning, depth\nestimation, and image super-resolution. We hope this paper contributes to\naccelerating progress in cross-modal media generation.\n","authors":["Qihao Liu","Xi Yin","Alan Yuille","Andrew Brown","Mannat Singh"],"pdf_url":"https://arxiv.org/pdf/2412.15213v1.pdf","comment":"Project page: https://cross-flow.github.io/"},{"id":"http://arxiv.org/abs/2412.15214v1","updated":"2024-12-19T18:59:56Z","published":"2024-12-19T18:59:56Z","title":"LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis","summary":"  The intuitive nature of drag-based interaction has led to its growing\nadoption for controlling object trajectories in image-to-video synthesis.\nStill, existing methods that perform dragging in the 2D space usually face\nambiguity when handling out-of-plane movements. In this work, we augment the\ninteraction with a new dimension, i.e., the depth dimension, such that users\nare allowed to assign a relative depth for each point on the trajectory. That\nway, our new interaction paradigm not only inherits the convenience from 2D\ndragging, but facilitates trajectory control in the 3D space, broadening the\nscope of creativity. We propose a pioneering method for 3D trajectory control\nin image-to-video synthesis by abstracting object masks into a few cluster\npoints. These points, accompanied by the depth information and the instance\ninformation, are finally fed into a video diffusion model as the control\nsignal. Extensive experiments validate the effectiveness of our approach,\ndubbed LeviTor, in precisely manipulating the object movements when producing\nphoto-realistic videos from static images. Project page:\nhttps://ppetrichor.github.io/levitor.github.io/\n","authors":["Hanlin Wang","Hao Ouyang","Qiuyu Wang","Wen Wang","Ka Leong Cheng","Qifeng Chen","Yujun Shen","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.15214v1.pdf","comment":"Project page available at\n  https://ppetrichor.github.io/levitor.github.io/"},{"id":"http://arxiv.org/abs/2412.15211v1","updated":"2024-12-19T18:59:51Z","published":"2024-12-19T18:59:51Z","title":"Generative Multiview Relighting for 3D Reconstruction under Extreme\n  Illumination Variation","summary":"  Reconstructing the geometry and appearance of objects from photographs taken\nin different environments is difficult as the illumination and therefore the\nobject appearance vary across captured images. This is particularly challenging\nfor more specular objects whose appearance strongly depends on the viewing\ndirection. Some prior approaches model appearance variation across images using\na per-image embedding vector, while others use physically-based rendering to\nrecover the materials and per-image illumination. Such approaches fail at\nfaithfully recovering view-dependent appearance given significant variation in\ninput illumination and tend to produce mostly diffuse results. We present an\napproach that reconstructs objects from images taken under different\nilluminations by first relighting the images under a single reference\nillumination with a multiview relighting diffusion model and then\nreconstructing the object's geometry and appearance with a radiance field\narchitecture that is robust to the small remaining inconsistencies among the\nrelit images. We validate our proposed approach on both synthetic and real\ndatasets and demonstrate that it greatly outperforms existing techniques at\nreconstructing high-fidelity appearance from images taken under extreme\nillumination variation. Moreover, our approach is particularly effective at\nrecovering view-dependent \"shiny\" appearance which cannot be reconstructed by\nprior methods.\n","authors":["Hadi Alzayer","Philipp Henzler","Jonathan T. Barron","Jia-Bin Huang","Pratul P. Srinivasan","Dor Verbin"],"pdf_url":"https://arxiv.org/pdf/2412.15211v1.pdf","comment":"Project page: https://relight-to-reconstruct.github.io/"},{"id":"http://arxiv.org/abs/2412.15212v1","updated":"2024-12-19T18:59:51Z","published":"2024-12-19T18:59:51Z","title":"Scaling 4D Representations","summary":"  Scaling has not yet been convincingly demonstrated for pure self-supervised\nlearning from video. However, prior work has focused evaluations on\nsemantic-related tasks $\\unicode{x2013}$ action classification, ImageNet\nclassification, etc. In this paper we focus on evaluating self-supervised\nlearning on non-semantic vision tasks that are more spatial (3D) and temporal\n(+1D = 4D), such as camera pose estimation, point and object tracking, and\ndepth estimation. We show that by learning from very large video datasets,\nmasked auto-encoding (MAE) with transformer video models actually scales,\nconsistently improving performance on these 4D tasks, as model size increases\nfrom 20M all the way to the largest by far reported self-supervised video model\n$\\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with\nmany recent image and video models demonstrates the benefits of scaling 4D\nrepresentations.\n","authors":["João Carreira","Dilara Gokay","Michael King","Chuhan Zhang","Ignacio Rocco","Aravindh Mahendran","Thomas Albert Keck","Joseph Heyward","Skanda Koppula","Etienne Pot","Goker Erdogan","Yana Hasson","Yi Yang","Klaus Greff","Guillaume Le Moing","Sjoerd van Steenkiste","Daniel Zoran","Drew A. Hudson","Pedro Vélez","Luisa Polanía","Luke Friedman","Chris Duvarney","Ross Goroshin","Kelsey Allen","Jacob Walker","Rishabh Kabra","Eric Aboussouan","Jennifer Sun","Thomas Kipf","Carl Doersch","Viorica Pătrăucean","Dima Damen","Pauline Luc","Mehdi S. M. Sajjadi","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2412.15212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15209v1","updated":"2024-12-19T18:59:44Z","published":"2024-12-19T18:59:44Z","title":"PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation","summary":"  Despite significant advancements in Large Vision-Language Models (LVLMs),\nexisting pixel-grounding models operate on single-image settings, limiting\ntheir ability to perform detailed, fine-grained comparisons across multiple\nimages. Conversely, current multi-image understanding models lack pixel-level\ngrounding. Our work addresses this gap by introducing the task of multi-image\npixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates\npixel-level grounding with robust multi-image reasoning capabilities to produce\ncontextually rich, pixel-grounded explanations. Central to PRIMA is an\nefficient vision module that queries fine-grained visual representations across\nmultiple images, reducing TFLOPs by $25.3\\%$. To support training and\nevaluation, we curate $M^4Seg$, a new reasoning segmentation benchmark\nconsisting of $\\sim$224K question-answer pairs that require fine-grained visual\nunderstanding across multiple images. Experimental results demonstrate PRIMA\noutperforms state-of-the-art baselines.\n","authors":["Muntasir Wahed","Kiet A. Nguyen","Adheesh Sunil Juvekar","Xinzhuo Li","Xiaona Zhou","Vedant Shah","Tianjiao Yu","Pinar Yanardag","Ismini Lourentzou"],"pdf_url":"https://arxiv.org/pdf/2412.15209v1.pdf","comment":"Project page: https://plan-lab.github.io/prima"},{"id":"http://arxiv.org/abs/2412.15208v1","updated":"2024-12-19T18:59:40Z","published":"2024-12-19T18:59:40Z","title":"OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving","summary":"  Since the advent of Multimodal Large Language Models (MLLMs), they have made\na significant impact across a wide range of real-world applications,\nparticularly in Autonomous Driving (AD). Their ability to process complex\nvisual data and reason about intricate driving scenarios has paved the way for\na new paradigm in end-to-end AD systems. However, the progress of developing\nend-to-end models for AD has been slow, as existing fine-tuning methods demand\nsubstantial resources, including extensive computational power, large-scale\ndatasets, and significant funding. Drawing inspiration from recent advancements\nin inference computing, we propose OpenEMMA, an open-source end-to-end\nframework based on MLLMs. By incorporating the Chain-of-Thought reasoning\nprocess, OpenEMMA achieves significant improvements compared to the baseline\nwhen leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates\neffectiveness, generalizability, and robustness across a variety of challenging\ndriving scenarios, offering a more efficient and effective approach to\nautonomous driving. We release all the codes in\nhttps://github.com/taco-group/OpenEMMA.\n","authors":["Shuo Xing","Chengyuan Qian","Yuping Wang","Hongyuan Hua","Kexin Tian","Yang Zhou","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2412.15208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15206v1","updated":"2024-12-19T18:59:33Z","published":"2024-12-19T18:59:33Z","title":"AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models\n  for Autonomous Driving","summary":"  Recent advancements in large vision language models (VLMs) tailored for\nautonomous driving (AD) have shown strong scene understanding and reasoning\ncapabilities, making them undeniable candidates for end-to-end driving systems.\nHowever, limited work exists on studying the trustworthiness of DriveVLMs -- a\ncritical factor that directly impacts public transportation safety. In this\npaper, we introduce AutoTrust, a comprehensive trustworthiness benchmark for\nlarge vision-language models in autonomous driving (DriveVLMs), considering\ndiverse perspectives -- including trustfulness, safety, robustness, privacy,\nand fairness. We constructed the largest visual question-answering dataset for\ninvestigating trustworthiness issues in driving scenarios, comprising over 10k\nunique scenes and 18k queries. We evaluated six publicly available VLMs,\nspanning from generalist to specialist, from open-source to commercial models.\nOur exhaustive evaluations have unveiled previously undiscovered\nvulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we found\nthat the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperform\nspecialized models fine-tuned for driving in terms of overall trustworthiness.\nDriveVLMs like DriveLM-Agent are particularly vulnerable to disclosing\nsensitive information. Additionally, both generalist and specialist VLMs remain\nsusceptible to adversarial attacks and struggle to ensure unbiased\ndecision-making across diverse environments and populations. Our findings call\nfor immediate and decisive action to address the trustworthiness of DriveVLMs\n-- an issue of critical importance to public safety and the welfare of all\ncitizens relying on autonomous transportation systems. Our benchmark is\npublicly available at \\url{https://github.com/taco-group/AutoTrust}, and the\nleaderboard is released at \\url{https://taco-group.github.io/AutoTrust/}.\n","authors":["Shuo Xing","Hongyuan Hua","Xiangbo Gao","Shenzhe Zhu","Renjie Li","Kexin Tian","Xiaopeng Li","Heng Huang","Tianbao Yang","Zhangyang Wang","Yang Zhou","Huaxiu Yao","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2412.15206v1.pdf","comment":"55 pages, 14 figures"},{"id":"http://arxiv.org/abs/2412.15205v1","updated":"2024-12-19T18:59:31Z","published":"2024-12-19T18:59:31Z","title":"FlowAR: Scale-wise Autoregressive Image Generation Meets Flow Matching","summary":"  Autoregressive (AR) modeling has achieved remarkable success in natural\nlanguage processing by enabling models to generate text with coherence and\ncontextual understanding through next token prediction. Recently, in image\ngeneration, VAR proposes scale-wise autoregressive modeling, which extends the\nnext token prediction to the next scale prediction, preserving the 2D structure\nof images. However, VAR encounters two primary challenges: (1) its complex and\nrigid scale design limits generalization in next scale prediction, and (2) the\ngenerator's dependence on a discrete tokenizer with the same complex scale\nstructure restricts modularity and flexibility in updating the tokenizer. To\naddress these limitations, we introduce FlowAR, a general next scale prediction\nmethod featuring a streamlined scale design, where each subsequent scale is\nsimply double the previous one. This eliminates the need for VAR's intricate\nmulti-scale residual tokenizer and enables the use of any off-the-shelf\nVariational AutoEncoder (VAE). Our simplified design enhances generalization in\nnext scale prediction and facilitates the integration of Flow Matching for\nhigh-quality image synthesis. We validate the effectiveness of FlowAR on the\nchallenging ImageNet-256 benchmark, demonstrating superior generation\nperformance compared to previous methods. Codes will be available at\n\\url{https://github.com/OliverRensu/FlowAR}.\n","authors":["Sucheng Ren","Qihang Yu","Ju He","Xiaohui Shen","Alan Yuille","Liang-Chieh Chen"],"pdf_url":"https://arxiv.org/pdf/2412.15205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15200v1","updated":"2024-12-19T18:58:46Z","published":"2024-12-19T18:58:46Z","title":"DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation\n  for High-quality 3D Asset Creation","summary":"  Procedural Content Generation (PCG) is powerful in creating high-quality 3D\ncontents, yet controlling it to produce desired shapes is difficult and often\nrequires extensive parameter tuning. Inverse Procedural Content Generation aims\nto automatically find the best parameters under the input condition. However,\nexisting sampling-based and neural network-based methods still suffer from\nnumerous sample iterations or limited controllability. In this work, we present\nDI-PCG, a novel and efficient method for Inverse PCG from general image\nconditions. At its core is a lightweight diffusion transformer model, where PCG\nparameters are directly treated as the denoising target and the observed images\nas conditions to control parameter generation. DI-PCG is efficient and\neffective. With only 7.6M network parameters and 30 GPU hours to train, it\ndemonstrates superior performance in recovering parameters accurately, and\ngeneralizing well to in-the-wild images. Quantitative and qualitative\nexperiment results validate the effectiveness of DI-PCG in inverse PCG and\nimage-to-3D generation tasks. DI-PCG offers a promising approach for efficient\ninverse PCG and represents a valuable exploration step towards a 3D generation\npath that models how to construct a 3D asset using parametric models.\n","authors":["Wang Zhao","Yan-Pei Cao","Jiale Xu","Yuejiang Dong","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2412.15200v1.pdf","comment":"Project page: https://thuzhaowang.github.io/projects/DI-PCG/"},{"id":"http://arxiv.org/abs/2412.15199v1","updated":"2024-12-19T18:58:36Z","published":"2024-12-19T18:58:36Z","title":"LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation","summary":"  This paper targets the challenge of real-time LiDAR re-simulation in dynamic\ndriving scenarios. Recent approaches utilize neural radiance fields combined\nwith the physical modeling of LiDAR sensors to achieve high-fidelity\nre-simulation results. Unfortunately, these methods face limitations due to\nhigh computational demands in large-scale scenes and cannot perform real-time\nLiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novel\nframework that supports real-time, physically accurate LiDAR re-simulation for\ndriving scenes. Our primary contribution is the development of an efficient and\neffective rendering pipeline, which integrates Gaussian primitives and\nhardware-accelerated ray tracing technology. Specifically, we model the\nphysical properties of LiDAR sensors using Gaussian primitives with learnable\nparameters and incorporate scene graphs to handle scene dynamics. Building upon\nthis scene representation, our framework first constructs a bounding volume\nhierarchy (BVH), then casts rays for each pixel and generates novel LiDAR views\nthrough a differentiable rendering algorithm. Importantly, our framework\nsupports realistic rendering with flexible scene editing operations and various\nsensor configurations. Extensive experiments across multiple public benchmarks\ndemonstrate that our method outperforms state-of-the-art methods in terms of\nrendering quality and efficiency. Our project page is at\nhttps://zju3dv.github.io/lidar-rt.\n","authors":["Chenxu Zhou","Lvchang Fu","Sida Peng","Yunzhi Yan","Zhanhua Zhang","Yong Chen","Jiazhi Xia","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.15199v1.pdf","comment":"Project page: https://zju3dv.github.io/lidar-rt"},{"id":"http://arxiv.org/abs/2412.15195v1","updated":"2024-12-19T18:58:14Z","published":"2024-12-19T18:58:14Z","title":"Preventing Local Pitfalls in Vector Quantization via Optimal Transport","summary":"  Vector-quantized networks (VQNs) have exhibited remarkable performance across\nvarious tasks, yet they are prone to training instability, which complicates\nthe training process due to the necessity for techniques such as subtle\ninitialization and model distillation. In this study, we identify the local\nminima issue as the primary cause of this instability. To address this, we\nintegrate an optimal transport method in place of the nearest neighbor search\nto achieve a more globally informed assignment. We introduce OptVQ, a novel\nvector quantization method that employs the Sinkhorn algorithm to optimize the\noptimal transport problem, thereby enhancing the stability and efficiency of\nthe training process. To mitigate the influence of diverse data distributions\non the Sinkhorn algorithm, we implement a straightforward yet effective\nnormalization strategy. Our comprehensive experiments on image reconstruction\ntasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses\ncurrent state-of-the-art VQNs in reconstruction quality.\n","authors":["Borui Zhang","Wenzhao Zheng","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2412.15195v1.pdf","comment":"Code is available at https://github.com/zbr17/OptVQ"},{"id":"http://arxiv.org/abs/2412.15191v1","updated":"2024-12-19T18:57:21Z","published":"2024-12-19T18:57:21Z","title":"AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal\n  Audio-Video Generation","summary":"  We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Video\ngeneration that leverages the activations of frozen video and audio diffusion\nmodels for temporally-aligned cross-modal conditioning. The key to our\nframework is a Fusion Block that enables bidirectional information exchange\nbetween our backbone video and audio diffusion models through a\ntemporally-aligned self attention operation. Unlike prior work that uses\nfeature extractors pretrained for other tasks for the conditioning signal,\nAV-Link can directly leverage features obtained by the complementary modality\nin a single framework i.e. video features to generate audio, or audio features\nto generate video. We extensively evaluate our design choices and demonstrate\nthe ability of our method to achieve synchronized and high-quality audiovisual\ncontent, showcasing its potential for applications in immersive media\ngeneration. Project Page: snap-research.github.io/AVLink/\n","authors":["Moayed Haji-Ali","Willi Menapace","Aliaksandr Siarohin","Ivan Skorokhodov","Alper Canberk","Kwot Sin Lee","Vicente Ordonez","Sergey Tulyakov"],"pdf_url":"https://arxiv.org/pdf/2412.15191v1.pdf","comment":"Project Page: snap-research.github.io/AVLink/"},{"id":"http://arxiv.org/abs/2412.15190v1","updated":"2024-12-19T18:57:13Z","published":"2024-12-19T18:57:13Z","title":"EarthDial: Turning Multi-sensory Earth Observations to Interactive\n  Dialogues","summary":"  Automated analysis of vast Earth observation data via interactive\nVision-Language Models (VLMs) can unlock new opportunities for environmental\nmonitoring, disaster response, and resource management. Existing generic VLMs\ndo not perform well on Remote Sensing data, while the recent Geo-spatial VLMs\nremain restricted to a fixed resolution and few sensor modalities. In this\npaper, we introduce EarthDial, a conversational assistant specifically designed\nfor Earth Observation (EO) data, transforming complex, multi-sensory Earth\nobservations into interactive, natural language dialogues. EarthDial supports\nmulti-spectral, multi-temporal, and multi-resolution imagery, enabling a wide\nrange of remote sensing tasks, including classification, detection, captioning,\nquestion answering, visual reasoning, and visual grounding. To achieve this, we\nintroduce an extensive instruction tuning dataset comprising over 11.11M\ninstruction pairs covering RGB, Synthetic Aperture Radar (SAR), and\nmultispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore,\nEarthDial handles bi-temporal and multi-temporal sequence analysis for\napplications like change detection. Our extensive experimental results on 37\ndownstream applications demonstrate that EarthDial outperforms existing generic\nand domain-specific models, achieving better generalization across various EO\ntasks.\n","authors":["Sagar Soni","Akshay Dudhane","Hiyam Debary","Mustansar Fiaz","Muhammad Akhtar Munir","Muhammad Sohail Danish","Paolo Fraccaro","Campbell D Watson","Levente J Klein","Fahad Shahbaz Khan","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2412.15190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15188v1","updated":"2024-12-19T18:56:24Z","published":"2024-12-19T18:56:24Z","title":"LlamaFusion: Adapting Pretrained Language Models for Multimodal\n  Generation","summary":"  We present LlamaFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLlamaFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LlamaFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLlamaFusion improves image understanding by 20% and image generation by 3.6%\nusing only 50% of the FLOPs while maintaining Llama-3's language capabilities.\nWe also demonstrate that this framework can adapt existing vision-language\nmodels with multimodal generation ability. Overall, this framework not only\nleverages existing computational investments in text-only LLMs but also enables\nthe parallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.\n","authors":["Weijia Shi","Xiaochuang Han","Chunting Zhou","Weixin Liang","Xi Victoria Lin","Luke Zettlemoyer","Lili Yu"],"pdf_url":"https://arxiv.org/pdf/2412.15188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15185v1","updated":"2024-12-19T18:55:25Z","published":"2024-12-19T18:55:25Z","title":"Tiled Diffusion","summary":"  Image tiling -- the seamless connection of disparate images to create a\ncoherent visual field -- is crucial for applications such as texture creation,\nvideo game asset development, and digital art. Traditionally, tiles have been\nconstructed manually, a method that poses significant limitations in\nscalability and flexibility. Recent research has attempted to automate this\nprocess using generative models. However, current approaches primarily focus on\ntiling textures and manipulating models for single-image generation, without\ninherently supporting the creation of multiple interconnected tiles across\ndiverse domains. This paper presents Tiled Diffusion, a novel approach that\nextends the capabilities of diffusion models to accommodate the generation of\ncohesive tiling patterns across various domains of image synthesis that require\ntiling. Our method supports a wide range of tiling scenarios, from self-tiling\nto complex many-to-many connections, enabling seamless integration of multiple\nimages. Tiled Diffusion automates the tiling process, eliminating the need for\nmanual intervention and enhancing creative possibilities in various\napplications, such as seamlessly tiling of existing images, tiled texture\ncreation, and 360{\\deg} synthesis.\n","authors":["Or Madar","Ohad Fried"],"pdf_url":"https://arxiv.org/pdf/2412.15185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07449v2","updated":"2024-12-19T18:51:28Z","published":"2024-11-12T00:20:11Z","title":"Tracing the Roots: Leveraging Temporal Dynamics in Diffusion\n  Trajectories for Origin Attribution","summary":"  Diffusion models have revolutionized image synthesis, garnering significant\nresearch interest in recent years. Diffusion is an iterative algorithm in which\nsamples are generated step-by-step, starting from pure noise. This process\nintroduces the notion of diffusion trajectories, i.e., paths from the standard\nGaussian distribution to the target image distribution. In this context, we\nstudy discriminative algorithms operating on these trajectories. Specifically,\ngiven a pre-trained diffusion model, we consider the problem of classifying\nimages as part of the training dataset, generated by the model or originating\nfrom an external source. Our approach demonstrates the presence of patterns\nacross steps that can be leveraged for classification. We also conduct ablation\nstudies, which reveal that using higher-order gradient features to characterize\nthe trajectories leads to significant performance gains and more robust\nalgorithms.\n","authors":["Andreas Floros","Seyed-Mohsen Moosavi-Dezfooli","Pier Luigi Dragotti"],"pdf_url":"https://arxiv.org/pdf/2411.07449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15171v1","updated":"2024-12-19T18:46:55Z","published":"2024-12-19T18:46:55Z","title":"SqueezeMe: Efficient Gaussian Avatars for VR","summary":"  Gaussian Splatting has enabled real-time 3D human avatars with unprecedented\nlevels of visual quality. While previous methods require a desktop GPU for\nreal-time inference of a single avatar, we aim to squeeze multiple Gaussian\navatars onto a portable virtual reality headset with real-time drivable\ninference. We begin by training a previous work, Animatable Gaussians, on a\nhigh quality dataset captured with 512 cameras. The Gaussians are animated by\ncontrolling base set of Gaussians with linear blend skinning (LBS) motion and\nthen further adjusting the Gaussians with a neural network decoder to correct\ntheir appearance. When deploying the model on a Meta Quest 3 VR headset, we\nfind two major computational bottlenecks: the decoder and the rendering. To\naccelerate the decoder, we train the Gaussians in UV-space instead of\npixel-space, and we distill the decoder to a single neural network layer.\nFurther, we discover that neighborhoods of Gaussians can share a single\ncorrective from the decoder, which provides an additional speedup. To\naccelerate the rendering, we develop a custom pipeline in Vulkan that runs on\nthe mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrently\nat 72 FPS on a VR headset. Demo videos are at\nhttps://forresti.github.io/squeezeme.\n","authors":["Shunsuke Saito","Stanislav Pidhorskyi","Igor Santesteban","Forrest Iandola","Divam Gupta","Anuj Pahuja","Nemanja Bartolovic","Frank Yu","Emanuel Garbin","Tomas Simon"],"pdf_url":"https://arxiv.org/pdf/2412.15171v1.pdf","comment":"Initial version"},{"id":"http://arxiv.org/abs/2412.15159v1","updated":"2024-12-19T18:34:50Z","published":"2024-12-19T18:34:50Z","title":"OnlineVPO: Align Video Diffusion Model with Online Video-Centric\n  Preference Optimization","summary":"  In recent years, the field of text-to-video (T2V) generation has made\nsignificant strides. Despite this progress, there is still a gap between\ntheoretical advancements and practical application, amplified by issues like\ndegraded image quality and flickering artifacts. Recent advancements in\nenhancing the video diffusion model (VDM) through feedback learning have shown\npromising results. However, these methods still exhibit notable limitations,\nsuch as misaligned feedback and inferior scalability. To tackle these issues,\nwe introduce OnlineVPO, a more efficient preference learning approach tailored\nspecifically for video diffusion models. Our method features two novel designs,\nfirstly, instead of directly using image-based reward feedback, we leverage the\nvideo quality assessment (VQA) model trained on synthetic data as the reward\nmodel to provide distribution and modality-aligned feedback on the video\ndiffusion model. Additionally, we introduce an online DPO algorithm to address\nthe off-policy optimization and scalability issue in existing video preference\nlearning frameworks. By employing the video reward model to offer concise video\nfeedback on the fly, OnlineVPO offers effective and efficient preference\nguidance. Extensive experiments on the open-source video-diffusion model\ndemonstrate OnlineVPO as a simple yet effective and more importantly scalable\npreference learning algorithm for video diffusion models, offering valuable\ninsights for future advancements in this domain.\n","authors":["Jiacheng Zhang","Jie Wu","Weifeng Chen","Yatai Ji","Xuefeng Xiao","Weilin Huang","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2412.15159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15156v1","updated":"2024-12-19T18:32:21Z","published":"2024-12-19T18:32:21Z","title":"Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned\n  LLM","summary":"  Text-to-video models have made remarkable advancements through optimization\non high-quality text-video pairs, where the textual prompts play a pivotal role\nin determining quality of output videos. However, achieving the desired output\noften entails multiple revisions and iterative inference to refine\nuser-provided prompts. Current automatic methods for refining prompts encounter\nchallenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware\nwhen applied to text-to-video diffusion models. To address these problem, we\nintroduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video,\nwhich excels in crafting Video-Centric, Labor-Free and Preference-Aligned\nprompts tailored to specific video diffusion model. Our approach involves a\nmeticulously crafted two-stage optimization and alignment system. Initially, we\nconduct a reward-guided prompt evolution pipeline to automatically create\noptimal prompts pool and leverage them for supervised fine-tuning (SFT) of the\nLLM. Then multi-dimensional rewards are employed to generate pairwise data for\nthe SFT model, followed by the direct preference optimization (DPO) algorithm\nto further facilitate preference alignment. Through extensive experimentation\nand comparative analyses, we validate the effectiveness of Prompt-A-Video\nacross diverse generation models, highlighting its potential to push the\nboundaries of video generation.\n","authors":["Yatai Ji","Jiacheng Zhang","Jie Wu","Shilong Zhang","Shoufa Chen","Chongjian GE","Peize Sun","Weifeng Chen","Wenqi Shao","Xuefeng Xiao","Weilin Huang","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2412.15156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15150v1","updated":"2024-12-19T18:28:37Z","published":"2024-12-19T18:28:37Z","title":"Leveraging Color Channel Independence for Improved Unsupervised Object\n  Detection","summary":"  Object-centric architectures can learn to extract distinct object\nrepresentations from visual scenes, enabling downstream applications on the\nobject level. Similarly to autoencoder-based image models, object-centric\napproaches have been trained on the unsupervised reconstruction loss of images\nencoded by RGB color spaces. In our work, we challenge the common assumption\nthat RGB images are the optimal color space for unsupervised learning in\ncomputer vision. We discuss conceptually and empirically that other color\nspaces, such as HSV, bear essential characteristics for object-centric\nrepresentation learning, like robustness to lighting conditions. We further\nshow that models improve when requiring them to predict additional color\nchannels. Specifically, we propose to transform the predicted targets to the\nRGB-S space, which extends RGB with HSV's saturation component and leads to\nmarkedly better reconstruction and disentanglement for five common evaluation\ndatasets. The use of composite color spaces can be implemented with basically\nno computational overhead, is agnostic of the models' architecture, and is\nuniversally applicable across a wide range of visual computing tasks and\ntraining types. The findings of our approach encourage additional\ninvestigations in computer vision tasks beyond object-centric learning.\n","authors":["Bastian Jäckl","Yannick Metz","Udo Schlegel","Daniel A. Keim","Maximilian T. Fischer"],"pdf_url":"https://arxiv.org/pdf/2412.15150v1.pdf","comment":"38 pages incl. references, 16 figures"},{"id":"http://arxiv.org/abs/2412.15129v1","updated":"2024-12-19T18:09:42Z","published":"2024-12-19T18:09:42Z","title":"Jet: A Modern Transformer-Based Normalizing Flow","summary":"  In the past, normalizing generative flows have emerged as a promising class\nof generative models for natural images. This type of model has many modeling\nadvantages: the ability to efficiently compute log-likelihood of the input\ndata, fast generation and simple overall structure. Normalizing flows remained\na topic of active research but later fell out of favor, as visual quality of\nthe samples was not competitive with other model classes, such as GANs,\nVQ-VAE-based approaches or diffusion models. In this paper we revisit the\ndesign of the coupling-based normalizing flow models by carefully ablating\nprior design choices and using computational blocks based on the Vision\nTransformer architecture, not convolutional neural networks. As a result, we\nachieve state-of-the-art quantitative and qualitative performance with a much\nsimpler architecture. While the overall visual quality is still behind the\ncurrent state-of-the-art models, we argue that strong normalizing flow models\ncan help advancing research frontier by serving as building components of more\npowerful generative models.\n","authors":["Alexander Kolesnikov","André Susano Pinto","Michael Tschannen"],"pdf_url":"https://arxiv.org/pdf/2412.15129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15119v1","updated":"2024-12-19T17:59:54Z","published":"2024-12-19T17:59:54Z","title":"Parallelized Autoregressive Visual Generation","summary":"  Autoregressive models have emerged as a powerful approach for visual\ngeneration but suffer from slow inference speed due to their sequential\ntoken-by-token prediction process. In this paper, we propose a simple yet\neffective approach for parallelized autoregressive visual generation that\nimproves generation efficiency while preserving the advantages of\nautoregressive modeling. Our key insight is that parallel generation depends on\nvisual token dependencies-tokens with weak dependencies can be generated in\nparallel, while strongly dependent adjacent tokens are difficult to generate\ntogether, as their independent sampling may lead to inconsistencies. Based on\nthis observation, we develop a parallel generation strategy that generates\ndistant tokens with weak dependencies in parallel while maintaining sequential\ngeneration for strongly dependent local tokens. Our approach can be seamlessly\nintegrated into standard autoregressive models without modifying the\narchitecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that\nour method achieves a 3.6x speedup with comparable quality and up to 9.5x\nspeedup with minimal quality degradation across both image and video generation\ntasks. We hope this work will inspire future research in efficient visual\ngeneration and unified autoregressive modeling. Project page:\nhttps://epiphqny.github.io/PAR-project.\n","authors":["Yuqing Wang","Shuhuai Ren","Zhijie Lin","Yujin Han","Haoyuan Guo","Zhenheng Yang","Difan Zou","Jiashi Feng","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2412.15119v1.pdf","comment":"Project page: https://epiphqny.github.io/PAR-project"},{"id":"http://arxiv.org/abs/2412.11917v3","updated":"2024-12-19T17:57:59Z","published":"2024-12-16T16:01:18Z","title":"Does VLM Classification Benefit from LLM Description Semantics?","summary":"  Accurately describing images with text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect, where multiple modified text prompts act as a noisy test-time\naugmentation for the original one. We propose an alternative evaluation\nscenario to decide if a performance boost of LLM-generated descriptions is\ncaused by such a noise augmentation effect or rather by genuine description\nsemantics. The proposed scenario avoids noisy test-time augmentation and\nensures that genuine, distinctive descriptions cause the performance boost.\nFurthermore, we propose a training-free method for selecting discriminative\ndescriptions that work independently of classname-ensembling effects. Our\napproach identifies descriptions that effectively differentiate classes within\na local CLIP label neighborhood, improving classification accuracy across seven\ndatasets. Additionally, we provide insights into the explainability of\ndescription-based image classification with VLMs.\n","authors":["Pingchuan Ma","Lennart Rietdorf","Dmytro Kotovenko","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2412.11917v3.pdf","comment":"AAAI-25 (extended version), Code: https://github.com/CompVis/DisCLIP"},{"id":"http://arxiv.org/abs/2412.15106v1","updated":"2024-12-19T17:51:49Z","published":"2024-12-19T17:51:49Z","title":"Knowing Where to Focus: Attention-Guided Alignment for Text-based Person\n  Search","summary":"  In the realm of Text-Based Person Search (TBPS), mainstream methods aim to\nexplore more efficient interaction frameworks between text descriptions and\nvisual data. However, recent approaches encounter two principal challenges.\nFirstly, the widely used random-based Masked Language Modeling (MLM) considers\nall the words in the text equally during training. However, massive\nsemantically vacuous words ('with', 'the', etc.) be masked fail to contribute\nefficient interaction in the cross-modal MLM and hampers the representation\nalignment. Secondly, manual descriptions in TBPS datasets are tedious and\ninevitably contain several inaccuracies. To address these issues, we introduce\nan Attention-Guided Alignment (AGA) framework featuring two innovative\ncomponents: Attention-Guided Mask (AGM) Modeling and Text Enrichment Module\n(TEM). AGM dynamically masks semantically meaningful words by aggregating the\nattention weight derived from the text encoding process, thereby cross-modal\nMLM can capture information related to the masked word from text context and\nimages and align their representations. Meanwhile, TEM alleviates low-quality\nrepresentations caused by repetitive and erroneous text descriptions by\nreplacing those semantically meaningful words with MLM's prediction. It not\nonly enriches text descriptions but also prevents overfitting. Extensive\nexperiments across three challenging benchmarks demonstrate the effectiveness\nof our AGA, achieving new state-of-the-art results with Rank-1 accuracy\nreaching 78.36%, 67.31%, and 67.4% on CUHK-PEDES, ICFG-PEDES, and RSTPReid,\nrespectively.\n","authors":["Lei Tan","Weihao Li","Pingyang Dai","Jie Chen","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2412.15106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13788v2","updated":"2024-12-19T17:51:42Z","published":"2024-03-20T17:51:53Z","title":"DepthFM: Fast Monocular Depth Estimation with Flow Matching","summary":"  Current discriminative depth estimation methods often produce blurry\nartifacts, while generative approaches suffer from slow sampling due to\ncurvatures in the noise-to-depth transport. Our method addresses these\nchallenges by framing depth estimation as a direct transport between image and\ndepth distributions. We are the first to explore flow matching in this field,\nand we demonstrate that its interpolation trajectories enhance both training\nand sampling efficiency while preserving high performance. While generative\nmodels typically require extensive training data, we mitigate this dependency\nby integrating external knowledge from a pre-trained image diffusion model,\nenabling effective transfer even across differing objectives. To further boost\nour model performance, we employ synthetic data and utilize image-depth pairs\ngenerated by a discriminative model on an in-the-wild image dataset. As a\ngenerative model, our model can reliably estimate depth confidence, which\nprovides an additional advantage. Our approach achieves competitive zero-shot\nperformance on standard benchmarks of complex natural scenes while improving\nsampling efficiency and only requiring minimal synthetic data for training.\n","authors":["Ming Gui","Johannes Schusterbauer","Ulrich Prestel","Pingchuan Ma","Dmytro Kotovenko","Olga Grebenkova","Stefan Andreas Baumann","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.13788v2.pdf","comment":"AAAI 2025, Project Page: https://github.com/CompVis/depth-fm"},{"id":"http://arxiv.org/abs/2412.15095v1","updated":"2024-12-19T17:45:08Z","published":"2024-12-19T17:45:08Z","title":"A Full Transformer-based Framework for Automatic Pain Estimation using\n  Videos","summary":"  The automatic estimation of pain is essential in designing an optimal pain\nmanagement system offering reliable assessment and reducing the suffering of\npatients. In this study, we present a novel full transformer-based framework\nconsisting of a Transformer in Transformer (TNT) model and a Transformer\nleveraging cross-attention and self-attention blocks. Elaborating on videos\nfrom the BioVid database, we demonstrate state-of-the-art performances, showing\nthe efficacy, efficiency, and generalization capability across all the primary\npain estimation tasks.\n","authors":["Stefanos Gkikas","Manolis Tsiknakis"],"pdf_url":"https://arxiv.org/pdf/2412.15095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15077v1","updated":"2024-12-19T17:26:07Z","published":"2024-12-19T17:26:07Z","title":"Till the Layers Collapse: Compressing a Deep Neural Network through the\n  Lenses of Batch Normalization Layers","summary":"  Today, deep neural networks are widely used since they can handle a variety\nof complex tasks. Their generality makes them very powerful tools in modern\ntechnology. However, deep neural networks are often overparameterized. The\nusage of these large models consumes a lot of computation resources. In this\npaper, we introduce a method called \\textbf{T}ill the \\textbf{L}ayers\n\\textbf{C}ollapse (TLC), which compresses deep neural networks through the\nlenses of batch normalization layers. By reducing the depth of these networks,\nour method decreases deep neural networks' computational requirements and\noverall latency. We validate our method on popular models such as Swin-T,\nMobileNet-V2, and RoBERTa, across both image classification and natural\nlanguage processing (NLP) tasks.\n","authors":["Zhu Liao","Nour Hezbri","Victor Quétu","Van-Tam Nguyen","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2412.15077v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.15058v1","updated":"2024-12-19T17:06:53Z","published":"2024-12-19T17:06:53Z","title":"MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging\n  Datasets with In-Context Guidance","summary":"  Medical researchers and clinicians often need to perform novel segmentation\ntasks on a set of related images. Existing methods for segmenting a new dataset\nare either interactive, requiring substantial human effort for each image, or\nrequire an existing set of manually labeled images. We introduce a system,\nMultiverSeg, that enables practitioners to rapidly segment an entire new\ndataset without requiring access to any existing labeled data from that task or\ndomain. Along with the image to segment, the model takes user interactions such\nas clicks, bounding boxes or scribbles as input, and predicts a segmentation.\nAs the user segments more images, those images and segmentations become\nadditional inputs to the model, providing context. As the context set of\nlabeled images grows, the number of interactions required to segment each new\nimage decreases. We demonstrate that MultiverSeg enables users to interactively\nsegment new datasets efficiently, by amortizing the number of interactions per\nimage to achieve an accurate segmentation. Compared to using a state-of-the-art\ninteractive segmentation method, using MultiverSeg reduced the total number of\nscribble steps by 53% and clicks by 36% to achieve 90% Dice on sets of images\nfrom unseen tasks. We release code and model weights at\nhttps://multiverseg.csail.mit.edu\n","authors":["Hallee E. Wong","Jose Javier Gonzalez Ortiz","John Guttag","Adrian V. Dalca"],"pdf_url":"https://arxiv.org/pdf/2412.15058v1.pdf","comment":"Project Website: https://multiverseg.csail.mit.edu Keywords:\n  interactive segmentation, in-context learning, medical image analysis,\n  biomedical imaging, image annotation, visual prompting"},{"id":"http://arxiv.org/abs/2412.15054v1","updated":"2024-12-19T17:02:03Z","published":"2024-12-19T17:02:03Z","title":"GIRAFE: Glottal Imaging Dataset for Advanced Segmentation, Analysis, and\n  Facilitative Playbacks Evaluation","summary":"  The advances in the development of Facilitative Playbacks extracted from\nHigh-Speed videoendoscopic sequences of the vocal folds are hindered by a\nnotable lack of publicly available datasets annotated with the semantic\nsegmentations corresponding to the area of the glottal gap. This fact also\nlimits the reproducibility and further exploration of existing research in this\nfield.\n  To address this gap, GIRAFE is a data repository designed to facilitate the\ndevelopment of advanced techniques for the semantic segmentation, analysis, and\nfast evaluation of High-Speed videoendoscopic sequences of the vocal folds. The\nrepository includes 65 high-speed videoendoscopic recordings from a cohort of\n50 patients (30 female, 20 male). The dataset comprises 15 recordings from\nhealthy controls, 26 from patients with diagnosed voice disorders, and 24 with\nan unknown health condition. All of them were manually annotated by an expert,\nincluding the masks corresponding to the semantic segmentation of the glottal\ngap. The repository is also complemented with the automatic segmentation of the\nglottal area using different state-of-the-art approaches.\n  This data set has already supported several studies, which demonstrates its\nusefulness for the development of new glottal gap segmentation algorithms from\nHigh-Speed-Videoendoscopic sequences to improve or create new Facilitative\nPlaybacks. Despite these advances and others in the field, the broader\nchallenge of performing an accurate and completely automatic semantic\nsegmentation method of the glottal area remains open.\n","authors":["G. Andrade-Miranda","K. Chatzipapas","J. D. Arias-Londoño","J. I. Godino-Llorente"],"pdf_url":"https://arxiv.org/pdf/2412.15054v1.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.15050v1","updated":"2024-12-19T16:57:45Z","published":"2024-12-19T16:57:45Z","title":"Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream\n  Diffusion","summary":"  Rendering and inverse rendering are pivotal tasks in both computer vision and\ngraphics. The rendering equation is the core of the two tasks, as an ideal\nconditional distribution transfer function from intrinsic properties to RGB\nimages. Despite achieving promising results of existing rendering methods, they\nmerely approximate the ideal estimation for a specific scene and come with a\nhigh computational cost. Additionally, the inverse conditional distribution\ntransfer is intractable due to the inherent ambiguity. To address these\nchallenges, we propose a data-driven method that jointly models rendering and\ninverse rendering as two conditional generation tasks within a single diffusion\nframework. Inspired by UniDiffuser, we utilize two distinct time schedules to\nmodel both tasks, and with a tailored dual streaming module, we achieve\ncross-conditioning of two pre-trained diffusion models. This unified approach,\nnamed Uni-Renderer, allows the two processes to facilitate each other through a\ncycle-consistent constrain, mitigating ambiguity by enforcing consistency\nbetween intrinsic properties and rendered images. Combined with a meticulously\nprepared dataset, our method effectively decomposition of intrinsic properties\nand demonstrates a strong capability to recognize changes during rendering. We\nwill open-source our training and inference code to the public, fostering\nfurther research and development in this area.\n","authors":["Zhifei Chen","Tianshuo Xu","Wenhang Ge","Leyi Wu","Dongyu Yan","Jing He","Luozhou Wang","Lu Zeng","Shunsi Zhang","Yingcong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.15050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.03767v2","updated":"2024-12-19T16:45:52Z","published":"2023-01-10T03:10:32Z","title":"Metric Compatible Training for Online Backfilling in Large-Scale\n  Retrieval","summary":"  Backfilling is the process of re-extracting all gallery embeddings from\nupgraded models in image retrieval systems. It inevitably requires a\nprohibitively large amount of computational cost and even entails the downtime\nof the service. Although backward-compatible learning sidesteps this challenge\nby tackling query-side representations, this leads to suboptimal solutions in\nprinciple because gallery embeddings cannot benefit from model upgrades. We\naddress this dilemma by introducing an online backfilling algorithm, which\nenables us to achieve a progressive performance improvement during the\nbackfilling process while not sacrificing the final performance of new model\nafter the completion of backfilling. To this end, we first propose a simple\ndistance rank merge technique for online backfilling. Then, we incorporate a\nreverse transformation module for more effective and efficient merging, which\nis further enhanced by adopting a metric-compatible contrastive learning\napproach. These two components help to make the distances of old and new models\ncompatible, resulting in desirable merge results during backfilling with no\nextra computational overhead. Extensive experiments show the effectiveness of\nour framework on four standard benchmarks in various settings.\n","authors":["Seonguk Seo","Mustafa Gokhan Uzunbas","Bohyung Han","Sara Cao","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2301.03767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15032v1","updated":"2024-12-19T16:44:01Z","published":"2024-12-19T16:44:01Z","title":"DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT\n  Space","summary":"  This paper explores image modeling from the frequency space and introduces\nDCTdiff, an end-to-end diffusion generative paradigm that efficiently models\nimages in the discrete cosine transform (DCT) space. We investigate the design\nspace of DCTdiff and reveal the key design factors. Experiments on different\nframeworks (UViT, DiT), generation tasks, and various diffusion samplers\ndemonstrate that DCTdiff outperforms pixel-based diffusion models regarding\ngenerative quality and training efficiency. Remarkably, DCTdiff can seamlessly\nscale up to high-resolution generation without using the latent diffusion\nparadigm. Finally, we illustrate several intriguing properties of DCT image\nmodeling. For example, we provide a theoretical proof of why `image diffusion\ncan be seen as spectral autoregression', bridging the gap between diffusion and\nautoregressive models. The effectiveness of DCTdiff and the introduced\nproperties suggest a promising direction for image modeling in the frequency\nspace. The code is at \\url{https://github.com/forever208/DCTdiff}.\n","authors":["Mang Ning","Mingxiao Li","Jianlin Su","Haozhe Jia","Lanmiao Liu","Martin Beneš","Albert Ali Salah","Itir Onal Ertugrul"],"pdf_url":"https://arxiv.org/pdf/2412.15032v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2302.10634v2","updated":"2024-12-19T16:41:57Z","published":"2023-02-21T12:48:44Z","title":"A Deep Learning-Based and Fully Automated Pipeline for Regurgitant\n  Mitral Valve Anatomy Analysis from 3D Echocardiography","summary":"  3D transesophageal echocardiography (3DTEE), is the recommended method for\ndiagnosing mitral regurgitation (MR). 3DTEE provides a high-quality 3D image of\nthe mitral valve (MV), allowing for precise segmentation and measurement of the\nregurgitant valve anatomy. However, manual TEE segmentations are time-consuming\nand prone to intra-operator variability, affecting the reliability of the\nmeasurements. To address this, we developed a fully automated pipeline using a\n3D convolutional neural network (CNN) to segment MV substructures (annulus,\nanterior leaflet, and posterior leaflet) and quantify MV anatomy. The 3D CNN,\nbased on a multi-decoder residual U-Net architecture, was trained and tested on\na dataset comprising 100 3DTEE images with corresponding segmentations. Within\nthe pipeline, a custom algorithm refines the CNN-based segmentations and\nextracts MV models, from which anatomical landmarks and features are\nquantified. The accuracy of the proposed method was assessed using Dice score\nand mean surface distance (MSD) against ground truth segmentations, and the\nextracted anatomical parameters were compared against a semiautomated\ncommercial software TomTec Image Arena. The trained 3D CNN achieved an average\nDice score of 0.79 and MSD of 0.47 mm for the combined segmentation of the\nannulus, anterior and posterior leaflet. The proposed CNN architecture\noutperformed a baseline residual U-Net architecture in MV substructure\nsegmentation, and the refinement of the predicted annulus segmentation improved\nMSD by 8.36%. The annular and leaflet linear measurements differed by less than\n7.94 mm and 3.67 mm, respectively, compared to the 3D measurements obtained\nwith TomTec Image Arena. The proposed pipeline was faster than the commercial\nsoftware, with a modeling time of 12.54 s and a quantification time of 54.42 s.\n","authors":["Riccardo Munafò","Simone Saitta","Giacomo Ingallina","Paolo Denti","Francesco Maisano","Eustachio Agricola","Alberto Redaelli","Emiliano Votta"],"pdf_url":"https://arxiv.org/pdf/2302.10634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15023v1","updated":"2024-12-19T16:37:19Z","published":"2024-12-19T16:37:19Z","title":"Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and\n  Semantic Controls","summary":"  Sound designers and Foley artists usually sonorize a scene, such as from a\nmovie or video game, by manually annotating and sonorizing each action of\ninterest in the video. In our case, the intent is to leave full creative\ncontrol to sound designers with a tool that allows them to bypass the more\nrepetitive parts of their work, thus being able to focus on the creative\naspects of sound production. We achieve this presenting Stable-V2A, a two-stage\nmodel consisting of: an RMS-Mapper that estimates an envelope representative of\nthe audio characteristics associated with the input video; and Stable-Foley, a\ndiffusion model based on Stable Audio Open that generates audio semantically\nand temporally aligned with the target video. Temporal alignment is guaranteed\nby the use of the envelope as a ControlNet input, while semantic alignment is\nachieved through the use of sound representations chosen by the designer as\ncross-attention conditioning of the diffusion process. We train and test our\nmodel on Greatest Hits, a dataset commonly used to evaluate V2A models. In\naddition, to test our model on a case study of interest, we introduce Walking\nThe Maps, a dataset of videos extracted from video games depicting animated\ncharacters walking in different locations. Samples and code available on our\ndemo page at https://ispamm.github.io/Stable-V2A.\n","authors":["Riccardo Fosco Gramaccioni","Christian Marinoni","Emilian Postolache","Marco Comunità","Luca Cosmo","Joshua D. Reiss","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.15023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15010v1","updated":"2024-12-19T16:22:37Z","published":"2024-12-19T16:22:37Z","title":"Robust Federated Learning in the Face of Covariate Shift: A Magnitude\n  Pruning with Hybrid Regularization Framework for Enhanced Model Aggregation","summary":"  The development of highly sophisticated neural networks has allowed for fast\nprogress in every field of computer vision, however, applications where\nannotated data is prohibited due to privacy or security concerns remain\nchallenging. Federated Learning (FL) offers a promising framework for\nindividuals aiming to collaboratively develop a shared model while preserving\ndata privacy. Nevertheless, our findings reveal that variations in data\ndistribution among clients can profoundly affect FL methodologies, primarily\ndue to instabilities in the aggregation process. We also propose a novel FL\nframework to mitigate the adverse effects of covariate shifts among federated\nclients by combining individual parameter pruning and regularization techniques\nto improve the robustness of individual clients' models to aggregate. Each\nclient's model is optimized through magnitude-based pruning and the addition of\ndropout and noise injection layers to build more resilient decision pathways in\nthe networks and improve the robustness of the model's parameter aggregation\nstep. The proposed framework is capable of extracting robust representations\neven in the presence of very large covariate shifts among client data\ndistributions and in the federation of a small number of clients. Empirical\nfindings substantiate the effectiveness of our proposed methodology across\ncommon benchmark datasets, including CIFAR10, MNIST, SVHN, and Fashion MNIST.\nFurthermore, we introduce the CelebA-Gender dataset, specifically designed to\nevaluate performance on a more realistic domain. The proposed method is capable\nof extracting robust representations even in the presence of both high and low\ncovariate shifts among client data distributions.\n","authors":["Ozgu Goksu","Nicolas Pugeault"],"pdf_url":"https://arxiv.org/pdf/2412.15010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14988v1","updated":"2024-12-19T16:00:10Z","published":"2024-12-19T16:00:10Z","title":"Stitch Contrast and Segment_Learning a Human Action Segmentation Model\n  Using Trimmed Skeleton Videos","summary":"  Existing skeleton-based human action classification models rely on\nwell-trimmed action-specific skeleton videos for both training and testing,\nprecluding their scalability to real-world applications where untrimmed videos\nexhibiting concatenated actions are predominant. To overcome this limitation,\nrecently introduced skeleton action segmentation models involve un-trimmed\nskeleton videos into end-to-end training. The model is optimized to provide\nframe-wise predictions for any length of testing videos, simultaneously\nrealizing action localization and classification. Yet, achieving such an\nimprovement im-poses frame-wise annotated skeleton videos, which remains\ntime-consuming in practice. This paper features a novel framework for\nskeleton-based action segmentation trained on short trimmed skeleton videos,\nbut that can run on longer un-trimmed videos. The approach is implemented in\nthree steps: Stitch, Contrast, and Segment. First, Stitch proposes a tem-poral\nskeleton stitching scheme that treats trimmed skeleton videos as elementary\nhuman motions that compose a semantic space and can be sampled to generate\nmulti-action stitched se-quences. Contrast learns contrastive representations\nfrom stitched sequences with a novel discrimination pretext task that enables a\nskeleton encoder to learn meaningful action-temporal contexts to improve action\nsegmentation. Finally, Segment relates the proposed method to action\nsegmentation by learning a segmentation layer while handling particular da-ta\navailability. Experiments involve a trimmed source dataset and an untrimmed\ntarget dataset in an adaptation formulation for real-world skeleton-based human\naction segmentation to evaluate the effectiveness of the proposed method.\n","authors":["Haitao Tian","Pierre Payeur"],"pdf_url":"https://arxiv.org/pdf/2412.14988v1.pdf","comment":"Accepted as AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08941v3","updated":"2024-12-19T15:59:19Z","published":"2024-12-12T05:08:05Z","title":"Optimized Gradient Clipping for Noisy Label Learning","summary":"  Previous research has shown that constraining the gradient of loss function\nwith respect to model-predicted probabilities can enhance the model robustness\nagainst noisy labels. These methods typically specify a fixed optimal threshold\nfor gradient clipping through validation data to obtain the desired robustness\nagainst noise. However, this common practice overlooks the dynamic distribution\nof gradients from both clean and noisy-labeled samples at different stages of\ntraining, significantly limiting the model capability to adapt to the variable\nnature of gradients throughout the training process. To address this issue, we\npropose a simple yet effective approach called Optimized Gradient Clipping\n(OGC), which dynamically adjusts the clipping threshold based on the ratio of\nnoise gradients to clean gradients after clipping, estimated by modeling the\ndistributions of clean and noisy samples. This approach allows us to modify the\nclipping threshold at each training step, effectively controlling the influence\nof noise gradients. Additionally, we provide statistical analysis to certify\nthe noise-tolerance ability of OGC. Our extensive experiments across various\ntypes of label noise, including symmetric, asymmetric, instance-dependent, and\nreal-world noise, demonstrate the effectiveness of our approach.\n","authors":["Xichen Ye","Yifan Wu","Weizhong Zhang","Xiaoqiang Li","Yifan Chen","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2412.08941v3.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14974v1","updated":"2024-12-19T15:48:51Z","published":"2024-12-19T15:48:51Z","title":"Arti-PG: A Toolbox for Procedurally Synthesizing Large-Scale and Diverse\n  Articulated Objects with Rich Annotations","summary":"  The acquisition of substantial volumes of 3D articulated object data is\nexpensive and time-consuming, and consequently the scarcity of 3D articulated\nobject data becomes an obstacle for deep learning methods to achieve remarkable\nperformance in various articulated object understanding tasks. Meanwhile,\npairing these object data with detailed annotations to enable training for\nvarious tasks is also difficult and labor-intensive to achieve. In order to\nexpeditiously gather a significant number of 3D articulated objects with\ncomprehensive and detailed annotations for training, we propose Articulated\nObject Procedural Generation toolbox, a.k.a. Arti-PG toolbox. Arti-PG toolbox\nconsists of i) descriptions of articulated objects by means of a generalized\nstructure program along with their analytic correspondence to the objects'\npoint cloud, ii) procedural rules about manipulations on the structure program\nto synthesize large-scale and diverse new articulated objects, and iii)\nmathematical descriptions of knowledge (e.g. affordance, semantics, etc.) to\nprovide annotations to the synthesized object. Arti-PG has two appealing\nproperties for providing training data for articulated object understanding\ntasks: i) objects are created with unlimited variations in shape through\nprogram-oriented structure manipulation, ii) Arti-PG is widely applicable to\ndiverse tasks by easily providing comprehensive and detailed annotations.\nArti-PG now supports the procedural generation of 26 categories of articulate\nobjects and provides annotations across a wide range of both vision and\nmanipulation tasks, and we provide exhaustive experiments which fully\ndemonstrate its advantages. We will make Arti-PG toolbox publicly available for\nthe community to use.\n","authors":["Jianhua Sun","Yuxuan Li","Jiude Wei","Longfei Xu","Nange Wang","Yining Zhang","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2412.14974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14969v1","updated":"2024-12-19T15:47:31Z","published":"2024-12-19T15:47:31Z","title":"PhotoHolmes: a Python library for forgery detection in digital images","summary":"  In this paper, we introduce PhotoHolmes, an open-source Python library\ndesigned to easily run and benchmark forgery detection methods on digital\nimages. The library includes implementations of popular and state-of-the-art\nmethods, dataset integration tools, and evaluation metrics. Utilizing the\nBenchmark tool in PhotoHolmes, users can effortlessly compare various methods.\nThis facilitates an accurate and reproducible comparison between their own\nmethods and those in the existing literature. Furthermore, PhotoHolmes includes\na command-line interface (CLI) to easily run the methods implemented in the\nlibrary on any suspicious image. As such, image forgery methods become more\naccessible to the community. The library has been built with extensibility and\nmodularity in mind, which makes adding new methods, datasets and metrics to the\nlibrary a straightforward process. The source code is available at\nhttps://github.com/photoholmes/photoholmes.\n","authors":["Julián O'Flaherty","Rodrigo Paganini","Juan Pablo Sotelo","Julieta Umpiérrez","Marina Gardella","Matías Tailanian","Pablo Musé"],"pdf_url":"https://arxiv.org/pdf/2412.14969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14965v1","updated":"2024-12-19T15:44:04Z","published":"2024-12-19T15:44:04Z","title":"Movie2Story: A framework for understanding videos and telling stories in\n  the form of novel text","summary":"  Multimodal video-to-text models have made considerable progress, primarily in\ngenerating brief descriptions of video content. However, there is still a\ndeficiency in generating rich long-form text descriptions that integrate both\nvideo and audio. In this paper, we introduce a framework called M2S, designed\nto generate novel-length text by combining audio, video, and character\nrecognition. M2S includes modules for video long-form text description and\ncomprehension, audio-based analysis of emotion, speech rate, and character\nalignment, and visual-based character recognition alignment. By integrating\nmultimodal information using the large language model GPT4o, M2S stands out in\nthe field of multimodal text generation. We demonstrate the effectiveness and\naccuracy of M2S through comparative experiments and human evaluation.\nAdditionally, the model framework has good scalability and significant\npotential for future research.\n","authors":["Kangning Li","Zheyang Jia","Anyu Ying"],"pdf_url":"https://arxiv.org/pdf/2412.14965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14963v1","updated":"2024-12-19T15:43:05Z","published":"2024-12-19T15:43:05Z","title":"IDOL: Instant Photorealistic 3D Human Creation from a Single Image","summary":"  Creating a high-fidelity, animatable 3D full-body avatar from a single image\nis a challenging task due to the diverse appearance and poses of humans and the\nlimited availability of high-quality training data. To achieve fast and\nhigh-quality human reconstruction, this work rethinks the task from the\nperspectives of dataset, model, and representation. First, we introduce a\nlarge-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K\ndiverse, photorealistic sets of human images. Each set contains 24-view frames\nin specific human poses, generated using a pose-controllable\nimage-to-multi-view model. Next, leveraging the diversity in views, poses, and\nappearances within HuGe100K, we develop a scalable feed-forward transformer\nmodel to predict a 3D human Gaussian representation in a uniform space from a\ngiven human image. This model is trained to disentangle human pose, body shape,\nclothing geometry, and texture. The estimated Gaussians can be animated without\npost-processing. We conduct comprehensive experiments to validate the\neffectiveness of the proposed dataset and method. Our model demonstrates the\nability to efficiently reconstruct photorealistic humans at 1K resolution from\na single input image using a single GPU instantly. Additionally, it seamlessly\nsupports various applications, as well as shape and texture editing tasks.\n","authors":["Yiyu Zhuang","Jiaxi Lv","Hao Wen","Qing Shuai","Ailing Zeng","Hao Zhu","Shifeng Chen","Yujiu Yang","Xun Cao","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14963v1.pdf","comment":"21 pages, 15 figures, includes main content, supplementary materials,\n  and references"},{"id":"http://arxiv.org/abs/2412.14961v1","updated":"2024-12-19T15:42:21Z","published":"2024-12-19T15:42:21Z","title":"TDCNet: Transparent Objects Depth Completion with CNN-Transformer\n  Dual-Branch Parallel Network","summary":"  The sensing and manipulation of transparent objects present a critical\nchallenge in industrial and laboratory robotics. Conventional sensors face\nchallenges in obtaining the full depth of transparent objects due to the\nrefraction and reflection of light on their surfaces and their lack of visible\ntexture. Previous research has attempted to obtain complete depth maps of\ntransparent objects from RGB and damaged depth maps (collected by depth sensor)\nusing deep learning models. However, existing methods fail to fully utilize the\noriginal depth map, resulting in limited accuracy for deep completion. To solve\nthis problem, we propose TDCNet, a novel dual-branch CNN-Transformer parallel\nnetwork for transparent object depth completion. The proposed framework\nconsists of two different branches: one extracts features from partial depth\nmaps, while the other processes RGB-D images. Experimental results demonstrate\nthat our model achieves state-of-the-art performance across multiple public\ndatasets. Our code and the pre-trained model are publicly available at\nhttps://github.com/XianghuiFan/TDCNet.\n","authors":["Xianghui Fan","Chao Ye","Anping Deng","Xiaotian Wu","Mengyang Pan","Hang Yang"],"pdf_url":"https://arxiv.org/pdf/2412.14961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14957v1","updated":"2024-12-19T15:38:15Z","published":"2024-12-19T15:38:15Z","title":"Dream to Manipulate: Compositional World Models Empowering Robot\n  Imitation Learning with Imagination","summary":"  A world model provides an agent with a representation of its environment,\nenabling it to predict the causal consequences of its actions. Current world\nmodels typically cannot directly and explicitly imitate the actual environment\nin front of a robot, often resulting in unrealistic behaviors and\nhallucinations that make them unsuitable for real-world applications. In this\npaper, we introduce a new paradigm for constructing world models that are\nexplicit representations of the real world and its dynamics. By integrating\ncutting-edge advances in real-time photorealism with Gaussian Splatting and\nphysics simulators, we propose the first compositional manipulation world\nmodel, which we call DreMa. DreMa replicates the observed world and its\ndynamics, allowing it to imagine novel configurations of objects and predict\nthe future consequences of robot actions. We leverage this capability to\ngenerate new data for imitation learning by applying equivariant\ntransformations to a small set of demonstrations. Our evaluations across\nvarious settings demonstrate significant improvements in both accuracy and\nrobustness by incrementing actions and object distributions, reducing the data\nneeded to learn a policy and improving the generalization of the agents. As a\nhighlight, we show that a real Franka Emika Panda robot, powered by DreMa's\nimagination, can successfully learn novel physical tasks from just a single\nexample per task variation (one-shot policy learning). Our project page and\nsource code can be found in https://leobarcellona.github.io/DreamToManipulate/\n","authors":["Leonardo Barcellona","Andrii Zadaianchuk","Davide Allegro","Samuele Papa","Stefano Ghidoni","Efstratios Gavves"],"pdf_url":"https://arxiv.org/pdf/2412.14957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13647v2","updated":"2024-12-19T15:37:55Z","published":"2024-12-18T09:23:12Z","title":"G-VEval: A Versatile Metric for Evaluating Image and Video Captions\n  Using GPT-4o","summary":"  Evaluation metric of visual captioning is important yet not thoroughly\nexplored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss\nsemantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are\nlimited in zero-shot scenarios. Advanced Language Model-based metrics also\nstruggle with aligning to nuanced human preferences. To address these issues,\nwe introduce G-VEval, a novel metric inspired by G-Eval and powered by the new\nGPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and\nsupports three modes: reference-free, reference-only, and combined,\naccommodating both video and image inputs. We also propose MSVD-Eval, a new\ndataset for video captioning evaluation, to establish a more transparent and\nconsistent framework for both human experts and evaluation metrics. It is\ndesigned to address the lack of clear criteria in existing datasets by\nintroducing distinct dimensions of Accuracy, Completeness, Conciseness, and\nRelevance (ACCR). Extensive results show that G-VEval outperforms existing\nmethods in correlation with human annotations, as measured by Kendall tau-b and\nKendall tau-c. This provides a flexible solution for diverse captioning tasks\nand suggests a straightforward yet effective approach for large language models\nto understand video content, paving the way for advancements in automated\ncaptioning. Codes are available at https://github.com/ztangaj/gveval\n","authors":["Tony Cheng Tong","Sirui He","Zhiwen Shao","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2412.13647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14954v1","updated":"2024-12-19T15:36:30Z","published":"2024-12-19T15:36:30Z","title":"Corn Ear Detection and Orientation Estimation Using Deep Learning","summary":"  Monitoring growth behavior of maize plants such as the development of ears\ncan give key insights into the plant's health and development. Traditionally,\nthe measurement of the angle of ears is performed manually, which can be\ntime-consuming and prone to human error. To address these challenges, this\npaper presents a computer vision-based system for detecting and tracking ears\nof corn in an image sequence. The proposed system could accurately detect,\ntrack, and predict the ear's orientation, which can be useful in monitoring\ntheir growth behavior. This can significantly save time compared to manual\nmeasurement and enables additional areas of ear orientation research and\npotential increase in efficiencies for maize production. Using an object\ndetector with keypoint detection, the algorithm proposed could detect 90\npercent of all ears. The cardinal estimation had a mean absolute error (MAE) of\n18 degrees, compared to a mean 15 degree difference between two people\nmeasuring by hand. These results demonstrate the feasibility of using computer\nvision techniques for monitoring maize growth and can lead to further research\nin this area.\n","authors":["Nathan Sprague","John Evans","Michael Mardikes"],"pdf_url":"https://arxiv.org/pdf/2412.14954v1.pdf","comment":"22 pages;15 figures"},{"id":"http://arxiv.org/abs/2406.16710v2","updated":"2024-12-19T15:28:26Z","published":"2024-06-24T15:11:35Z","title":"ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait\n  Image","summary":"  While recent works have achieved great success on image-to-3D object\ngeneration, high quality and fidelity 3D head generation from a single image\nremains a great challenge. Previous text-based methods for generating 3D heads\nwere limited by text descriptions and image-based methods struggled to produce\nhigh-quality head geometry. To handle this challenging problem, we propose a\nnovel framework, ID-Sculpt, to generate high-quality 3D heads while preserving\ntheir identities. Our work incorporates the identity information of the\nportrait image into three parts: 1) geometry initialization, 2) geometry\nsculpting, and 3) texture generation stages. Given a reference portrait image,\nwe first align the identity features with text features to realize ID-aware\nguidance enhancement, which contains the control signals representing the face\ninformation. We then use the canny map, ID features of the portrait image, and\na pre-trained text-to-normal/depth diffusion model to generate ID-aware\ngeometry supervision, and 3D-GAN inversion is employed to generate ID-aware\ngeometry initialization. Furthermore, with the ability to inject identity\ninformation into 3D head generation, we use ID-aware guidance to calculate\nID-aware Score Distillation (ISD) for geometry sculpting. For texture\ngeneration, we adopt the ID Consistent Texture Inpainting and Refinement which\nprogressively expands the view for texture inpainting to obtain an\ninitialization UV texture map. We then use the ID-aware guidance to provide\nimage-level supervision for noisy multi-view images to obtain a refined texture\nmap. Extensive experiments demonstrate that we can generate high-quality 3D\nheads with accurate geometry and texture from a single in-the-wild portrait\nimage.\n","authors":["Jinkun Hao","Junshu Tang","Jiangning Zhang","Ran Yi","Yijia Hong","Moran Li","Weijian Cao","Yating Wang","Chengjie Wang","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2406.16710v2.pdf","comment":"Accepted by AAAI 2025; Project page:\n  https://jinkun-hao.github.io/ID-Sculpt/"},{"id":"http://arxiv.org/abs/2411.10958v2","updated":"2024-12-19T15:26:20Z","published":"2024-11-17T04:35:49Z","title":"SageAttention2: Efficient Attention with Thorough Outlier Smoothing and\n  Per-thread INT4 Quantization","summary":"  Although quantization for linear layers has been widely used, its application\nto accelerate the attention process remains limited. To further enhance the\nefficiency of attention computation compared to SageAttention while maintaining\nprecision, we propose SageAttention2, which utilizes significantly faster 4-bit\nmatrix multiplication (Matmul) alongside additional precision-enhancing\ntechniques. First, we propose to quantize matrixes $(Q, K)$ to INT4 in a\nhardware-friendly thread-level granularity and quantize matrixes $(\\widetilde\nP, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the\naccuracy of INT4 $QK$. Third, we propose to use an FP32 Matmul buffer for $PV$\nto enhance the accuracy of FP8 $\\widetilde PV$. The operations per second (OPS)\nof SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on\nRTX4090, respectively. Comprehensive experiments confirm that our approach\nincurs negligible end-to-end metrics loss across diverse models, including\nthose for large language processing, image generation, and video generation.\nThe codes are available at https://github.com/thu-ml/SageAttention.\n","authors":["Jintao Zhang","Haofeng Huang","Pengle Zhang","Jia Wei","Jun Zhu","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2411.10958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14939v1","updated":"2024-12-19T15:15:03Z","published":"2024-12-19T15:15:03Z","title":"GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface\n  Reconstruction","summary":"  Neural surface representation has demonstrated remarkable success in the\nareas of novel view synthesis and 3D reconstruction. However, assessing the\ngeometric quality of 3D reconstructions in the absence of ground truth mesh\nremains a significant challenge, due to its rendering-based optimization\nprocess and entangled learning of appearance and geometry with photometric\nlosses. In this paper, we present a novel framework, i.e, GURecon, which\nestablishes a geometric uncertainty field for the neural surface based on\ngeometric consistency. Different from existing methods that rely on\nrendering-based measurement, GURecon models a continuous 3D uncertainty field\nfor the reconstructed surface, and is learned by an online distillation\napproach without introducing real geometric information for supervision.\nMoreover, in order to mitigate the interference of illumination on geometric\nconsistency, a decoupled field is learned and exploited to finetune the\nuncertainty field. Experiments on various datasets demonstrate the superiority\nof GURecon in modeling 3D geometric uncertainty, as well as its plug-and-play\nextension to various neural surface representations and improvement on\ndownstream tasks such as incremental reconstruction. The code and supplementary\nmaterial are available on the project website:\nhttps://zju3dv.github.io/GURecon/.\n","authors":["Zesong Yang","Ru Zhang","Jiale Shi","Zixiang Ai","Boming Zhao","Hujun Bao","Luwei Yang","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2412.14939v1.pdf","comment":"Accepted by AAAI 2025. Project page:\n  https://zju3dv.github.io/gurecon/"},{"id":"http://arxiv.org/abs/2302.11947v2","updated":"2024-12-19T15:13:46Z","published":"2023-02-23T11:44:43Z","title":"Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight\n  Convolutional Neural Networks","summary":"  The health and safety hazards posed by worn crane lifting ropes mandate\nperiodic inspection for damage. This task is time-consuming, prone to human\nerror, halts operation, and may result in the premature disposal of ropes.\nTherefore, we propose using efficient deep learning and computer vision methods\nto automate the process of detecting damaged ropes. Specifically, we present a\nvision-based system for detecting damage in synthetic fiber rope images using\nlightweight convolutional neural networks. We develop a camera-based apparatus\nto photograph the lifting rope's surface, while in operation, and capture the\nprogressive wear-and-tear as well as the more significant degradation in the\nrope's health state. Experts from Konecranes annotate the collected images in\naccordance with the rope's condition; normal or damaged. Then, we pre-process\nthe images, systematically design a deep learning model, evaluate its detection\nand prediction performance, analyze its computational complexity, and compare\nit with various other models. Experimental results show the proposed model\noutperforms other similar techniques with 96.5% accuracy, 94.8% precision,\n98.3% recall, 96.5% F1-score, and 99.3% AUC. Besides, they demonstrate the\nmodel's real-time operation, low memory footprint, robustness to various\nenvironmental and operational conditions, and adequacy for deployment in\nindustrial applications such as lifting, mooring, towing, climbing, and\nsailing.\n","authors":["Tuomas Jalonen","Mohammad Al-Sa'd","Roope Mellanen","Serkan Kiranyaz","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2302.11947v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14925v1","updated":"2024-12-19T15:02:50Z","published":"2024-12-19T15:02:50Z","title":"Automatic Spectral Calibration of Hyperspectral Images:Method, Dataset\n  and Benchmark","summary":"  Hyperspectral image (HSI) densely samples the world in both the space and\nfrequency domain and therefore is more distinctive than RGB images. Usually,\nHSI needs to be calibrated to minimize the impact of various illumination\nconditions. The traditional way to calibrate HSI utilizes a physical reference,\nwhich involves manual operations, occlusions, and/or limits camera mobility.\nThese limitations inspire this paper to automatically calibrate HSIs using a\nlearning-based method. Towards this goal, a large-scale HSI calibration dataset\nis created, which has 765 high-quality HSI pairs covering diversified natural\nscenes and illuminations. The dataset is further expanded to 7650 pairs by\ncombining with 10 different physically measured illuminations. A spectral\nillumination transformer (SIT) together with an illumination attention module\nis proposed. Extensive benchmarks demonstrate the SoTA performance of the\nproposed SIT. The benchmarks also indicate that low-light conditions are more\nchallenging than normal conditions. The dataset and codes are available\nonline:https://github.com/duranze/Automatic-spectral-calibration-of-HSI\n","authors":["Zhuoran Du","Shaodi You","Cheng Cheng","Shikui Wei"],"pdf_url":"https://arxiv.org/pdf/2412.14925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04272v2","updated":"2024-12-19T15:02:37Z","published":"2024-09-06T13:28:05Z","title":"Cycle Pixel Difference Network for Crisp Edge Detection","summary":"  Edge detection, as a fundamental task in computer vision, has garnered\nincreasing attention. The advent of deep learning has significantly advanced\nthis field. However, recent deep learning-based methods generally face two\nsignificant issues: 1) reliance on large-scale pre-trained weights, and 2)\ngeneration of thick edges. We construct a U-shape encoder-decoder model named\nCPD-Net that successfully addresses these two issues simultaneously. In\nresponse to issue 1), we propose a novel cycle pixel difference convolution\n(CPDC), which effectively integrates edge prior knowledge with modern\nconvolution operations, consequently successfully eliminating the dependence on\nlarge-scale pre-trained weights. As for issue 2), we construct a multi-scale\ninformation enhancement module (MSEM) and a dual residual connection-based\n(DRC) decoder to enhance the edge location ability of the model, thereby\ngenerating crisp and clean contour maps. Comprehensive experiments conducted on\nfour standard benchmarks demonstrate that our method achieves competitive\nperformance on the BSDS500 dataset (ODS=0.813 and AC=0.352), NYUD-V2 (ODS=0.760\nand AC=0.223), BIPED dataset (ODS=0.898 and AC=0.426), and CID (ODS=0.59). Our\napproach provides a novel perspective for addressing these challenges in edge\ndetection.\n","authors":["Changsong Liu","Wei Zhang","Yanyan Liu","Mingyang Li","Wenlin Li","Yimeng Fan","Xiangnan Bai","Liang Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.04272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16571v4","updated":"2024-12-19T14:47:05Z","published":"2024-04-25T12:34:23Z","title":"MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth\n  Estimation of Endoscopic Images","summary":"  Photometric constraint is indispensable for self-supervised monocular depth\nestimation. It involves warping a source image onto a target view using\nestimated depth&pose, and then minimizing the difference between the warped and\ntarget images. However, the endoscopic built-in light causes significant\nbrightness fluctuations, and thus makes the photometric constraint unreliable.\nPrevious efforts only mitigate this relying on extra models to calibrate image\nbrightness. In this paper, we propose MonoPCC to address the brightness\ninconsistency radically by reshaping the photometric constraint into a cycle\nform. Instead of only warping the source image, MonoPCC constructs a closed\nloop consisting of two opposite forward-backward warping paths: from target to\nsource and then back to target. Thus, the target image finally receives an\nimage cycle-warped from itself, which naturally makes the constraint invariant\nto brightness changes. Moreover, MonoPCC transplants the source image's\nphase-frequency into the intermediate warped image to avoid structure lost, and\nalso stabilizes the training via an exponential moving average (EMA) strategy\nto avoid frequent changes in the forward warping. The comprehensive and\nextensive experimental results on four endoscopic datasets demonstrate that our\nproposed MonoPCC shows a great robustness to the brightness inconsistency, and\nexceeds other state-of-the-arts by reducing the absolute relative error by at\nleast 7.27%, 9.38%, 9.90% and 3.17%, respectively.\n","authors":["Zhiwei Wang","Ying Zhou","Shiquan He","Ting Li","Fan Huang","Qiang Ding","Xinxia Feng","Mei Liu","Qiang Li"],"pdf_url":"https://arxiv.org/pdf/2404.16571v4.pdf","comment":"14 pages, 12 figures"},{"id":"http://arxiv.org/abs/2311.18512v2","updated":"2024-12-19T14:46:05Z","published":"2023-11-30T12:40:23Z","title":"Union-over-Intersections: Object Detection beyond Winner-Takes-All","summary":"  This paper revisits the problem of predicting box locations in object\ndetection architectures. Typically, each box proposal or box query aims to\ndirectly maximize the intersection-over-union score with the ground truth,\nfollowed by a winner-takes-all non-maximum suppression where only the highest\nscoring box in each region is retained. We observe that both steps are\nsub-optimal: the first involves regressing proposals to the entire ground\ntruth, which is a difficult task even with large receptive fields, and the\nsecond neglects valuable information from boxes other than the top candidate.\nInstead of regressing proposals to the whole ground truth, we propose a simpler\napproach: regress only to the area of intersection between the proposal and the\nground truth. This avoids the need for proposals to extrapolate beyond their\nvisual scope, improving localization accuracy. Rather than adopting a\nwinner-takes-all strategy, we take the union over the regressed intersections\nof all boxes in a region to generate the final box outputs. Our plug-and-play\nmethod integrates seamlessly into proposal-based, grid-based, and query-based\ndetection architectures with minimal modifications, consistently improving\nobject localization and instance segmentation. We demonstrate its broad\napplicability and versatility across various detection and segmentation tasks.\n","authors":["Aritra Bhowmik","Pascal Mettes","Martin R. Oswald","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2311.18512v2.pdf","comment":"17 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2312.06259v2","updated":"2024-12-19T14:38:47Z","published":"2023-12-11T09:57:09Z","title":"Point Cloud Semantic Segmentation with Sparse and Inhomogeneous\n  Annotations","summary":"  Utilizing uniformly distributed sparse annotations, weakly supervised\nlearning alleviates the heavy reliance on fine-grained annotations in point\ncloud semantic segmentation tasks. However, few works discuss the inhomogeneity\nof sparse annotations, albeit it is common in real-world scenarios. Therefore,\nthis work introduces the probability density function into the gradient\nsampling approximation method to qualitatively analyze the impact of annotation\nsparsity and inhomogeneity under weakly supervised learning. Based on our\nanalysis, we propose an Adaptive Annotation Distribution Network (AADNet)\ncapable of robust learning on arbitrarily distributed sparse annotations.\nSpecifically, we propose a label-aware point cloud downsampling strategy to\nincrease the proportion of annotations involved in the training stage.\nFurthermore, we design the multiplicative dynamic entropy as the gradient\ncalibration function to mitigate the gradient bias caused by non-uniformly\ndistributed sparse annotations and explicitly reduce the epistemic uncertainty.\nWithout any prior restrictions and additional information, our proposed method\nachieves comprehensive performance improvements at multiple label rates and\ndifferent annotation distributions.\n","authors":["Zhiyi Pan","Nan Zhang","Wei Gao","Shan Liu","Ge Li"],"pdf_url":"https://arxiv.org/pdf/2312.06259v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14902v1","updated":"2024-12-19T14:32:11Z","published":"2024-12-19T14:32:11Z","title":"MagicNaming: Consistent Identity Generation by Finding a \"Name Space\" in\n  T2I Diffusion Models","summary":"  Large-scale text-to-image diffusion models, (e.g., DALL-E, SDXL) are capable\nof generating famous persons by simply referring to their names. Is it possible\nto make such models generate generic identities as simple as the famous ones,\ne.g., just use a name? In this paper, we explore the existence of a \"Name\nSpace\", where any point in the space corresponds to a specific identity.\nFortunately, we find some clues in the feature space spanned by text embedding\nof celebrities' names. Specifically, we first extract the embeddings of\ncelebrities' names in the Laion5B dataset with the text encoder of diffusion\nmodels. Such embeddings are used as supervision to learn an encoder that can\npredict the name (actually an embedding) of a given face image. We\nexperimentally find that such name embeddings work well in promising the\ngenerated image with good identity consistency. Note that like the names of\ncelebrities, our predicted name embeddings are disentangled from the semantics\nof text inputs, making the original generation capability of text-to-image\nmodels well-preserved. Moreover, by simply plugging such name embeddings, all\nvariants (e.g., from Civitai) derived from the same base model (i.e., SDXL)\nreadily become identity-aware text-to-image models. Project homepage:\n\\url{https://magicfusion.github.io/MagicNaming/}.\n","authors":["Jing Zhao","Heliang Zheng","Chaoyue Wang","Long Lan","Wanrong Hunag","Yuhua Tang"],"pdf_url":"https://arxiv.org/pdf/2412.14902v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13099v2","updated":"2024-12-19T14:23:45Z","published":"2024-12-17T17:10:02Z","title":"Accuracy Limits as a Barrier to Biometric System Security","summary":"  Biometric systems are widely used for identity verification and\nidentification, including authentication (i.e., one-to-one matching to verify a\nclaimed identity) and identification (i.e., one-to-many matching to find a\nsubject in a database). The matching process relies on measuring similarities\nor dissimilarities between a fresh biometric template and enrolled templates.\nThe False Match Rate FMR is a key metric for assessing the accuracy and\nreliability of such systems. This paper analyzes biometric systems based on\ntheir FMR, with two main contributions. First, we explore untargeted attacks,\nwhere an adversary aims to impersonate any user within a database. We determine\nthe number of trials required for an attacker to successfully impersonate a\nuser and derive the critical population size (i.e., the maximum number of users\nin the database) required to maintain a given level of security. Furthermore,\nwe compute the critical FMR value needed to ensure resistance against\nuntargeted attacks as the database size increases. Second, we revisit the\nbiometric birthday problem to evaluate the approximate and exact probabilities\nthat two users in a database collide (i.e., can impersonate each other). Based\non this analysis, we derive both the approximate critical population size and\nthe critical FMR value needed to bound the likelihood of such collisions\noccurring with a given probability. These thresholds offer insights for\ndesigning systems that mitigate the risk of impersonation and collisions,\nparticularly in large-scale biometric databases. Our findings indicate that\ncurrent biometric systems fail to deliver sufficient accuracy to achieve an\nadequate security level against untargeted attacks, even in small-scale\ndatabases. Moreover, state-of-the-art systems face significant challenges in\naddressing the biometric birthday problem, especially as database sizes grow.\n","authors":["Axel Durbet","Paul-Marie Grollemund","Pascal Lafourcade","Kevin Thiry-Atighehchi"],"pdf_url":"https://arxiv.org/pdf/2412.13099v2.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.14118v2","updated":"2024-12-19T14:18:57Z","published":"2024-12-18T18:04:12Z","title":"GaraMoSt: Parallel Multi-Granularity Motion and Structural Modeling for\n  Efficient Multi-Frame Interpolation in DSA Images","summary":"  The rapid and accurate direct multi-frame interpolation method for Digital\nSubtraction Angiography (DSA) images is crucial for reducing radiation and\nproviding real-time assistance to physicians for precise diagnostics and\ntreatment. DSA images contain complex vascular structures and various motions.\nApplying natural scene Video Frame Interpolation (VFI) methods results in\nmotion artifacts, structural dissipation, and blurriness. Recently, MoSt-DSA\nhas specifically addressed these issues for the first time and achieved SOTA\nresults. However, MoSt-DSA's focus on real-time performance leads to\ninsufficient suppression of high-frequency noise and incomplete filtering of\nlow-frequency noise in the generated images. To address these issues within the\nsame computational time scale, we propose GaraMoSt. Specifically, we optimize\nthe network pipeline with a parallel design and propose a module named MG-MSFE.\nMG-MSFE extracts frame-relative motion and structural features at various\ngranularities in a fully convolutional parallel manner and supports\nindependent, flexible adjustment of context-aware granularity at different\nscales, thus enhancing computational efficiency and accuracy. Extensive\nexperiments demonstrate that GaraMoSt achieves the SOTA performance in\naccuracy, robustness, visual effects, and noise suppression, comprehensively\nsurpassing MoSt-DSA and other natural scene VFI methods. The code and models\nare available at https://github.com/ZyoungXu/GaraMoSt.\n","authors":["Ziyang Xu","Huangxuan Zhao","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14118v2.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2411.13093v2","updated":"2024-12-19T14:17:13Z","published":"2024-11-20T07:44:34Z","title":"Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension","summary":"  Existing large video-language models (LVLMs) struggle to comprehend long\nvideos correctly due to limited context. To address this problem, fine-tuning\nlong-context LVLMs and employing GPT-based agents have emerged as promising\nsolutions. However, fine-tuning LVLMs would require extensive high-quality data\nand substantial GPU resources, while GPT-based agents would rely on proprietary\nmodels (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented\nGeneration (Video-RAG), a training-free and cost-effective pipeline that\nemploys visually-aligned auxiliary texts to help facilitate cross-modality\nalignment while providing additional information beyond the visual content.\nSpecifically, we leverage open-source external tools to extract\nvisually-aligned information from pure video data (e.g., audio, optical\ncharacter, and object detection), and incorporate the extracted information\ninto an existing LVLM as auxiliary texts, alongside video frames and queries,\nin a plug-and-play manner. Our Video-RAG offers several key advantages: (i)\nlightweight with low computing overhead due to single-turn retrieval; (ii) easy\nimplementation and compatibility with any LVLM; and (iii) significant,\nconsistent performance gains across long video understanding benchmarks,\nincluding Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates\nsuperior performance over proprietary models like Gemini-1.5-Pro and GPT-4o\nwhen utilized with a 72B model.\n","authors":["Yongdong Luo","Xiawu Zheng","Xiao Yang","Guilin Li","Haojia Lin","Jinfa Huang","Jiayi Ji","Fei Chao","Jiebo Luo","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2411.13093v2.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.14880v1","updated":"2024-12-19T14:17:09Z","published":"2024-12-19T14:17:09Z","title":"Multimodal Hypothetical Summary for Retrieval-based Multi-image Question\n  Answering","summary":"  Retrieval-based multi-image question answering (QA) task involves retrieving\nmultiple question-related images and synthesizing these images to generate an\nanswer. Conventional \"retrieve-then-answer\" pipelines often suffer from\ncascading errors because the training objective of QA fails to optimize the\nretrieval stage. To address this issue, we propose a novel method to\neffectively introduce and reference retrieved information into the QA. Given\nthe image set to be retrieved, we employ a multimodal large language model\n(visual perspective) and a large language model (textual perspective) to obtain\nmultimodal hypothetical summary in question-form and description-form. By\ncombining visual and textual perspectives, MHyS captures image content more\nspecifically and replaces real images in retrieval, which eliminates the\nmodality gap by transforming into text-to-text retrieval and helps improve\nretrieval. To more advantageously introduce retrieval with QA, we employ\ncontrastive learning to align queries (questions) with MHyS. Moreover, we\npropose a coarse-to-fine strategy for calculating both sentence-level and\nword-level similarity scores, to further enhance retrieval and filter out\nirrelevant details. Our approach achieves a 3.7% absolute improvement over\nstate-of-the-art methods on RETVQA and a 14.5% improvement over CLIP.\nComprehensive experiments and detailed ablation studies demonstrate the\nsuperiority of our method.\n","authors":["Peize Li","Qingyi Si","Peng Fu","Zheng Lin","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14880v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14873v1","updated":"2024-12-19T14:11:49Z","published":"2024-12-19T14:11:49Z","title":"Zero-Shot Artifact2Artifact: Self-incentive artifact removal for\n  photoacoustic imaging without any data","summary":"  Photoacoustic imaging (PAI) uniquely combines optical contrast with the\npenetration depth of ultrasound, making it critical for clinical applications.\nHowever, the quality of 3D PAI is often degraded due to reconstruction\nartifacts caused by the sparse and angle-limited configuration of detector\narrays. Existing iterative or deep learning-based methods are either\ntime-consuming or require large training datasets, significantly limiting their\npractical application. Here, we propose Zero-Shot Artifact2Artifact (ZS-A2A), a\nzero-shot self-supervised artifact removal method based on a super-lightweight\nnetwork, which leverages the fact that reconstruction artifacts are sensitive\nto irregularities caused by data loss. By introducing random perturbations to\nthe acquired PA data, it spontaneously generates subset data, which in turn\nstimulates the network to learn the artifact patterns in the reconstruction\nresults, thus enabling zero-shot artifact removal. This approach requires\nneither training data nor prior knowledge of the artifacts, and is capable of\nartifact removal for 3D PAI. For maximum amplitude projection (MAP) images or\nslice images in 3D PAI acquired with arbitrarily sparse or angle-limited\ndetector arrays, ZS-A2A employs a self-incentive strategy to complete artifact\nremoval and improves the Contrast-to-Noise Ratio (CNR). We validated ZS-A2A in\nboth simulation study and $ in\\ vivo $ animal experiments. Results demonstrate\nthat ZS-A2A achieves state-of-the-art (SOTA) performance compared to existing\nzero-shot methods, and for the $ in\\ vivo $ rat liver, ZS-A2A improves CNR from\n17.48 to 43.46 in just 8 seconds. The project for ZS-A2A will be available in\nthe following GitHub repository: https://github.com/JaegerCQ/ZS-A2A.\n","authors":["Shuang Li","Qian Chen","Chulhong Kim","Seongwook Choi","Yibing Wang","Yu Zhang","Changhui Li"],"pdf_url":"https://arxiv.org/pdf/2412.14873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.13620v3","updated":"2024-12-19T14:08:42Z","published":"2022-04-28T16:35:04Z","title":"Generative Adversarial Networks for Image Super-Resolution: A Survey","summary":"  Single image super-resolution (SISR) has played an important role in the\nfield of image processing. Recent generative adversarial networks (GANs) can\nachieve excellent results on low-resolution images with small samples. However,\nthere are little literatures summarizing different GANs in SISR. In this paper,\nwe conduct a comparative study of GANs from different perspectives. We first\ntake a look at developments of GANs. Second, we present popular architectures\nfor GANs in big and small samples for image applications. Then, we analyze\nmotivations, implementations and differences of GANs based optimization methods\nand discriminative learning for image super-resolution in terms of supervised,\nsemi-supervised and unsupervised manners, where these GANs are analyzed via\nintegrating different network architectures, prior knowledge, loss functions\nand multiple tasks. Next, we compare performance of these popular GANs on\npublic datasets via quantitative and qualitative analysis in SISR. Finally, we\nhighlight challenges of GANs and potential research points for SISR.\n","authors":["Chunwei Tian","Xuanyu Zhang","Qi Zhu","Bob Zhang","Jerry Chun-Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2204.13620v3.pdf","comment":"31pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.07024v3","updated":"2024-12-19T14:07:44Z","published":"2024-07-09T16:44:04Z","title":"Exploring Scalability of Self-Training for Open-Vocabulary Temporal\n  Action Localization","summary":"  The vocabulary size in temporal action localization (TAL) is limited by the\nscarcity of large-scale annotated datasets. To overcome this, recent works\nintegrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL\n(OV-TAL). However, despite the success of VLMs trained on extensive datasets,\nexisting OV-TAL methods still rely on human-labeled TAL datasets of limited\nsize to train action localizers, limiting their generalizability. In this\npaper, we explore the scalability of self-training with unlabeled YouTube\nvideos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic\naction localizer is trained on a human-labeled TAL dataset to generate\npseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled\ndataset is then used to train the localizer. Extensive experiments demonstrate\nthat leveraging web-scale videos in self-training significantly enhances the\ngeneralizability of an action localizer. Additionally, we identify limitations\nin existing OV-TAL evaluation schemes and propose a new benchmark for thorough\nassessment. Finally, we showcase the TAL performance of the large multimodal\nmodel Gemini-1.5 on our new benchmark. Code is released at\nhttps://github.com/HYUNJS/STOV-TAL.\n","authors":["Jeongseok Hyun","Su Ho Han","Hyolim Kang","Joon-Young Lee","Seon Joo Kim"],"pdf_url":"https://arxiv.org/pdf/2407.07024v3.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2412.14870v1","updated":"2024-12-19T14:06:56Z","published":"2024-12-19T14:06:56Z","title":"Large-scale School Mapping using Weakly Supervised Deep Learning for\n  Universal School Connectivity","summary":"  Improving global school connectivity is critical for ensuring inclusive and\nequitable quality education. To reliably estimate the cost of connecting\nschools, governments and connectivity providers require complete and accurate\nschool location data - a resource that is often scarce in many low- and\nmiddle-income countries. To address this challenge, we propose a\ncost-effective, scalable approach to locating schools in high-resolution\nsatellite images using weakly supervised deep learning techniques. Our best\nmodels, which combine vision transformers and convolutional neural networks,\nachieve AUPRC values above 0.96 across 10 pilot African countries. Leveraging\nexplainable AI techniques, our approach can approximate the precise\ngeographical coordinates of the school locations using only low-cost,\nclassification-level annotations. To demonstrate the scalability of our method,\nwe generate nationwide maps of school location predictions in African countries\nand present a detailed analysis of our results, using Senegal as our case\nstudy. Finally, we demonstrate the immediate usability of our work by\nintroducing an interactive web mapping tool to streamline human-in-the-loop\nmodel validation efforts by government partners. This work successfully\nshowcases the real-world utility of deep learning and satellite images for\nplanning regional infrastructure and accelerating universal school\nconnectivity.\n","authors":["Isabelle Tingzon","Utku Can Ozturk","Ivan Dotu"],"pdf_url":"https://arxiv.org/pdf/2412.14870v1.pdf","comment":"Accepted at AAAI-25 Special Track on AI for Social Impact (AISI)"},{"id":"http://arxiv.org/abs/2412.14869v1","updated":"2024-12-19T14:06:44Z","published":"2024-12-19T14:06:44Z","title":"AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional\n  Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature\n  Screening","summary":"  Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood\nwithin the skull, which occurs due to the rupture of blood vessels in or around\nthe brain. If this condition is not diagnosed in a timely manner and\nappropriately treated, it can lead to serious complications such as decreased\nconsciousness, permanent neurological disabilities, or even death.The primary\naim of this study is to detect the occurrence or non-occurrence of ICH,\nfollowed by determining the type of subdural hemorrhage (SDH). These tasks are\nframed as two separate binary classification problems. By adding two layers to\nthe co-scale convolutional attention (CCA) classifier architecture, we\nintroduce a novel approach for ICH detection. In the first layer, after\nextracting features from different slices of computed tomography (CT) scan\nimages, we combine these features and select the 50 components that capture the\nhighest variance in the data, considering them as informative features. We then\nassess the discriminative power of these features using the bootstrap forest\nalgorithm, discarding those that lack sufficient discriminative ability between\ndifferent classes. This algorithm explicitly determines the contribution of\neach feature to the final prediction, assisting us in developing an explainable\nAI model. The features feed into a boosting neural network as a latent feature\nspace. In the second layer, we introduce a novel uncertainty-based fuzzy\nintegral operator to fuse information from different CT scan slices. This\noperator, by accounting for the dependencies between consecutive slices,\nsignificantly improves detection accuracy.\n","authors":["Mehdi Hosseini Chagahi","Md. Jalil Piran","Niloufar Delfan","Behzad Moshiri","Jaber Hatam Parikhan"],"pdf_url":"https://arxiv.org/pdf/2412.14869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20213v4","updated":"2024-12-19T13:46:40Z","published":"2024-03-29T14:50:43Z","title":"VHM: Versatile and Honest Vision Language Model for Remote Sensing Image\n  Analysis","summary":"  This paper develops a Versatile and Honest vision language Model (VHM) for\nremote sensing image analysis. VHM is built on a large-scale remote sensing\nimage-text dataset with rich-content captions (VersaD), and an honest\ninstruction dataset comprising both factual and deceptive questions (HnstD).\nUnlike prevailing remote sensing image-text datasets, in which image captions\nfocus on a few prominent objects and their relationships, VersaD captions\nprovide detailed information about image properties, object attributes, and the\noverall scene. This comprehensive captioning enables VHM to thoroughly\nunderstand remote sensing images and perform diverse remote sensing tasks.\nMoreover, different from existing remote sensing instruction datasets that only\ninclude factual questions, HnstD contains additional deceptive questions\nstemming from the non-existence of objects. This feature prevents VHM from\nproducing affirmative answers to nonsense queries, thereby ensuring its\nhonesty. In our experiments, VHM significantly outperforms various vision\nlanguage models on common tasks of scene classification, visual question\nanswering, and visual grounding. Additionally, VHM achieves competent\nperformance on several unexplored tasks, such as building vectorizing,\nmulti-label classification and honest question answering. We will release the\ncode, data and model weights at https://github.com/opendatalab/VHM .\n","authors":["Chao Pang","Xingxing Weng","Jiang Wu","Jiayu Li","Yi Liu","Jiaxing Sun","Weijia Li","Shuai Wang","Litong Feng","Gui-Song Xia","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2403.20213v4.pdf","comment":"Equal contribution: Chao Pang, Xingxing Weng, Jiang Wu; Corresponding\n  author: Gui-Song Xia, Conghui He"},{"id":"http://arxiv.org/abs/2411.04865v4","updated":"2024-12-19T13:45:39Z","published":"2024-11-07T16:58:18Z","title":"ZAHA: Introducing the Level of Facade Generalization and the Large-Scale\n  Point Cloud Facade Semantic Segmentation Benchmark Dataset","summary":"  Facade semantic segmentation is a long-standing challenge in photogrammetry\nand computer vision. Although the last decades have witnessed the influx of\nfacade segmentation methods, there is a lack of comprehensive facade classes\nand data covering the architectural variability. In ZAHA, we introduce Level of\nFacade Generalization (LoFG), novel hierarchical facade classes designed based\non international urban modeling standards, ensuring compatibility with\nreal-world challenging classes and uniform methods' comparison. Realizing the\nLoFG, we present to date the largest semantic 3D facade segmentation dataset,\nproviding 601 million annotated points at five and 15 classes of LoFG2 and\nLoFG3, respectively. Moreover, we analyze the performance of baseline semantic\nsegmentation methods on our introduced LoFG classes and data, complementing it\nwith a discussion on the unresolved challenges for facade segmentation. We\nfirmly believe that ZAHA shall facilitate further development of 3D facade\nsemantic segmentation methods, enabling robust segmentation indispensable in\ncreating urban digital twins.\n","authors":["Olaf Wysocki","Yue Tan","Thomas Froech","Yan Xia","Magdalena Wysocki","Ludwig Hoegner","Daniel Cremers","Christoph Holst"],"pdf_url":"https://arxiv.org/pdf/2411.04865v4.pdf","comment":"Accepted to WACV 2025 (IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV))"},{"id":"http://arxiv.org/abs/2412.13913v2","updated":"2024-12-19T13:41:08Z","published":"2024-12-18T14:53:38Z","title":"A Black-Box Evaluation Framework for Semantic Robustness in Bird's Eye\n  View Detection","summary":"  Camera-based Bird's Eye View (BEV) perception models receive increasing\nattention for their crucial role in autonomous driving, a domain where concerns\nabout the robustness and reliability of deep learning have been raised. While\nonly a few works have investigated the effects of randomly generated semantic\nperturbations, aka natural corruptions, on the multi-view BEV detection task,\nwe develop a black-box robustness evaluation framework that adversarially\noptimises three common semantic perturbations: geometric transformation, colour\nshifting, and motion blur, to deceive BEV models, serving as the first approach\nin this emerging field. To address the challenge posed by optimising the\nsemantic perturbation, we design a smoothed, distance-based surrogate function\nto replace the mAP metric and introduce SimpleDIRECT, a deterministic\noptimisation algorithm that utilises observed slopes to guide the optimisation\nprocess. By comparing with randomised perturbation and two optimisation\nbaselines, we demonstrate the effectiveness of the proposed framework.\nAdditionally, we provide a benchmark on the semantic robustness of ten recent\nBEV models. The results reveal that PolarFormer, which emphasises geometric\ninformation from multi-view images, exhibits the highest robustness, whereas\nBEVDet is fully compromised, with its precision reduced to zero.\n","authors":["Fu Wang","Yanghao Zhang","Xiangyu Yin","Guangliang Cheng","Zeyu Fu","Xiaowei Huang","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2412.13913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10929v6","updated":"2024-12-19T13:39:55Z","published":"2024-10-14T16:35:27Z","title":"ASTM :Autonomous Smart Traffic Management System Using Artificial\n  Intelligence CNN and LSTM","summary":"  In the modern world, the development of Artificial Intelligence (AI) has\ncontributed to improvements in various areas, including automation, computer\nvision, fraud detection, and more. AI can be leveraged to enhance the\nefficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce\ntraffic congestion rates. This paper presents an Autonomous Smart Traffic\nManagement (STM) system that uses AI to improve traffic flow rates. The system\nemploys the YOLO V5 Convolutional Neural Network to detect vehicles in traffic\nmanagement images. Additionally, it predicts the number of vehicles for the\nnext 12 hours using a Recurrent Neural Network with Long Short-Term Memory\n(RNN-LSTM). The Smart Traffic Management Cycle Length Analysis manages the\ntraffic cycle length based on these vehicle predictions, aided by AI. From the\nresults of the RNN-LSTM model for predicting vehicle numbers over the next 12\nhours, we observe that the model predicts traffic with a Mean Squared Error\n(MSE) of 4.521 vehicles and a Root Mean Squared Error (RMSE) of 2.232 vehicles.\nAfter simulating the STM system in the CARLA simulation environment, we found\nthat the Traffic Management Congestion Flow Rate with ASTM (21 vehicles per\nminute) is 50\\% higher than the rate without STM (around 15 vehicles per\nminute). Additionally, the Traffic Management Vehicle Pass Delay with STM (5\nseconds per vehicle) is 70\\% lower than without STM (around 12 seconds per\nvehicle). These results demonstrate that the STM system using AI can increase\ntraffic flow by 50\\% and reduce vehicle pass delays by 70\\%.\n","authors":["Christofel Rio Goenawan"],"pdf_url":"https://arxiv.org/pdf/2410.10929v6.pdf","comment":"In process to IEEE Intelligent Vehicle Symposium 2025"},{"id":"http://arxiv.org/abs/2412.14846v1","updated":"2024-12-19T13:38:20Z","published":"2024-12-19T13:38:20Z","title":"Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy\n  with Pre-training, Data Augmentation and Dual Flow UNet","summary":"  Head and neck tumors and metastatic lymph nodes are crucial for treatment\nplanning and prognostic analysis. Accurate segmentation and quantitative\nanalysis of these structures require pixel-level annotation, making automated\nsegmentation techniques essential for the diagnosis and treatment of head and\nneck cancer. In this study, we investigated the effects of multiple strategies\non the segmentation of pre-radiotherapy (pre-RT) and mid-radiotherapy (mid-RT)\nimages. For the segmentation of pre-RT images, we utilized: 1) a fully\nsupervised learning approach, and 2) the same approach enhanced with\npre-trained weights and the MixUp data augmentation technique. For mid-RT\nimages, we introduced a novel computational-friendly network architecture that\nfeatures separate encoders for mid-RT images and registered pre-RT images with\ntheir labels. The mid-RT encoder branch integrates information from pre-RT\nimages and labels progressively during the forward propagation. We selected the\nhighest-performing model from each fold and used their predictions to create an\nensemble average for inference. In the final test, our models achieved a\nsegmentation performance of 82.38% for pre-RT and 72.53% for mid-RT on\naggregated Dice Similarity Coefficient (DSC) as HiLab. Our code is available at\nhttps://github.com/WltyBY/HNTS-MRG2024_train_code.\n","authors":["Litingyu Wang","Wenjun Liao","Shichuan Zhang","Guotai Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14837v1","updated":"2024-12-19T13:27:58Z","published":"2024-12-19T13:27:58Z","title":"ObjVariantEnsemble: Advancing Point Cloud LLM Evaluation in Challenging\n  Scenes with Subtly Distinguished Objects","summary":"  3D scene understanding is an important task, and there has been a recent\nsurge of research interest in aligning 3D representations of point clouds with\ntext to empower embodied AI. However, due to the lack of comprehensive 3D\nbenchmarks, the capabilities of 3D models in real-world scenes, particularly\nthose that are challenging with subtly distinguished objects, remain\ninsufficiently investigated. To facilitate a more thorough evaluation of 3D\nmodels' capabilities, we propose a scheme, ObjVariantEnsemble, to\nsystematically introduce more scenes with specified object classes, colors,\nshapes, quantities, and spatial relationships to meet model evaluation needs.\nMore importantly, we intentionally construct scenes with similar objects to a\ncertain degree and design an LLM-VLM-cooperated annotator to capture key\ndistinctions as annotations. The resultant benchmark can better challenge 3D\nmodels, reveal their shortcomings in understanding, and potentially aid in the\nfurther development of 3D models.\n","authors":["Qihang Cao","Huangxun Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14837v1.pdf","comment":"Accepted to AAAI2025"},{"id":"http://arxiv.org/abs/2412.14835v1","updated":"2024-12-19T13:25:39Z","published":"2024-12-19T13:25:39Z","title":"Progressive Multimodal Reasoning via Active Retrieval","summary":"  Multi-step multimodal reasoning tasks pose significant challenges for\nmultimodal large language models (MLLMs), and finding effective ways to enhance\ntheir performance in such scenarios remains an unresolved issue. In this paper,\nwe propose AR-MCTS, a universal framework designed to progressively improve the\nreasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo\nTree Search (MCTS). Our approach begins with the development of a unified\nretrieval module that retrieves key supporting insights for solving complex\nreasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in\nautomated multimodal reasoning verification, we employ the MCTS algorithm\ncombined with an active retrieval mechanism, which enables the automatic\ngeneration of step-wise annotations. This strategy dynamically retrieves key\ninsights for each reasoning step, moving beyond traditional beam search\nsampling to improve the diversity and reliability of the reasoning space.\nAdditionally, we introduce a process reward model that aligns progressively to\nsupport the automatic verification of multimodal reasoning tasks. Experimental\nresults across three complex multimodal reasoning benchmarks confirm the\neffectiveness of the AR-MCTS framework in enhancing the performance of various\nmultimodal models. Further analysis demonstrates that AR-MCTS can optimize\nsampling diversity and accuracy, yielding reliable multimodal reasoning.\n","authors":["Guanting Dong","Chenghao Zhang","Mengjie Deng","Yutao Zhu","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2412.14835v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2412.14833v1","updated":"2024-12-19T13:21:04Z","published":"2024-12-19T13:21:04Z","title":"Synchronized and Fine-Grained Head for Skeleton-Based Ambiguous Action\n  Recognition","summary":"  Skeleton-based action recognition using GCNs has achieved remarkable\nperformance, but recognizing ambiguous actions, such as \"waving\" and\n\"saluting\", remains a significant challenge. Existing methods typically rely on\na serial combination of GCNs and TCNs, where spatial and temporal features are\nextracted independently, leading to an unbalanced spatial-temporal information,\nwhich hinders accurate action recognition. Moreover, existing methods for\nambiguous actions often overemphasize local details, resulting in the loss of\ncrucial global context, which further complicates the task of differentiating\nambiguous actions. To address these challenges, we propose a lightweight\nplug-and-play module called Synchronized and Fine-grained Head (SF-Head),\ninserted between GCN and TCN layers. SF-Head first conducts Synchronized\nSpatial-Temporal Extraction (SSTE) with a Feature Redundancy Loss (F-RL),\nensuring a balanced interaction between the two types of features. It then\nperforms Adaptive Cross-dimensional Feature Aggregation (AC-FA), with a Feature\nConsistency Loss (F-CL), which aligns the aggregated feature with their\noriginal spatial-temporal feature. This aggregation step effectively combines\nboth global context and local details. Experimental results on NTU RGB+D 60,\nNTU RGB+D 120, and NW-UCLA datasets demonstrate significant improvements in\ndistinguishing ambiguous actions. Our code will be made available at\nhttps://github.com/HaoHuang2003/SFHead.\n","authors":["Hao Huang","Yujie Lin","Siyu Chen","Haiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14833v1.pdf","comment":"20pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.14821v1","updated":"2024-12-19T13:12:15Z","published":"2024-12-19T13:12:15Z","title":"PC-BEV: An Efficient Polar-Cartesian BEV Fusion Framework for LiDAR\n  Semantic Segmentation","summary":"  Although multiview fusion has demonstrated potential in LiDAR segmentation,\nits dependence on computationally intensive point-based interactions, arising\nfrom the lack of fixed correspondences between views such as range view and\nBird's-Eye View (BEV), hinders its practical deployment. This paper challenges\nthe prevailing notion that multiview fusion is essential for achieving high\nperformance. We demonstrate that significant gains can be realized by directly\nfusing Polar and Cartesian partitioning strategies within the BEV space. Our\nproposed BEV-only segmentation model leverages the inherent fixed grid\ncorrespondences between these partitioning schemes, enabling a fusion process\nthat is orders of magnitude faster (170$\\times$ speedup) than conventional\npoint-based methods. Furthermore, our approach facilitates dense feature\nfusion, preserving richer contextual information compared to sparse point-based\nalternatives. To enhance scene understanding while maintaining inference\nefficiency, we also introduce a hybrid Transformer-CNN architecture. Extensive\nevaluation on the SemanticKITTI and nuScenes datasets provides compelling\nevidence that our method outperforms previous multiview fusion approaches in\nterms of both performance and inference speed, highlighting the potential of\nBEV-based fusion for LiDAR segmentation. Code is available at\n\\url{https://github.com/skyshoumeng/PC-BEV.}\n","authors":["Shoumeng Qiu","Xinrun Li","XiangYang Xue","Jian Pu"],"pdf_url":"https://arxiv.org/pdf/2412.14821v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14819v1","updated":"2024-12-19T13:10:38Z","published":"2024-12-19T13:10:38Z","title":"Multi-Level Embedding and Alignment Network with Consistency and\n  Invariance Learning for Cross-View Geo-Localization","summary":"  Cross-View Geo-Localization (CVGL) involves determining the localization of\ndrone images by retrieving the most similar GPS-tagged satellite images.\nHowever, the imaging gaps between platforms are often significant and the\nvariations in viewpoints are substantial, which limits the ability of existing\nmethods to effectively associate cross-view features and extract consistent and\ninvariant characteristics. Moreover, existing methods often overlook the\nproblem of increased computational and storage requirements when improving\nmodel performance. To handle these limitations, we propose a lightweight\nenhanced alignment network, called the Multi-Level Embedding and Alignment\nNetwork (MEAN). The MEAN network uses a progressive multi-level enhancement\nstrategy, global-to-local associations, and cross-domain alignment, enabling\nfeature communication across levels. This allows MEAN to effectively connect\nfeatures at different levels and learn robust cross-view consistent mappings\nand modality-invariant features. Moreover, MEAN adopts a shallow backbone\nnetwork combined with a lightweight branch design, effectively reducing\nparameter count and computational complexity. Experimental results on the\nUniversity-1652 and SUES-200 datasets demonstrate that MEAN reduces parameter\ncount by 62.17% and computational complexity by 70.99% compared to\nstate-of-the-art models, while maintaining competitive or even superior\nperformance. The codes will be released soon.\n","authors":["Zhongwei Chen","Zhao-Xu Yang","Hai-Jun Rong"],"pdf_url":"https://arxiv.org/pdf/2412.14819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14816v1","updated":"2024-12-19T13:10:03Z","published":"2024-12-19T13:10:03Z","title":"Explainable Tampered Text Detection via Multimodal Large Models","summary":"  Recently, tampered text detection has attracted increasing attention due to\nits essential role in information security. Although existing methods can\ndetect the tampered text region, the interpretation of such detection remains\nunclear, making the prediction unreliable. To address this black-box problem,\nwe propose to explain the basis of tampered text detection with natural\nlanguage via large multimodal models. To fill the data gap for this task, we\npropose a large-scale, comprehensive dataset, ETTD, which contains both\npixel-level annotations indicating the tampered text region and natural\nlanguage annotations describing the anomaly of the tampered text. Multiple\nmethods are employed to improve the quality of the proposed data. For example,\na fused mask prompt is proposed to reduce confusion when querying GPT4o to\ngenerate anomaly descriptions. By weighting the input image with the mask\nannotation, the tampered region can be clearly indicated and the content in and\naround the tampered region can also be preserved. We also propose prompting\nGPT4o to recognize tampered texts and filtering out the responses with low OCR\naccuracy, which can effectively improve annotation quality in an automatic\nmanner. To further improve explainable tampered text detection, we propose a\nsimple yet effective model called TTD, which benefits from improved\nfine-grained perception by paying attention to the suspected region with\nauxiliary reference grounding query. Extensive experiments on both the ETTD\ndataset and the public dataset have verified the effectiveness of the proposed\nmethods. In-depth analysis is also provided to inspire further research. The\ndataset and code will be made publicly available.\n","authors":["Chenfan Qu","Jian Liu","Haoxing Chen","Baihan Yu","Jingjing Liu","Weiqiang Wang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2412.14816v1.pdf","comment":"The first work for explainable tampered text detection"},{"id":"http://arxiv.org/abs/2304.02488v4","updated":"2024-12-19T13:00:35Z","published":"2023-04-05T15:02:30Z","title":"SCB-dataset: A Dataset for Detecting Student Classroom Behavior","summary":"  The use of deep learning methods for automatic detection of students'\nclassroom behavior is a promising approach to analyze their class performance\nand enhance teaching effectiveness. However, the lack of publicly available\ndatasets on student behavior poses a challenge for researchers in this field.\nTo address this issue, we propose a Student Classroom Behavior dataset\n(SCB-dataset) that reflects real-life scenarios. Our dataset includes 11,248\nlabels and 4,003 images, with a focus on hand-raising behavior. We evaluated\nthe dataset using the YOLOv7 algorithm, achieving a mean average precision\n(map) of up to 85.3%. We believe that our dataset can serve as a robust\nfoundation for future research in the field of student behavior detection and\npromote further advancements in this area.Our SCB-dataset can be downloaded\nfrom: https://github.com/Whiffe/SCB-dataset\n","authors":["Fan Yang"],"pdf_url":"https://arxiv.org/pdf/2304.02488v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13735v2","updated":"2024-12-19T12:59:31Z","published":"2024-12-18T11:14:01Z","title":"3D Registration in 30 Years: A Survey","summary":"  3D point cloud registration is a fundamental problem in computer vision,\ncomputer graphics, robotics, remote sensing, and etc. Over the last thirty\nyears, we have witnessed the amazing advancement in this area with numerous\nkinds of solutions. Although a handful of relevant surveys have been conducted,\ntheir coverage is still limited. In this work, we present a comprehensive\nsurvey on 3D point cloud registration, covering a set of sub-areas such as\npairwise coarse registration, pairwise fine registration, multi-view\nregistration, cross-scale registration, and multi-instance registration. The\ndatasets, evaluation metrics, method taxonomy, discussions of the merits and\ndemerits, insightful thoughts of future directions are comprehensively\npresented in this survey. The regularly updated project page of the survey is\navailable at https://github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.\n","authors":["Jiaqi Yang","Chu'ai Zhang","Zhengbao Wang","Xinyue Cao","Xuan Ouyang","Xiyu Zhang","Zhenxuan Zeng","Zhao Zeng","Borui Lu","Zhiyi Xia","Qian Zhang","Yulan Guo","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14803v1","updated":"2024-12-19T12:48:40Z","published":"2024-12-19T12:48:40Z","title":"Video Prediction Policy: A Generalist Robot Policy with Predictive\n  Visual Representations","summary":"  Recent advancements in robotics have focused on developing generalist\npolicies capable of performing multiple tasks. Typically, these policies\nutilize pre-trained vision encoders to capture crucial information from current\nobservations. However, previous vision encoders, which trained on two-image\ncontrastive learning or single-image reconstruction, can not perfectly capture\nthe sequential information essential for embodied tasks. Recently, video\ndiffusion models (VDMs) have demonstrated the capability to accurately predict\nfuture image sequences, exhibiting a good understanding of physical dynamics.\nMotivated by the strong visual prediction capabilities of VDMs, we hypothesize\nthat they inherently possess visual representations that reflect the evolution\nof the physical world, which we term predictive visual representations.\nBuilding on this hypothesis, we propose the Video Prediction Policy (VPP), a\ngeneralist robotic policy conditioned on the predictive visual representations\nfrom VDMs. To further enhance these representations, we incorporate diverse\nhuman or robotic manipulation datasets, employing unified video-generation\ntraining objectives. VPP consistently outperforms existing methods across two\nsimulated and two real-world benchmarks. Notably, it achieves a 28.1\\% relative\nimprovement in the Calvin ABC-D benchmark compared to the previous\nstate-of-the-art and delivers a 28.8\\% increase in success rates for complex\nreal-world dexterous manipulation tasks.\n","authors":["Yucheng Hu","Yanjiang Guo","Pengchao Wang","Xiaoyu Chen","Yen-Jen Wang","Jianke Zhang","Koushil Sreenath","Chaochao Lu","Jianyu Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14803v1.pdf","comment":"The first two authors contribute equally. Project Page at\n  https://video-prediction-policy.github.io/"},{"id":"http://arxiv.org/abs/2410.05317v3","updated":"2024-12-19T12:38:23Z","published":"2024-10-05T03:47:06Z","title":"Accelerating Diffusion Transformers with Token-wise Feature Caching","summary":"  Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.\n","authors":["Chang Zou","Xuyang Liu","Ting Liu","Siteng Huang","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.05317v3.pdf","comment":"In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix"},{"id":"http://arxiv.org/abs/2412.13803v2","updated":"2024-12-19T12:31:34Z","published":"2024-12-18T12:50:11Z","title":"M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object\n  Segmentation","summary":"  Intelligent robots need to interact with diverse objects across various\nenvironments. The appearance and state of objects frequently undergo complex\ntransformations depending on the object properties, e.g., phase transitions.\nHowever, in the vision community, segmenting dynamic objects with phase\ntransitions is overlooked. In light of this, we introduce the concept of phase\nin segmentation, which categorizes real-world objects based on their visual\ncharacteristics and potential morphological and appearance changes. Then, we\npresent a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video\nObject Segmentation (M$^3$-VOS), to verify the ability of models to understand\nobject phases, which consists of 479 high-resolution videos spanning over 10\ndistinct everyday scenarios. It provides dense instance mask annotations that\ncapture both object phases and their transitions. We evaluate state-of-the-art\nmethods on M$^3$-VOS, yielding several key insights. Notably, current\nappearancebased approaches show significant room for improvement when handling\nobjects with phase transitions. The inherent changes in disorder suggest that\nthe predictive performance of the forward entropy-increasing process can be\nimproved through a reverse entropy-reducing process. These findings lead us to\npropose ReVOS, a new plug-andplay model that improves its performance by\nreversal refinement. Our data and code will be publicly available at\nhttps://zixuan-chen.github.io/M-cubeVOS.github.io/.\n","authors":["Zixuan Chen","Jiaxin Li","Liming Tan","Yejie Guo","Junxuan Liang","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2412.13803v2.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.14790v1","updated":"2024-12-19T12:29:31Z","published":"2024-12-19T12:29:31Z","title":"YOLOv11 Optimization for Efficient Resource Utilization","summary":"  The objective of this research is to optimize the eleventh iteration of You\nOnly Look Once (YOLOv11) by developing size-specific modified versions of the\narchitecture. These modifications involve pruning unnecessary layers and\nreconfiguring the main architecture of YOLOv11. Each proposed version is\ntailored to detect objects of specific size ranges, from small to large. To\nensure proper model selection based on dataset characteristics, we introduced\nan object classifier program. This program identifies the most suitable\nmodified version for a given dataset. The proposed models were evaluated on\nvarious datasets and compared with the original YOLOv11 and YOLOv8 models. The\nexperimental results highlight significant improvements in computational\nresource efficiency, with the proposed models maintaining the accuracy of the\noriginal YOLOv11. In some cases, the modified versions outperformed the\noriginal model regarding detection performance. Furthermore, the proposed\nmodels demonstrated reduced model sizes and faster inference times. Models\nweights and the object size classifier can be found in this repository\n","authors":["Areeg Fagad Rasheed","M. Zarkoosh"],"pdf_url":"https://arxiv.org/pdf/2412.14790v1.pdf","comment":"12 pages, 13 figures, 4 tables"},{"id":"http://arxiv.org/abs/2412.09401v2","updated":"2024-12-19T12:23:39Z","published":"2024-12-12T16:08:03Z","title":"SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos","summary":"  In this paper, we introduce SLAM3R, a novel and effective monocular RGB SLAM\nsystem for real-time and high-quality dense 3D reconstruction. SLAM3R provides\nan end-to-end solution by seamlessly integrating local 3D reconstruction and\nglobal coordinate registration through feed-forward neural networks. Given an\ninput video, the system first converts it into overlapping clips using a\nsliding window mechanism. Unlike traditional pose optimization-based methods,\nSLAM3R directly regresses 3D pointmaps from RGB images in each window and\nprogressively aligns and deforms these local pointmaps to create a globally\nconsistent scene reconstruction - all without explicitly solving any camera\nparameters. Experiments across datasets consistently show that SLAM3R achieves\nstate-of-the-art reconstruction accuracy and completeness while maintaining\nreal-time performance at 20+ FPS. Code and weights at:\nhttps://github.com/PKU-VCL-3DV/SLAM3R.\n","authors":["Yuzheng Liu","Siyan Dong","Shuzhe Wang","Yanchao Yang","Qingnan Fan","Baoquan Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09401v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16302v2","updated":"2024-12-19T12:14:03Z","published":"2024-07-23T08:57:11Z","title":"DeepClean: Integrated Distortion Identification and Algorithm Selection\n  for Rectifying Image Corruptions","summary":"  Distortion identification and rectification in images and videos is vital for\nachieving good performance in downstream vision applications. Instead of\nrelying on fixed trial-and-error based image processing pipelines, we propose a\ntwo-level sequential planning approach for automated image distortion\nclassification and rectification. At the higher level it detects the class of\ncorruptions present in the input image, if any. The lower level selects a\nspecific algorithm to be applied, from a set of externally provided candidate\nalgorithms. The entire two-level setup runs in the form of a single forward\npass during inference and it is to be queried iteratively until the retrieval\nof the original image. We demonstrate improvements compared to three baselines\non the object detection task on COCO image dataset with rich set of\ndistortions. The advantage of our approach is its dynamic reconfiguration,\nconditioned on the input image and generalisability to unseen candidate\nalgorithms at inference time, since it relies only on the comparison of their\noutput of the image embeddings.\n","authors":["Aditya Kapoor","Harshad Khadilkar","Jayvardhana Gubbi"],"pdf_url":"https://arxiv.org/pdf/2407.16302v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.14168v2","updated":"2024-12-19T11:59:46Z","published":"2024-12-18T18:59:50Z","title":"FashionComposer: Compositional Fashion Image Generation","summary":"  We present FashionComposer for compositional fashion image generation. Unlike\nprevious methods, FashionComposer is highly flexible. It takes multi-modal\ninput (i.e., text prompt, parametric human model, garment image, and face\nimage) and supports personalizing the appearance, pose, and figure of the human\nand assigning multiple garments in one pass. To achieve this, we first develop\na universal framework capable of handling diverse input modalities. We\nconstruct scaled training data to enhance the model's robust compositional\ncapabilities. To accommodate multiple reference images (garments and faces)\nseamlessly, we organize these references in a single image as an \"asset\nlibrary\" and employ a reference UNet to extract appearance features. To inject\nthe appearance features into the correct pixels in the generated result, we\npropose subject-binding attention. It binds the appearance features from\ndifferent \"assets\" with the corresponding text features. In this way, the model\ncould understand each asset according to their semantics, supporting arbitrary\nnumbers and types of reference images. As a comprehensive solution,\nFashionComposer also supports many other applications like human album\ngeneration, diverse virtual try-on tasks, etc.\n","authors":["Sihui Ji","Yiyang Wang","Xi Chen","Xiaogang Xu","Hao Luo","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.14168v2.pdf","comment":"https://sihuiji.github.io/FashionComposer-Page"},{"id":"http://arxiv.org/abs/2412.14768v1","updated":"2024-12-19T11:51:45Z","published":"2024-12-19T11:51:45Z","title":"FLAMe: Federated Learning with Attention Mechanism using Spatio-Temporal\n  Keypoint Transformers for Pedestrian Fall Detection in Smart Cities","summary":"  In smart cities, detecting pedestrian falls is a major challenge to ensure\nthe safety and quality of life of citizens. In this study, we propose a novel\nfall detection system using FLAMe (Federated Learning with Attention\nMechanism), a federated learning (FL) based algorithm. FLAMe trains around\nimportant keypoint information and only transmits the trained important weights\nto the server, reducing communication costs and preserving data privacy.\nFurthermore, the lightweight keypoint transformer model is integrated into the\nFL framework to effectively learn spatio-temporal features. We validated the\nexperiment using 22,672 video samples from the \"Fall Accident Risk Behavior\nVideo-Sensor Pair data\" dataset from AI-Hub. As a result of the experiment, the\nFLAMe-based system achieved an accuracy of 94.02% with about 190,000\ntransmission parameters, maintaining performance similar to that of existing\ncentralized learning while maximizing efficiency by reducing communication\ncosts by about 40% compared to the existing FL algorithm, FedAvg. Therefore,\nthe FLAMe algorithm has demonstrated that it provides robust performance in the\ndistributed environment of smart cities and is a practical and effective\nsolution for public safety.\n","authors":["Byeonghun Kim","Byeongjoon Noh"],"pdf_url":"https://arxiv.org/pdf/2412.14768v1.pdf","comment":"8 pages, 7 figures, AAAI 2025 FLUID Workshop"},{"id":"http://arxiv.org/abs/2408.01812v3","updated":"2024-12-19T11:29:09Z","published":"2024-08-03T15:43:56Z","title":"SkyDiffusion: Ground-to-Aerial Image Synthesis with Diffusion Models and\n  BEV Paradigm","summary":"  Ground-to-aerial image synthesis focuses on generating realistic aerial\nimages from corresponding ground street view images while maintaining\nconsistent content layout, simulating a top-down view. The significant\nviewpoint difference leads to domain gaps between views, and dense urban scenes\nlimit the visible range of street views, making this cross-view generation task\nparticularly challenging. In this paper, we introduce SkyDiffusion, a novel\ncross-view generation method for synthesizing aerial images from street view\nimages, utilizing a diffusion model and the Bird's-Eye View (BEV) paradigm. The\nCurved-BEV method in SkyDiffusion converts street-view images into a BEV\nperspective, effectively bridging the domain gap, and employs a \"multi-to-one\"\nmapping strategy to address occlusion issues in dense urban scenes. Next,\nSkyDiffusion designed a BEV-guided diffusion model to generate\ncontent-consistent and realistic aerial images. Additionally, we introduce a\nnovel dataset, Ground2Aerial-3, designed for diverse ground-to-aerial image\nsynthesis applications, including disaster scene aerial synthesis, historical\nhigh-resolution satellite image synthesis, and low-altitude UAV image synthesis\ntasks. Experimental results demonstrate that SkyDiffusion outperforms\nstate-of-the-art methods on cross-view datasets across natural (CVUSA),\nsuburban (CVACT), urban (VIGOR-Chicago), and various application scenarios\n(G2A-3), achieving realistic and content-consistent aerial image generation.\nMore result and dataset information can be found at\nhttps://opendatalab.github.io/skydiffusion/ .\n","authors":["Junyan Ye","Jun He","Weijia Li","Zhutao Lv","Yi Lin","Jinhua Yu","Haote Yang","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2408.01812v3.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2401.17981v3","updated":"2024-12-19T11:25:34Z","published":"2024-01-31T16:38:32Z","title":"From Training-Free to Adaptive: Empirical Insights into MLLMs'\n  Understanding of Detection Information","summary":"  Despite the impressive capabilities of Multimodal Large Language Models\n(MLLMs) in integrating text and image modalities, challenges remain in\naccurately interpreting detailed visual elements. Vision detection models excel\nat recognizing fine-grained image details, prompting researchers to use them to\nenhance MLLMs. One effective strategy is to infuse detection information in\ntext format, which has proven simple and effective. However, most studies\nutilize this method without training, leaving the potential of adaptive\ntraining largely unexplored. Adaptive training could significantly enhance\nMLLMs' comprehension of unique inputs while filtering out irrelevant\ninformation. This paper addresses the crucial question: How does training\nimpact MLLMs' understanding of infused textual detection information? We\nsystematically experiment with various representative models to evaluate the\neffects of training-free, retraining, and fine-tuning strategies. We also\nexamine the influence of training on MLLMs' original abilities and the\ninterchangeability of detection models. Our findings indicate that fine-tuning\na pre-trained MLLM to incorporate textual detection information delivers\nsuperior results compared to training-free and retraining methods, improving\nperformance by 6.71% across 10 widely recognized benchmarks. Furthermore,\nfine-tuning enables MLLMs to retain performance enhancements even when\ndetection models are swapped, indicating improved understanding of formatted\ntextual data. We release our codes to support further exploration of fusion\nstrategies for vision detection models and the enhancement of MLLMs'\nfine-grained multimodal capabilities.\n","authors":["Qirui Jiao","Daoyuan Chen","Yilun Huang","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2401.17981v3.pdf","comment":"32 pages, 22 tables, 7 figures"},{"id":"http://arxiv.org/abs/2410.23091v5","updated":"2024-12-19T11:18:58Z","published":"2024-10-30T15:06:44Z","title":"CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for\n  Adversarial Defense","summary":"  Despite ongoing efforts to defend neural classifiers from adversarial\nattacks, they remain vulnerable, especially to unseen attacks. In contrast,\nhumans are difficult to be cheated by subtle manipulations, since we make\njudgments only based on essential factors. Inspired by this observation, we\nattempt to model label generation with essential label-causative factors and\nincorporate label-non-causative factors to assist data generation. For an\nadversarial example, we aim to discriminate the perturbations as non-causative\nfactors and make predictions only based on the label-causative factors.\nConcretely, we propose a casual diffusion model (CausalDiff) that adapts\ndiffusion models for conditional data generation and disentangles the two types\nof casual factors by learning towards a novel casual information bottleneck\nobjective. Empirically, CausalDiff has significantly outperformed\nstate-of-the-art defense methods on various unseen attacks, achieving an\naverage robustness of 86.39% (+4.01%) on CIFAR-10, 56.25% (+3.13%) on\nCIFAR-100, and 82.62% (+4.93%) on GTSRB (German Traffic Sign Recognition\nBenchmark). The code is available at\nhttps://github.com/CAS-AISafetyBasicResearchGroup/CausalDiff\n","authors":["Mingkun Zhang","Keping Bi","Wei Chen","Quanrun Chen","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.23091v5.pdf","comment":"accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.11383v2","updated":"2024-12-19T11:12:30Z","published":"2024-09-17T17:34:24Z","title":"Training Datasets Generation for Machine Learning: Application to Vision\n  Based Navigation","summary":"  Vision Based Navigation consists in utilizing cameras as precision sensors\nfor GNC after extracting information from images. To enable the adoption of\nmachine learning for space applications, one of obstacles is the demonstration\nthat available training datasets are adequate to validate the algorithms. The\nobjective of the study is to generate datasets of images and metadata suitable\nfor training machine learning algorithms. Two use cases were selected and a\nrobust methodology was developed to validate the datasets including the ground\ntruth. The first use case is in-orbit rendezvous with a man-made object: a\nmockup of satellite ENVISAT. The second use case is a Lunar landing scenario.\nDatasets were produced from archival datasets (Chang'e 3), from the laboratory\nat DLR TRON facility and at Airbus Robotic laboratory, from SurRender software\nhigh fidelity image simulator using Model Capture and from Generative\nAdversarial Networks. The use case definition included the selection of\nalgorithms as benchmark: an AI-based pose estimation algorithm and a dense\noptical flow algorithm were selected. Eventually it is demonstrated that\ndatasets produced with SurRender and selected laboratory facilities are\nadequate to train machine learning algorithms.\n","authors":["Jérémy Lebreton","Ingo Ahrns","Roland Brochard","Christoph Haskamp","Hans Krüger","Matthieu Le Goff","Nicolas Menga","Nicolas Ollagnier","Ralf Regele","Francesco Capolupo","Massimo Casasco"],"pdf_url":"https://arxiv.org/pdf/2409.11383v2.pdf","comment":"6 pages, 4 figures, preprint of the proceedings of ESA SPAICE\n  conference 2024"},{"id":"http://arxiv.org/abs/2408.04594v3","updated":"2024-12-19T11:04:20Z","published":"2024-08-08T17:10:16Z","title":"Img-Diff: Contrastive Data Synthesis for Multimodal Large Language\n  Models","summary":"  High-performance Multimodal Large Language Models (MLLMs) are heavily\ndependent on data quality. To advance fine-grained image recognition within\nMLLMs, we introduce a novel data synthesis method inspired by contrastive\nlearning and image difference captioning. Our key idea involves challenging the\nmodel to discern both matching and distinct elements by scrutinizing object\ndifferences in detailed regions across similar images. We begin by generating\npairs of similar images that emphasize object variations. Following this, we\nemploy a Difference Area Generator to pinpoint object differences, and\nsubsequently, a Difference Captions Generator to articulate these differences.\nThis process results in a high-quality dataset of \"object replacement\" samples,\ntermed Img-Diff, which can be scaled as needed due to its automated nature. We\nleverage this generated dataset to fine-tune state-of-the-art (SOTA) MLLMs,\nsuch as InternVL2, achieving substantial improvements across various image\ndifference and Visual Question Answering tasks. Notably, the trained models\nsignificantly outperform existing SOTA models like GPT-4V and Gemini on the\nMMVP benchmark. Additionally, we conduct comprehensive evaluations to validate\nthe dataset's diversity, quality, and robustness, offering several insights\ninto the synthesis of such contrastive datasets. We release our codes and\ndataset to encourage further research on multimodal data synthesis and MLLMs'\nfundamental capabilities for image understanding.\n","authors":["Qirui Jiao","Daoyuan Chen","Yilun Huang","Bolin Ding","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2408.04594v3.pdf","comment":"22 pages, 10 figures, 16 tables"},{"id":"http://arxiv.org/abs/2412.10681v2","updated":"2024-12-19T10:59:24Z","published":"2024-12-14T05:01:46Z","title":"One Pixel is All I Need","summary":"  Vision Transformers (ViTs) have achieved record-breaking performance in\nvarious visual tasks. However, concerns about their robustness against backdoor\nattacks have grown. Backdoor attacks involve associating a specific trigger\nwith a target label, causing the model to predict the attacker-specified label\nwhen the trigger is present, while correctly identifying clean images.We found\nthat ViTs exhibit higher attack success rates for quasi-triggers(patterns\ndifferent from but similar to the original training triggers)compared to CNNs.\nMoreover, some backdoor features in clean samples can suppress the original\ntrigger, making quasi-triggers more effective.To better understand and exploit\nthese vulnerabilities, we developed a tool called the Perturbation Sensitivity\nDistribution Map (PSDM). PSDM computes and sums gradients over many inputs to\nshow how sensitive the model is to small changes in the input. In ViTs, PSDM\nreveals a patch-like pattern where central pixels are more sensitive than\nedges. We use PSDM to guide the creation of quasi-triggers.Based on these\nfindings, we designed \"WorstVIT,\" a simple yet effective data poisoning\nbackdoor for ViT models. This attack requires an extremely low poisoning rate,\ntrains for just one epoch, and modifies a single pixel to successfully attack\nall validation images.\n","authors":["Deng Siqin","Zhou Xiaoyi"],"pdf_url":"https://arxiv.org/pdf/2412.10681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16729v3","updated":"2024-12-19T10:53:48Z","published":"2024-08-29T17:20:59Z","title":"Prediction-Feedback DETR for Temporal Action Detection","summary":"  Temporal Action Detection (TAD) is fundamental yet challenging for real-world\nvideo applications. Leveraging the unique benefits of transformers, various\nDETR-based approaches have been adopted in TAD. However, it has recently been\nidentified that the attention collapse in self-attention causes the performance\ndegradation of DETR for TAD. Building upon previous research, this paper newly\naddresses the attention collapse problem in cross-attention within DETR-based\nTAD methods. Moreover, our findings reveal that cross-attention exhibits\npatterns distinct from predictions, indicating a short-cut phenomenon. To\nresolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR),\nwhich utilizes predictions to restore the collapse and align the cross- and\nself-attention with predictions. Specifically, we devise novel\nprediction-feedback objectives using guidance from the relations of the\npredictions. As a result, Pred-DETR significantly alleviates the collapse and\nachieves state-of-the-art performance among DETR-based methods on various\nchallenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and\nFineAction.\n","authors":["Jihwan Kim","Miso Lee","Cheol-Ho Cho","Jihyun Lee","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2408.16729v3.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2406.02507v3","updated":"2024-12-19T10:43:11Z","published":"2024-06-04T17:25:59Z","title":"Guiding a Diffusion Model with a Bad Version of Itself","summary":"  The primary axes of interest in image-generating diffusion models are image\nquality, the amount of variation in the results, and how well the results align\nwith a given condition, e.g., a class label or a text prompt. The popular\nclassifier-free guidance approach uses an unconditional model to guide a\nconditional model, leading to simultaneously better prompt alignment and\nhigher-quality images at the cost of reduced variation. These effects seem\ninherently entangled, and thus hard to control. We make the surprising\nobservation that it is possible to obtain disentangled control over image\nquality without compromising the amount of variation by guiding generation\nusing a smaller, less-trained version of the model itself rather than an\nunconditional model. This leads to significant improvements in ImageNet\ngeneration, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using\npublicly available networks. Furthermore, the method is also applicable to\nunconditional diffusion models, drastically improving their quality.\n","authors":["Tero Karras","Miika Aittala","Tuomas Kynkäänniemi","Jaakko Lehtinen","Timo Aila","Samuli Laine"],"pdf_url":"https://arxiv.org/pdf/2406.02507v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.14719v1","updated":"2024-12-19T10:41:24Z","published":"2024-12-19T10:41:24Z","title":"Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition","summary":"  Micro-Action Recognition (MAR) has gained increasing attention due to its\ncrucial role as a form of non-verbal communication in social interactions, with\npromising potential for applications in human communication and emotion\nanalysis. However, current approaches often overlook the inherent ambiguity in\nmicro-actions, which arises from the wide category range and subtle visual\ndifferences between categories. This oversight hampers the accuracy of\nmicro-action recognition. In this paper, we propose a novel Prototypical\nCalibrating Ambiguous Network (\\textbf{PCAN}) to unleash and mitigate the\nambiguity of MAR. \\textbf{Firstly}, we employ a hierarchical action-tree to\nidentify the ambiguous sample, categorizing them into distinct sets of\nambiguous samples of false negatives and false positives, considering both\nbody- and action-level categories. \\textbf{Secondly}, we implement an ambiguous\ncontrastive refinement module to calibrate these ambiguous samples by\nregulating the distance between ambiguous samples and their corresponding\nprototypes. This calibration process aims to pull false negative\n($\\mathbb{FN}$) samples closer to their respective prototypes and push false\npositive ($\\mathbb{FP}$) samples apart from their affiliated prototypes. In\naddition, we propose a new prototypical diversity amplification loss to\nstrengthen the model's capacity by amplifying the differences between different\nprototypes. \\textbf{Finally}, we propose a prototype-guided rectification to\nrectify prediction by incorporating the representability of prototypes.\nExtensive experiments conducted on the benchmark dataset demonstrate the\nsuperior performance of our method compared to existing approaches. The code is\navailable at https://github.com/kunli-cs/PCAN.\n","authors":["Kun Li","Dan Guo","Guoliang Chen","Chunxiao Fan","Jingyuan Xu","Zhiliang Wu","Hehe Fan","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14719v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14706v1","updated":"2024-12-19T10:19:43Z","published":"2024-12-19T10:19:43Z","title":"EnergyMoGen: Compositional Human Motion Generation with Energy-Based\n  Diffusion Model in Latent Space","summary":"  Diffusion models, particularly latent diffusion models, have demonstrated\nremarkable success in text-driven human motion generation. However, it remains\nchallenging for latent diffusion models to effectively compose multiple\nsemantic concepts into a single, coherent motion sequence. To address this\nissue, we propose EnergyMoGen, which includes two spectrums of Energy-Based\nModels: (1) We interpret the diffusion model as a latent-aware energy-based\nmodel that generates motions by composing a set of diffusion models in latent\nspace; (2) We introduce a semantic-aware energy model based on cross-attention,\nwhich enables semantic composition and adaptive gradient descent for text\nembeddings. To overcome the challenges of semantic inconsistency and motion\ndistortion across these two spectrums, we introduce Synergistic Energy Fusion.\nThis design allows the motion latent diffusion model to synthesize\nhigh-quality, complex motions by combining multiple energy terms corresponding\nto textual descriptions. Experiments show that our approach outperforms\nexisting state-of-the-art models on various motion generation tasks, including\ntext-to-motion generation, compositional motion generation, and multi-concept\nmotion generation. Additionally, we demonstrate that our method can be used to\nextend motion datasets and improve the text-to-motion task.\n","authors":["Jianrong Zhang","Hehe Fan","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2412.14706v1.pdf","comment":"Project page: https://jiro-zhang.github.io/EnergyMoGen/"},{"id":"http://arxiv.org/abs/2412.14705v1","updated":"2024-12-19T10:17:50Z","published":"2024-12-19T10:17:50Z","title":"Event-assisted 12-stop HDR Imaging of Dynamic Scene","summary":"  High dynamic range (HDR) imaging is a crucial task in computational\nphotography, which captures details across diverse lighting conditions.\nTraditional HDR fusion methods face limitations in dynamic scenes with extreme\nexposure differences, as aligning low dynamic range (LDR) frames becomes\nchallenging due to motion and brightness variation. In this work, we propose a\nnovel 12-stop HDR imaging approach for dynamic scenes, leveraging a dual-camera\nsystem with an event camera and an RGB camera. The event camera provides\ntemporally dense, high dynamic range signals that improve alignment between LDR\nframes with large exposure differences, reducing ghosting artifacts caused by\nmotion. Also, a real-world finetuning strategy is proposed to increase the\ngeneralization of alignment module on real-world events. Additionally, we\nintroduce a diffusion-based fusion module that incorporates image priors from\npre-trained diffusion models to address artifacts in high-contrast regions and\nminimize errors from the alignment process. To support this work, we developed\nthe ESHDR dataset, the first dataset for 12-stop HDR imaging with synchronized\nevent signals, and validated our approach on both simulated and real-world\ndata. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance, successfully extending HDR imaging to 12 stops in\ndynamic scenes.\n","authors":["Shi Guo","Zixuan Chen","Ziran Zhang","Yutian Chen","Gangwei Xu","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2412.14705v1.pdf","comment":"Project page:\n  https://openimaginglab.github.io/Event-Assisted-12stops-HDR/"},{"id":"http://arxiv.org/abs/2410.17098v2","updated":"2024-12-19T10:03:18Z","published":"2024-10-22T15:22:53Z","title":"Activity Recognition on Avatar-Anonymized Datasets with Masked\n  Differential Privacy","summary":"  Privacy-preserving computer vision is an important emerging problem in\nmachine learning and artificial intelligence. Prevalent methods tackling this\nproblem use differential privacy (DP) or obfuscation techniques to protect the\nprivacy of individuals. In both cases, the utility of the trained model is\nsacrificed heavily in this process. In this work, we present an anonymization\npipeline that replaces sensitive human subjects in video datasets with\nsynthetic avatars within context, employing a combined rendering and stable\ndiffusion-based strategy. Additionally we propose masked differential privacy\n({MaskDP}) to protect non-anonymized but privacy sensitive background\ninformation. MaskDP allows for controlling sensitive regions where differential\nprivacy is applied, in contrast to applying DP on the entire input. This\ncombined methodology provides strong privacy protection while minimizing the\nusual performance penalty of privacy preserving methods. Experiments on\nmultiple challenging action recognition datasets demonstrate that our proposed\ntechniques result in better utility-privacy trade-offs compared to standard\ndifferentially private training in the especially demanding $\\epsilon<1$\nregime.\n","authors":["David Schneider","Sina Sajadmanesh","Vikash Sehwag","Saquib Sarfraz","Rainer Stiefelhagen","Lingjuan Lyu","Vivek Sharma"],"pdf_url":"https://arxiv.org/pdf/2410.17098v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14692v1","updated":"2024-12-19T09:51:45Z","published":"2024-12-19T09:51:45Z","title":"Explicit Relational Reasoning Network for Scene Text Detection","summary":"  Connected component (CC) is a proper text shape representation that aligns\nwith human reading intuition. However, CC-based text detection methods have\nrecently faced a developmental bottleneck that their time-consuming\npost-processing is difficult to eliminate. To address this issue, we introduce\nan explicit relational reasoning network (ERRNet) to elegantly model the\ncomponent relationships without post-processing. Concretely, we first represent\neach text instance as multiple ordered text components, and then treat these\ncomponents as objects in sequential movement. In this way, scene text detection\ncan be innovatively viewed as a tracking problem. From this perspective, we\ndesign an end-to-end tracking decoder to achieve a CC-based method dispensing\nwith post-processing entirely. Additionally, we observe that there is an\ninconsistency between classification confidence and localization quality, so we\npropose a Polygon Monte-Carlo method to quickly and accurately evaluate the\nlocalization quality. Based on this, we introduce a position-supervised\nclassification loss to guide the task-aligned learning of ERRNet. Experiments\non challenging benchmarks demonstrate the effectiveness of our ERRNet. It\nconsistently achieves state-of-the-art accuracy while holding highly\ncompetitive inference speed.\n","authors":["Yuchen Su","Zhineng Chen","Yongkun Du","Zhilong Ji","Kai Hu","Jinfeng Bai","Xieping Gao"],"pdf_url":"https://arxiv.org/pdf/2412.14692v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14680v1","updated":"2024-12-19T09:32:53Z","published":"2024-12-19T09:32:53Z","title":"A Light-Weight Framework for Open-Set Object Detection with Decoupled\n  Feature Alignment in Joint Space","summary":"  Open-set object detection (OSOD) is highly desirable for robotic manipulation\nin unstructured environments. However, existing OSOD methods often fail to meet\nthe requirements of robotic applications due to their high computational burden\nand complex deployment. To address this issue, this paper proposes a\nlight-weight framework called Decoupled OSOD (DOSOD), which is a practical and\nhighly efficient solution to support real-time OSOD tasks in robotic systems.\nSpecifically, DOSOD builds upon the YOLO-World pipeline by integrating a\nvision-language model (VLM) with a detector. A Multilayer Perceptron (MLP)\nadaptor is developed to transform text embeddings extracted by the VLM into a\njoint space, within which the detector learns the region representations of\nclass-agnostic proposals. Cross-modality features are directly aligned in the\njoint space, avoiding the complex feature interactions and thereby improving\ncomputational efficiency. DOSOD operates like a traditional closed-set detector\nduring the testing phase, effectively bridging the gap between closed-set and\nopen-set detection. Compared to the baseline YOLO-World, the proposed DOSOD\nsignificantly enhances real-time performance while maintaining comparable\naccuracy. The slight DOSOD-S model achieves a Fixed AP of $26.7\\%$, compared to\n$26.2\\%$ for YOLO-World-v1-S and $22.7\\%$ for YOLO-World-v2-S, using similar\nbackbones on the LVIS minival dataset. Meanwhile, the FPS of DOSOD-S is\n$57.1\\%$ higher than YOLO-World-v1-S and $29.6\\%$ higher than YOLO-World-v2-S.\nMeanwhile, we demonstrate that the DOSOD model facilitates the deployment of\nedge devices. The codes and models are publicly available at\nhttps://github.com/D-Robotics-AI-Lab/DOSOD.\n","authors":["Yonghao He","Hu Su","Haiyong Yu","Cong Yang","Wei Sui","Cong Wang","Song Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14678v1","updated":"2024-12-19T09:31:53Z","published":"2024-12-19T09:31:53Z","title":"Efficient Few-Shot Neural Architecture Search by Counting the Number of\n  Nonlinear Functions","summary":"  Neural architecture search (NAS) enables finding the best-performing\narchitecture from a search space automatically. Most NAS methods exploit an\nover-parameterized network (i.e., a supernet) containing all possible\narchitectures (i.e., subnets) in the search space. However, the subnets that\nshare the same set of parameters are likely to have different characteristics,\ninterfering with each other during training. To address this, few-shot NAS\nmethods have been proposed that divide the space into a few subspaces and\nemploy a separate supernet for each subspace to limit the extent of weight\nsharing. They achieve state-of-the-art performance, but the computational cost\nincreases accordingly. We introduce in this paper a novel few-shot NAS method\nthat exploits the number of nonlinear functions to split the search space. To\nbe specific, our method divides the space such that each subspace consists of\nsubnets with the same number of nonlinear functions. Our splitting criterion is\nefficient, since it does not require comparing gradients of a supernet to split\nthe space. In addition, we have found that dividing the space allows us to\nreduce the channel dimensions required for each supernet, which enables\ntraining multiple supernets in an efficient manner. We also introduce a\nsupernet-balanced sampling (SBS) technique, sampling several subnets at each\ntraining step, to train different supernets evenly within a limited number of\ntraining steps. Extensive experiments on standard NAS benchmarks demonstrate\nthe effectiveness of our approach. Our code is available at\nhttps://cvlab.yonsei.ac.kr/projects/EFS-NAS.\n","authors":["Youngmin Oh","Hyunju Lee","Bumsub Ham"],"pdf_url":"https://arxiv.org/pdf/2412.14678v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14672v1","updated":"2024-12-19T09:24:10Z","published":"2024-12-19T09:24:10Z","title":"FiVL: A Framework for Improved Vision-Language Alignment","summary":"  Large Vision Language Models (LVLMs) have achieved significant progress in\nintegrating visual and textual inputs for multimodal reasoning. However, a\nrecurring challenge is ensuring these models utilize visual information as\neffectively as linguistic content when both modalities are necessary to\nformulate an accurate answer. We hypothesize that hallucinations arise due to\nthe lack of effective visual grounding in current LVLMs. This issue extends to\nvision-language benchmarks, where it is difficult to make the image\nindispensable for accurate answer generation, particularly in vision\nquestion-answering tasks. In this work, we introduce FiVL, a novel method for\nconstructing datasets designed to train LVLMs for enhanced visual grounding and\nto evaluate their effectiveness in achieving it. These datasets can be utilized\nfor both training and assessing an LVLM's ability to use image content as\nsubstantive evidence rather than relying solely on linguistic priors, providing\ninsights into the model's reliance on visual information. To demonstrate the\nutility of our dataset, we introduce an innovative training task that\noutperforms baselines alongside a validation method and application for\nexplainability. The code is available at https://github.com/IntelLabs/fivl.\n","authors":["Estelle Aflalo","Gabriela Ben Melech Stan","Tiep Le","Man Luo","Shachar Rosenman","Sayak Paul","Shao-Yen Tseng","Vasudev Lal"],"pdf_url":"https://arxiv.org/pdf/2412.14672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14671v1","updated":"2024-12-19T09:22:19Z","published":"2024-12-19T09:22:19Z","title":"MUSTER: Longitudinal Deformable Registration by Composition of\n  Consecutive Deformations","summary":"  Longitudinal imaging allows for the study of structural changes over time.\nOne approach to detecting such changes is by non-linear image registration.\nThis study introduces Multi-Session Temporal Registration (MUSTER), a novel\nmethod that facilitates longitudinal analysis of changes in extended series of\nmedical images. MUSTER improves upon conventional pairwise registration by\nincorporating more than two imaging sessions to recover longitudinal\ndeformations. Longitudinal analysis at a voxel-level is challenging due to\neffects of a changing image contrast as well as instrumental and environmental\nsources of bias between sessions. We show that local normalized\ncross-correlation as an image similarity metric leads to biased results and\npropose a robust alternative. We test the performance of MUSTER on a synthetic\nmulti-site, multi-session neuroimaging dataset and show that, in various\nscenarios, using MUSTER significantly enhances the estimated deformations\nrelative to pairwise registration. Additionally, we apply MUSTER on a sample of\nolder adults from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study.\nThe results show that MUSTER can effectively identify patterns of\nneuro-degeneration from T1-weighted images and that these changes correlate\nwith changes in cognition, matching the performance of state of the art\nsegmentation methods. By leveraging GPU acceleration, MUSTER efficiently\nhandles large datasets, making it feasible also in situations with limited\ncomputational resources.\n","authors":["Edvard O. S. Grødem","Donatas Sederevičius","Esten H. Leonardsen","Bradley J. MacIntosh","Atle Bjørnerud","Till Schellhorn","Øystein Sørensen","Inge Amlien","Pablo F. Garrido","Anders M. Fjell"],"pdf_url":"https://arxiv.org/pdf/2412.14671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01220v3","updated":"2024-12-19T09:16:19Z","published":"2024-07-01T12:07:26Z","title":"Fast and Efficient: Mask Neural Fields for 3D Scene Segmentation","summary":"  Understanding 3D scenes is a crucial challenge in computer vision research\nwith applications spanning multiple domains. Recent advancements in distilling\n2D vision-language foundation models into neural fields, like NeRF and 3DGS,\nenable open-vocabulary segmentation of 3D scenes from 2D multi-view images\nwithout the need for precise 3D annotations. However, while effective, these\nmethods typically rely on the per-pixel distillation of high-dimensional CLIP\nfeatures, introducing ambiguity and necessitating complex regularization\nstrategies, which adds inefficiency during training. This paper presents\nMaskField, which enables efficient 3D open-vocabulary segmentation with neural\nfields from a novel perspective. Unlike previous methods, MaskField decomposes\nthe distillation of mask and semantic features from foundation models by\nformulating a mask feature field and queries. MaskField overcomes ambiguous\nobject boundaries by naturally introducing SAM segmented object shapes without\nextra regularization during training. By circumventing the direct handling of\ndense high-dimensional CLIP features during training, MaskField is particularly\ncompatible with explicit scene representations like 3DGS. Our extensive\nexperiments show that MaskField not only surpasses prior state-of-the-art\nmethods but also achieves remarkably fast convergence. We hope that MaskField\nwill inspire further exploration into how neural fields can be trained to\ncomprehend 3D scenes from 2D models.\n","authors":["Zihan Gao","Lingling Li","Licheng Jiao","Fang Liu","Xu Liu","Wenping Ma","Yuwei Guo","Shuyuan Yang"],"pdf_url":"https://arxiv.org/pdf/2407.01220v3.pdf","comment":"15 pages, 9 figures, Code:https://github.com/keloee/MaskField"},{"id":"http://arxiv.org/abs/2412.14660v1","updated":"2024-12-19T09:10:07Z","published":"2024-12-19T09:10:07Z","title":"Unveiling Uncertainty: A Deep Dive into Calibration and Performance of\n  Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) combine visual and textual data for\ntasks such as image captioning and visual question answering. Proper\nuncertainty calibration is crucial, yet challenging, for reliable use in areas\nlike healthcare and autonomous driving. This paper investigates representative\nMLLMs, focusing on their calibration across various scenarios, including before\nand after visual fine-tuning, as well as before and after multimodal training\nof the base LLMs. We observed miscalibration in their performance, and at the\nsame time, no significant differences in calibration across these scenarios. We\nalso highlight how uncertainty differs between text and images and how their\nintegration affects overall uncertainty. To better understand MLLMs'\nmiscalibration and their ability to self-assess uncertainty, we construct the\nIDK (I don't know) dataset, which is key to evaluating how they handle\nunknowns. Our findings reveal that MLLMs tend to give answers rather than admit\nuncertainty, but this self-assessment improves with proper prompt adjustments.\nFinally, to calibrate MLLMs and enhance model reliability, we propose\ntechniques such as temperature scaling and iterative prompt optimization. Our\nresults provide insights into improving MLLMs for effective and responsible\ndeployment in multimodal applications. Code and IDK dataset:\n\\href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}.\n","authors":["Zijun Chen","Wenbo Hu","Guande He","Zhijie Deng","Zheng Zhang","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2412.14660v1.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2403.15031v2","updated":"2024-12-19T09:00:34Z","published":"2024-03-22T08:26:31Z","title":"Image Classification with Rotation-Invariant Variational Quantum\n  Circuits","summary":"  Variational quantum algorithms are gaining attention as an early application\nof Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of\nvariational methods lies in the phenomenon of Barren Plateaus, present in the\noptimization of variational parameters. Adding geometric inductive bias to the\nquantum models has been proposed as a potential solution to mitigate this\nproblem, leading to a new field called Geometric Quantum Machine Learning. In\nthis work, an equivariant architecture for variational quantum classifiers is\nintroduced to create a label-invariant model for image classification with\n$C_4$ rotational label symmetry. The equivariant circuit is benchmarked against\ntwo different architectures, and it is experimentally observed that the\ngeometric approach boosts the model's performance. Finally, a classical\nequivariant convolution operation is proposed to extend the quantum model for\nthe processing of larger images, employing the resources available in NISQ\ndevices.\n","authors":["Paul San Sebastian","Mikel Cañizo","Román Orús"],"pdf_url":"https://arxiv.org/pdf/2403.15031v2.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.00143v2","updated":"2024-12-19T08:58:15Z","published":"2024-05-31T19:13:09Z","title":"Diversifying Query: Region-Guided Transformer for Temporal Sentence\n  Grounding","summary":"  Temporal sentence grounding is a challenging task that aims to localize the\nmoment spans relevant to a language description. Although recent DETR-based\nmodels have achieved notable progress by leveraging multiple learnable moment\nqueries, they suffer from overlapped and redundant proposals, leading to\ninaccurate predictions. We attribute this limitation to the lack of\ntask-related guidance for the learnable queries to serve a specific mode.\nFurthermore, the complex solution space generated by variable and\nopen-vocabulary language descriptions complicates optimization, making it\nharder for learnable queries to distinguish each other adaptively. To tackle\nthis limitation, we present a Region-Guided TRansformer (RGTR) for temporal\nsentence grounding, which diversifies moment queries to eliminate overlapped\nand redundant predictions. Instead of using learnable queries, RGTR adopts a\nset of anchor pairs as moment queries to introduce explicit regional guidance.\nEach anchor pair takes charge of moment prediction for a specific temporal\nregion, which reduces the optimization difficulty and ensures the diversity of\nthe final predictions. In addition, we design an IoU-aware scoring head to\nimprove proposal quality. Extensive experiments demonstrate the effectiveness\nof RGTR, outperforming state-of-the-art methods on QVHighlights, Charades-STA\nand TACoS datasets. Codes are available at https://github.com/TensorsSun/RGTR\n","authors":["Xiaolong Sun","Liushuai Shi","Le Wang","Sanping Zhou","Kun Xia","Yabing Wang","Gang Hua"],"pdf_url":"https://arxiv.org/pdf/2406.00143v2.pdf","comment":"Accepted by AAAI-25. Code is available at\n  https://github.com/TensorsSun/RGTR"},{"id":"http://arxiv.org/abs/2412.11953v2","updated":"2024-12-19T08:52:56Z","published":"2024-12-16T16:37:03Z","title":"Reliable Breast Cancer Molecular Subtype Prediction based on\n  uncertainty-aware Bayesian Deep Learning by Mammography","summary":"  Breast cancer is a heterogeneous disease with different molecular subtypes,\nclinical behavior, treatment responses as well as survival outcomes. The\ndevelopment of a reliable, accurate, available and inexpensive method to\npredict the molecular subtypes using medical images plays an important role in\nthe diagnosis and prognosis of breast cancer. Recently, deep learning methods\nhave shown good performance in the breast cancer classification tasks using\nvarious medical images. Despite all that success, classical deep learning\ncannot deliver the predictive uncertainty. The uncertainty represents the\nvalidity of the predictions. Therefore, the high predicted uncertainty might\ncause a negative effect in the accurate diagnosis of breast cancer molecular\nsubtypes. To overcome this, uncertainty quantification methods are used to\ndetermine the predictive uncertainty. Accordingly, in this study, we proposed\nan uncertainty-aware Bayesian deep learning model using the full mammogram\nimages. In addition, to increase the performance of the multi-class molecular\nsubtype classification task, we proposed a novel hierarchical classification\nstrategy, named the two-stage classification strategy. The separate AUC of the\nproposed model for each subtype was 0.71, 0.75 and 0.86 for HER2-enriched,\nluminal and triple-negative classes, respectively. The proposed model not only\nhas a comparable performance to other studies in the field of breast cancer\nmolecular subtypes prediction, even using full mammography images, but it is\nalso more reliable, due to quantify the predictive uncertainty.\n","authors":["Mohaddeseh Chegini","Ali Mahloojifar"],"pdf_url":"https://arxiv.org/pdf/2412.11953v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14643v1","updated":"2024-12-19T08:51:57Z","published":"2024-12-19T08:51:57Z","title":"RefHCM: A Unified Model for Referring Perceptions in Human-Centric\n  Scenarios","summary":"  Human-centric perceptions play a crucial role in real-world applications.\nWhile recent human-centric works have achieved impressive progress, these\nefforts are often constrained to the visual domain and lack interaction with\nhuman instructions, limiting their applicability in broader scenarios such as\nchatbots and sports analysis. This paper introduces Referring Human\nPerceptions, where a referring prompt specifies the person of interest in an\nimage. To tackle the new task, we propose RefHCM (Referring Human-Centric\nModel), a unified framework to integrate a wide range of human-centric\nreferring tasks. Specifically, RefHCM employs sequence mergers to convert raw\nmultimodal data -- including images, text, coordinates, and parsing maps --\ninto semantic tokens. This standardized representation enables RefHCM to\nreformulate diverse human-centric referring tasks into a sequence-to-sequence\nparadigm, solved using a plain encoder-decoder transformer architecture.\nBenefiting from a unified learning strategy, RefHCM effectively facilitates\nknowledge transfer across tasks and exhibits unforeseen capabilities in\nhandling complex reasoning. This work represents the first attempt to address\nreferring human perceptions with a general-purpose framework, while\nsimultaneously establishing a corresponding benchmark that sets new standards\nfor the field. Extensive experiments showcase RefHCM's competitive and even\nsuperior performance across multiple human-centric referring tasks. The code\nand data are publicly at https://github.com/JJJYmmm/RefHCM.\n","authors":["Jie Huang","Ruibing Hou","Jiahe Zhao","Hong Chang","Shiguang Shan"],"pdf_url":"https://arxiv.org/pdf/2412.14643v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2412.14640v1","updated":"2024-12-19T08:51:01Z","published":"2024-12-19T08:51:01Z","title":"Adaptive Prompt Tuning: Vision Guided Prompt Tuning with Cross-Attention\n  for Fine-Grained Few-Shot Learning","summary":"  Few-shot, fine-grained classification in computer vision poses significant\nchallenges due to the need to differentiate subtle class distinctions with\nlimited data. This paper presents a novel method that enhances the Contrastive\nLanguage-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided\nby real-time visual inputs. Unlike existing techniques such as Context\nOptimization (CoOp) and Visual Prompt Tuning (VPT), which are constrained by\nstatic prompts or visual token reliance, the proposed approach leverages a\ncross-attention mechanism to dynamically refine text prompts for the image at\nhand. This enables an image-specific alignment of textual features with image\npatches extracted from the Vision Transformer, making the model more effective\nfor datasets with high intra-class variance and low inter-class differences.\nThe method is evaluated on several datasets, including CUBirds, Oxford Flowers,\nand FGVC Aircraft, showing significant performance gains over static prompt\ntuning approaches. To ensure these performance gains translate into trustworthy\npredictions, we integrate Monte-Carlo Dropout in our approach to improve the\nreliability of the model predictions and uncertainty estimates. This\nintegration provides valuable insights into the model's predictive confidence,\nhelping to identify when predictions can be trusted and when additional\nverification is necessary. This dynamic approach offers a robust solution,\nadvancing the state-of-the-art for few-shot fine-grained classification.\n","authors":["Eric Brouwer","Jan Erik van Woerden","Gertjan Burghouts","Matias Valedenegro-Toro","Marco Zullich"],"pdf_url":"https://arxiv.org/pdf/2412.14640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18459v3","updated":"2024-12-19T08:47:07Z","published":"2024-04-29T06:35:34Z","title":"Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in\n  the Wild","summary":"  Large language models have evolved data-efficient generalists, benefiting\nfrom the universal language interface and large-scale pre-training. However,\nconstructing a data-efficient generalist for dense visual prediction presents a\ndistinct challenge due to the variation in label structures across different\ntasks. Consequently, generalization to unseen dense prediction tasks in the\nlow-data regime is not straightforward and has received less attention from\nprevious vision generalists. In this study, we explore a universal model that\ncan flexibly adapt to unseen dense label structures with a few examples,\nenabling it to serve as a data-efficient vision generalist in diverse\nreal-world scenarios. To this end, we base our method on a powerful\nmeta-learning framework and explore several axes to improve its performance and\nversatility for real-world problems, such as flexible adaptation mechanisms and\nscalability. We evaluate our model across a spectrum of unseen real-world\nscenarios where low-shot learning is desirable, including video, 3D, medical,\nbiological, and user-interactive tasks. Equipped with a generic architecture\nand an effective adaptation mechanism, our model flexibly adapts to all of\nthese tasks with at most 50 labeled images, showcasing a significant\nadvancement over existing data-efficient generalist approaches. Codes are\navailable at https://github.com/GitGyun/chameleon.\n","authors":["Donggyun Kim","Seongwoong Cho","Semin Kim","Chong Luo","Seunghoon Hong"],"pdf_url":"https://arxiv.org/pdf/2404.18459v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12974v3","updated":"2024-12-19T08:41:19Z","published":"2024-12-17T14:56:59Z","title":"Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential\n  via Self-Attention Redirection Guidance","summary":"  Recently, diffusion models have emerged as promising newcomers in the field\nof generative models, shining brightly in image generation. However, when\nemployed for object removal tasks, they still encounter issues such as\ngenerating random artifacts and the incapacity to repaint foreground object\nareas with appropriate content after removal. To tackle these problems, we\npropose Attentive Eraser, a tuning-free method to empower pre-trained diffusion\nmodels for stable and effective object removal. Firstly, in light of the\nobservation that the self-attention maps influence the structure and shape\ndetails of the generated images, we propose Attention Activation and\nSuppression (ASS), which re-engineers the self-attention mechanism within the\npre-trained diffusion models based on the given mask, thereby prioritizing the\nbackground over the foreground object during the reverse generation process.\nMoreover, we introduce Self-Attention Redirection Guidance (SARG), which\nutilizes the self-attention redirected by ASS to guide the generation process,\neffectively removing foreground objects within the mask while simultaneously\ngenerating content that is both plausible and coherent. Experiments demonstrate\nthe stability and effectiveness of Attentive Eraser in object removal across a\nvariety of pre-trained diffusion models, outperforming even training-based\nmethods. Furthermore, Attentive Eraser can be implemented in various diffusion\nmodel architectures and checkpoints, enabling excellent scalability. Code is\navailable at https://github.com/Anonym0u3/AttentiveEraser.\n","authors":["Wenhao Sun","Benlei Cui","Xue-Mei Dong","Jingqun Tang"],"pdf_url":"https://arxiv.org/pdf/2412.12974v3.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14633v1","updated":"2024-12-19T08:38:59Z","published":"2024-12-19T08:38:59Z","title":"Progressive Fine-to-Coarse Reconstruction for Accurate Low-Bit\n  Post-Training Quantization in Vision Transformers","summary":"  Due to its efficiency, Post-Training Quantization (PTQ) has been widely\nadopted for compressing Vision Transformers (ViTs). However, when quantized\ninto low-bit representations, there is often a significant performance drop\ncompared to their full-precision counterparts. To address this issue,\nreconstruction methods have been incorporated into the PTQ framework to improve\nperformance in low-bit quantization settings. Nevertheless, existing related\nmethods predefine the reconstruction granularity and seldom explore the\nprogressive relationships between different reconstruction granularities, which\nleads to sub-optimal quantization results in ViTs. To this end, in this paper,\nwe propose a Progressive Fine-to-Coarse Reconstruction (PFCR) method for\naccurate PTQ, which significantly improves the performance of low-bit quantized\nvision transformers. Specifically, we define multi-head self-attention and\nmulti-layer perceptron modules along with their shortcuts as the finest\nreconstruction units. After reconstructing these two fine-grained units, we\ncombine them to form coarser blocks and reconstruct them at a coarser\ngranularity level. We iteratively perform this combination and reconstruction\nprocess, achieving progressive fine-to-coarse reconstruction. Additionally, we\nintroduce a Progressive Optimization Strategy (POS) for PFCR to alleviate the\ndifficulty of training, thereby further enhancing model performance.\nExperimental results on the ImageNet dataset demonstrate that our proposed\nmethod achieves the best Top-1 accuracy among state-of-the-art methods,\nparticularly attaining 75.61% for 3-bit quantized ViT-B in PTQ. Besides,\nquantization results on the COCO dataset reveal the effectiveness and\ngeneralization of our proposed method on other computer vision tasks like\nobject detection and instance segmentation.\n","authors":["Rui Ding","Liang Yong","Sihuan Zhao","Jing Nie","Lihui Chen","Haijun Liu","Xichuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14631v1","updated":"2024-12-19T08:36:32Z","published":"2024-12-19T08:36:32Z","title":"Review of Fruit Tree Image Segmentation","summary":"  Fruit tree image segmentation is an essential problem in automating a variety\nof agricultural tasks such as phenotyping, harvesting, spraying, and pruning.\nMany research papers have proposed a diverse spectrum of solutions suitable to\nspecific tasks and environments. The review scope of this paper is confined to\nthe front views of fruit trees and based on 158 relevant papers collected using\na newly designed crawling review method. These papers are systematically\nreviewed based on a taxonomy that sequentially considers the method, image,\ntask, and fruit. This taxonomy will assist readers to intuitively grasp the big\npicture of these research activities. Our review reveals that the most\nnoticeable deficiency of the previous studies was the lack of a versatile\ndataset and segmentation model that could be applied to a variety of tasks and\nenvironments. Six important future research tasks are suggested, with the\nexpectation that these will pave the way to building a versatile tree\nsegmentation module.\n","authors":["Il-Seok Oh"],"pdf_url":"https://arxiv.org/pdf/2412.14631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14630v1","updated":"2024-12-19T08:33:33Z","published":"2024-12-19T08:33:33Z","title":"Unified Image Restoration and Enhancement: Degradation Calibrated Cycle\n  Reconstruction Diffusion Model","summary":"  Image restoration and enhancement are pivotal for numerous computer vision\napplications, yet unifying these tasks efficiently remains a significant\nchallenge. Inspired by the iterative refinement capabilities of diffusion\nmodels, we propose CycleRDM, a novel framework designed to unify restoration\nand enhancement tasks while achieving high-quality mapping. Specifically,\nCycleRDM first learns the mapping relationships among the degraded domain, the\nrough normal domain, and the normal domain through a two-stage diffusion\ninference process. Subsequently, we transfer the final calibration process to\nthe wavelet low-frequency domain using discrete wavelet transform, performing\nfine-grained calibration from a frequency domain perspective by leveraging\ntask-specific frequency spaces. To improve restoration quality, we design a\nfeature gain module for the decomposed wavelet high-frequency domain to\neliminate redundant features. Additionally, we employ multimodal textual\nprompts and Fourier transform to drive stable denoising and reduce randomness\nduring the inference process. After extensive validation, CycleRDM can be\neffectively generalized to a wide range of image restoration and enhancement\ntasks while requiring only a small number of training samples to be\nsignificantly superior on various benchmarks of reconstruction quality and\nperceptual quality. The source code will be available at\nhttps://github.com/hejh8/CycleRDM.\n","authors":["Minglong Xue","Jinhong He","Shivakumara Palaiahnakote","Mingliang Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11216v2","updated":"2024-12-19T08:32:20Z","published":"2024-12-15T15:13:14Z","title":"Distribution-Consistency-Guided Multi-modal Hashing","summary":"  Multi-modal hashing methods have gained popularity due to their fast speed\nand low storage requirements. Among them, the supervised methods demonstrate\nbetter performance by utilizing labels as supervisory signals compared with\nunsupervised methods. Currently, for almost all supervised multi-modal hashing\nmethods, there is a hidden assumption that training sets have no noisy labels.\nHowever, labels are often annotated incorrectly due to manual labeling in\nreal-world scenarios, which will greatly harm the retrieval performance. To\naddress this issue, we first discover a significant distribution consistency\npattern through experiments, i.e., the 1-0 distribution of the presence or\nabsence of each category in the label is consistent with the high-low\ndistribution of similarity scores of the hash codes relative to category\ncenters. Then, inspired by this pattern, we propose a novel\nDistribution-Consistency-Guided Multi-modal Hashing (DCGMH), which aims to\nfilter and reconstruct noisy labels to enhance retrieval performance.\nSpecifically, the proposed method first randomly initializes several category\ncenters, which are used to compute the high-low distribution of similarity\nscores; Noisy and clean labels are then separately filtered out via the\ndiscovered distribution consistency pattern to mitigate the impact of noisy\nlabels; Subsequently, a correction strategy, which is indirectly designed via\nthe distribution consistency pattern, is applied to the filtered noisy labels,\ncorrecting high-confidence ones while treating low-confidence ones as unlabeled\nfor unsupervised learning, thereby further enhancing the model's performance.\nExtensive experiments on three widely used datasets demonstrate the superiority\nof the proposed method compared to state-of-the-art baselines in multi-modal\nretrieval tasks. The code is available at\nhttps://github.com/LiuJinyu1229/DCGMH.\n","authors":["Jin-Yu Liu","Xian-Ling Mao","Tian-Yi Che","Rong-Cheng Tu"],"pdf_url":"https://arxiv.org/pdf/2412.11216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14629v1","updated":"2024-12-19T08:31:42Z","published":"2024-12-19T08:31:42Z","title":"Robust PCA Based on Adaptive Weighted Least Squares and Low-Rank Matrix\n  Factorization","summary":"  Robust Principal Component Analysis (RPCA) is a fundamental technique for\ndecomposing data into low-rank and sparse components, which plays a critical\nrole for applications such as image processing and anomaly detection.\nTraditional RPCA methods commonly use $\\ell_1$ norm regularization to enforce\nsparsity, but this approach can introduce bias and result in suboptimal\nestimates, particularly in the presence of significant noise or outliers.\nNon-convex regularization methods have been proposed to mitigate these\nchallenges, but they tend to be complex to optimize and sensitive to initial\nconditions, leading to potential instability in solutions. To overcome these\nchallenges, in this paper, we propose a novel RPCA model that integrates\nadaptive weighted least squares (AWLS) and low-rank matrix factorization\n(LRMF). The model employs a {self-attention-inspired} mechanism in its weight\nupdate process, allowing the weight matrix to dynamically adjust and emphasize\nsignificant components during each iteration. By employing a weighted F-norm\nfor the sparse component, our method effectively reduces bias while simplifying\nthe computational process compared to traditional $\\ell_1$-norm-based methods.\nWe use an alternating minimization algorithm, where each subproblem has an\nexplicit solution, thereby improving computational efficiency. Despite its\nsimplicity, numerical experiments demonstrate that our method outperforms\nexisting non-convex regularization approaches, offering superior performance\nand stability, as well as enhanced accuracy and robustness in practical\napplications.\n","authors":["Kexin Li","You-wei Wen","Xu Xiao","Mingchao Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.14629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14628v1","updated":"2024-12-19T08:30:54Z","published":"2024-12-19T08:30:54Z","title":"Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models","summary":"  Diffusion Models (DM) have democratized AI image generation through an\niterative denoising process. Quantization is a major technique to alleviate the\ninference cost and reduce the size of DM denoiser networks. However, as\ndenoisers evolve from variants of convolutional U-Nets toward newer Transformer\narchitectures, it is of growing importance to understand the quantization\nsensitivity of different weight layers, operations and architecture types to\nperformance. In this work, we address this challenge with Qua$^2$SeDiMo, a\nmixed-precision Post-Training Quantization framework that generates explainable\ninsights on the cost-effectiveness of various model weight quantization methods\nfor different denoiser operation types and block structures. We leverage these\ninsights to make high-quality mixed-precision quantization decisions for a\nmyriad of diffusion models ranging from foundational U-Nets to state-of-the-art\nTransformers. As a result, Qua$^2$SeDiMo can construct 3.4-bit, 3.9-bit,\n3.65-bit and 3.7-bit weight quantization on PixArt-${\\alpha}$,\nPixArt-${\\Sigma}$, Hunyuan-DiT and SDXL, respectively. We further pair our\nweight-quantization configurations with 6-bit activation quantization and\noutperform existing approaches in terms of quantitative metrics and generative\nimage quality.\n","authors":["Keith G. Mills","Mohammad Salameh","Ruichen Chen","Negar Hassanpour","Wei Lu","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2412.14628v1.pdf","comment":"AAAI 2025; version includes supplementary material; 22 Pages, 18\n  Figures, 8 Tables"},{"id":"http://arxiv.org/abs/2412.14623v1","updated":"2024-12-19T08:21:28Z","published":"2024-12-19T08:21:28Z","title":"FRIDAY: Mitigating Unintentional Facial Identity in Deepfake Detectors\n  Guided by Facial Recognizers","summary":"  Previous Deepfake detection methods perform well within their training\ndomains, but their effectiveness diminishes significantly with new synthesis\ntechniques. Recent studies have revealed that detection models often create\ndecision boundaries based on facial identity rather than synthetic artifacts,\nresulting in poor performance on cross-domain datasets. To address this\nlimitation, we propose Facial Recognition Identity Attenuation (FRIDAY), a\nnovel training method that mitigates facial identity influence using a face\nrecognizer. Specifically, we first train a face recognizer using the same\nbackbone as the Deepfake detector. The recognizer is then frozen and employed\nduring the detector's training to reduce facial identity information. This is\nachieved by feeding input images into both the recognizer and the detector, and\nminimizing the similarity of their feature embeddings through our Facial\nIdentity Attenuating loss. This process encourages the detector to generate\nembeddings distinct from the recognizer, effectively reducing the impact of\nfacial identity. Extensive experiments demonstrate that our approach\nsignificantly enhances detection performance on both in-domain and cross-domain\ndatasets.\n","authors":["Younhun Kim","Myung-Joon Kwon","Wonjun Lee","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2412.14623v1.pdf","comment":"5 pages, 4 figures. In 2024 IEEE International Conference on Visual\n  Communications and Image Processing (VCIP) Oral"},{"id":"http://arxiv.org/abs/2409.17671v3","updated":"2024-12-19T08:19:41Z","published":"2024-09-26T09:30:37Z","title":"Leveraging Anthropometric Measurements to Improve Human Mesh Estimation\n  and Ensure Consistent Body Shapes","summary":"  The basic body shape (i.e., the body shape in T-pose) of a person does not\nchange within a single video. However, most SOTA human mesh estimation (HME)\nmodels output a slightly different, thus inconsistent basic body shape for each\nvideo frame. Furthermore, we find that SOTA 3D human pose estimation (HPE)\nmodels outperform HME models regarding the precision of the estimated 3D\nkeypoint positions. We solve the problem of inconsistent body shapes by\nleveraging anthropometric measurements like taken by tailors from humans. We\ncreate a model called A2B that converts given anthropometric measurements to\nbasic body shape parameters of human mesh models. We obtain superior and\nconsistent human meshes by combining the A2B model results with the keypoints\nof 3D HPE models using inverse kinematics. We evaluate our approach on\nchallenging datasets like ASPset or fit3D, where we can lower the MPJPE by over\n30 mm compared to SOTA HME models. Further, replacing estimates of the body\nshape parameters from existing HME models with A2B results not only increases\nthe performance of these HME models, but also guarantees consistent body\nshapes.\n","authors":["Katja Ludwig","Julian Lorenz","Daniel Kienzle","Tuan Bui","Rainer Lienhart"],"pdf_url":"https://arxiv.org/pdf/2409.17671v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14619v1","updated":"2024-12-19T08:11:42Z","published":"2024-12-19T08:11:42Z","title":"Pitfalls of topology-aware image segmentation","summary":"  Topological correctness, i.e., the preservation of structural integrity and\nspecific characteristics of shape, is a fundamental requirement for medical\nimaging tasks, such as neuron or vessel segmentation. Despite the recent surge\nin topology-aware methods addressing this challenge, their real-world\napplicability is hindered by flawed benchmarking practices. In this paper, we\nidentify critical pitfalls in model evaluation that include inadequate\nconnectivity choices, overlooked topological artifacts in ground truth\nannotations, and inappropriate use of evaluation metrics. Through detailed\nempirical analysis, we uncover these issues' profound impact on the evaluation\nand ranking of segmentation methods. Drawing from our findings, we propose a\nset of actionable recommendations to establish fair and robust evaluation\nstandards for topology-aware medical image segmentation methods.\n","authors":["Alexander H. Berger","Laurin Lux","Alexander Weers","Martin Menten","Daniel Rueckert","Johannes C. Paetzold"],"pdf_url":"https://arxiv.org/pdf/2412.14619v1.pdf","comment":"Code is available at\n  https://github.com/AlexanderHBerger/topo-pitfalls"},{"id":"http://arxiv.org/abs/2412.14613v1","updated":"2024-12-19T08:03:16Z","published":"2024-12-19T08:03:16Z","title":"HarmonicEval: Multi-modal, Multi-task, Multi-criteria Automatic\n  Evaluation Using a Vision Language Model","summary":"  Vision-language models (VLMs) have shown impressive abilities in text and\nimage understanding. However, existing metrics for evaluating the text\ngenerated by VLMs focus exclusively on overall quality, leading to two\nlimitations: 1) it is challenging to identify which aspects of the text need\nimprovement from the overall score; 2) metrics may overlook specific evaluation\ncriteria when predicting an overall score. To address these limitations, we\npropose HarmonicEval, a reference-free evaluation metric that aggregates\ncriterion-wise scores to produce the overall score in a bottom-up manner.\nFurthermore, we construct the Multi-task Multi-criteria Human Evaluation (MMHE)\ndataset, which comprises 18,000 expert human judgments across four\nvision-language tasks. Our experiments demonstrate that HarmonicEval achieves\nhigher correlations with human judgments than conventional metrics while\nproviding numerical scores for each criterion.\n","authors":["Masanari Ohi","Masahiro Kaneko","Naoaki Okazaki","Nakamasa Inoue"],"pdf_url":"https://arxiv.org/pdf/2412.14613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19101v4","updated":"2024-12-19T08:00:44Z","published":"2024-06-27T11:28:36Z","title":"DocKylin: A Large Multimodal Model for Visual Document Understanding\n  with Efficient Visual Slimming","summary":"  Current multimodal large language models (MLLMs) face significant challenges\nin visual document understanding (VDU) tasks due to the high resolution, dense\ntext, and complex layouts typical of document images. These characteristics\ndemand a high level of detail perception ability from MLLMs. While increasing\ninput resolution improves detail perception capability, it also leads to longer\nsequences of visual tokens, increasing computational costs and straining the\nmodels' ability to handle long contexts. To address these challenges, we\nintroduce DocKylin, a document-centric MLLM that performs visual content\nslimming at both the pixel and token levels, thereby reducing token sequence\nlength in VDU scenarios. We introduce an Adaptive Pixel Slimming (APS)\npreprocessing module to perform pixel-level slimming, increasing the proportion\nof informative pixels. Moreover, we propose a novel Dynamic Token Slimming\n(DTS) module to conduct token-level slimming, filtering essential tokens and\nremoving others to adaptively create a more compact visual sequence.\nExperiments demonstrate DocKylin's promising performance across various VDU\nbenchmarks and the effectiveness of each component.\n","authors":["Jiaxin Zhang","Wentao Yang","Songxuan Lai","Zecheng Xie","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2406.19101v4.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14603v1","updated":"2024-12-19T07:49:40Z","published":"2024-12-19T07:49:40Z","title":"Successive optimization of optics and post-processing with\n  differentiable coherent PSF operator and field information","summary":"  Recently, the joint design of optical systems and downstream algorithms is\nshowing significant potential. However, existing rays-described methods are\nlimited to optimizing geometric degradation, making it difficult to fully\nrepresent the optical characteristics of complex, miniaturized lenses\nconstrained by wavefront aberration or diffraction effects. In this work, we\nintroduce a precise optical simulation model, and every operation in pipeline\nis differentiable. This model employs a novel initial value strategy to enhance\nthe reliability of intersection calculation on high aspherics. Moreover, it\nutilizes a differential operator to reduce memory consumption during coherent\npoint spread function calculations. To efficiently address various degradation,\nwe design a joint optimization procedure that leverages field information.\nGuided by a general restoration network, the proposed method not only enhances\nthe image quality, but also successively improves the optical performance\nacross multiple lenses that are already in professional level. This joint\noptimization pipeline offers innovative insights into the practical design of\nsophisticated optical systems and post-processing algorithms. The source code\nwill be made publicly available at\nhttps://github.com/Zrr-ZJU/Successive-optimization\n","authors":["Zheng Ren","Jingwen Zhou","Wenguan Zhang","Jiapu Yan","Bingkun Chen","Huajun Feng","Shiqi Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14598v1","updated":"2024-12-19T07:39:06Z","published":"2024-12-19T07:39:06Z","title":"Can We Get Rid of Handcrafted Feature Extractors? SparseViT:\n  Nonsemantics-Centered, Parameter-Efficient Image Manipulation Localization\n  Through Spare-Coding Transformer","summary":"  Non-semantic features or semantic-agnostic features, which are irrelevant to\nimage context but sensitive to image manipulations, are recognized as\nevidential to Image Manipulation Localization (IML). Since manual labels are\nimpossible, existing works rely on handcrafted methods to extract non-semantic\nfeatures. Handcrafted non-semantic features jeopardize IML model's\ngeneralization ability in unseen or complex scenarios. Therefore, for IML, the\nelephant in the room is: How to adaptively extract non-semantic features?\nNon-semantic features are context-irrelevant and manipulation-sensitive. That\nis, within an image, they are consistent across patches unless manipulation\noccurs. Then, spare and discrete interactions among image patches are\nsufficient for extracting non-semantic features. However, image semantics vary\ndrastically on different patches, requiring dense and continuous interactions\namong image patches for learning semantic representations. Hence, in this\npaper, we propose a Sparse Vision Transformer (SparseViT), which reformulates\nthe dense, global self-attention in ViT into a sparse, discrete manner. Such\nsparse self-attention breaks image semantics and forces SparseViT to adaptively\nextract non-semantic features for images. Besides, compared with existing IML\nmodels, the sparse self-attention mechanism largely reduced the model size (max\n80% in FLOPs), achieving stunning parameter efficiency and computation\nreduction. Extensive experiments demonstrate that, without any handcrafted\nfeature extractors, SparseViT is superior in both generalization and efficiency\nacross benchmark datasets.\n","authors":["Lei Su","Xiaochen Ma","Xuekang Zhu","Chaoqun Niu","Zeyu Lei","Ji-Zhe Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14598v1.pdf","comment":"12 page, 8 figures, published to AAAI"},{"id":"http://arxiv.org/abs/2412.14596v1","updated":"2024-12-19T07:31:40Z","published":"2024-12-19T07:31:40Z","title":"LDP: Generalizing to Multilingual Visual Information Extraction by\n  Language Decoupled Pretraining","summary":"  Visual Information Extraction (VIE) plays a crucial role in the comprehension\nof semi-structured documents, and several pre-trained models have been\ndeveloped to enhance performance. However, most of these works are monolingual\n(usually English). Due to the extremely unbalanced quantity and quality of\npre-training corpora between English and other languages, few works can extend\nto non-English scenarios. In this paper, we conduct systematic experiments to\nshow that vision and layout modality hold invariance among images with\ndifferent languages. If decoupling language bias from document images, a\nvision-layout-based model can achieve impressive cross-lingual generalization.\nAccordingly, we present a simple but effective multilingual training paradigm\nLDP (Language Decoupled Pre-training) for better utilization of monolingual\npre-training data. Our proposed model LDM (Language Decoupled Model) is first\npre-trained on the language-independent data, where the language knowledge is\ndecoupled by a diffusion model, and then the LDM is fine-tuned on the\ndownstream languages. Extensive experiments show that the LDM outperformed all\nSOTA multilingual pre-trained models, and also maintains competitiveness on\ndownstream monolingual/English benchmarks.\n","authors":["Huawen Shen","Gengluo Li","Jinwen Zhong","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14596v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14592v1","updated":"2024-12-19T07:23:17Z","published":"2024-12-19T07:23:17Z","title":"Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry,\n  and Internal Properties","summary":"  Object anomaly detection is essential for industrial quality inspection, yet\ntraditional single-sensor methods face critical limitations. They fail to\ncapture the wide range of anomaly types, as single sensors are often\nconstrained to either external appearance, geometric structure, or internal\nproperties. To overcome these challenges, we introduce MulSen-AD, the first\nhigh-resolution, multi-sensor anomaly detection dataset tailored for industrial\napplications. MulSen-AD unifies data from RGB cameras, laser scanners, and\nlock-in infrared thermography, effectively capturing external appearance,\ngeometric deformations, and internal defects. The dataset spans 15 industrial\nproducts with diverse, real-world anomalies. We also present MulSen-AD Bench, a\nbenchmark designed to evaluate multi-sensor methods, and propose\nMulSen-TripleAD, a decision-level fusion algorithm that integrates these three\nmodalities for robust, unsupervised object anomaly detection. Our experiments\ndemonstrate that multi-sensor fusion substantially outperforms single-sensor\napproaches, achieving 96.1% AUROC in object-level detection accuracy. These\nresults highlight the importance of integrating multi-sensor data for\ncomprehensive industrial anomaly detection.\n","authors":["Wenqiao Li","Bozhong Zheng","Xiaohao Xu","Jinye Gan","Fading Lu","Xiang Li","Na Ni","Zheng Tian","Xiaonan Huang","Shenghua Gao","Yingna Wu"],"pdf_url":"https://arxiv.org/pdf/2412.14592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09583v4","updated":"2024-12-19T07:21:43Z","published":"2024-10-12T16:28:40Z","title":"POPoS: Improving Efficient and Robust Facial Landmark Detection with\n  Parallel Optimal Position Search","summary":"  Achieving a balance between accuracy and efficiency is a critical challenge\nin facial landmark detection (FLD). This paper introduces Parallel Optimal\nPosition Search (POPoS), a high-precision encoding-decoding framework designed\nto address the limitations of traditional FLD methods. POPoS employs three key\ncontributions: (1) Pseudo-range multilateration is utilized to correct heatmap\nerrors, improving landmark localization accuracy. By integrating multiple\nanchor points, it reduces the impact of individual heatmap inaccuracies,\nleading to robust overall positioning. (2) To enhance the pseudo-range accuracy\nof selected anchor points, a new loss function, named multilateration anchor\nloss, is proposed. This loss function enhances the accuracy of the distance\nmap, mitigates the risk of local optima, and ensures optimal solutions. (3) A\nsingle-step parallel computation algorithm is introduced, boosting\ncomputational efficiency and reducing processing time. Extensive evaluations\nacross five benchmark datasets demonstrate that POPoS consistently outperforms\nexisting methods, particularly excelling in low-resolution heatmaps scenarios\nwith minimal computational overhead. These advantages make POPoS a highly\nefficient and accurate tool for FLD, with broad applicability in real-world\nscenarios.\n","authors":["Chong-Yang Xiang","Jun-Yan He","Zhi-Qi Cheng","Xiao Wu","Xian-Sheng Hua"],"pdf_url":"https://arxiv.org/pdf/2410.09583v4.pdf","comment":"Accepted to AAAI 2025, 9 pages, 6 figures. Code:\n  https://github.com/teslatasy/POPoS"},{"id":"http://arxiv.org/abs/2410.20815v2","updated":"2024-12-19T07:19:52Z","published":"2024-10-28T08:02:34Z","title":"Grid4D: 4D Decomposed Hash Encoding for High-fidelity Dynamic Gaussian\n  Splatting","summary":"  Recently, Gaussian splatting has received more and more attention in the\nfield of static scene rendering. Due to the low computational overhead and\ninherent flexibility of explicit representations, plane-based explicit methods\nare popular ways to predict deformations for Gaussian-based dynamic scene\nrendering models. However, plane-based methods rely on the inappropriate\nlow-rank assumption and excessively decompose the space-time 4D encoding,\nresulting in overmuch feature overlap and unsatisfactory rendering quality. To\ntackle these problems, we propose Grid4D, a dynamic scene rendering model based\non Gaussian splatting and employing a novel explicit encoding method for the 4D\ninput through the hash encoding. Different from plane-based explicit\nrepresentations, we decompose the 4D encoding into one spatial and three\ntemporal 3D hash encodings without the low-rank assumption. Additionally, we\ndesign a novel attention module that generates the attention scores in a\ndirectional range to aggregate the spatial and temporal features. The\ndirectional attention enables Grid4D to more accurately fit the diverse\ndeformations across distinct scene components based on the spatial encoded\nfeatures. Moreover, to mitigate the inherent lack of smoothness in explicit\nrepresentation methods, we introduce a smooth regularization term that keeps\nour model from the chaos of deformation prediction. Our experiments demonstrate\nthat Grid4D significantly outperforms the state-of-the-art models in visual\nquality and rendering speed.\n","authors":["Jiawei Xu","Zexin Fan","Jian Yang","Jin Xie"],"pdf_url":"https://arxiv.org/pdf/2410.20815v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.14587v1","updated":"2024-12-19T07:13:15Z","published":"2024-12-19T07:13:15Z","title":"Spike2Former: Efficient Spiking Transformer for High-performance Image\n  Segmentation","summary":"  Spiking Neural Networks (SNNs) have a low-power advantage but perform poorly\nin image segmentation tasks. The reason is that directly converting neural\nnetworks with complex architectural designs for segmentation tasks into spiking\nversions leads to performance degradation and non-convergence. To address this\nchallenge, we first identify the modules in the architecture design that lead\nto the severe reduction in spike firing, make targeted improvements, and\npropose Spike2Former architecture. Second, we propose normalized integer\nspiking neurons to solve the training stability problem of SNNs with complex\narchitectures. We set a new state-of-the-art for SNNs in various semantic\nsegmentation datasets, with a significant improvement of +12.7% mIoU and 5.0\nefficiency on ADE20K, +14.3% mIoU and 5.2 efficiency on VOC2012, and +9.1% mIoU\nand 6.6 efficiency on CityScapes.\n","authors":["Zhenxin Lei","Man Yao","Jiakui Hu","Xinhao Luo","Yanye Lu","Bo Xu","Guoqi Li"],"pdf_url":"https://arxiv.org/pdf/2412.14587v1.pdf","comment":"This work has been accepted on Association for the Advancement of\n  Artificial Intelligence 2025"},{"id":"http://arxiv.org/abs/2412.14585v1","updated":"2024-12-19T07:06:25Z","published":"2024-12-19T07:06:25Z","title":"HiCM$^2$: Hierarchical Compact Memory Modeling for Dense Video\n  Captioning","summary":"  With the growing demand for solutions to real-world video challenges,\ninterest in dense video captioning (DVC) has been on the rise. DVC involves the\nautomatic captioning and localization of untrimmed videos. Several studies\nhighlight the challenges of DVC and introduce improved methods utilizing prior\nknowledge, such as pre-training and external memory. In this research, we\npropose a model that leverages the prior knowledge of human-oriented\nhierarchical compact memory inspired by human memory hierarchy and cognition.\nTo mimic human-like memory recall, we construct a hierarchical memory and a\nhierarchical memory reading module. We build an efficient hierarchical compact\nmemory by employing clustering of memory events and summarization using large\nlanguage models. Comparative experiments demonstrate that this hierarchical\nmemory recall process improves the performance of DVC by achieving\nstate-of-the-art performance on YouCook2 and ViTT datasets.\n","authors":["Minkuk Kim","Hyeon Bae Kim","Jinyoung Moon","Jinwoo Choi","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2412.14585v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.14580v1","updated":"2024-12-19T07:00:03Z","published":"2024-12-19T07:00:03Z","title":"DiffSim: Taming Diffusion Models for Evaluating Visual Similarity","summary":"  Diffusion models have fundamentally transformed the field of generative\nmodels, making the assessment of similarity between customized model outputs\nand reference inputs critically important. However, traditional perceptual\nsimilarity metrics operate primarily at the pixel and patch levels, comparing\nlow-level colors and textures but failing to capture mid-level similarities and\ndifferences in image layout, object pose, and semantic content. Contrastive\nlearning-based CLIP and self-supervised learning-based DINO are often used to\nmeasure semantic similarity, but they highly compress image features,\ninadequately assessing appearance details. This paper is the first to discover\nthat pretrained diffusion models can be utilized for measuring visual\nsimilarity and introduces the DiffSim method, addressing the limitations of\ntraditional metrics in capturing perceptual consistency in custom generation\ntasks. By aligning features in the attention layers of the denoising U-Net,\nDiffSim evaluates both appearance and style similarity, showing superior\nalignment with human visual preferences. Additionally, we introduce the Sref\nand IP benchmarks to evaluate visual similarity at the level of style and\ninstance, respectively. Comprehensive evaluations across multiple benchmarks\ndemonstrate that DiffSim achieves state-of-the-art performance, providing a\nrobust tool for measuring visual coherence in generative models.\n","authors":["Yiren Song","Xiaokang Liu","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2412.14580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14579v1","updated":"2024-12-19T06:57:37Z","published":"2024-12-19T06:57:37Z","title":"GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D\n  Gaussian Splatting","summary":"  3D occupancy perception is gaining increasing attention due to its capability\nto offer detailed and precise environment representations. Previous\nweakly-supervised NeRF methods balance efficiency and accuracy, with mIoU\nvarying by 5-10 points due to sampling count along camera rays. Recently,\nreal-time Gaussian splatting has gained widespread popularity in 3D\nreconstruction, and the occupancy prediction task can also be viewed as a\nreconstruction task. Consequently, we propose GSRender, which naturally employs\n3D Gaussian Splatting for occupancy prediction, simplifying the sampling\nprocess. In addition, the limitations of 2D supervision result in duplicate\npredictions along the same camera ray. We implemented the Ray Compensation (RC)\nmodule, which mitigates this issue by compensating for features from adjacent\nframes. Finally, we redesigned the loss to eliminate the impact of dynamic\nobjects from adjacent frames. Extensive experiments demonstrate that our\napproach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while\nnarrowing the gap with 3D supervision methods. Our code will be released soon.\n","authors":["Qianpu Sun","Changyong Shu","Sifan Zhou","Zichen Yu","Yan Chen","Dawei Yang","Yuan Chun"],"pdf_url":"https://arxiv.org/pdf/2412.14579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14576v1","updated":"2024-12-19T06:52:12Z","published":"2024-12-19T06:52:12Z","title":"Alignment-Free RGB-T Salient Object Detection: A Large-scale Dataset and\n  Progressive Correlation Network","summary":"  Alignment-free RGB-Thermal (RGB-T) salient object detection (SOD) aims to\nachieve robust performance in complex scenes by directly leveraging the\ncomplementary information from unaligned visible-thermal image pairs, without\nrequiring manual alignment. However, the labor-intensive process of collecting\nand annotating image pairs limits the scale of existing benchmarks, hindering\nthe advancement of alignment-free RGB-T SOD. In this paper, we construct a\nlarge-scale and high-diversity unaligned RGB-T SOD dataset named UVT20K,\ncomprising 20,000 image pairs, 407 scenes, and 1256 object categories. All\nsamples are collected from real-world scenarios with various challenges, such\nas low illumination, image clutter, complex salient objects, and so on. To\nsupport the exploration for further research, each sample in UVT20K is\nannotated with a comprehensive set of ground truths, including saliency masks,\nscribbles, boundaries, and challenge attributes. In addition, we propose a\nProgressive Correlation Network (PCNet), which models inter- and intra-modal\ncorrelations on the basis of explicit alignment to achieve accurate predictions\nin unaligned image pairs. Extensive experiments conducted on unaligned and\naligned datasets demonstrate the effectiveness of our method.Code and dataset\nare available at https://github.com/Angknpng/PCNet.\n","authors":["Kunpeng Wang","Keke Chen","Chenglong Li","Zhengzheng Tu","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2412.14576v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14571v1","updated":"2024-12-19T06:42:25Z","published":"2024-12-19T06:42:25Z","title":"SCKD: Semi-Supervised Cross-Modality Knowledge Distillation for 4D Radar\n  Object Detection","summary":"  3D object detection is one of the fundamental perception tasks for autonomous\nvehicles. Fulfilling such a task with a 4D millimeter-wave radar is very\nattractive since the sensor is able to acquire 3D point clouds similar to Lidar\nwhile maintaining robust measurements under adverse weather. However, due to\nthe high sparsity and noise associated with the radar point clouds, the\nperformance of the existing methods is still much lower than expected. In this\npaper, we propose a novel Semi-supervised Cross-modality Knowledge Distillation\n(SCKD) method for 4D radar-based 3D object detection. It characterizes the\ncapability of learning the feature from a Lidar-radar-fused teacher network\nwith semi-supervised distillation. We first propose an adaptive fusion module\nin the teacher network to boost its performance. Then, two feature distillation\nmodules are designed to facilitate the cross-modality knowledge transfer.\nFinally, a semi-supervised output distillation is proposed to increase the\neffectiveness and flexibility of the distillation framework. With the same\nnetwork structure, our radar-only student trained by SCKD boosts the mAP by\n10.38% over the baseline and outperforms the state-of-the-art works on the VoD\ndataset. The experiment on ZJUODset also shows 5.12% mAP improvements on the\nmoderate difficulty level over the baseline when extra unlabeled data are\navailable. Code is available at https://github.com/Ruoyu-Xu/SCKD.\n","authors":["Ruoyu Xu","Zhiyu Xiang","Chenwei Zhang","Hanzhi Zhong","Xijun Zhao","Ruina Dang","Peng Xu","Tianyu Pu","Eryun Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14571v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2410.04749v2","updated":"2024-12-19T06:41:40Z","published":"2024-10-07T04:59:08Z","title":"LLaVA Needs More Knowledge: Retrieval Augmented Natural Language\n  Generation with Knowledge Graph for Explaining Thoracic Pathologies","summary":"  Generating Natural Language Explanations (NLEs) for model predictions on\nmedical images, particularly those depicting thoracic pathologies, remains a\ncritical and challenging task. Existing methodologies often struggle due to\ngeneral models' insufficient domain-specific medical knowledge and privacy\nconcerns associated with retrieval-based augmentation techniques. To address\nthese issues, we propose a novel Vision-Language framework augmented with a\nKnowledge Graph (KG)-based datastore, which enhances the model's understanding\nby incorporating additional domain-specific medical knowledge essential for\ngenerating accurate and informative NLEs. Our framework employs a KG-based\nretrieval mechanism that not only improves the precision of the generated\nexplanations but also preserves data privacy by avoiding direct data retrieval.\nThe KG datastore is designed as a plug-and-play module, allowing for seamless\nintegration with various model architectures. We introduce and evaluate three\ndistinct frameworks within this paradigm: KG-LLaVA, which integrates the\npre-trained LLaVA model with KG-RAG; Med-XPT, a custom framework combining\nMedCLIP, a transformer-based projector, and GPT-2; and Bio-LLaVA, which adapts\nLLaVA by incorporating the Bio-ViT-L vision model. These frameworks are\nvalidated on the MIMIC-NLE dataset, where they achieve state-of-the-art\nresults, underscoring the effectiveness of KG augmentation in generating\nhigh-quality NLEs for thoracic pathologies.\n","authors":["Ameer Hamza"," Abdullah","Yong Hyun Ahn","Sungyoung Lee","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2410.04749v2.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.14568v1","updated":"2024-12-19T06:39:28Z","published":"2024-12-19T06:39:28Z","title":"Improving Geometry in Sparse-View 3DGS via Reprojection-based DoF\n  Separation","summary":"  Recent learning-based Multi-View Stereo models have demonstrated\nstate-of-the-art performance in sparse-view 3D reconstruction. However,\ndirectly applying 3D Gaussian Splatting (3DGS) as a refinement step following\nthese models presents challenges. We hypothesize that the excessive positional\ndegrees of freedom (DoFs) in Gaussians induce geometry distortion, fitting\ncolor patterns at the cost of structural fidelity. To address this, we propose\nreprojection-based DoF separation, a method distinguishing positional DoFs in\nterms of uncertainty: image-plane-parallel DoFs and ray-aligned DoF. To\nindependently manage each DoF, we introduce a reprojection process along with\ntailored constraints for each DoF. Through experiments across various datasets,\nwe confirm that separating the positional DoFs of Gaussians and applying\ntargeted constraints effectively suppresses geometric artifacts, producing\nreconstruction results that are both visually and geometrically plausible.\n","authors":["Yongsung Kim","Minjun Park","Jooyoung Choi","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2412.14568v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2412.11530v2","updated":"2024-12-19T06:32:22Z","published":"2024-12-16T08:08:35Z","title":"RoMeO: Robust Metric Visual Odometry","summary":"  Visual odometry (VO) aims to estimate camera poses from visual inputs -- a\nfundamental building block for many applications such as VR/AR and robotics.\nThis work focuses on monocular RGB VO where the input is a monocular RGB video\nwithout IMU or 3D sensors. Existing approaches lack robustness under this\nchallenging scenario and fail to generalize to unseen data (especially\noutdoors); they also cannot recover metric-scale poses. We propose Robust\nMetric Visual Odometry (RoMeO), a novel method that resolves these issues\nleveraging priors from pre-trained depth models. RoMeO incorporates both\nmonocular metric depth and multi-view stereo (MVS) models to recover\nmetric-scale, simplify correspondence search, provide better initialization and\nregularize optimization. Effective strategies are proposed to inject noise\nduring training and adaptively filter noisy depth priors, which ensure the\nrobustness of RoMeO on in-the-wild data. As shown in Fig.1, RoMeO advances the\nstate-of-the-art (SOTA) by a large margin across 6 diverse datasets covering\nboth indoor and outdoor scenes. Compared to the current SOTA DPVO, RoMeO\nreduces the relative (align the trajectory scale with GT) and absolute\ntrajectory errors both by >50%. The performance gain also transfers to the full\nSLAM pipeline (with global BA & loop closure). Code will be released upon\nacceptance.\n","authors":["Junda Cheng","Zhipeng Cai","Zhaoxing Zhang","Wei Yin","Matthias Muller","Michael Paulitsch","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11973v6","updated":"2024-12-19T06:29:38Z","published":"2023-12-19T09:11:49Z","title":"Continual Learning: Forget-free Winning Subnetworks for Video\n  Representations","summary":"  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the\nexistence of efficient subnetworks within larger, dense networks, a\nhigh-performing Winning Subnetwork (WSN) in terms of task performance under\nappropriate sparsity conditions is considered for various continual learning\ntasks. It leverages pre-existing weights from dense networks to achieve\nefficient learning in Task Incremental Learning (TIL) and Task-agnostic\nIncremental Learning (TaIL) scenarios. In Few-Shot Class Incremental Learning\n(FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is\ndesigned to prevent overfitting when the data samples are scarce. Furthermore,\nthe sparse reuse of WSN weights is considered for Video Incremental Learning\n(VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It\nenables compact encoding of videos and identifies reusable subnetworks across\nvarying bandwidths. We have integrated FSO into different architectural\nframeworks for continual learning, including VIL, TIL, and FSCIL. Our\ncomprehensive experiments demonstrate FSO's effectiveness, significantly\nimproving task performance at various convolutional representational levels.\nSpecifically, FSO enhances higher-layer performance in TIL and FSCIL and\nlower-layer performance in VIL.\n","authors":["Haeyong Kang","Jaehong Yoon","Sung Ju Hwang","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2312.11973v6.pdf","comment":"IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (T-PAMI)"},{"id":"http://arxiv.org/abs/2412.14561v1","updated":"2024-12-19T06:26:16Z","published":"2024-12-19T06:26:16Z","title":"GBRIP: Granular Ball Representation for Imbalanced Partial Label\n  Learning","summary":"  Partial label learning (PLL) is a complicated weakly supervised\nmulti-classification task compounded by class imbalance. Currently, existing\nmethods only rely on inter-class pseudo-labeling from inter-class features,\noften overlooking the significant impact of the intra-class imbalanced features\ncombined with the inter-class. To address these limitations, we introduce\nGranular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for\nimbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and\nmulti-center loss to construct a granular ball-based nfeature space through\nunsupervised learning, effectively capturing the feature distribution within\neach class. GBRIP mitigates the impact of confusing features by systematically\nrefining label disambiguation and estimating imbalance distributions. The novel\nmulti-center loss function enhances learning by emphasizing the relationships\nbetween samples and their respective centers within the granular balls.\nExtensive experiments on standard benchmarks demonstrate that GBRIP outperforms\nexisting state-of-the-art methods, offering a robust solution to the challenges\nof imbalanced PLL.\n","authors":["Jintao Huang","Yiu-ming Cheung","Chi-man Vong","Wenbin Qian"],"pdf_url":"https://arxiv.org/pdf/2412.14561v1.pdf","comment":"AAAI25"},{"id":"http://arxiv.org/abs/2409.01179v3","updated":"2024-12-19T06:26:04Z","published":"2024-09-02T11:19:54Z","title":"Recoverable Compression: A Multimodal Vision Token Recovery Mechanism\n  Guided by Text Information","summary":"  With the advancement of large-scale language modeling techniques, large\nmultimodal models combining visual encoders with large language models have\ndemonstrated exceptional performance in various visual tasks. Most of the\ncurrent large-scale multimodal models achieve this by mapping visual features\nobtained from the visual encoder into a large language model and using them as\ninputs alongside text for downstream tasks. Therefore, the number of visual\ntokens directly affects the training and inference speed of the model. There\nhas been significant work on token pruning for visual transformers, but for\nlarge multimodal models, only relying on visual information for token pruning\nor compression may lead to significant loss of important information. On the\nother hand, the textual input in the form of a question may contain valuable\ninformation that can aid in answering the question, providing additional\nknowledge to the model. To address the potential oversimplification and\nexcessive pruning that can occur with most purely visual token pruning methods,\nwe propose a text information-guided dynamic visual token recovery mechanism\nthat does not require training. This mechanism leverages the similarity between\nthe question text and visual tokens to recover visually meaningful tokens with\nimportant text information while merging other less important tokens.\nExperimental results demonstrate that our proposed method achieves comparable\nperformance to the original approach while compressing the visual tokens to an\naverage of 10% of the original quantity. Our source code will be made publicly\navailable following acceptance.\n","authors":["Yi Chen","Jian Xu","Xu-Yao Zhang","Wen-Zhuo Liu","Yang-Yang Liu","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2409.01179v3.pdf","comment":"AAAI2025 Accepted"},{"id":"http://arxiv.org/abs/2403.10650v3","updated":"2024-12-19T06:25:45Z","published":"2024-03-15T19:35:10Z","title":"PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time\n  Adaptation","summary":"  Real-world vision models in dynamic environments face rapid shifts in domain\ndistributions, leading to decreased recognition performance. Using unlabeled\ntest data, continuous test-time adaptation (CTTA) directly adjusts a\npre-trained source discriminative model to these changing domains. A highly\neffective CTTA method involves applying layer-wise adaptive learning rates for\nselectively adapting pre-trained layers. However, it suffers from the poor\nestimation of domain shift and the inaccuracies arising from the pseudo-labels.\nThis work aims to overcome these limitations by identifying layers for\nadaptation via quantifying model prediction uncertainty without relying on\npseudo-labels. We utilize the magnitude of gradients as a metric, calculated by\nbackpropagating the KL divergence between the softmax output and a uniform\ndistribution, to select layers for further adaptation. Subsequently, for the\nparameters exclusively belonging to these selected layers, with the remaining\nones frozen, we evaluate their sensitivity to approximate the domain shift and\nadjust their learning rates accordingly. We conduct extensive image\nclassification experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C,\ndemonstrating the superior efficacy of our method compared to prior approaches.\n","authors":["Sarthak Kumar Maharana","Baoming Zhang","Yunhui Guo"],"pdf_url":"https://arxiv.org/pdf/2403.10650v3.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14559v1","updated":"2024-12-19T06:22:19Z","published":"2024-12-19T06:22:19Z","title":"ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation\n  Model","summary":"  The scaling law has been validated in various domains, such as natural\nlanguage processing (NLP) and massive computer vision tasks; however, its\napplication to motion generation remains largely unexplored. In this paper, we\nintroduce a scalable motion generation framework that includes the motion\ntokenizer Motion FSQ-VAE and a text-prefix autoregressive transformer. Through\ncomprehensive experiments, we observe the scaling behavior of this system. For\nthe first time, we confirm the existence of scaling laws within the context of\nmotion generation. Specifically, our results demonstrate that the normalized\ntest loss of our prefix autoregressive models adheres to a logarithmic law in\nrelation to compute budgets. Furthermore, we also confirm the power law between\nNon-Vocabulary Parameters, Vocabulary Parameters, and Data Tokens with respect\nto compute budgets respectively. Leveraging the scaling law, we predict the\noptimal transformer size, vocabulary size, and data requirements for a compute\nbudget of $1e18$. The test loss of the system, when trained with the optimal\nmodel size, vocabulary size, and required data, aligns precisely with the\npredicted test loss, thereby validating the scaling law.\n","authors":["Shunlin Lu","Jingbo Wang","Zeyu Lu","Ling-Hao Chen","Wenxun Dai","Junting Dong","Zhiyang Dou","Bo Dai","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.14559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20633v3","updated":"2024-12-19T06:22:05Z","published":"2024-05-31T05:49:37Z","title":"Skeleton-OOD: An End-to-End Skeleton-Based Model for Robust\n  Out-of-Distribution Human Action Detection","summary":"  Human action recognition is crucial in computer vision systems. However, in\nreal-world scenarios, human actions often fall outside the distribution of\ntraining data, requiring a model to both recognize in-distribution (ID) actions\nand reject out-of-distribution (OOD) ones. Despite its importance, there has\nbeen limited research on OOD detection in human actions. Existing works on OOD\ndetection mainly focus on image data with RGB structure, and many methods are\npost-hoc in nature. While these methods are convenient and computationally\nefficient, they often lack sufficient accuracy, fail to consider the exposure\nof OOD samples, and ignore the application in skeleton structure data. To\naddress these challenges, we propose a novel end-to-end skeleton-based model\ncalled Skeleton-OOD, which is committed to improving the effectiveness of OOD\ntasks while ensuring the accuracy of ID recognition. Through extensive\nexperiments conducted on NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics-400\ndatasets, Skeleton-OOD demonstrates the superior performance of our proposed\napproach compared to state-of-the-art methods. Our findings underscore the\neffectiveness of classic OOD detection techniques in the context of\nskeleton-based action recognition tasks, offering promising avenues for future\nresearch in this field. Code is available at\nhttps://github.com/YilliaJing/Skeleton-OOD.git.\n","authors":["Jing Xu","Anqi Zhu","Jingyu Lin","Qiuhong Ke","Cunjian Chen"],"pdf_url":"https://arxiv.org/pdf/2405.20633v3.pdf","comment":"Accepted by Neurocomputing"},{"id":"http://arxiv.org/abs/2412.14547v1","updated":"2024-12-19T05:55:18Z","published":"2024-12-19T05:55:18Z","title":"Bright-NeRF:Brightening Neural Radiance Field with Color Restoration\n  from Low-light Raw Images","summary":"  Neural Radiance Fields (NeRFs) have demonstrated prominent performance in\nnovel view synthesis. However, their input heavily relies on image acquisition\nunder normal light conditions, making it challenging to learn accurate scene\nrepresentation in low-light environments where images typically exhibit\nsignificant noise and severe color distortion. To address these challenges, we\npropose a novel approach, Bright-NeRF, which learns enhanced and high-quality\nradiance fields from multi-view low-light raw images in an unsupervised manner.\nOur method simultaneously achieves color restoration, denoising, and enhanced\nnovel view synthesis. Specifically, we leverage a physically-inspired model of\nthe sensor's response to illumination and introduce a chromatic adaptation loss\nto constrain the learning of response, enabling consistent color perception of\nobjects regardless of lighting conditions. We further utilize the raw data's\nproperties to expose the scene's intensity automatically. Additionally, we have\ncollected a multi-view low-light raw image dataset to advance research in this\nfield. Experimental results demonstrate that our proposed method significantly\noutperforms existing 2D and 3D approaches. Our code and dataset will be made\npublicly available.\n","authors":["Min Wang","Xin Huang","Guoqing Zhou","Qifeng Guo","Qing Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14547v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14546v1","updated":"2024-12-19T05:52:16Z","published":"2024-12-19T05:52:16Z","title":"{S$^3$-Mamba}: Small-Size-Sensitive Mamba for Lesion Segmentation","summary":"  Small lesions play a critical role in early disease diagnosis and\nintervention of severe infections. Popular models often face challenges in\nsegmenting small lesions, as it occupies only a minor portion of an image,\nwhile down\\_sampling operations may inevitably lose focus on local features of\nsmall lesions. To tackle the challenges, we propose a {\\bf S}mall-{\\bf\nS}ize-{\\bf S}ensitive {\\bf Mamba} ({\\bf S$^3$-Mamba}), which promotes the\nsensitivity to small lesions across three dimensions: channel, spatial, and\ntraining strategy. Specifically, an Enhanced Visual State Space block is\ndesigned to focus on small lesions through multiple residual connections to\npreserve local features, and selectively amplify important details while\nsuppressing irrelevant ones through channel-wise attention. A Tensor-based\nCross-feature Multi-scale Attention is designed to integrate input image\nfeatures and intermediate-layer features with edge features and exploit the\nattentive support of features across multiple scales, thereby retaining spatial\ndetails of small lesions at various granularities. Finally, we introduce a\nnovel regularized curriculum learning to automatically assess lesion size and\nsample difficulty, and gradually focus from easy samples to hard ones like\nsmall lesions. Extensive experiments on three medical image segmentation\ndatasets show the superiority of our S$^3$-Mamba, especially in segmenting\nsmall lesions. Our code is available at\nhttps://github.com/ErinWang2023/S3-Mamba.\n","authors":["Gui Wang","Yuexiang Li","Wenting Chen","Meidan Ding","Wooi Ping Cheah","Rong Qu","Jianfeng Ren","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2412.14546v1.pdf","comment":"Accept by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14545v1","updated":"2024-12-19T05:51:46Z","published":"2024-12-19T05:51:46Z","title":"Summary of Point Transformer with Federated Learning for Predicting\n  Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide\n  Images","summary":"  This study introduces a federated learning-based approach to predict HER2\nstatus from hematoxylin and eosin (HE)-stained whole slide images (WSIs),\nreducing costs and speeding up treatment decisions. To address label imbalance\nand feature representation challenges in multisite datasets, a point\ntransformer is proposed, incorporating dynamic label distribution, an auxiliary\nclassifier, and farthest cosine sampling. Extensive experiments demonstrate\nstate-of-the-art performance across four sites (2687 WSIs) and strong\ngeneralization to two unseen sites (229 WSIs).\n","authors":["Kamorudeen A. Amuda","Almustapha A. Wakili"],"pdf_url":"https://arxiv.org/pdf/2412.14545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16214v2","updated":"2024-12-19T05:50:06Z","published":"2024-07-23T06:42:55Z","title":"Diff-Shadow: Global-guided Diffusion Model for Shadow Removal","summary":"  We propose Diff-Shadow, a global-guided diffusion model for shadow removal.\nPrevious transformer-based approaches can utilize global information to relate\nshadow and non-shadow regions but are limited in their synthesis ability and\nrecover images with obvious boundaries. In contrast, diffusion-based methods\ncan generate better content but they are not exempt from issues related to\ninconsistent illumination. In this work, we combine the advantages of diffusion\nmodels and global guidance to achieve shadow-free restoration. Specifically, we\npropose a parallel UNets architecture: 1) the local branch performs the\npatch-based noise estimation in the diffusion process, and 2) the global branch\nrecovers the low-resolution shadow-free images. A Reweight Cross Attention\n(RCA) module is designed to integrate global contextual information of\nnon-shadow regions into the local branch. We further design a Global-guided\nSampling Strategy (GSS) that mitigates patch boundary issues and ensures\nconsistent illumination across shaded and unshaded regions in the recovered\nimage. Comprehensive experiments on datasets ISTD, ISTD+, and SRD have\ndemonstrated the effectiveness of Diff-Shadow. Compared to state-of-the-art\nmethods, our method achieves a significant improvement in terms of PSNR,\nincreasing from 32.33dB to 33.69dB on the ISTD dataset.\n","authors":["Jinting Luo","Ru Li","Chengzhi Jiang","Xiaoming Zhang","Mingyan Han","Ting Jiang","Haoqiang Fan","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2407.16214v2.pdf","comment":"Proceedings of the 39th Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2412.08125v2","updated":"2024-12-19T05:46:29Z","published":"2024-12-11T06:21:33Z","title":"Progressive Multi-granular Alignments for Grounded Reasoning in Large\n  Vision-Language Models","summary":"  Existing Large Vision-Language Models (LVLMs) excel at matching concepts\nacross multi-modal inputs but struggle with compositional concepts and\nhigh-level relationships between entities. This paper introduces Progressive\nmulti-granular Vision-Language alignments (PromViL), a novel framework to\nenhance LVLMs' ability in performing grounded compositional visual reasoning\ntasks. Our approach constructs a hierarchical structure of multi-modal\nalignments, ranging from simple to complex concepts. By progressively aligning\ntextual descriptions with corresponding visual regions, our model learns to\nleverage contextual information from lower levels to inform higher-level\nreasoning. To facilitate this learning process, we introduce a data generation\nprocess that creates a novel dataset derived from Visual Genome, providing a\nwide range of nested compositional vision-language pairs. Experimental results\ndemonstrate that our PromViL framework significantly outperforms baselines on\nvarious visual grounding and compositional question answering tasks. The code\nis available at: https://github.com/lqh52/PromViL.\n","authors":["Quang-Hung Le","Long Hoang Dang","Ngan Le","Truyen Tran","Thao Minh Le"],"pdf_url":"https://arxiv.org/pdf/2412.08125v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06842v4","updated":"2024-12-19T05:45:43Z","published":"2024-04-10T09:14:28Z","title":"MoCha-Stereo: Motif Channel Attention Network for Stereo Matching","summary":"  Learning-based stereo matching techniques have made significant progress.\nHowever, existing methods inevitably lose geometrical structure information\nduring the feature channel generation process, resulting in edge detail\nmismatches. In this paper, the Motif Cha}nnel Attention Stereo Matching Network\n(MoCha-Stereo) is designed to address this problem. We provide the Motif\nChannel Correlation Volume (MCCV) to determine more accurate edge matching\ncosts. MCCV is achieved by projecting motif channels, which capture common\ngeometric structures in feature channels, onto feature maps and cost volumes.\nIn addition, edge variations in %potential feature channels of the\nreconstruction error map also affect details matching, we propose the\nReconstruction Error Motif Penalty (REMP) module to further refine the\nfull-resolution disparity estimation. REMP integrates the frequency information\nof typical channel features from the reconstruction error. MoCha-Stereo ranks\n1st on the KITTI-2015 and KITTI-2012 Reflective leaderboards. Our structure\nalso shows excellent performance in Multi-View Stereo. Code is avaliable at\nhttps://github.com/ZYangChen/MoCha-Stereo.\n","authors":["Ziyang Chen","Wei Long","He Yao","Yongjun Zhang","Bingshu Wang","Yongbin Qin","Jia Wu"],"pdf_url":"https://arxiv.org/pdf/2404.06842v4.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2412.14539v1","updated":"2024-12-19T05:36:52Z","published":"2024-12-19T05:36:52Z","title":"Downscaling Precipitation with Bias-informed Conditional Diffusion Model","summary":"  Climate change is intensifying rainfall extremes, making high-resolution\nprecipitation projections crucial for society to better prepare for impacts\nsuch as flooding. However, current Global Climate Models (GCMs) operate at\nspatial resolutions too coarse for localized analyses. To address this\nlimitation, deep learning-based statistical downscaling methods offer promising\nsolutions, providing high-resolution precipitation projections with a moderate\ncomputational cost. In this work, we introduce a bias-informed conditional\ndiffusion model for statistical downscaling of precipitation. Specifically, our\nmodel leverages a conditional diffusion approach to learn distribution priors\nfrom large-scale, high-resolution precipitation datasets. The long-tail\ndistribution of precipitation poses a unique challenge for training diffusion\nmodels; to address this, we apply gamma correction during preprocessing.\nAdditionally, to correct biases in the downscaled results, we employ a\nguided-sampling strategy to enhance bias correction. Our experiments\ndemonstrate that the proposed model achieves highly accurate results in an 8\ntimes downscaling setting, outperforming previous deterministic methods. The\ncode and dataset are available at\nhttps://github.com/RoseLV/research_super-resolution\n","authors":["Ran Lyu","Linhan Wang","Yanshen Sun","Hedanqiu Bai","Chang-Tien Lu"],"pdf_url":"https://arxiv.org/pdf/2412.14539v1.pdf","comment":"3 pages, 2 figures. Accepted by Proceedings of IEEE International\n  Conference on Big Data, Dec 15-18, 2024"},{"id":"http://arxiv.org/abs/2403.15249v2","updated":"2024-12-19T05:30:55Z","published":"2024-03-22T14:47:18Z","title":"Spectral Motion Alignment for Video Motion Transfer using Diffusion\n  Models","summary":"  The evolution of diffusion models has greatly impacted video generation and\nunderstanding. Particularly, text-to-video diffusion models (VDMs) have\nsignificantly facilitated the customization of input video with target\nappearance, motion, etc. Despite these advances, challenges persist in\naccurately distilling motion information from video frames. While existing\nworks leverage the consecutive frame residual as the target motion vector, they\ninherently lack global motion context and are vulnerable to frame-wise\ndistortions. To address this, we present Spectral Motion Alignment (SMA), a\nnovel framework that refines and aligns motion vectors using Fourier and\nwavelet transforms. SMA learns motion patterns by incorporating\nfrequency-domain regularization, facilitating the learning of whole-frame\nglobal motion dynamics, and mitigating spatial artifacts. Extensive experiments\ndemonstrate SMA's efficacy in improving motion transfer while maintaining\ncomputational efficiency and compatibility across various video customization\nframeworks.\n","authors":["Geon Yeong Park","Hyeonho Jeong","Sang Wan Lee","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2403.15249v2.pdf","comment":"AAAI 2025, Project page:\n  https://geonyeong-park.github.io/spectral-motion-alignment/"},{"id":"http://arxiv.org/abs/2406.13362v3","updated":"2024-12-19T05:26:14Z","published":"2024-06-19T09:07:31Z","title":"VisualRWKV: Exploring Recurrent Neural Networks for Visual Language\n  Models","summary":"  Visual Language Models (VLMs) have rapidly progressed with the recent success\nof large language models. However, there have been few attempts to incorporate\nefficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In\nthis study, we introduce VisualRWKV, the first application of a linear RNN\nmodel to multimodal learning tasks, leveraging the pre-trained RWKV language\nmodel. We propose a data-dependent recurrence and sandwich prompts to enhance\nour modeling capabilities, along with a 2D image scanning mechanism to enrich\nthe processing of visual sequences. Extensive experiments demonstrate that\nVisualRWKV achieves competitive performance compared to Transformer-based\nmodels like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV\nhas a speed advantage of 3.98 times and can save 54% of GPU memory when\nreaching an inference length of 24K tokens. To facilitate further research and\nanalysis, we have made the checkpoints and the associated code publicly\naccessible at the following GitHub repository: see\nhttps://github.com/howard-hou/VisualRWKV.\n","authors":["Haowen Hou","Peigen Zeng","Fei Ma","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2406.13362v3.pdf","comment":"Accepted at COLING 2025 main conference"},{"id":"http://arxiv.org/abs/2412.14535v1","updated":"2024-12-19T05:23:49Z","published":"2024-12-19T05:23:49Z","title":"DAMPER: A Dual-Stage Medical Report Generation Framework with\n  Coarse-Grained MeSH Alignment and Fine-Grained Hypergraph Matching","summary":"  Medical report generation is crucial for clinical diagnosis and patient\nmanagement, summarizing diagnoses and recommendations based on medical imaging.\nHowever, existing work often overlook the clinical pipeline involved in report\nwriting, where physicians typically conduct an initial quick review followed by\na detailed examination. Moreover, current alignment methods may lead to\nmisaligned relationships. To address these issues, we propose DAMPER, a\ndual-stage framework for medical report generation that mimics the clinical\npipeline of report writing in two stages. In the first stage, a MeSH-Guided\nCoarse-Grained Alignment (MCG) stage that aligns chest X-ray (CXR) image\nfeatures with medical subject headings (MeSH) features to generate a rough\nkeyphrase representation of the overall impression. In the second stage, a\nHypergraph-Enhanced Fine-Grained Alignment (HFG) stage that constructs\nhypergraphs for image patches and report annotations, modeling high-order\nrelationships within each modality and performing hypergraph matching to\ncapture semantic correlations between image regions and textual phrases.\nFinally,the coarse-grained visual features, generated MeSH representations, and\nvisual hypergraph features are fed into a report decoder to produce the final\nmedical report. Extensive experiments on public datasets demonstrate the\neffectiveness of DAMPER in generating comprehensive and accurate medical\nreports, outperforming state-of-the-art methods across various evaluation\nmetrics.\n","authors":["Xiaofei Huang","Wenting Chen","Jie Liu","Qisheng Lu","Xiaoling Luo","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2412.14535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14531v1","updated":"2024-12-19T05:02:30Z","published":"2024-12-19T05:02:30Z","title":"Consistent Human Image and Video Generation with Spatially Conditioned\n  Diffusion","summary":"  Consistent human-centric image and video synthesis aims to generate images or\nvideos with new poses while preserving appearance consistency with a given\nreference image, which is crucial for low-cost visual content creation. Recent\nadvances based on diffusion models typically rely on separate networks for\nreference appearance feature extraction and target visual generation, leading\nto inconsistent domain gaps between references and targets. In this paper, we\nframe the task as a spatially-conditioned inpainting problem, where the target\nimage is inpainted to maintain appearance consistency with the reference. This\napproach enables the reference features to guide the generation of\npose-compliant targets within a unified denoising network, thereby mitigating\ndomain gaps. Additionally, to better maintain the reference appearance\ninformation, we impose a causal feature interaction framework, in which\nreference features can only query from themselves, while target features can\nquery appearance information from both the reference and the target. To further\nenhance computational efficiency and flexibility, in practical implementation,\nwe decompose the spatially-conditioned generation process into two stages:\nreference appearance extraction and conditioned target generation. Both stages\nshare a single denoising network, with interactions restricted to\nself-attention layers. This proposed method ensures flexible control over the\nappearance of generated human images and videos. By fine-tuning existing base\ndiffusion models on human video data, our method demonstrates strong\ngeneralization to unseen human identities and poses without requiring\nadditional per-instance fine-tuning. Experimental results validate the\neffectiveness of our approach, showing competitive performance compared to\nexisting methods for consistent human image and video synthesis.\n","authors":["Mingdeng Cao","Chong Mou","Ziyang Yuan","Xintao Wang","Zhaoyang Zhang","Ying Shan","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.14531v1.pdf","comment":"Project page: https://github.com/ljzycmd/SCD"},{"id":"http://arxiv.org/abs/2412.08108v2","updated":"2024-12-19T05:01:33Z","published":"2024-12-11T05:23:34Z","title":"Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language\n  Models Across Both Images and Text with a Single Perturbation","summary":"  Large Vision-Language Models (VLMs) have demonstrated remarkable performance\nacross multimodal tasks by integrating vision encoders with large language\nmodels (LLMs). However, these models remain vulnerable to adversarial attacks.\nAmong such attacks, Universal Adversarial Perturbations (UAPs) are especially\npowerful, as a single optimized perturbation can mislead the model across\nvarious input images. In this work, we introduce a novel UAP specifically\ndesigned for VLMs: the Doubly-Universal Adversarial Perturbation (Doubly-UAP),\ncapable of universally deceiving VLMs across both image and text inputs. To\nsuccessfully disrupt the vision encoder's fundamental process, we analyze the\ncore components of the attention mechanism. After identifying value vectors in\nthe middle-to-late layers as the most vulnerable, we optimize Doubly-UAP in a\nlabel-free manner with a frozen model. Despite being developed as a black-box\nto the LLM, Doubly-UAP achieves high attack success rates on VLMs, consistently\noutperforming baseline methods across vision-language tasks. Extensive ablation\nstudies and analyses further demonstrate the robustness of Doubly-UAP and\nprovide insights into how it influences internal attention mechanisms.\n","authors":["Hee-Seon Kim","Minbeom Kim","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2412.08108v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05282v2","updated":"2024-12-19T04:42:41Z","published":"2024-07-07T06:50:22Z","title":"UltraEdit: Instruction-based Fine-Grained Image Editing at Scale","summary":"  This paper presents UltraEdit, a large-scale (approximately 4 million editing\nsamples), automatically generated dataset for instruction-based image editing.\nOur key idea is to address the drawbacks in existing image editing datasets\nlike InstructPix2Pix and MagicBrush, and provide a systematic approach to\nproducing massive and high-quality image editing samples. UltraEdit offers\nseveral distinct advantages: 1) It features a broader range of editing\ninstructions by leveraging the creativity of large language models (LLMs)\nalongside in-context editing examples from human raters; 2) Its data sources\nare based on real images, including photographs and artworks, which provide\ngreater diversity and reduced bias compared to datasets solely generated by\ntext-to-image models; 3) It also supports region-based editing, enhanced by\nhigh-quality, automatically produced region annotations. Our experiments show\nthat canonical diffusion-based editing baselines trained on UltraEdit set new\nrecords on MagicBrush and Emu-Edit benchmarks. Our analysis further confirms\nthe crucial role of real image anchors and region-based editing data. The\ndataset, code, and models can be found in https://ultra-editing.github.io.\n","authors":["Haozhe Zhao","Xiaojian Ma","Liang Chen","Shuzheng Si","Rujie Wu","Kaikai An","Peiyu Yu","Minjia Zhang","Qing Li","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2407.05282v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.14518v1","updated":"2024-12-19T04:33:22Z","published":"2024-12-19T04:33:22Z","title":"Efficient Self-Supervised Video Hashing with Selective State Spaces","summary":"  Self-supervised video hashing (SSVH) is a practical task in video indexing\nand retrieval. Although Transformers are predominant in SSVH for their\nimpressive temporal modeling capabilities, they often suffer from computational\nand memory inefficiencies. Drawing inspiration from Mamba, an advanced\nstate-space model, we explore its potential in SSVH to achieve a better balance\nbetween efficacy and efficiency. We introduce S5VH, a Mamba-based video hashing\nmodel with an improved self-supervised learning paradigm. Specifically, we\ndesign bidirectional Mamba layers for both the encoder and decoder, which are\neffective and efficient in capturing temporal relationships thanks to the\ndata-dependent selective scanning mechanism with linear complexity. In our\nlearning strategy, we transform global semantics in the feature space into\nsemantically consistent and discriminative hash centers, followed by a center\nalignment loss as a global learning signal. Our self-local-global (SLG)\nparadigm significantly improves learning efficiency, leading to faster and\nbetter convergence. Extensive experiments demonstrate S5VH's improvements over\nstate-of-the-art methods, superior transferability, and scalable advantages in\ninference efficiency. Code is available at\nhttps://github.com/gimpong/AAAI25-S5VH.\n","authors":["Jinpeng Wang","Niu Lian","Jun Li","Yuting Wang","Yan Feng","Bin Chen","Yongbing Zhang","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2412.14518v1.pdf","comment":"Accepted by AAAI'25. 9 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2412.14509v1","updated":"2024-12-19T04:17:32Z","published":"2024-12-19T04:17:32Z","title":"A Super-pixel-based Approach to the Stable Interpretation of Neural\n  Networks","summary":"  Saliency maps are widely used in the computer vision community for\ninterpreting neural network classifiers. However, due to the randomness of\ntraining samples and optimization algorithms, the resulting saliency maps\nsuffer from a significant level of stochasticity, making it difficult for\ndomain experts to capture the intrinsic factors that influence the neural\nnetwork's decision. In this work, we propose a novel pixel partitioning\nstrategy to boost the stability and generalizability of gradient-based saliency\nmaps. Through both theoretical analysis and numerical experiments, we\ndemonstrate that the grouping of pixels reduces the variance of the saliency\nmap and improves the generalization behavior of the interpretation method.\nFurthermore, we propose a sensible grouping strategy based on super-pixels\nwhich cluster pixels into groups that align well with the semantic meaning of\nthe images. We perform several numerical experiments on CIFAR-10 and ImageNet.\nOur empirical results suggest that the super-pixel-based interpretation maps\nconsistently improve the stability and quality over the pixel-based saliency\nmaps.\n","authors":["Shizhan Gong","Jingwei Zhang","Qi Dou","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2412.14509v1.pdf","comment":"BMVC 2024"},{"id":"http://arxiv.org/abs/2406.04906v2","updated":"2024-12-19T04:09:02Z","published":"2024-06-07T12:58:14Z","title":"RU-AI: A Large Multimodal Dataset for Machine-Generated Content\n  Detection","summary":"  The recent generative AI models' capability of creating realistic and\nhuman-like content is significantly transforming the ways in which people\ncommunicate, create and work. The appropriate use of generative AI models can\nbenefit society, while their misuse poses threats to the society. However, the\nlack of aligned multimodal datasets has inhibited the development of effective\nand robust methods for detecting machine-generated content, particularly in\ntriple-modality settings (e.g., text, image, and voice). In this paper, we\nintroduce RU-AI, a new large-scale multimodal dataset for robust and efficient\ndetection of machine-generated content in text, image and voice. Our dataset is\nconstructed on the basis of three large publicly available datasets: Flickr8K,\nCOCO and Places205, by adding their corresponding AI duplicates, resulting\ntotal of 1,475,370 data instances. In addition, we create a noise variant of\neach modality of the datasets aiming to analyse the models' robustness. Given\nour dataset, we conduct extensive experiments with the current SOTA detection\nmethods. The results reveal that existing models still struggle to achieve\naccurate and robust classification after training on our dataset. The RU-AI\ndataset is designed to support the development of detection methods across\nmodalities and can be effectively utilised for identifying machine-generated\ncontent. The source code and dataset are available at\nhttps://github.com/ZhihaoZhang97/RU-AI.\n","authors":["Liting Huang","Zhihao Zhang","Yiran Zhang","Xiyue Zhou","Shoujin Wang"],"pdf_url":"https://arxiv.org/pdf/2406.04906v2.pdf","comment":"Submitted to WWW'25 Resource Track"},{"id":"http://arxiv.org/abs/2412.11586v2","updated":"2024-12-19T03:43:18Z","published":"2024-12-16T09:17:36Z","title":"StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair\n  Geometric Priors","summary":"  While haircut indicates distinct personality, existing avatar generation\nmethods fail to model practical hair due to the general or entangled\nrepresentation. We propose StrandHead, a novel text to 3D head avatar\ngeneration method capable of generating disentangled 3D hair with strand\nrepresentation. Without using 3D data for supervision, we demonstrate that\nrealistic hair strands can be generated from prompts by distilling 2D\ngenerative diffusion models. To this end, we propose a series of reliable\npriors on shape initialization, geometric primitives, and statistical haircut\nfeatures, leading to a stable optimization and text-aligned performance.\nExtensive experiments show that StrandHead achieves the state-of-the-art\nreality and diversity of generated 3D head and hair. The generated 3D hair can\nalso be easily implemented in the Unreal Engine for physical simulation and\nother applications. The code will be available at\nhttps://xiaokunsun.github.io/StrandHead.github.io.\n","authors":["Xiaokun Sun","Zeyu Cai","Ying Tai","Jian Yang","Zhenyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11586v2.pdf","comment":"Project page: https://xiaokunsun.github.io/StrandHead.github.io"},{"id":"http://arxiv.org/abs/2412.14496v1","updated":"2024-12-19T03:42:58Z","published":"2024-12-19T03:42:58Z","title":"Content-style disentangled representation for controllable artistic\n  image stylization and generation","summary":"  Controllable artistic image stylization and generation aims to render the\ncontent provided by text or image with the learned artistic style, where\ncontent and style decoupling is the key to achieve satisfactory results.\nHowever, current methods for content and style disentanglement primarily rely\non image information for supervision, which leads to two problems: 1) models\ncan only support one modality for style or content input;2) incomplete\ndisentanglement resulting in semantic interference from the reference image. To\naddress the above issues, this paper proposes a content-style representation\ndisentangling method for controllable artistic image stylization and\ngeneration. We construct a WikiStyle+ dataset consists of artworks with\ncorresponding textual descriptions for style and content. Based on the\nmultimodal dataset, we propose a disentangled content and style representations\nguided diffusion model. The disentangled representations are first learned by\nQ-Formers and then injected into a pre-trained diffusion model using learnable\nmulti-step cross-attention layers for better controllable stylization. This\napproach allows model to accommodate inputs from different modalities.\nExperimental results show that our method achieves a thorough disentanglement\nof content and style in reference images under multimodal supervision, thereby\nenabling a harmonious integration of content and style in the generated\noutputs, successfully producing style-consistent and expressive stylized\nimages.\n","authors":["Ma Zhuoqi","Zhang Yixuan","You Zejun","Tian Long","Liu Xiyang"],"pdf_url":"https://arxiv.org/pdf/2412.14496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14494v1","updated":"2024-12-19T03:39:13Z","published":"2024-12-19T03:39:13Z","title":"Drive-1-to-3: Enriching Diffusion Priors for Novel View Synthesis of\n  Real Vehicles","summary":"  The recent advent of large-scale 3D data, e.g. Objaverse, has led to\nimpressive progress in training pose-conditioned diffusion models for novel\nview synthesis. However, due to the synthetic nature of such 3D data, their\nperformance drops significantly when applied to real-world images. This paper\nconsolidates a set of good practices to finetune large pretrained models for a\nreal-world task -- harvesting vehicle assets for autonomous driving\napplications. To this end, we delve into the discrepancies between the\nsynthetic data and real driving data, then develop several strategies to\naccount for them properly. Specifically, we start with a virtual camera\nrotation of real images to ensure geometric alignment with synthetic data and\nconsistency with the pose manifold defined by pretrained models. We also\nidentify important design choices in object-centric data curation to account\nfor varying object distances in real driving scenes -- learn across varying\nobject scales with fixed camera focal length. Further, we perform\nocclusion-aware training in latent spaces to account for ubiquitous occlusions\nin real data, and handle large viewpoint changes by leveraging a symmetric\nprior. Our insights lead to effective finetuning that results in a $68.8\\%$\nreduction in FID for novel view synthesis over prior arts.\n","authors":["Chuang Lin","Bingbing Zhuang","Shanlin Sun","Ziyu Jiang","Jianfei Cai","Manmohan Chandraker"],"pdf_url":"https://arxiv.org/pdf/2412.14494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10230v3","updated":"2024-12-19T03:31:05Z","published":"2023-09-19T00:52:36Z","title":"LiON: Learning Point-wise Abstaining Penalty for LiDAR Outlier DetectioN\n  Using Diverse Synthetic Data","summary":"  LiDAR-based semantic scene understanding is an important module in the modern\nautonomous driving perception stack. However, identifying outlier points in a\nLiDAR point cloud is challenging as LiDAR point clouds lack semantically-rich\ninformation. While former SOTA methods adopt heuristic architectures, we\nrevisit this problem from the perspective of Selective Classification, which\nintroduces a selective function into the standard closed-set classification\nsetup. Our solution is built upon the basic idea of abstaining from choosing\nany inlier categories but learns a point-wise abstaining penalty with a\nmargin-based loss. Apart from learning paradigms, synthesizing outliers to\napproximate unlimited real outliers is also critical, so we propose a strong\nsynthesis pipeline that generates outliers originated from various factors:\nobject categories, sampling patterns and sizes. We demonstrate that learning\ndifferent abstaining penalties, apart from point-wise penalty, for different\ntypes of (synthesized) outliers can further improve the performance. We\nbenchmark our method on SemanticKITTI and nuScenes and achieve SOTA results.\nCodes are available at https://github.com/Daniellli/LiON/.\n","authors":["Shaocong Xu","Pengfei Li","Qianpu Sun","Xinyu Liu","Yang Li","Shihui Guo","Zhen Wang","Bo Jiang","Rui Wang","Kehua Sheng","Bo Zhang","Li Jiang","Hao Zhao","Yilun Chen"],"pdf_url":"https://arxiv.org/pdf/2309.10230v3.pdf","comment":"Accepted by AAAI2025. Codes are available at\n  https://github.com/Daniellli/LiON/"},{"id":"http://arxiv.org/abs/2412.14489v1","updated":"2024-12-19T03:26:51Z","published":"2024-12-19T03:26:51Z","title":"QADM-Net: Quality-adaptive Dynamic Network for Reliable Multimodal\n  Classification","summary":"  Integrating complementary information from different data modalities can\nyield representation with stronger expressive ability. However, data quality\nvaries across multimodal samples, highlighting the need for learning reliable\nmultimodal representations, especially in safety-critical applications. This\npaper focuses on an aspect that existing methods in this domain commonly\noverlook: the importance of network dynamics and adaptability in providing\nreliable results from diverse samples. Specifically, it highlights the model's\nability to dynamically adjust its capacity and behaviour according to different\nsamples, using the adjusted network for predicting each sample. To this end, we\npropose a novel framework for multimodal reliable classification termed\nQuality-adaptive Dynamic Multimodal Network (QADM-Net). QADM-Net first\nintroduces a confidence-guided dynamic depths mechanism to achieve the\nappropriate network capacity. This mechanism adjusts the network depth\naccording to the difficulty of each sample, which is determined by the quality\nof its modalities. Subsequently, we develop an informativeness-based dynamic\nparameters mechanism that enables QADM-Net to perform unique inference\nbehaviour on each of the diverse samples with feature-level quality variation\npresented in their feature vectors. In this way, QADM-Net adequately adapts its\ncapacity and behaviour on each sample by investigating the quality variation of\nsamples at both modality and feature levels, thus enhancing the reliability of\nclassification results. Experiments conducted on four datasets demonstrate that\nQADM-Net significantly outperforms state-of-the-art methods in classification\nperformance and exhibits strong adaptability to data with diverse quality.\n","authors":["Shu Shen","Tong Zhang","C. L. Philip Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14489v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.14487v1","updated":"2024-12-19T03:21:01Z","published":"2024-12-19T03:21:01Z","title":"Token Preference Optimization with Self-Calibrated Visual-Anchored\n  Rewards for Hallucination Mitigation","summary":"  Direct Preference Optimization (DPO) has been demonstrated to be highly\neffective in mitigating hallucinations in Large Vision Language Models (LVLMs)\nby aligning their outputs more closely with human preferences. Despite the\nrecent progress, existing methods suffer from two drawbacks: 1) Lack of\nscalable token-level rewards; and 2) Neglect of visual-anchored tokens. To this\nend, we propose a novel Token Preference Optimization model with\nself-calibrated rewards (dubbed as TPO), which adaptively attends to\nvisual-correlated tokens without fine-grained annotations. Specifically, we\nintroduce a token-level \\emph{visual-anchored} \\emph{reward} as the difference\nof the logistic distributions of generated tokens conditioned on the raw image\nand the corrupted one. In addition, to highlight the informative\nvisual-anchored tokens, a visual-aware training objective is proposed to\nenhance more accurate token-level optimization. Extensive experimental results\nhave manifested the state-of-the-art performance of the proposed TPO. For\nexample, by building on top of LLAVA-1.5-7B, our TPO boosts the performance\nabsolute improvement for hallucination benchmarks.\n","authors":["Jihao Gu","Yingyao Wang","Meng Cao","Pi Bu","Jun Song","Yancheng He","Shilong Li","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.14487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13609v2","updated":"2024-12-19T03:12:19Z","published":"2024-12-18T08:36:35Z","title":"Sign-IDD: Iconicity Disentangled Diffusion for Sign Language Production","summary":"  Sign Language Production (SLP) aims to generate semantically consistent sign\nvideos from textual statements, where the conversion from textual glosses to\nsign poses (G2P) is a crucial step. Existing G2P methods typically treat sign\nposes as discrete three-dimensional coordinates and directly fit them, which\noverlooks the relative positional relationships among joints. To this end, we\nprovide a new perspective, constraining joint associations and gesture details\nby modeling the limb bones to improve the accuracy and naturalness of the\ngenerated poses. In this work, we propose a pioneering iconicity disentangled\ndiffusion framework, termed Sign-IDD, specifically designed for SLP. Sign-IDD\nincorporates a novel Iconicity Disentanglement (ID) module to bridge the gap\nbetween relative positions among joints. The ID module disentangles the\nconventional 3D joint representation into a 4D bone representation, comprising\nthe 3D spatial direction vector and 1D spatial distance vector between adjacent\njoints. Additionally, an Attribute Controllable Diffusion (ACD) module is\nintroduced to further constrain joint associations, in which the attribute\nseparation layer aims to separate the bone direction and length attributes, and\nthe attribute control layer is designed to guide the pose generation by\nleveraging the above attributes. The ACD module utilizes the gloss embeddings\nas semantic conditions and finally generates sign poses from noise embeddings.\nExtensive experiments on PHOENIX14T and USTC-CSL datasets validate the\neffectiveness of our method. The code is available at:\nhttps://github.com/NaVi-start/Sign-IDD.\n","authors":["Shengeng Tang","Jiayi He","Dan Guo","Yanyan Wei","Feng Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2412.13609v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2401.05055v2","updated":"2024-12-19T03:11:39Z","published":"2024-01-10T10:30:18Z","title":"Deep learning in motion deblurring: current status, benchmarks and\n  future prospects","summary":"  Motion deblurring is one of the fundamental problems of computer vision and\nhas received continuous attention. The variability in blur, both within and\nacross images, imposes limitations on non-blind deblurring techniques that rely\non estimating the blur kernel. As a response, blind motion deblurring has\nemerged, aiming to restore clear and detailed images without prior knowledge of\nthe blur type, fueled by the advancements in deep learning methodologies.\nDespite strides in this field, a comprehensive synthesis of recent progress in\ndeep learning-based blind motion deblurring is notably absent. This paper fills\nthat gap by providing an exhaustive overview of the role of deep learning in\nblind motion deblurring, encompassing datasets, evaluation metrics, and methods\ndeveloped over the last six years. Specifically, we first introduce the types\nof motion blur and the fundamental principles of deblurring. Next, we outline\nthe shortcomings of traditional non-blind deblurring algorithms, emphasizing\nthe advantages of employing deep learning techniques for deblurring tasks.\nFollowing this, we categorize and summarize existing blind motion deblurring\nmethods based on different backbone networks, including convolutional neural\nnetworks, generative adversarial networks, recurrent neural networks, and\nTransformer networks. Subsequently, we elaborate not only on the fundamental\nprinciples of these different categories but also provide a comprehensive\nsummary and comparison of their advantages and limitations. Qualitative and\nquantitative experimental results conducted on four widely used datasets\nfurther compare the performance of SOTA methods. Finally, an analysis of\npresent challenges and future pathways. All collected models, benchmark\ndatasets, source code links, and codes for evaluation have been made publicly\navailable at https://github.com/VisionVerse/Blind-Motion-Deblurring-Survey\n","authors":["Yawen Xiang","Heng Zhou","Chengyang Li","Fangwei Sun","Zhongbo Li","Yongqiang Xie"],"pdf_url":"https://arxiv.org/pdf/2401.05055v2.pdf","comment":"29 pages, 13 figures, more than 150 papers have been included"},{"id":"http://arxiv.org/abs/2412.14484v1","updated":"2024-12-19T03:10:26Z","published":"2024-12-19T03:10:26Z","title":"DirectorLLM for Human-Centric Video Generation","summary":"  In this paper, we introduce DirectorLLM, a novel video generation model that\nemploys a large language model (LLM) to orchestrate human poses within videos.\nAs foundational text-to-video models rapidly evolve, the demand for\nhigh-quality human motion and interaction grows. To address this need and\nenhance the authenticity of human motions, we extend the LLM from a text\ngenerator to a video director and human motion simulator. Utilizing open-source\nresources from Llama 3, we train the DirectorLLM to generate detailed\ninstructional signals, such as human poses, to guide video generation. This\napproach offloads the simulation of human motion from the video generator to\nthe LLM, effectively creating informative outlines for human-centric scenes.\nThese signals are used as conditions by the video renderer, facilitating more\nrealistic and prompt-following video generation. As an independent LLM module,\nit can be applied to different video renderers, including UNet and DiT, with\nminimal effort. Experiments on automatic evaluation benchmarks and human\nevaluations show that our model outperforms existing ones in generating videos\nwith higher human motion fidelity, improved prompt faithfulness, and enhanced\nrendered subject naturalness.\n","authors":["Kunpeng Song","Tingbo Hou","Zecheng He","Haoyu Ma","Jialiang Wang","Animesh Sinha","Sam Tsai","Yaqiao Luo","Xiaoliang Dai","Li Chen","Xide Xia","Peizhao Zhang","Peter Vajda","Ahmed Elgammal","Felix Juefei-Xu"],"pdf_url":"https://arxiv.org/pdf/2412.14484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09529v2","updated":"2024-12-19T03:05:27Z","published":"2024-12-12T18:20:16Z","title":"Can Modern LLMs Act as Agent Cores in Radiology Environments?","summary":"  Advancements in large language models (LLMs) have paved the way for LLM-based\nagent systems that offer enhanced accuracy and interpretability across various\ndomains. Radiology, with its complex analytical requirements, is an ideal field\nfor the application of these agents. This paper aims to investigate the\npre-requisite question for building concrete radiology agents which is, `Can\nmodern LLMs act as agent cores in radiology environments?' To investigate it,\nwe introduce RadABench with three-fold contributions: First, we present\nRadABench-Data, a comprehensive synthetic evaluation dataset for LLM-based\nagents, generated from an extensive taxonomy encompassing 6 anatomies, 5\nimaging modalities, 10 tool categories, and 11 radiology tasks. Second, we\npropose RadABench-EvalPlat, a novel evaluation platform for agents featuring a\nprompt-driven workflow and the capability to simulate a wide range of radiology\ntoolsets. Third, we assess the performance of 7 leading LLMs on our benchmark\nfrom 5 perspectives with multiple metrics. Our findings indicate that while\ncurrent LLMs demonstrate strong capabilities in many areas, they are still not\nsufficiently advanced to serve as the central agent core in a fully operational\nradiology agent system. Additionally, we identify key factors influencing the\nperformance of LLM-based agent cores, offering insights for clinicians on how\nto apply agent systems in real-world radiology practices effectively. All of\nour code and data are open-sourced in\nhttps://github.com/MAGIC-AI4Med/RadABench.\n","authors":["Qiaoyu Zheng","Chaoyi Wu","Pengcheng Qiu","Lisong Dai","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2412.09529v2.pdf","comment":"22 pages,7 figures"},{"id":"http://arxiv.org/abs/2412.14480v1","updated":"2024-12-19T03:04:34Z","published":"2024-12-19T03:04:34Z","title":"GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question\n  Answering","summary":"  In Embodied Question Answering (EQA), agents must explore and develop a\nsemantic understanding of an unseen environment in order to answer a situated\nquestion with confidence. This remains a challenging problem in robotics, due\nto the difficulties in obtaining useful semantic representations, updating\nthese representations online, and leveraging prior world knowledge for\nefficient exploration and planning. Aiming to address these limitations, we\npropose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic\nscene graphs (3DSGs) and task relevant images as multi-modal memory for\ngrounding Vision-Language Models (VLMs) to perform EQA tasks in unseen\nenvironments. We employ a hierarchical planning approach that exploits the\nhierarchical nature of 3DSGs for structured planning and semantic-guided\nexploration. Through experiments in simulation on the HM-EQA dataset and in the\nreal world in home and office environments, we demonstrate that our method\noutperforms key baselines by completing EQA tasks with higher success rates and\nfewer planning steps.\n","authors":["Saumya Saxena","Blake Buchanan","Chris Paxton","Bingqing Chen","Narunas Vaskevicius","Luigi Palmieri","Jonathan Francis","Oliver Kroemer"],"pdf_url":"https://arxiv.org/pdf/2412.14480v1.pdf","comment":"Project website: https://saumyasaxena.github.io/grapheqa"},{"id":"http://arxiv.org/abs/2402.18292v6","updated":"2024-12-19T03:03:58Z","published":"2024-02-28T12:37:30Z","title":"FSL-Rectifier: Rectify Outliers in Few-Shot Learning via Test-Time\n  Augmentation","summary":"  Few-shot learning (FSL) commonly requires a model to identify images\n(queries) that belong to classes unseen during training, based on a few\nlabelled samples of the new classes (support set) as reference. So far, plenty\nof algorithms involve training data augmentation to improve the generalization\ncapability of FSL models, but outlier queries or support images during\ninference can still pose great generalization challenges. In this work, to\nreduce the bias caused by the outlier samples, we generate additional\ntest-class samples by combining original samples with suitable train-class\nsamples via a generative image combiner. Then, we obtain averaged features via\nan augmentor, which leads to more typical representations through the\naveraging. We experimentally and theoretically demonstrate the effectiveness of\nour method, obtaining a test accuracy improvement proportion of around 10\\%\n(e.g., from 46.86\\% to 53.28\\%) for trained FSL models. Importantly, given a\npretrained image combiner, our method is training-free for off-the-shelf FSL\nmodels, whose performance can be improved without extra datasets nor further\ntraining of the models themselves. Codes are available at\nhttps://github.com/WendyBaiYunwei/FSL-Rectifier-Pub.\n","authors":["Yunwei Bai","Ying Kiat Tan","Shiming Chen","Yao Shu","Tsuhan Chen"],"pdf_url":"https://arxiv.org/pdf/2402.18292v6.pdf","comment":"To be published in AAAI 2025"},{"id":"http://arxiv.org/abs/2403.13352v5","updated":"2024-12-19T02:57:25Z","published":"2024-03-20T07:31:07Z","title":"AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in\n  Text-to-Image Generation","summary":"  Text-to-Image (T2I) diffusion models have achieved remarkable success in\nimage generation. Despite their progress, challenges remain in both\nprompt-following ability, image quality and lack of high-quality datasets,\nwhich are essential for refining these models. As acquiring labeled data is\ncostly, we introduce AGFSync, a framework that enhances T2I diffusion models\nthrough Direct Preference Optimization (DPO) in a fully AI-driven approach.\nAGFSync utilizes Vision-Language Models (VLM) to assess image quality across\nstyle, coherence, and aesthetics, generating feedback data within an AI-driven\nloop. By applying AGFSync to leading T2I models such as SD v1.4, v1.5, and\nSDXL-base, our extensive experiments on the TIFA dataset demonstrate notable\nimprovements in VQA scores, aesthetic evaluations, and performance on the HPSv2\nbenchmark, consistently outperforming the base models. AGFSync's method of\nrefining T2I diffusion models paves the way for scalable alignment techniques.\nOur code and dataset are publicly available at\nhttps://anjingkun.github.io/AGFSync.\n","authors":["Jingkun An","Yinghao Zhu","Zongjian Li","Enshen Zhou","Haoran Feng","Xijie Huang","Bohua Chen","Yemin Shi","Chengwei Pan"],"pdf_url":"https://arxiv.org/pdf/2403.13352v5.pdf","comment":"Accepted by AAAI-2025"},{"id":"http://arxiv.org/abs/2412.14475v1","updated":"2024-12-19T02:49:55Z","published":"2024-12-19T02:49:55Z","title":"MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval","summary":"  Despite the rapidly growing demand for multimodal retrieval, progress in this\nfield remains severely constrained by a lack of training data. In this paper,\nwe introduce MegaPairs, a novel data synthesis method that leverages vision\nlanguage models (VLMs) and open-domain images, together with a massive\nsynthetic dataset generated from this method. Our empirical analysis shows that\nMegaPairs generates high-quality data, enabling the multimodal retriever to\nsignificantly outperform the baseline model trained on 70$\\times$ more data\nfrom existing datasets. Moreover, since MegaPairs solely relies on general\nimage corpora and open-source VLMs, it can be easily scaled up, enabling\ncontinuous improvements in retrieval performance. In this stage, we produced\nmore than 26 million training instances and trained several models of varying\nsizes using this data. These new models achieve state-of-the-art zero-shot\nperformance across 4 popular composed image retrieval (CIR) benchmarks and the\nhighest overall performance on the 36 datasets provided by MMEB. They also\ndemonstrate notable performance improvements with additional downstream\nfine-tuning. Our produced dataset, well-trained models, and data synthesis\npipeline will be made publicly available to facilitate the future development\nof this field.\n","authors":["Junjie Zhou","Zheng Liu","Ze Liu","Shitao Xiao","Yueze Wang","Bo Zhao","Chen Jason Zhang","Defu Lian","Yongping Xiong"],"pdf_url":"https://arxiv.org/pdf/2412.14475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14473v1","updated":"2024-12-19T02:47:17Z","published":"2024-12-19T02:47:17Z","title":"Promptable Representation Distribution Learning and Data Augmentation\n  for Gigapixel Histopathology WSI Analysis","summary":"  Gigapixel image analysis, particularly for whole slide images (WSIs), often\nrelies on multiple instance learning (MIL). Under the paradigm of MIL, patch\nimage representations are extracted and then fixed during the training of the\nMIL classifiers for efficiency consideration. However, the invariance of\nrepresentations makes it difficult to perform data augmentation for WSI-level\nmodel training, which significantly limits the performance of the downstream\nWSI analysis. The current data augmentation methods for gigapixel images either\nintroduce additional computational costs or result in a loss of semantic\ninformation, which is hard to meet the requirements for efficiency and\nstability needed for WSI model training. In this paper, we propose a Promptable\nRepresentation Distribution Learning framework (PRDL) for both patch-level\nrepresentation learning and WSI-level data augmentation. Meanwhile, we explore\nthe use of prompts to guide data augmentation in feature space, which achieves\npromptable data augmentation for training robust WSI-level models. The\nexperimental results have demonstrated that the proposed method stably\noutperforms state-of-the-art methods.\n","authors":["Kunming Tang","Zhiguo Jiang","Jun Shi","Wei Wang","Haibo Wu","Yushan Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.14473v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.09191v3","updated":"2024-12-19T02:44:14Z","published":"2024-12-12T11:38:46Z","title":"RAD: Region-Aware Diffusion Models for Image Inpainting","summary":"  Diffusion models have achieved remarkable success in image generation, with\napplications broadening across various domains. Inpainting is one such\napplication that can benefit significantly from diffusion models. Existing\nmethods either hijack the reverse process of a pretrained diffusion model or\ncast the problem into a larger framework, \\ie, conditioned generation. However,\nthese approaches often require nested loops in the generation process or\nadditional components for conditioning. In this paper, we present region-aware\ndiffusion models (RAD) for inpainting with a simple yet effective reformulation\nof the vanilla diffusion models. RAD utilizes a different noise schedule for\neach pixel, which allows local regions to be generated asynchronously while\nconsidering the global image context. A plain reverse process requires no\nadditional components, enabling RAD to achieve inference time up to 100 times\nfaster than the state-of-the-art approaches. Moreover, we employ low-rank\nadaptation (LoRA) to fine-tune RAD based on other pretrained diffusion models,\nreducing computational burdens in training as well. Experiments demonstrated\nthat RAD provides state-of-the-art results both qualitatively and\nquantitatively, on the FFHQ, LSUN Bedroom, and ImageNet datasets.\n","authors":["Sora Kim","Sungho Suh","Minsik Lee"],"pdf_url":"https://arxiv.org/pdf/2412.09191v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14170v2","updated":"2024-12-19T02:42:46Z","published":"2024-12-18T18:59:53Z","title":"E-CAR: Efficient Continuous Autoregressive Image Generation via\n  Multistage Modeling","summary":"  Recent advances in autoregressive (AR) models with continuous tokens for\nimage generation show promising results by eliminating the need for discrete\ntokenization. However, these models face efficiency challenges due to their\nsequential token generation nature and reliance on computationally intensive\ndiffusion-based sampling. We present ECAR (Efficient Continuous Auto-Regressive\nImage Generation via Multistage Modeling), an approach that addresses these\nlimitations through two intertwined innovations: (1) a stage-wise continuous\ntoken generation strategy that reduces computational complexity and provides\nprogressively refined token maps as hierarchical conditions, and (2) a\nmultistage flow-based distribution modeling method that transforms only\npartial-denoised distributions at each stage comparing to complete denoising in\nnormal diffusion models. Holistically, ECAR operates by generating tokens at\nincreasing resolutions while simultaneously denoising the image at each stage.\nThis design not only reduces token-to-image transformation cost by a factor of\nthe stage number but also enables parallel processing at the token level. Our\napproach not only enhances computational efficiency but also aligns naturally\nwith image generation principles by operating in continuous token space and\nfollowing a hierarchical generation process from coarse to fine details.\nExperimental results demonstrate that ECAR achieves comparable image quality to\nDiT Peebles & Xie [2023] while requiring 10$\\times$ FLOPs reduction and\n5$\\times$ speedup to generate a 256$\\times$256 image.\n","authors":["Zhihang Yuan","Yuzhang Shang","Hanling Zhang","Tongcheng Fang","Rui Xie","Bingxin Xu","Yan Yan","Shengen Yan","Guohao Dai","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14170v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08475v3","updated":"2024-12-19T02:42:45Z","published":"2024-09-13T02:02:07Z","title":"RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense\n  Positive Supervision","summary":"  RT-DETR is the first real-time end-to-end transformer-based object detector.\nIts efficiency comes from the framework design and the Hungarian matching.\nHowever, compared to dense supervision detectors like the YOLO series, the\nHungarian matching provides much sparser supervision, leading to insufficient\nmodel training and difficult to achieve optimal results. To address these\nissues, we proposed a hierarchical dense positive supervision method based on\nRT-DETR, named RT-DETRv3. Firstly, we introduce a CNN-based auxiliary branch\nthat provides dense supervision that collaborates with the original decoder to\nenhance the encoder feature representation. Secondly, to address insufficient\ndecoder training, we propose a novel learning strategy involving self-attention\nperturbation. This strategy diversifies label assignment for positive samples\nacross multiple query groups, thereby enriching positive supervisions.\nAdditionally, we introduce a shared-weight decoder branch for dense positive\nsupervision to ensure more high-quality queries matching each ground truth.\nNotably, all aforementioned modules are training-only. We conduct extensive\nexperiments to demonstrate the effectiveness of our approach on COCO val2017.\nRT-DETRv3 significantly outperforms existing real-time detectors, including the\nRT-DETR series and the YOLO series. For example, RT-DETRv3-R18 achieves 48.1%\nAP (+1.6%/+1.4%) compared to RT-DETR-R18/RT-DETRv2-R18, while maintaining the\nsame latency. Furthermore, RT-DETRv3-R101 can attain an impressive 54.6% AP\noutperforming YOLOv10-X. The code will be released at\nhttps://github.com/clxia12/RT-DETRv3.\n","authors":["Shuo Wang","Chunlong Xia","Feng Lv","Yifeng Shi"],"pdf_url":"https://arxiv.org/pdf/2409.08475v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14205v3","updated":"2024-12-19T02:35:48Z","published":"2024-05-23T06:03:19Z","title":"Agent Planning with World Knowledge Model","summary":"  Recent endeavors towards directly using large language models (LLMs) as agent\nmodels to execute interactive planning tasks have shown commendable results.\nDespite their achievements, however, they still struggle with brainless\ntrial-and-error in global planning and generating hallucinatory actions in\nlocal planning due to their poor understanding of the ``real'' physical world.\nImitating humans' mental world knowledge model which provides global prior\nknowledge before the task and maintains local dynamic knowledge during the\ntask, in this paper, we introduce parametric World Knowledge Model (WKM) to\nfacilitate agent planning. Concretely, we steer the agent model to\nself-synthesize knowledge from both expert and sampled trajectories. Then we\ndevelop WKM, providing prior task knowledge to guide the global planning and\ndynamic state knowledge to assist the local planning. Experimental results on\nthree complex real-world simulated datasets with three state-of-the-art\nopen-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our\nmethod can achieve superior performance compared to various strong baselines.\nBesides, we analyze to illustrate that our WKM can effectively alleviate the\nblind trial-and-error and hallucinatory action issues, providing strong support\nfor the agent's understanding of the world. Other interesting findings include:\n1) our instance-level task knowledge can generalize better to unseen tasks, 2)\nweak WKM can guide strong agent model planning, and 3) unified WKM training has\npromising potential for further development. The code is available at\nhttps://github.com/zjunlp/WKM.\n","authors":["Shuofei Qiao","Runnan Fang","Ningyu Zhang","Yuqi Zhu","Xiang Chen","Shumin Deng","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.14205v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.14465v1","updated":"2024-12-19T02:24:35Z","published":"2024-12-19T02:24:35Z","title":"DiffusionTrend: A Minimalist Approach to Virtual Fashion Try-On","summary":"  We introduce DiffusionTrend for virtual fashion try-on, which forgoes the\nneed for retraining diffusion models. Using advanced diffusion models,\nDiffusionTrend harnesses latent information rich in prior information to\ncapture the nuances of garment details. Throughout the diffusion denoising\nprocess, these details are seamlessly integrated into the model image\ngeneration, expertly directed by a precise garment mask crafted by a\nlightweight and compact CNN. Although our DiffusionTrend model initially\ndemonstrates suboptimal metric performance, our exploratory approach offers\nsome important advantages: (1) It circumvents resource-intensive retraining of\ndiffusion models on large datasets. (2) It eliminates the necessity for various\ncomplex and user-unfriendly model inputs. (3) It delivers a visually compelling\ntry-on experience, underscoring the potential of training-free diffusion model.\nThis initial foray into the application of untrained diffusion models in\nvirtual try-on technology potentially paves the way for further exploration and\nrefinement in this industrially and academically valuable field.\n","authors":["Wengyi Zhan","Mingbao Lin","Shuicheng Yan","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2412.14465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14464v1","updated":"2024-12-19T02:23:55Z","published":"2024-12-19T02:23:55Z","title":"LiftRefine: Progressively Refined View Synthesis from 3D Lifting with\n  Volume-Triplane Representations","summary":"  We propose a new view synthesis method via synthesizing a 3D neural field\nfrom both single or few-view input images. To address the ill-posed nature of\nthe image-to-3D generation problem, we devise a two-stage method that involves\na reconstruction model and a diffusion model for view synthesis. Our\nreconstruction model first lifts one or more input images to the 3D space from\na volume as the coarse-scale 3D representation followed by a tri-plane as the\nfine-scale 3D representation. To mitigate the ambiguity in occluded regions,\nour diffusion model then hallucinates missing details in the rendered images\nfrom tri-planes. We then introduce a new progressive refinement technique that\niteratively applies the reconstruction and diffusion model to gradually\nsynthesize novel views, boosting the overall quality of the 3D representations\nand their rendering. Empirical evaluation demonstrates the superiority of our\nmethod over state-of-the-art methods on the synthetic SRN-Car dataset, the\nin-the-wild CO3D dataset, and large-scale Objaverse dataset while achieving\nboth sampling efficacy and multi-view consistency.\n","authors":["Tung Do","Thuan Hoang Nguyen","Anh Tuan Tran","Rang Nguyen","Binh-Son Hua"],"pdf_url":"https://arxiv.org/pdf/2412.14464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14462v1","updated":"2024-12-19T02:23:13Z","published":"2024-12-19T02:23:13Z","title":"Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion","summary":"  As a common image editing operation, image composition involves integrating\nforeground objects into background scenes. In this paper, we expand the\napplication of the concept of Affordance from human-centered image composition\ntasks to a more general object-scene composition framework, addressing the\ncomplex interplay between foreground objects and background scenes. Following\nthe principle of Affordance, we define the affordance-aware object insertion\ntask, which aims to seamlessly insert any object into any scene with various\nposition prompts. To address the limited data issue and incorporate this task,\nwe constructed the SAM-FB dataset, which contains over 3 million examples\nacross more than 3,000 object categories. Furthermore, we propose the\nMask-Aware Dual Diffusion (MADD) model, which utilizes a dual-stream\narchitecture to simultaneously denoise the RGB image and the insertion mask. By\nexplicitly modeling the insertion mask in the diffusion process, MADD\neffectively facilitates the notion of affordance. Extensive experimental\nresults show that our method outperforms the state-of-the-art methods and\nexhibits strong generalization performance on in-the-wild images. Please refer\nto our code on https://github.com/KaKituken/affordance-aware-any.\n","authors":["Jixuan He","Wanhua Li","Ye Liu","Junsik Kim","Donglai Wei","Hanspeter Pfister"],"pdf_url":"https://arxiv.org/pdf/2412.14462v1.pdf","comment":"Code is available at:\n  https://github.com/KaKituken/affordance-aware-any. Project page at:\n  https://kakituken.github.io/affordance-any.github.io/"},{"id":"http://arxiv.org/abs/2405.14768v3","updated":"2024-12-19T02:18:54Z","published":"2024-05-23T16:35:52Z","title":"WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of\n  Large Language Models","summary":"  Large language models (LLMs) need knowledge updates to meet the ever-growing\nworld facts and correct the hallucinated responses, facilitating the methods of\nlifelong model editing. Where the updated knowledge resides in memories is a\nfundamental question for model editing. In this paper, we find that editing\neither long-term memory (direct model parameters) or working memory\n(non-parametric knowledge of neural network activations/representations by\nretrieval) will result in an impossible triangle -- reliability,\ngeneralization, and locality can not be realized together in the lifelong\nediting settings. For long-term memory, directly editing the parameters will\ncause conflicts with irrelevant pretrained knowledge or previous edits (poor\nreliability and locality). For working memory, retrieval-based activations can\nhardly make the model understand the edits and generalize (poor\ngeneralization). Therefore, we propose WISE to bridge the gap between memories.\nIn WISE, we design a dual parametric memory scheme, which consists of the main\nmemory for the pretrained knowledge and a side memory for the edited knowledge.\nWe only edit the knowledge in the side memory and train a router to decide\nwhich memory to go through when given a query. For continual editing, we devise\na knowledge-sharding mechanism where different sets of edits reside in distinct\nsubspaces of parameters, and are subsequently merged into a shared memory\nwithout conflicts. Extensive experiments show that WISE can outperform previous\nmodel editing methods and overcome the impossible triangle under lifelong model\nediting of question answering, hallucination, and out-of-distribution settings\nacross trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is\navailable at https://github.com/zjunlp/EasyEdit.\n","authors":["Peng Wang","Zexi Li","Ningyu Zhang","Ziwen Xu","Yunzhi Yao","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.14768v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.14456v1","updated":"2024-12-19T02:15:55Z","published":"2024-12-19T02:15:55Z","title":"LEDiff: Latent Exposure Diffusion for HDR Generation","summary":"  While consumer displays increasingly support more than 10 stops of dynamic\nrange, most image assets such as internet photographs and generative AI content\nremain limited to 8-bit low dynamic range (LDR), constraining their utility\nacross high dynamic range (HDR) applications. Currently, no generative model\ncan produce high-bit, high-dynamic range content in a generalizable way.\nExisting LDR-to-HDR conversion methods often struggle to produce photorealistic\ndetails and physically-plausible dynamic range in the clipped areas. We\nintroduce LEDiff, a method that enables a generative model with HDR content\ngeneration through latent space fusion inspired by image-space exposure fusion\ntechniques. It also functions as an LDR-to-HDR converter, expanding the dynamic\nrange of existing low-dynamic range images. Our approach uses a small HDR\ndataset to enable a pretrained diffusion model to recover detail and dynamic\nrange in clipped highlights and shadows. LEDiff brings HDR capabilities to\nexisting generative models and converts any LDR image to HDR, creating\nphotorealistic HDR outputs for image generation, image-based lighting (HDR\nenvironment map generation), and photographic effects such as depth of field\nsimulation, where linear HDR data is essential for realistic quality.\n","authors":["Chao Wang","Zhihao Xia","Thomas Leimkuehler","Karol Myszkowski","Xuaner Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.14456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17969v3","updated":"2024-12-19T02:10:00Z","published":"2024-05-28T08:56:33Z","title":"Knowledge Circuits in Pretrained Transformers","summary":"  The remarkable capabilities of modern large language models are rooted in\ntheir vast repositories of knowledge encoded within their parameters, enabling\nthem to perceive the world and engage in reasoning. The inner workings of how\nthese models store knowledge have long been a subject of intense interest and\ninvestigation among researchers. To date, most studies have concentrated on\nisolated components within these models, such as the Multilayer Perceptrons and\nattention head. In this paper, we delve into the computation graph of the\nlanguage model to uncover the knowledge circuits that are instrumental in\narticulating specific knowledge. The experiments, conducted with GPT2 and\nTinyLLAMA, have allowed us to observe how certain information heads, relation\nheads, and Multilayer Perceptrons collaboratively encode knowledge within the\nmodel. Moreover, we evaluate the impact of current knowledge editing techniques\non these knowledge circuits, providing deeper insights into the functioning and\nconstraints of these editing methodologies. Finally, we utilize knowledge\ncircuits to analyze and interpret language model behaviors such as\nhallucinations and in-context learning. We believe the knowledge circuits hold\npotential for advancing our understanding of Transformers and guiding the\nimproved design of knowledge editing. Code and data are available in\nhttps://github.com/zjunlp/KnowledgeCircuits.\n","authors":["Yunzhi Yao","Ningyu Zhang","Zekun Xi","Mengru Wang","Ziwen Xu","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.17969v3.pdf","comment":"NeurIPS 2024, 26 pages"},{"id":"http://arxiv.org/abs/2406.17256v2","updated":"2024-12-19T02:05:58Z","published":"2024-06-25T03:50:20Z","title":"Disentangled Motion Modeling for Video Frame Interpolation","summary":"  Video Frame Interpolation (VFI) aims to synthesize intermediate frames\nbetween existing frames to enhance visual smoothness and quality. Beyond the\nconventional methods based on the reconstruction loss, recent works have\nemployed generative models for improved perceptual quality. However, they\nrequire complex training and large computational costs for pixel space\nmodeling. In this paper, we introduce disentangled Motion Modeling (MoMo), a\ndiffusion-based approach for VFI that enhances visual quality by focusing on\nintermediate motion modeling. We propose a disentangled two-stage training\nprocess. In the initial stage, frame synthesis and flow models are trained to\ngenerate accurate frames and flows optimal for synthesis. In the subsequent\nstage, we introduce a motion diffusion model, which incorporates our novel\nU-Net architecture specifically designed for optical flow, to generate\nbi-directional flows between frames. By learning the simpler low-frequency\nrepresentation of motions, MoMo achieves superior perceptual quality with\nreduced computational demands compared to the generative modeling methods on\nthe pixel space. MoMo surpasses state-of-the-art methods in perceptual metrics\nacross various benchmarks, demonstrating its efficacy and efficiency in VFI.\n","authors":["Jaihyun Lew","Jooyoung Choi","Chaehun Shin","Dahuin Jung","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2406.17256v2.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14453v1","updated":"2024-12-19T02:05:28Z","published":"2024-12-19T02:05:28Z","title":"Multimodal Latent Diffusion Model for Complex Sewing Pattern Generation","summary":"  Generating sewing patterns in garment design is receiving increasing\nattention due to its CG-friendly and flexible-editing nature. Previous sewing\npattern generation methods have been able to produce exquisite clothing, but\nstruggle to design complex garments with detailed control. To address these\nissues, we propose SewingLDM, a multi-modal generative model that generates\nsewing patterns controlled by text prompts, body shapes, and garment sketches.\nInitially, we extend the original vector of sewing patterns into a more\ncomprehensive representation to cover more intricate details and then compress\nthem into a compact latent space. To learn the sewing pattern distribution in\nthe latent space, we design a two-step training strategy to inject the\nmulti-modal conditions, \\ie, body shapes, text prompts, and garment sketches,\ninto a diffusion model, ensuring the generated garments are body-suited and\ndetail-controlled. Comprehensive qualitative and quantitative experiments show\nthe effectiveness of our proposed method, significantly surpassing previous\napproaches in terms of complex garment design and various body adaptability.\nOur project page: https://shengqiliu1.github.io/SewingLDM.\n","authors":["Shengqi Liu","Yuhao Cheng","Zhuo Chen","Xingyu Ren","Wenhan Zhu","Lincheng Li","Mengxiao Bi","Xiaokang Yang","Yichao Yan"],"pdf_url":"https://arxiv.org/pdf/2412.14453v1.pdf","comment":"Our project page: https://shengqiliu1.github.io/SewingLDM"},{"id":"http://arxiv.org/abs/2412.14449v1","updated":"2024-12-19T01:58:00Z","published":"2024-12-19T01:58:00Z","title":"Color Enhancement for V-PCC Compressed Point Cloud via 2D Attribute Map\n  Optimization","summary":"  Video-based point cloud compression (V-PCC) converts the dynamic point cloud\ndata into video sequences using traditional video codecs for efficient\nencoding. However, this lossy compression scheme introduces artifacts that\ndegrade the color attributes of the data. This paper introduces a framework\ndesigned to enhance the color quality in the V-PCC compressed point clouds. We\npropose the lightweight de-compression Unet (LDC-Unet), a 2D neural network, to\noptimize the projection maps generated during V-PCC encoding. The optimized 2D\nmaps will then be back-projected to the 3D space to enhance the corresponding\npoint cloud attributes. Additionally, we introduce a transfer learning strategy\nand develop a customized natural image dataset for the initial training. The\nmodel was then fine-tuned using the projection maps of the compressed point\nclouds. The whole strategy effectively addresses the scarcity of point cloud\ntraining data. Our experiments, conducted on the public 8i voxelized full\nbodies long sequences (8iVSLF) dataset, demonstrate the effectiveness of our\nproposed method in improving the color quality.\n","authors":["Jingwei Bao","Yu Liu","Zeliang Li","Shuyuan Zhu","Siu-Kei Au Yeung"],"pdf_url":"https://arxiv.org/pdf/2412.14449v1.pdf","comment":"IEEE VCIP 2024"},{"id":"http://arxiv.org/abs/2412.12603v2","updated":"2024-12-19T01:57:56Z","published":"2024-12-17T07:00:07Z","title":"RemoteTrimmer: Adaptive Structural Pruning for Remote Sensing Image\n  Classification","summary":"  Since high resolution remote sensing image classification often requires a\nrelatively high computation complexity, lightweight models tend to be practical\nand efficient. Model pruning is an effective method for model compression.\nHowever, existing methods rarely take into account the specificity of remote\nsensing images, resulting in significant accuracy loss after pruning. To this\nend, we propose an effective structural pruning approach for remote sensing\nimage classification. Specifically, a pruning strategy that amplifies the\ndifferences in channel importance of the model is introduced. Then an adaptive\nmining loss function is designed for the fine-tuning process of the pruned\nmodel. Finally, we conducted experiments on two remote sensing classification\ndatasets. The experimental results demonstrate that our method achieves minimal\naccuracy loss after compressing remote sensing classification models, achieving\nstate-of-the-art (SoTA) performance.\n","authors":["Guangwenjie Zou","Liang Yao","Fan Liu","Chuanyi Zhang","Xin Li","Ning Chen","Shengxiang Xu","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12603v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14446v1","updated":"2024-12-19T01:53:36Z","published":"2024-12-19T01:53:36Z","title":"VLM-AD: End-to-End Autonomous Driving through Vision-Language Model\n  Supervision","summary":"  Human drivers rely on commonsense reasoning to navigate diverse and dynamic\nreal-world scenarios. Existing end-to-end (E2E) autonomous driving (AD) models\nare typically optimized to mimic driving patterns observed in data, without\ncapturing the underlying reasoning processes. This limitation constrains their\nability to handle challenging driving scenarios. To close this gap, we propose\nVLM-AD, a method that leverages vision-language models (VLMs) as teachers to\nenhance training by providing additional supervision that incorporates\nunstructured reasoning information and structured action labels. Such\nsupervision enhances the model's ability to learn richer feature\nrepresentations that capture the rationale behind driving patterns.\nImportantly, our method does not require a VLM during inference, making it\npractical for real-time deployment. When integrated with state-of-the-art\nmethods, VLM-AD achieves significant improvements in planning accuracy and\nreduced collision rates on the nuScenes dataset.\n","authors":["Yi Xu","Yuxin Hu","Zaiwei Zhang","Gregory P. Meyer","Siva Karthik Mustikovela","Siddhartha Srinivasa","Eric M. Wolff","Xin Huang"],"pdf_url":"https://arxiv.org/pdf/2412.14446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14444v1","updated":"2024-12-19T01:45:58Z","published":"2024-12-19T01:45:58Z","title":"GenHMR: Generative Human Mesh Recovery","summary":"  Human mesh recovery (HMR) is crucial in many computer vision applications;\nfrom health to arts and entertainment. HMR from monocular images has\npredominantly been addressed by deterministic methods that output a single\nprediction for a given 2D image. However, HMR from a single image is an\nill-posed problem due to depth ambiguity and occlusions. Probabilistic methods\nhave attempted to address this by generating and fusing multiple plausible 3D\nreconstructions, but their performance has often lagged behind deterministic\napproaches. In this paper, we introduce GenHMR, a novel generative framework\nthat reformulates monocular HMR as an image-conditioned generative task,\nexplicitly modeling and mitigating uncertainties in the 2D-to-3D mapping\nprocess. GenHMR comprises two key components: (1) a pose tokenizer to convert\n3D human poses into a sequence of discrete tokens in a latent space, and (2) an\nimage-conditional masked transformer to learn the probabilistic distributions\nof the pose tokens, conditioned on the input image prompt along with randomly\nmasked token sequence. During inference, the model samples from the learned\nconditional distribution to iteratively decode high-confidence pose tokens,\nthereby reducing 3D reconstruction uncertainties. To further refine the\nreconstruction, a 2D pose-guided refinement technique is proposed to directly\nfine-tune the decoded pose tokens in the latent space, which forces the\nprojected 3D body mesh to align with the 2D pose clues. Experiments on\nbenchmark datasets demonstrate that GenHMR significantly outperforms\nstate-of-the-art methods. Project website can be found at\nhttps://m-usamasaleem.github.io/publication/GenHMR/GenHMR.html\n","authors":["Muhammad Usama Saleem","Ekkasit Pinyoanuntapong","Pu Wang","Hongfei Xue","Srijan Das","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14432v1","updated":"2024-12-19T01:21:23Z","published":"2024-12-19T01:21:23Z","title":"IntroStyle: Training-Free Introspective Style Attribution using\n  Diffusion Features","summary":"  Text-to-image (T2I) models have gained widespread adoption among content\ncreators and the general public. However, this has sparked significant concerns\nregarding data privacy and copyright infringement among artists. Consequently,\nthere is an increasing demand for T2I models to incorporate mechanisms that\nprevent the generation of specific artistic styles, thereby safeguarding\nintellectual property rights. Existing methods for style extraction typically\nnecessitate the collection of custom datasets and the training of specialized\nmodels. This, however, is resource-intensive, time-consuming, and often\nimpractical for real-time applications. Moreover, it may not adequately address\nthe dynamic nature of artistic styles and the rapidly evolving landscape of\ndigital art. We present a novel, training-free framework to solve the style\nattribution problem, using the features produced by a diffusion model alone,\nwithout any external modules or retraining. This is denoted as introspective\nstyle attribution (IntroStyle) and demonstrates superior performance to\nstate-of-the-art models for style retrieval. We also introduce a synthetic\ndataset of Style Hacks (SHacks) to isolate artistic style and evaluate\nfine-grained style attribution performance.\n","authors":["Anand Kumar","Jiteng Mu","Nuno Vasconcelos"],"pdf_url":"https://arxiv.org/pdf/2412.14432v1.pdf","comment":"16 pages, 17 figures"},{"id":"http://arxiv.org/abs/2410.23904v3","updated":"2024-12-19T01:20:09Z","published":"2024-10-31T13:06:29Z","title":"EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI\n  Detection","summary":"  Detecting Human-Object Interactions (HOI) in zero-shot settings, where models\nmust handle unseen classes, poses significant challenges. Existing methods that\nrely on aligning visual encoders with large Vision-Language Models (VLMs) to\ntap into the extensive knowledge of VLMs, require large, computationally\nexpensive models and encounter training difficulties. Adapting VLMs with prompt\nlearning offers an alternative to direct alignment. However, fine-tuning on\ntask-specific datasets often leads to overfitting to seen classes and\nsuboptimal performance on unseen classes, due to the absence of unseen class\nlabels. To address these challenges, we introduce a novel prompt learning-based\nframework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce\nLarge Language Model (LLM) and VLM guidance for learnable prompts, integrating\ndetailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks.\nHowever, because training datasets contain seen-class labels alone, fine-tuning\nVLMs on such datasets tends to optimize learnable prompts for seen classes\ninstead of unseen ones. Therefore, we design prompt learning for unseen classes\nusing information from related seen classes, with LLMs utilized to highlight\nthe differences between unseen and related seen classes. Quantitative\nevaluations on benchmark datasets demonstrate that our EZ-HOI achieves\nstate-of-the-art performance across various zero-shot settings with only 10.35%\nto 33.95% of the trainable parameters compared to existing methods. Code is\navailable at https://github.com/ChelsieLei/EZ-HOI.\n","authors":["Qinqian Lei","Bo Wang","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2410.23904v3.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.07157v2","updated":"2024-12-19T00:53:53Z","published":"2024-12-10T03:34:56Z","title":"Multi-Scale Contrastive Learning for Video Temporal Grounding","summary":"  Temporal grounding, which localizes video moments related to a natural\nlanguage query, is a core problem of vision-language learning and video\nunderstanding. To encode video moments of varying lengths, recent methods\nemploy a multi-level structure known as a feature pyramid. In this structure,\nlower levels concentrate on short-range video moments, while higher levels\naddress long-range moments. Because higher levels experience downsampling to\naccommodate increasing moment length, their capacity to capture information is\nreduced and consequently leads to degraded information in moment\nrepresentations. To resolve this problem, we propose a contrastive learning\nframework to capture salient semantics among video moments. Our key methodology\nis to leverage samples from the feature space emanating from multiple stages of\nthe video encoder itself requiring neither data augmentation nor online memory\nbanks to obtain positive and negative samples. To enable such an extension, we\nintroduce a sampling process to draw multiple video moments corresponding to a\ncommon query. Subsequently, by utilizing these moments' representations across\nvideo encoder layers, we instantiate a novel form of multi-scale and\ncross-scale contrastive learning that links local short-range video moments\nwith global long-range video moments. Extensive experiments demonstrate the\neffectiveness of our framework for not only long-form but also short-form video\ngrounding.\n","authors":["Thong Thanh Nguyen","Yi Bin","Xiaobao Wu","Zhiyuan Hu","Cong-Duy T Nguyen","See-Kiong Ng","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2412.07157v2.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14428v1","updated":"2024-12-19T00:52:25Z","published":"2024-12-19T00:52:25Z","title":"WildSAT: Learning Satellite Image Representations from Wildlife\n  Observations","summary":"  What does the presence of a species reveal about a geographic location? We\nposit that habitat, climate, and environmental preferences reflected in species\ndistributions provide a rich source of supervision for learning satellite image\nrepresentations. We introduce WildSAT, which pairs satellite images with\nmillions of geo-tagged wildlife observations readily-available on citizen\nscience platforms. WildSAT uses a contrastive learning framework to combine\ninformation from species distribution maps with text descriptions that capture\nhabitat and range details, alongside satellite images, to train or fine-tune\nmodels. On a range of downstream satellite image recognition tasks, this\nsignificantly improves the performance of both randomly initialized models and\npre-trained models from sources like ImageNet or specialized satellite image\ndatasets. Additionally, the alignment with text enables zero-shot retrieval,\nallowing for search based on general descriptions of locations. We demonstrate\nthat WildSAT achieves better representations than recent methods that utilize\nother forms of cross-modal supervision, such as aligning satellite images with\nground images or wildlife photos. Finally, we analyze the impact of various\ndesign choices on downstream performance, highlighting the general\napplicability of our approach.\n","authors":["Rangel Daroya","Elijah Cole","Oisin Mac Aodha","Grant Van Horn","Subhransu Maji"],"pdf_url":"https://arxiv.org/pdf/2412.14428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07160v2","updated":"2024-12-19T00:48:45Z","published":"2024-12-10T03:41:07Z","title":"Motion-aware Contrastive Learning for Temporal Panoptic Scene Graph\n  Generation","summary":"  To equip artificial intelligence with a comprehensive understanding towards a\ntemporal world, video and 4D panoptic scene graph generation abstracts visual\ndata into nodes to represent entities and edges to capture temporal relations.\nExisting methods encode entity masks tracked across temporal dimensions (mask\ntubes), then predict their relations with temporal pooling operation, which\ndoes not fully utilize the motion indicative of the entities' relation. To\novercome this limitation, we introduce a contrastive representation learning\nframework that focuses on motion pattern for temporal scene graph generation.\nFirstly, our framework encourages the model to learn close representations for\nmask tubes of similar subject-relation-object triplets. Secondly, we seek to\npush apart mask tubes from their temporally shuffled versions. Moreover, we\nalso learn distant representations for mask tubes belonging to the same video\nbut different triplets. Extensive experiments show that our motion-aware\ncontrastive framework significantly improves state-of-the-art methods on both\nvideo and 4D datasets.\n","authors":["Thong Thanh Nguyen","Xiaobao Wu","Yi Bin","Cong-Duy T Nguyen","See-Kiong Ng","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2412.07160v2.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.09545v2","updated":"2024-12-19T00:30:08Z","published":"2024-12-12T18:35:26Z","title":"SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing","summary":"  We introduce SimAvatar, a framework designed to generate simulation-ready\nclothed 3D human avatars from a text prompt. Current text-driven human avatar\ngeneration methods either model hair, clothing, and the human body using a\nunified geometry or produce hair and garments that are not easily adaptable for\nsimulation within existing simulation pipelines. The primary challenge lies in\nrepresenting the hair and garment geometry in a way that allows leveraging\nestablished prior knowledge from foundational image diffusion models (e.g.,\nStable Diffusion) while being simulation-ready using either physics or neural\nsimulators. To address this task, we propose a two-stage framework that\ncombines the flexibility of 3D Gaussians with simulation-ready hair strands and\ngarment meshes. Specifically, we first employ three text-conditioned 3D\ngenerative models to generate garment mesh, body shape and hair strands from\nthe given text prompt. To leverage prior knowledge from foundational diffusion\nmodels, we attach 3D Gaussians to the body mesh, garment mesh, as well as hair\nstrands and learn the avatar appearance through optimization. To drive the\navatar given a pose sequence, we first apply physics simulators onto the\ngarment meshes and hair strands. We then transfer the motion onto 3D Gaussians\nthrough carefully designed mechanisms for each body part. As a result, our\nsynthesized avatars have vivid texture and realistic dynamic motion. To the\nbest of our knowledge, our method is the first to produce highly realistic,\nfully simulation-ready 3D avatars, surpassing the capabilities of current\napproaches.\n","authors":["Xueting Li","Ye Yuan","Shalini De Mello","Gilles Daviet","Jonathan Leaf","Miles Macklin","Jan Kautz","Umar Iqbal"],"pdf_url":"https://arxiv.org/pdf/2412.09545v2.pdf","comment":"Project website: https://nvlabs.github.io/SimAvatar/"},{"id":"http://arxiv.org/abs/2409.14747v5","updated":"2024-12-19T00:25:07Z","published":"2024-09-23T06:51:10Z","title":"Distribution-Level Feature Distancing for Machine Unlearning: Towards a\n  Better Trade-off Between Model Utility and Forgetting","summary":"  With the explosive growth of deep learning applications and increasing\nprivacy concerns, the right to be forgotten has become a critical requirement\nin various AI industries. For example, given a facial recognition system, some\nindividuals may wish to remove their personal data that might have been used in\nthe training phase. Unfortunately, deep neural networks sometimes unexpectedly\nleak personal identities, making this removal challenging. While recent machine\nunlearning algorithms aim to enable models to forget specific data, we identify\nan unintended utility drop-correlation collapse-in which the essential\ncorrelations between image features and true labels weaken during the\nforgetting process. To address this challenge, we propose Distribution-Level\nFeature Distancing (DLFD), a novel method that efficiently forgets instances\nwhile preserving task-relevant feature correlations. Our method synthesizes\ndata samples by optimizing the feature distribution to be distinctly different\nfrom that of forget samples, achieving effective results within a single\ntraining epoch. Through extensive experiments on facial recognition datasets,\nwe demonstrate that our approach significantly outperforms state-of-the-art\nmachine unlearning methods in both forgetting performance and model utility\npreservation.\n","authors":["Dasol Choi","Dongbin Na"],"pdf_url":"https://arxiv.org/pdf/2409.14747v5.pdf","comment":"10 pages, 6 figures, AAAI 2025 camera ready version"},{"id":"http://arxiv.org/abs/2412.14424v1","updated":"2024-12-19T00:24:00Z","published":"2024-12-19T00:24:00Z","title":"FedPIA -- Permuting and Integrating Adapters leveraging Wasserstein\n  Barycenters for Finetuning Foundation Models in Multi-Modal Federated\n  Learning","summary":"  Large Vision-Language Models typically require large text and image datasets\nfor effective fine-tuning. However, collecting data from various sites,\nespecially in healthcare, is challenging due to strict privacy regulations. An\nalternative is to fine-tune these models on end-user devices, such as in\nmedical clinics, without sending data to a server. These local clients\ntypically have limited computing power and small datasets, which are not enough\nfor fully fine-tuning large VLMs on their own. A naive solution to these\nscenarios is to leverage parameter-efficient fine-tuning (PEFT) strategies and\napply federated learning (FL) algorithms to combine the learned adapter\nweights, thereby respecting the resource limitations and data privacy. However,\nthis approach does not fully leverage the knowledge from multiple adapters\ntrained on diverse data distributions and for diverse tasks. The adapters are\nadversely impacted by data heterogeneity and task heterogeneity across clients\nresulting in suboptimal convergence. To this end, we propose a novel framework\ncalled FedPIA that improves upon the naive combinations of FL and PEFT by\nintroducing Permutation and Integration of the local Adapters in the server and\nglobal Adapters in the clients exploiting Wasserstein barycenters for improved\nblending of client-specific and client-agnostic knowledge. This layerwise\npermutation helps to bridge the gap in the parameter space of local and global\nadapters before integration. We conduct over 2000 client-level experiments\nutilizing 48 medical image datasets across five different medical\nvision-language FL task settings encompassing visual question answering as well\nas image and report-based multi-label disease detection. Our experiments\ninvolving diverse client settings, ten different modalities, and two VLM\nbackbones demonstrate that FedPIA consistently outperforms the state-of-the-art\nPEFT-FL baselines.\n","authors":["Pramit Saha","Divyanshu Mishra","Felix Wagner","Konstantinos Kamnitsas","J. Alison Noble"],"pdf_url":"https://arxiv.org/pdf/2412.14424v1.pdf","comment":"Accepted for publication in AAAI 2025 (Main Track)"},{"id":"http://arxiv.org/abs/2412.15447v1","updated":"2024-12-19T22:59:55Z","published":"2024-12-19T22:59:55Z","title":"LiHi-GS: LiDAR-Supervised Gaussian Splatting for Highway Driving Scene\n  Reconstruction","summary":"  Photorealistic 3D scene reconstruction plays an important role in autonomous\ndriving, enabling the generation of novel data from existing datasets to\nsimulate safety-critical scenarios and expand training data without additional\nacquisition costs. Gaussian Splatting (GS) facilitates real-time,\nphotorealistic rendering with an explicit 3D Gaussian representation of the\nscene, providing faster processing and more intuitive scene editing than the\nimplicit Neural Radiance Fields (NeRFs). While extensive GS research has\nyielded promising advancements in autonomous driving applications, they\noverlook two critical aspects: First, existing methods mainly focus on\nlow-speed and feature-rich urban scenes and ignore the fact that highway\nscenarios play a significant role in autonomous driving. Second, while LiDARs\nare commonplace in autonomous driving platforms, existing methods learn\nprimarily from images and use LiDAR only for initial estimates or without\nprecise sensor modeling, thus missing out on leveraging the rich depth\ninformation LiDAR offers and limiting the ability to synthesize LiDAR data. In\nthis paper, we propose a novel GS method for dynamic scene synthesis and\nediting with improved scene reconstruction through LiDAR supervision and\nsupport for LiDAR rendering. Unlike prior works that are tested mostly on urban\ndatasets, to the best of our knowledge, we are the first to focus on the more\nchallenging and highly relevant highway scenes for autonomous driving, with\nsparse sensor views and monotone backgrounds.\n","authors":["Pou-Chun Kung","Xianling Zhang","Katherine A. Skinner","Nikita Jaipuria"],"pdf_url":"https://arxiv.org/pdf/2412.15447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12960v2","updated":"2024-12-19T22:48:46Z","published":"2024-03-19T17:58:04Z","title":"FaceXFormer: A Unified Transformer for Facial Analysis","summary":"  In this work, we introduce FaceXFormer, an end-to-end unified transformer\nmodel capable of performing nine facial analysis tasks including face parsing,\nlandmark detection, head pose estimation, attribute prediction, and estimation\nof age, gender, race, expression, and face visibility within a single\nframework. Conventional methods in face analysis have often relied on\ntask-specific designs and pre-processing techniques, which limit their\nscalability and integration into a unified architecture. Unlike these\nconventional methods, FaceXFormer leverages a transformer-based encoder-decoder\narchitecture where each task is treated as a learnable token, enabling the\nseamless integration and simultaneous processing of multiple tasks within a\nsingle framework. Moreover, we propose a novel parameter-efficient decoder,\nFaceX, which jointly processes face and task tokens, thereby learning\ngeneralized and robust face representations across different tasks. We jointly\ntrained FaceXFormer on nine face perception datasets and conducted experiments\nagainst specialized and multi-task models in both intra-dataset and\ncross-dataset evaluations across multiple benchmarks, showcasing\nstate-of-the-art or competitive performance. Further, we performed a\ncomprehensive analysis of different backbones for unified face task processing\nand evaluated our model \"in-the-wild\", demonstrating its robustness and\ngeneralizability. To the best of our knowledge, this is the first work to\npropose a single model capable of handling nine facial analysis tasks while\nmaintaining real-time performance at 33.21 FPS.\n","authors":["Kartik Narayan","Vibashan VS","Rama Chellappa","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2403.12960v2.pdf","comment":"Project page: https://kartik-3004.github.io/facexformer/"},{"id":"http://arxiv.org/abs/2412.15439v1","updated":"2024-12-19T22:42:29Z","published":"2024-12-19T22:42:29Z","title":"Uncertainty Estimation for Super-Resolution using ESRGAN","summary":"  Deep Learning-based image super-resolution (SR) has been gaining traction\nwith the aid of Generative Adversarial Networks. Models like SRGAN and ESRGAN\nare constantly ranked between the best image SR tools. However, they lack\nprincipled ways for estimating predictive uncertainty. In the present work, we\nenhance these models using Monte Carlo-Dropout and Deep Ensemble, allowing the\ncomputation of predictive uncertainty. When coupled with a prediction,\nuncertainty estimates can provide more information to the model users,\nhighlighting pixels where the SR output might be uncertain, hence potentially\ninaccurate, if these estimates were to be reliable. Our findings suggest that\nthese uncertainty estimates are decently calibrated and can hence fulfill this\ngoal, while providing no performance drop with respect to the corresponding\nmodels without uncertainty estimation.\n","authors":["Maniraj Sai Adapa","Marco Zullich","Matias Valdenegro-Toro"],"pdf_url":"https://arxiv.org/pdf/2412.15439v1.pdf","comment":"8 pages, 6 figures. VISAPP 2025 camera ready"},{"id":"http://arxiv.org/abs/2412.15438v1","updated":"2024-12-19T22:41:26Z","published":"2024-12-19T22:41:26Z","title":"Efficient Neural Network Encoding for 3D Color Lookup Tables","summary":"  3D color lookup tables (LUTs) enable precise color manipulation by mapping\ninput RGB values to specific output RGB values. 3D LUTs are instrumental in\nvarious applications, including video editing, in-camera processing,\nphotographic filters, computer graphics, and color processing for displays.\nWhile an individual LUT does not incur a high memory overhead, software and\ndevices may need to store dozens to hundreds of LUTs that can take over 100 MB.\nThis work aims to develop a neural network architecture that can encode\nhundreds of LUTs in a single compact representation. To this end, we propose a\nmodel with a memory footprint of less than 0.25 MB that can reconstruct 512\nLUTs with only minor color distortion ($\\bar{\\Delta}E_M$ $\\leq$ 2.0) over the\nentire color gamut. We also show that our network can weight colors to provide\nfurther quality gains on natural image colors ($\\bar{\\Delta}{E}_M$ $\\leq$ 1.0).\nFinally, we show that minor modifications to the network architecture enable a\nbijective encoding that produces LUTs that are invertible, allowing for reverse\ncolor processing. Our code is available at https://github.com/vahidzee/ennelut.\n","authors":["Vahid Zehtab","David B. Lindell","Marcus A. Brubaker","Michael S. Brown"],"pdf_url":"https://arxiv.org/pdf/2412.15438v1.pdf","comment":"14 pages, 13 figures; extended version; to appear in AAAI 2025"},{"id":"http://arxiv.org/abs/2304.10727v5","updated":"2024-12-19T22:34:56Z","published":"2023-04-21T03:45:59Z","title":"RoCOCO: Robustness Benchmark of MS-COCO to Stress-test Image-Text\n  Matching Models","summary":"  With the extensive use of vision-language models in various downstream tasks,\nevaluating their robustness is crucial. In this paper, we propose a benchmark\nfor assessing the robustness of vision-language models. We believe that a\nrobust model should properly understand both linguistic and visual semantics\nand be resilient to explicit variations. In pursuit of this goal, we create new\nvariants of texts and images in the MS-COCO test set and re-evaluate the\nstate-of-the-art (SOTA) models with the new data. Specifically, we alter the\nmeaning of text by replacing a word, and generate visually altered images that\nmaintain some visual context while introducing noticeable pixel changes through\nimage mixing techniques.Our evaluations on the proposed benchmark reveal\nsubstantial performance degradation in many SOTA models (e.g., Image-to-Text\nRecall@1: 81.9\\% $\\rightarrow$ 48.4\\% in BLIP, 66.1\\% $\\rightarrow$ 37.6\\% in\nVSE$\\infty$), with the models often favoring the altered texts/images over the\noriginal ones. This indicates the current vision-language models struggle with\nsubtle changes and often fail to understand the overall context of texts and\nimages. Based on these findings, we propose semantic contrastive loss and\nvisual contrastive loss to learn more robust embedding. Datasets and code are\navailable at {\\url{https://github.com/pseulki/rococo}}.\n","authors":["Seulki Park","Daeho Um","Hajung Yoon","Sanghyuk Chun","Sangdoo Yun","Jin Young Choi"],"pdf_url":"https://arxiv.org/pdf/2304.10727v5.pdf","comment":"Accepted to ECCV Synthetic Data for Computer Vision Workshop (Oral)"},{"id":"http://arxiv.org/abs/2409.16252v2","updated":"2024-12-19T21:41:29Z","published":"2024-09-24T17:20:58Z","title":"Fields of The World: A Machine Learning Benchmark Dataset For Global\n  Agricultural Field Boundary Segmentation","summary":"  Crop field boundaries are foundational datasets for agricultural monitoring\nand assessments but are expensive to collect manually. Machine learning (ML)\nmethods for automatically extracting field boundaries from remotely sensed\nimages could help realize the demand for these datasets at a global scale.\nHowever, current ML methods for field instance segmentation lack sufficient\ngeographic coverage, accuracy, and generalization capabilities. Further,\nresearch on improving ML methods is restricted by the lack of labeled datasets\nrepresenting the diversity of global agricultural fields. We present Fields of\nThe World (FTW) -- a novel ML benchmark dataset for agricultural field instance\nsegmentation spanning 24 countries on four continents (Europe, Africa, Asia,\nand South America). FTW is an order of magnitude larger than previous datasets\nwith 70,462 samples, each containing instance and semantic segmentation masks\npaired with multi-date, multi-spectral Sentinel-2 satellite images. We provide\nresults from baseline models for the new FTW benchmark, show that models\ntrained on FTW have better zero-shot and fine-tuning performance in held-out\ncountries than models that aren't pre-trained with diverse datasets, and show\npositive qualitative zero-shot results of FTW models in a real-world scenario\n-- running on Sentinel-2 scenes over Ethiopia.\n","authors":["Hannah Kerner","Snehal Chaudhari","Aninda Ghosh","Caleb Robinson","Adeel Ahmad","Eddie Choi","Nathan Jacobs","Chris Holmes","Matthias Mohr","Rahul Dodhia","Juan M. Lavista Ferres","Jennifer Marcus"],"pdf_url":"https://arxiv.org/pdf/2409.16252v2.pdf","comment":"Accepted at the AAAI-2025 Artificial Intelligence for Social Impact\n  (AISI) track"},{"id":"http://arxiv.org/abs/2403.10045v2","updated":"2024-12-19T21:39:24Z","published":"2024-03-15T06:31:03Z","title":"Towards Adversarially Robust Dataset Distillation by Curvature\n  Regularization","summary":"  Dataset distillation (DD) allows datasets to be distilled to fractions of\ntheir original size while preserving the rich distributional information so\nthat models trained on the distilled datasets can achieve a comparable accuracy\nwhile saving significant computational loads. Recent research in this area has\nbeen focusing on improving the accuracy of models trained on distilled\ndatasets. In this paper, we aim to explore a new perspective of DD. We study\nhow to embed adversarial robustness in distilled datasets, so that models\ntrained on these datasets maintain the high accuracy and meanwhile acquire\nbetter adversarial robustness. We propose a new method that achieves this goal\nby incorporating curvature regularization into the distillation process with\nmuch less computational overhead than standard adversarial training. Extensive\nempirical experiments suggest that our method not only outperforms standard\nadversarial training on both accuracy and robustness with less computation\noverhead but is also capable of generating robust distilled datasets that can\nwithstand various adversarial attacks.\n","authors":["Eric Xue","Yijiang Li","Haoyang Liu","Peiran Wang","Yifan Shen","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.10045v2.pdf","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.15400v1","updated":"2024-12-19T21:04:43Z","published":"2024-12-19T21:04:43Z","title":"SolidGS: Consolidating Gaussian Surfel Splatting for Sparse-View Surface\n  Reconstruction","summary":"  Gaussian splatting has achieved impressive improvements for both novel-view\nsynthesis and surface reconstruction from multi-view images. However, current\nmethods still struggle to reconstruct high-quality surfaces from only sparse\nview input images using Gaussian splatting. In this paper, we propose a novel\nmethod called SolidGS to address this problem. We observed that the\nreconstructed geometry can be severely inconsistent across multi-views, due to\nthe property of Gaussian function in geometry rendering. This motivates us to\nconsolidate all Gaussians by adopting a more solid kernel function, which\neffectively improves the surface reconstruction quality. With the additional\nhelp of geometrical regularization and monocular normal estimation, our method\nachieves superior performance on the sparse view surface reconstruction than\nall the Gaussian splatting methods and neural field methods on the widely used\nDTU, Tanks-and-Temples, and LLFF datasets.\n","authors":["Zhuowen Shen","Yuan Liu","Zhang Chen","Zhong Li","Jiepeng Wang","Yongqing Liang","Zhengming Yu","Jingdong Zhang","Yi Xu","Scott Schaefer","Xin Li","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2412.15400v1.pdf","comment":"Project page: https://mickshen7558.github.io/projects/SolidGS/"},{"id":"http://arxiv.org/abs/2412.15396v1","updated":"2024-12-19T20:58:26Z","published":"2024-12-19T20:58:26Z","title":"Learning Visual Composition through Improved Semantic Guidance","summary":"  Visual imagery does not consist of solitary objects, but instead reflects the\ncomposition of a multitude of fluid concepts. While there have been great\nadvances in visual representation learning, such advances have focused on\nbuilding better representations for a small number of discrete objects bereft\nof an understanding of how these objects are interacting. One can observe this\nlimitation in representations learned through captions or contrastive learning\n-- where the learned model treats an image essentially as a bag of words.\nSeveral works have attempted to address this limitation through the development\nof bespoke learned architectures to directly address the shortcomings in\ncompositional learning. In this work, we focus on simple, and scalable\napproaches. In particular, we demonstrate that by substantially improving\nweakly labeled data, i.e. captions, we can vastly improve the performance of\nstandard contrastive learning approaches. Previous CLIP models achieved near\nchance rate on challenging tasks probing compositional learning. However, our\nsimple approach boosts performance of CLIP substantially and surpasses all\nbespoke architectures. Furthermore, we showcase our results on a relatively new\ncaptioning benchmark derived from DOCCI. We demonstrate through a series of\nablations that a standard CLIP model trained with enhanced data may demonstrate\nimpressive performance on image retrieval tasks.\n","authors":["Austin Stone","Hagen Soltau","Robert Geirhos","Xi Yi","Ye Xia","Bingyi Cao","Kaifeng Chen","Abhijit Ogale","Jonathon Shlens"],"pdf_url":"https://arxiv.org/pdf/2412.15396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15392v1","updated":"2024-12-19T20:45:29Z","published":"2024-12-19T20:45:29Z","title":"Leveraging Weak Supervision for Cell Localization in Digital Pathology\n  Using Multitask Learning and Consistency Loss","summary":"  Cell detection and segmentation are integral parts of automated systems in\ndigital pathology. Encoder-decoder networks have emerged as a promising\nsolution for these tasks. However, training of these networks has typically\nrequired full boundary annotations of cells, which are labor-intensive and\ndifficult to obtain on a large scale. However, in many applications, such as\ncell counting, weaker forms of annotations--such as point annotations or\napproximate cell counts--can provide sufficient supervision for training. This\nstudy proposes a new mixed-supervision approach for training multitask networks\nin digital pathology by incorporating cell counts derived from the eyeballing\nprocess--a quick visual estimation method commonly used by pathologists. This\nstudy has two main contributions: (1) It proposes a mixed-supervision strategy\nfor digital pathology that utilizes cell counts obtained by eyeballing as an\nauxiliary supervisory signal to train a multitask network for the first time.\n(2) This multitask network is designed to concurrently learn the tasks of cell\ncounting and cell localization, and this study introduces a consistency loss\nthat regularizes training by penalizing inconsistencies between the predictions\nof these two tasks. Our experiments on two datasets of hematoxylin-eosin\nstained tissue images demonstrate that the proposed approach effectively\nutilizes the weakest form of annotation, improving performance when stronger\nannotations are limited. These results highlight the potential of integrating\neyeballing-derived ground truths into the network training, reducing the need\nfor resource-intensive annotations.\n","authors":["Berke Levent Cesur","Ayse Humeyra Dur Karasayar","Pinar Bulutay","Nilgun Kapucuoglu","Cisel Aydin Mericoz","Handan Eren","Omer Faruk Dilbaz","Javidan Osmanli","Burhan Soner Yetkili","Ibrahim Kulac","Can Fahrettin Koyuncu","Cigdem Gunduz-Demir"],"pdf_url":"https://arxiv.org/pdf/2412.15392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15389v1","updated":"2024-12-19T20:43:22Z","published":"2024-12-19T20:43:22Z","title":"Maximising Histopathology Segmentation using Minimal Labels via\n  Self-Supervision","summary":"  Histopathology, the microscopic examination of tissue samples, is essential\nfor disease diagnosis and prognosis. Accurate segmentation and identification\nof key regions in histopathology images are crucial for developing automated\nsolutions. However, state-of-art deep learning segmentation methods like UNet\nrequire extensive labels, which is both costly and time-consuming, particularly\nwhen dealing with multiple stainings. To mitigate this, multi-stain\nsegmentation methods such as MDS1 and UDAGAN have been developed, which reduce\nthe need for labels by requiring only one (source) stain to be labelled.\nNonetheless, obtaining source stain labels can still be challenging, and\nsegmentation models fail when they are unavailable. This article shows that\nthrough self-supervised pre-training, including SimCLR, BYOL, and a novel\napproach, HR-CS-CO, the performance of these segmentation methods (UNet, MDS1,\nand UDAGAN) can be retained even with 95% fewer labels. Notably, with\nself-supervised pre-training and using only 5% labels, the performance drops\nare minimal: 5.9% for UNet, 4.5% for MDS1, and 6.2% for UDAGAN, compared to\ntheir respective fully supervised counterparts (without pre-training, using\n100% labels). The code is available from\nhttps://github.com/zeeshannisar/improve_kidney_glomeruli_segmentation [to be\nmade public upon acceptance].\n","authors":["Zeeshan Nisar","Thomas Lampert"],"pdf_url":"https://arxiv.org/pdf/2412.15389v1.pdf","comment":"35 pages, 10 figures, 3 Tables"},{"id":"http://arxiv.org/abs/2412.15380v1","updated":"2024-12-19T20:16:58Z","published":"2024-12-19T20:16:58Z","title":"Uncertainty-Guided Cross Attention Ensemble Mean Teacher for\n  Semi-supervised Medical Image Segmentation","summary":"  This work proposes a novel framework, Uncertainty-Guided Cross Attention\nEnsemble Mean Teacher (UG-CEMT), for achieving state-of-the-art performance in\nsemi-supervised medical image segmentation. UG-CEMT leverages the strengths of\nco-training and knowledge distillation by combining a Cross-attention Ensemble\nMean Teacher framework (CEMT) inspired by Vision Transformers (ViT) with\nuncertainty-guided consistency regularization and Sharpness-Aware Minimization\nemphasizing uncertainty. UG-CEMT improves semi-supervised performance while\nmaintaining a consistent network architecture and task setting by fostering\nhigh disparity between sub-networks. Experiments demonstrate significant\nadvantages over existing methods like Mean Teacher and Cross-pseudo Supervision\nin terms of disparity, domain generalization, and medical image segmentation\nperformance. UG-CEMT achieves state-of-the-art results on multi-center prostate\nMRI and cardiac MRI datasets, where object segmentation is particularly\nchallenging. Our results show that using only 10\\% labeled data, UG-CEMT\napproaches the performance of fully supervised methods, demonstrating its\neffectiveness in exploiting unlabeled data for robust medical image\nsegmentation. The code is publicly available at\n\\url{https://github.com/Meghnak13/UG-CEMT}\n","authors":["Meghana Karri","Amit Soni Arya","Koushik Biswas","Nicol`o Gennaro","Vedat Cicek","Gorkem Durak","Yuri S. Velichko","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2412.15380v1.pdf","comment":"Accepted in WACV 2025"},{"id":"http://arxiv.org/abs/2412.15358v1","updated":"2024-12-19T19:42:22Z","published":"2024-12-19T19:42:22Z","title":"Dataset Augmentation by Mixing Visual Concepts","summary":"  This paper proposes a dataset augmentation method by fine-tuning pre-trained\ndiffusion models. Generating images using a pre-trained diffusion model with\ntextual conditioning often results in domain discrepancy between real data and\ngenerated images. We propose a fine-tuning approach where we adapt the\ndiffusion model by conditioning it with real images and novel text embeddings.\nWe introduce a unique procedure called Mixing Visual Concepts (MVC) where we\ncreate novel text embeddings from image captions. The MVC enables us to\ngenerate multiple images which are diverse and yet similar to the real data\nenabling us to perform effective dataset augmentation. We perform comprehensive\nqualitative and quantitative evaluations with the proposed dataset augmentation\napproach showcasing both coarse-grained and finegrained changes in generated\nimages. Our approach outperforms state-of-the-art augmentation techniques on\nbenchmark classification tasks.\n","authors":["Abdullah Al Rahat","Hemanth Venkateswara"],"pdf_url":"https://arxiv.org/pdf/2412.15358v1.pdf","comment":"Accepted at WACV 2025 main conference"},{"id":"http://arxiv.org/abs/2412.15347v1","updated":"2024-12-19T19:27:31Z","published":"2024-12-19T19:27:31Z","title":"Exploring Machine Learning Engineering for Object Detection and Tracking\n  by Unmanned Aerial Vehicle (UAV)","summary":"  With the advancement of deep learning methods it is imperative that\nautonomous systems will increasingly become intelligent with the inclusion of\nadvanced machine learning algorithms to execute a variety of autonomous\noperations. One such task involves the design and evaluation for a subsystem of\nthe perception system for object detection and tracking. The challenge in the\ncreation of software to solve the task is in discovering the need for a\ndataset, annotation of the dataset, selection of features, integration and\nrefinement of existing algorithms, while evaluating performance metrics through\ntraining and testing. This research effort focuses on the development of a\nmachine learning pipeline emphasizing the inclusion of assurance methods with\nincreasing automation. In the process, a new dataset was created by collecting\nvideos of moving object such as Roomba vacuum cleaner, emulating search and\nrescue (SAR) for indoor environment. Individual frames were extracted from the\nvideos and labeled using a combination of manual and automated techniques. This\nannotated dataset was refined for accuracy by initially training it on YOLOv4.\nAfter the refinement of the dataset it was trained on a second YOLOv4 and a\nMask R-CNN model, which is deployed on a Parrot Mambo drone to perform\nreal-time object detection and tracking. Experimental results demonstrate the\neffectiveness of the models in accurately detecting and tracking the Roomba\nacross multiple trials, achieving an average loss of 0.1942 and 96% accuracy.\n","authors":["Aneesha Guna","Parth Ganeriwala","Siddhartha Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2412.15347v1.pdf","comment":"Accepted at ICMLA '24"},{"id":"http://arxiv.org/abs/2412.07676v2","updated":"2024-12-19T19:18:15Z","published":"2024-12-10T17:04:10Z","title":"BATIS: Bootstrapping, Autonomous Testing, and Initialization System for\n  Quantum Dot Devices","summary":"  Semiconductor quantum dot (QD) devices have become central to advancements in\nspin-based quantum computing. As the complexity of QD devices grows, manual\ntuning becomes increasingly infeasible, necessitating robust and scalable\nautotuning solutions. Tuning large arrays of QD qubits depends on efficient\nchoices of automated protocols. Here, we introduce a bootstrapping, autonomous\ntesting, and initialization system (BATIS) designed to streamline QD device\nevaluation and calibration. BATIS navigates high-dimensional gate voltage\nspaces, automating essential steps such as leakage testing and gate\ncharacterization. For forming the current channels, BATIS follows a\nnon-standard approach that requires a single measurement regardless of the\nnumber of channels. Demonstrated at 1.3 K on a quad-QD Si/Si$_x$Ge$_{1-x}$\ndevice, BATIS eliminates the need for deep cryogenic environments during\ninitial device diagnostics, significantly enhancing scalability and reducing\nsetup times. By requiring only minimal prior knowledge of the device\narchitecture, BATIS represents a platform-agnostic solution, adaptable to\nvarious QD systems, which bridges a critical gap in QD autotuning.\n","authors":["Tyler J. Kovach","Daniel Schug","M. A. Wolfe","E. R. MacQuarrie","Patrick J. Walsh","Jared Benson","Mark Friesen","M. A. Eriksson","Justyna P. Zwolak"],"pdf_url":"https://arxiv.org/pdf/2412.07676v2.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.15342v1","updated":"2024-12-19T19:17:26Z","published":"2024-12-19T19:17:26Z","title":"DCRA-Net: Attention-Enabled Reconstruction Model for Dynamic Fetal\n  Cardiac MRI","summary":"  Dynamic fetal heart magnetic resonance imaging (MRI) presents unique\nchallenges due to the fast heart rate of the fetus compared to adult subjects\nand uncontrolled fetal motion. This requires high temporal and spatial\nresolutions over a large field of view, in order to encompass surrounding\nmaternal anatomy. In this work, we introduce Dynamic Cardiac Reconstruction\nAttention Network (DCRA-Net) - a novel deep learning model that employs\nattention mechanisms in spatial and temporal domains and temporal frequency\nrepresentation of data to reconstruct the dynamics of the fetal heart from\nhighly accelerated free-running (non-gated) MRI acquisitions. DCRA-Net was\ntrained on retrospectively undersampled complex-valued cardiac MRIs from 42\nfetal subjects and separately from 153 adult subjects, and evaluated on data\nfrom 14 fetal and 39 adult subjects respectively. Its performance was compared\nto L+S and k-GIN methods in both fetal and adult cases for an undersampling\nfactor of 8x. The proposed network performed better than the comparators for\nboth fetal and adult data, for both regular lattice and centrally weighted\nrandom undersampling. Aliased signals due to the undersampling were\ncomprehensively resolved, and both the spatial details of the heart and its\ntemporal dynamics were recovered with high fidelity. The highest performance\nwas achieved when using lattice undersampling, data consistency and temporal\nfrequency representation, yielding PSNR of 38 for fetal and 35 for adult cases.\nOur method is publicly available at https://github.com/denproc/DCRA-Net.\n","authors":["Denis Prokopenko","David F. A. Lloyd","Amedeo Chiribiri","Daniel Rueckert","Joseph V. Hajnal"],"pdf_url":"https://arxiv.org/pdf/2412.15342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15341v1","updated":"2024-12-19T19:13:18Z","published":"2024-12-19T19:13:18Z","title":"Efficient Fine-Tuning and Concept Suppression for Pruned Diffusion\n  Models","summary":"  Recent advances in diffusion generative models have yielded remarkable\nprogress. While the quality of generated content continues to improve, these\nmodels have grown considerably in size and complexity. This increasing\ncomputational burden poses significant challenges, particularly in\nresource-constrained deployment scenarios such as mobile devices. The\ncombination of model pruning and knowledge distillation has emerged as a\npromising solution to reduce computational demands while preserving generation\nquality. However, this technique inadvertently propagates undesirable\nbehaviors, including the generation of copyrighted content and unsafe concepts,\neven when such instances are absent from the fine-tuning dataset. In this\npaper, we propose a novel bilevel optimization framework for pruned diffusion\nmodels that consolidates the fine-tuning and unlearning processes into a\nunified phase. Our approach maintains the principal advantages of\ndistillation-namely, efficient convergence and style transfer\ncapabilities-while selectively suppressing the generation of unwanted content.\nThis plug-in framework is compatible with various pruning and concept\nunlearning methods, facilitating efficient, safe deployment of diffusion models\nin controlled environments.\n","authors":["Reza Shirkavand","Peiran Yu","Shangqian Gao","Gowthami Somepalli","Tom Goldstein","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2412.15341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03553v4","updated":"2024-12-19T19:02:29Z","published":"2024-09-05T14:13:05Z","title":"Organized Grouped Discrete Representation for Object-Centric Learning","summary":"  Object-Centric Learning (OCL) represents dense image or video pixels as\nsparse object features. Representative methods utilize discrete representation\ncomposed of Variational Autoencoder (VAE) template features to suppress\npixel-level information redundancy and guide object-level feature aggregation.\nThe most recent advancement, Grouped Discrete Representation (GDR), further\ndecomposes these template features into attributes. However, its naive channel\ngrouping as decomposition may erroneously group channels belonging to different\nattributes together and discretize them as sub-optimal template attributes,\nwhich losses information and harms expressivity. We propose Organized GDR\n(OGDR) to organize channels belonging to the same attributes together for\ncorrect decomposition from features into attributes. In unsupervised\nsegmentation experiments, OGDR is fully superior to GDR in augmentating\nclassical transformer-based OCL methods; it even improves state-of-the-art\ndiffusion-based ones. Codebook PCA and representation similarity analyses show\nthat compared with GDR, our OGDR eliminates redundancy and preserves\ninformation better for guiding object representation learning. The source code\nis available in the supplementary material.\n","authors":["Rongzhen Zhao","Vivienne Wang","Juho Kannala","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2409.03553v4.pdf","comment":"This paper, along with arXiv:2407.01726, was merged into\n  arXiv:2411.02299"},{"id":"http://arxiv.org/abs/2407.01726v3","updated":"2024-12-19T19:01:41Z","published":"2024-07-01T19:00:40Z","title":"Grouped Discrete Representation Guides Object-Centric Learning","summary":"  Similar to humans perceiving visual scenes as objects, Object-Centric\nLearning (OCL) can abstract dense images or videos into sparse object-level\nfeatures. Transformer-based OCL handles complex textures well due to the\ndecoding guidance of discrete representation, obtained by discretizing noisy\nfeatures in image or video feature maps using template features from a\ncodebook. However, treating features as minimal units overlooks their composing\nattributes, thus impeding model generalization; indexing features with natural\nnumbers loses attribute-level commonalities and characteristics, thus\ndiminishing heuristics for model convergence. We propose \\textit{Grouped\nDiscrete Representation} (GDR) to address these issues by grouping features\ninto attributes and indexing them with tuple numbers. In extensive experiments\nacross different query initializations, dataset modalities, and model\narchitectures, GDR consistently improves convergence and generalizability.\nVisualizations show that our method effectively captures attribute-level\ninformation in features. The source code will be available upon acceptance.\n","authors":["Rongzhen Zhao","Vivienne Wang","Juho Kannala","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2407.01726v3.pdf","comment":"This paper, along with arXiv:2409.03553, was merged into\n  arXiv:2411.02299"},{"id":"http://arxiv.org/abs/2412.15322v1","updated":"2024-12-19T18:59:55Z","published":"2024-12-19T18:59:55Z","title":"Taming Multimodal Joint Training for High-Quality Video-to-Audio\n  Synthesis","summary":"  We propose to synthesize high-quality and synchronized audio, given video and\noptional text conditions, using a novel multimodal joint training framework\nMMAudio. In contrast to single-modality training conditioned on (limited) video\ndata only, MMAudio is jointly trained with larger-scale, readily available\ntext-audio data to learn to generate semantically aligned high-quality audio\nsamples. Additionally, we improve audio-visual synchrony with a conditional\nsynchronization module that aligns video conditions with audio latents at the\nframe level. Trained with a flow matching objective, MMAudio achieves new\nvideo-to-audio state-of-the-art among public models in terms of audio quality,\nsemantic alignment, and audio-visual synchronization, while having a low\ninference time (1.23s to generate an 8s clip) and just 157M parameters. MMAudio\nalso achieves surprisingly competitive performance in text-to-audio generation,\nshowing that joint training does not hinder single-modality performance. Code\nand demo are available at: https://hkchengrex.github.io/MMAudio\n","authors":["Ho Kei Cheng","Masato Ishii","Akio Hayakawa","Takashi Shibuya","Alexander Schwing","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2412.15322v1.pdf","comment":"Project page: https://hkchengrex.github.io/MMAudio"},{"id":"http://arxiv.org/abs/2412.15321v1","updated":"2024-12-19T18:59:36Z","published":"2024-12-19T18:59:36Z","title":"Next Patch Prediction for Autoregressive Visual Generation","summary":"  Autoregressive models, built based on the Next Token Prediction (NTP)\nparadigm, show great potential in developing a unified framework that\nintegrates both language and vision tasks. In this work, we rethink the NTP for\nautoregressive image generation and propose a novel Next Patch Prediction (NPP)\nparadigm. Our key idea is to group and aggregate image tokens into patch tokens\ncontaining high information density. With patch tokens as a shorter input\nsequence, the autoregressive model is trained to predict the next patch,\nthereby significantly reducing the computational cost. We further propose a\nmulti-scale coarse-to-fine patch grouping strategy that exploits the natural\nhierarchical property of image data. Experiments on a diverse range of models\n(100M-1.4B parameters) demonstrate that the next patch prediction paradigm\ncould reduce the training cost to around 0.6 times while improving image\ngeneration quality by up to 1.0 FID score on the ImageNet benchmark. We\nhighlight that our method retains the original autoregressive model\narchitecture without introducing additional trainable parameters or\nspecifically designing a custom image tokenizer, thus ensuring flexibility and\nseamless adaptation to various autoregressive models for visual generation.\n","authors":["Yatian Pang","Peng Jin","Shuo Yang","Bin Lin","Bin Zhu","Zhenyu Tang","Liuhan Chen","Francis E. H. Tay","Ser-Nam Lim","Harry Yang","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.15321v1.pdf","comment":"Code: https://github.com/PKU-YuanGroup/Next-Patch-Prediction"},{"id":"http://arxiv.org/abs/2412.15320v1","updated":"2024-12-19T18:59:05Z","published":"2024-12-19T18:59:05Z","title":"Multi-concept Model Immunization through Differentiable Model Merging","summary":"  Model immunization is an emerging direction that aims to mitigate the\npotential risk of misuse associated with open-sourced models and advancing\nadaptation methods. The idea is to make the released models' weights difficult\nto fine-tune on certain harmful applications, hence the name ``immunized''.\nRecent work on model immunization focuses on the single-concept setting.\nHowever, models need to be immunized against multiple concepts in real-world\nsituations. To address this gap, we propose an immunization algorithm that,\nsimultaneously, learns a single ``difficult initialization'' for adaptation\nmethods over a set of concepts. We achieve this by incorporating a\ndifferentiable merging layer that combines a set of model weights adapted over\nmultiple concepts. In our experiments, we demonstrate the effectiveness of\nmulti-concept immunization by generalizing prior work's experiment setup of\nre-learning and personalization adaptation to multiple concepts.\n","authors":["Amber Yijia Zheng","Raymond A. Yeh"],"pdf_url":"https://arxiv.org/pdf/2412.15320v1.pdf","comment":"AAAI 2025"}]},"2024-12-20T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.16158v1","updated":"2024-12-20T18:59:59Z","published":"2024-12-20T18:59:59Z","title":"HoVLE: Unleashing the Power of Monolithic Vision-Language Models with\n  Holistic Vision-Language Embedding","summary":"  The rapid advance of Large Language Models (LLMs) has catalyzed the\ndevelopment of Vision-Language Models (VLMs). Monolithic VLMs, which avoid\nmodality-specific encoders, offer a promising alternative to the compositional\nones but face the challenge of inferior performance. Most existing monolithic\nVLMs require tuning pre-trained LLMs to acquire vision abilities, which may\ndegrade their language capabilities. To address this dilemma, this paper\npresents a novel high-performance monolithic VLM named HoVLE. We note that LLMs\nhave been shown capable of interpreting images, when image embeddings are\naligned with text embeddings. The challenge for current monolithic VLMs\nactually lies in the lack of a holistic embedding module for both vision and\nlanguage inputs. Therefore, HoVLE introduces a holistic embedding module that\nconverts visual and textual inputs into a shared space, allowing LLMs to\nprocess images in the same way as texts. Furthermore, a multi-stage training\nstrategy is carefully designed to empower the holistic embedding module. It is\nfirst trained to distill visual features from a pre-trained vision encoder and\ntext embeddings from the LLM, enabling large-scale training with unpaired\nrandom images and text tokens. The whole model further undergoes next-token\nprediction on multi-modal data to align the embeddings. Finally, an\ninstruction-tuning stage is incorporated. Our experiments show that HoVLE\nachieves performance close to leading compositional models on various\nbenchmarks, outperforming previous monolithic models by a large margin. Model\navailable at https://huggingface.co/OpenGVLab/HoVLE.\n","authors":["Chenxin Tao","Shiqian Su","Xizhou Zhu","Chenyu Zhang","Zhe Chen","Jiawen Liu","Wenhai Wang","Lewei Lu","Gao Huang","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2412.16158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16156v1","updated":"2024-12-20T18:59:03Z","published":"2024-12-20T18:59:03Z","title":"Personalized Representation from Personalized Generation","summary":"  Modern vision models excel at general purpose downstream tasks. It is\nunclear, however, how they may be used for personalized vision tasks, which are\nboth fine-grained and data-scarce. Recent works have successfully applied\nsynthetic data to general-purpose representation learning, while advances in\nT2I diffusion models have enabled the generation of personalized images from\njust a few real examples. Here, we explore a potential connection between these\nideas, and formalize the challenge of using personalized synthetic data to\nlearn personalized representations, which encode knowledge about an object of\ninterest and may be flexibly applied to any downstream task relating to the\ntarget object. We introduce an evaluation suite for this challenge, including\nreformulations of two existing datasets and a novel dataset explicitly\nconstructed for this purpose, and propose a contrastive learning approach that\nmakes creative use of image generators. We show that our method improves\npersonalized representation learning for diverse downstream tasks, from\nrecognition to segmentation, and analyze characteristics of image generation\napproaches that are key to this gain.\n","authors":["Shobhita Sundaram","Julia Chae","Yonglong Tian","Sara Beery","Phillip Isola"],"pdf_url":"https://arxiv.org/pdf/2412.16156v1.pdf","comment":"S.S. and J.C contributed equally; S.B. and P.I. co-supervised.\n  Project page: https://personalized-rep.github.io/"},{"id":"http://arxiv.org/abs/2412.16155v1","updated":"2024-12-20T18:58:24Z","published":"2024-12-20T18:58:24Z","title":"Can Generative Video Models Help Pose Estimation?","summary":"  Pairwise pose estimation from images with little or no overlap is an open\nchallenge in computer vision. Existing methods, even those trained on\nlarge-scale datasets, struggle in these scenarios due to the lack of\nidentifiable correspondences or visual overlap. Inspired by the human ability\nto infer spatial relationships from diverse scenes, we propose a novel\napproach, InterPose, that leverages the rich priors encoded within pre-trained\ngenerative video models. We propose to use a video model to hallucinate\nintermediate frames between two input images, effectively creating a dense,\nvisual transition, which significantly simplifies the problem of pose\nestimation. Since current video models can still produce implausible motion or\ninconsistent geometry, we introduce a self-consistency score that evaluates the\nconsistency of pose predictions from sampled videos. We demonstrate that our\napproach generalizes among three state-of-the-art video models and show\nconsistent improvements over the state-of-the-art DUSt3R on four diverse\ndatasets encompassing indoor, outdoor, and object-centric scenes. Our findings\nsuggest a promising avenue for improving pose estimation models by leveraging\nlarge generative models trained on vast amounts of video data, which is more\nreadily available than 3D data. See our project page for results:\nhttps://inter-pose.github.io/.\n","authors":["Ruojin Cai","Jason Y. Zhang","Philipp Henzler","Zhengqi Li","Noah Snavely","Ricardo Martin-Brualla"],"pdf_url":"https://arxiv.org/pdf/2412.16155v1.pdf","comment":"Project page: https://inter-pose.github.io/"},{"id":"http://arxiv.org/abs/2412.16153v1","updated":"2024-12-20T18:57:06Z","published":"2024-12-20T18:57:06Z","title":"MotiF: Making Text Count in Image Animation with Motion Focal Loss","summary":"  Text-Image-to-Video (TI2V) generation aims to generate a video from an image\nfollowing a text description, which is also referred to as text-guided image\nanimation. Most existing methods struggle to generate videos that align well\nwith the text prompts, particularly when motion is specified. To overcome this\nlimitation, we introduce MotiF, a simple yet effective approach that directs\nthe model's learning to the regions with more motion, thereby improving the\ntext alignment and motion generation. We use optical flow to generate a motion\nheatmap and weight the loss according to the intensity of the motion. This\nmodified objective leads to noticeable improvements and complements existing\nmethods that utilize motion priors as model inputs. Additionally, due to the\nlack of a diverse benchmark for evaluating TI2V generation, we propose TI2V\nBench, a dataset consists of 320 image-text pairs for robust evaluation. We\npresent a human evaluation protocol that asks the annotators to select an\noverall preference between two videos followed by their justifications. Through\na comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced\nmodels, achieving an average preference of 72%. The TI2V Bench is released in\nhttps://wang-sj16.github.io/motif/.\n","authors":["Shijie Wang","Samaneh Azadi","Rohit Girdhar","Saketh Rambhatla","Chen Sun","Xi Yin"],"pdf_url":"https://arxiv.org/pdf/2412.16153v1.pdf","comment":"TI2V Bench is released in https://wang-sj16.github.io/motif/"},{"id":"http://arxiv.org/abs/2412.16148v1","updated":"2024-12-20T18:51:41Z","published":"2024-12-20T18:51:41Z","title":"Frequency Is What You Need: Word-frequency Masking Benefits\n  Vision-Language Model Pre-training","summary":"  Vision Language Models (VLMs) can be trained more efficiently if training\nsets can be reduced in size. Recent work has shown the benefits of masking text\nduring VLM training using a variety of approaches: truncation, random masking,\nblock masking and syntax masking. In this paper, we show that the best masking\nstrategy changes over training epochs and that, given sufficient training\nepochs, word frequency information is what you need to achieve the best\nperformance. Experiments on a large range of data sets demonstrate the\nadvantages of our approach, called Contrastive Language-Image Pre-training with\nword Frequency Masking (CLIPF). The benefits are particularly evident as the\nnumber of input tokens decreases. We analyze the impact of CLIPF vs. other\nmasking approaches on word frequency balance and discuss the apparently\ncritical contribution of CLIPF in maintaining word frequency balance across POS\ncategories.\n","authors":["Mingliang Liang","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2412.16148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16147v1","updated":"2024-12-20T18:50:54Z","published":"2024-12-20T18:50:54Z","title":"SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage\n  Estimation in the Wild","summary":"  Seagrass meadows play a crucial role in marine ecosystems, providing\nimportant services such as carbon sequestration, water quality improvement, and\nhabitat provision. Monitoring the distribution and abundance of seagrass is\nessential for environmental impact assessments and conservation efforts.\nHowever, the current manual methods of analyzing underwater video transects to\nassess seagrass coverage are time-consuming and subjective. This work explores\nthe use of deep learning models to automate the process of seagrass detection\nand coverage estimation from underwater video data. A dataset of over 8,300\nannotated underwater images was created, and several deep learning\narchitectures, including ResNet, InceptionNetV3, DenseNet, and Vision\nTransformer, were evaluated for the task of binary classification of ``Eelgrass\nPresent'' and ``Eelgrass Absent'' images. The results demonstrate that deep\nlearning models, particularly the Vision Transformer, can achieve high\nperformance in predicting eelgrass presence, with AUROC scores exceeding 0.95\non the final test dataset. The use of transfer learning and the application of\nthe Deep WaveNet underwater image enhancement model further improved the\nmodels' capabilities. The proposed methodology allows for the efficient\nprocessing of large volumes of video data, enabling the acquisition of much\nmore detailed information on seagrass distributions compared to current manual\nmethods. This information is crucial for environmental impact assessments and\nmonitoring programs, as seagrasses are important indicators of coastal\necosystem health. Overall, this project demonstrates the value that deep\nlearning can bring to the field of marine ecology and environmental monitoring.\n","authors":["Jannik Elsäßer","Laura Weihl","Veronika Cheplygina","Lisbeth Tangaa Nielsen"],"pdf_url":"https://arxiv.org/pdf/2412.16147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16146v1","updated":"2024-12-20T18:50:36Z","published":"2024-12-20T18:50:36Z","title":"Mamba2D: A Natively Multi-Dimensional State-Space Model for Vision Tasks","summary":"  State-Space Models (SSMs) have recently emerged as a powerful and efficient\nalternative to the long-standing transformer architecture. However, existing\nSSM conceptualizations retain deeply rooted biases from their roots in natural\nlanguage processing. This constrains their ability to appropriately model the\nspatially-dependent characteristics of visual inputs. In this paper, we address\nthese limitations by re-deriving modern selective state-space techniques,\nstarting from a natively multidimensional formulation. Currently, prior works\nattempt to apply natively 1D SSMs to 2D data (i.e. images) by relying on\narbitrary combinations of 1D scan directions to capture spatial dependencies.\nIn contrast, Mamba2D improves upon this with a single 2D scan direction that\nfactors in both dimensions of the input natively, effectively modelling spatial\ndependencies when constructing hidden states. Mamba2D shows comparable\nperformance to prior adaptations of SSMs for vision tasks, on standard image\nclassification evaluations with the ImageNet-1K dataset.\n","authors":["Enis Baty","Alejandro Hernández Díaz","Chris Bridges","Rebecca Davidson","Steve Eckersley","Simon Hadfield"],"pdf_url":"https://arxiv.org/pdf/2412.16146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10889v2","updated":"2024-12-20T18:42:11Z","published":"2024-01-19T18:59:11Z","title":"Synthesizing Moving People with 3D Control","summary":"  In this paper, we present a diffusion model-based framework for animating\npeople from a single image for a given target 3D motion sequence. Our approach\nhas two core components: a) learning priors about invisible parts of the human\nbody and clothing, and b) rendering novel body poses with proper clothing and\ntexture. For the first part, we learn an in-filling diffusion model to\nhallucinate unseen parts of a person given a single image. We train this model\non texture map space, which makes it more sample-efficient since it is\ninvariant to pose and viewpoint. Second, we develop a diffusion-based rendering\npipeline, which is controlled by 3D human poses. This produces realistic\nrenderings of novel poses of the person, including clothing, hair, and\nplausible in-filling of unseen regions. This disentangled approach allows our\nmethod to generate a sequence of images that are faithful to the target motion\nin the 3D pose and, to the input image in terms of visual similarity. In\naddition to that, the 3D control allows various synthetic camera trajectories\nto render a person. Our experiments show that our method is resilient in\ngenerating prolonged motions and varied challenging and complex poses compared\nto prior methods. Please check our website for more details:\nhttps://boyiliee.github.io/3DHM.github.io/.\n","authors":["Boyi Li","Junming Chen","Jathushan Rajasegaran","Yossi Gandelsman","Alexei A. Efros","Jitendra Malik"],"pdf_url":"https://arxiv.org/pdf/2401.10889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16141v1","updated":"2024-12-20T18:40:53Z","published":"2024-12-20T18:40:53Z","title":"NeRF-To-Real Tester: Neural Radiance Fields as Test Image Generators for\n  Vision of Autonomous Systems","summary":"  Autonomous inspection of infrastructure on land and in water is a quickly\ngrowing market, with applications including surveying constructions, monitoring\nplants, and tracking environmental changes in on- and off-shore wind energy\nfarms. For Autonomous Underwater Vehicles and Unmanned Aerial Vehicles\noverfitting of controllers to simulation conditions fundamentally leads to poor\nperformance in the operation environment. There is a pressing need for more\ndiverse and realistic test data that accurately represents the challenges faced\nby these systems. We address the challenge of generating perception test data\nfor autonomous systems by leveraging Neural Radiance Fields to generate\nrealistic and diverse test images, and integrating them into a metamorphic\ntesting framework for vision components such as vSLAM and object detection. Our\ntool, N2R-Tester, allows training models of custom scenes and rendering test\nimages from perturbed positions. An experimental evaluation of N2R-Tester on\neight different vision components in AUVs and UAVs demonstrates the efficacy\nand versatility of the approach.\n","authors":["Laura Weihl","Bilal Wehbe","Andrzej Wąsowski"],"pdf_url":"https://arxiv.org/pdf/2412.16141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16137v1","updated":"2024-12-20T18:35:53Z","published":"2024-12-20T18:35:53Z","title":"Camera-Based Localization and Enhanced Normalized Mutual Information","summary":"  Robust and fine localization algorithms are crucial for autonomous driving.\nFor the production of such vehicles as a commodity, affordable sensing\nsolutions and reliable localization algorithms must be designed. This work\nconsiders scenarios where the sensor data comes from images captured by an\ninexpensive camera mounted on the vehicle and where the vehicle contains a fine\nglobal map. Such localization algorithms typically involve finding the section\nin the global map that best matches the captured image. In harsh environments,\nboth the global map and the captured image can be noisy. Because of physical\nconstraints on camera placement, the image captured by the camera can be viewed\nas a noisy perspective transformed version of the road in the global map. Thus,\nan optimal algorithm should take into account the unequal noise power in\nvarious regions of the captured image, and the intrinsic uncertainty in the\nglobal map due to environmental variations. This article briefly reviews two\nmatching methods: (i) standard inner product (SIP) and (ii) normalized mutual\ninformation (NMI). It then proposes novel and principled modifications to\nimprove the performance of these algorithms significantly in noisy\nenvironments. These enhancements are inspired by the physical constraints\nassociated with autonomous vehicles. They are grounded in statistical signal\nprocessing and, in some context, are provably better. Numerical simulations\ndemonstrate the effectiveness of such modifications.\n","authors":["Vishnu Teja Kunde","Jean-Francois Chamberland","Siddharth Agarwal"],"pdf_url":"https://arxiv.org/pdf/2412.16137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16129v1","updated":"2024-12-20T18:26:10Z","published":"2024-12-20T18:26:10Z","title":"LEDA: Log-Euclidean Diffeomorphic Autoencoder for Efficient Statistical\n  Analysis of Diffeomorphism","summary":"  Image registration is a core task in computational anatomy that establishes\ncorrespondences between images. Invertible deformable registration, which\ncomputes a deformation field and handles complex, non-linear transformation, is\nessential for tracking anatomical variations, especially in neuroimaging\napplications where inter-subject differences and longitudinal changes are key.\nAnalyzing the deformation fields is challenging due to their non-linearity,\nlimiting statistical analysis. However, traditional approaches for analyzing\ndeformation fields are computationally expensive, sensitive to initialization,\nand prone to numerical errors, especially when the deformation is far from the\nidentity. To address these limitations, we propose the Log-Euclidean\nDiffeomorphic Autoencoder (LEDA), an innovative framework designed to compute\nthe principal logarithm of deformation fields by efficiently predicting\nconsecutive square roots. LEDA operates within a linearized latent space that\nadheres to the diffeomorphisms group action laws, enhancing our model's\nrobustness and applicability. We also introduce a loss function to enforce\ninverse consistency, ensuring accurate latent representations of deformation\nfields. Extensive experiments with the OASIS-1 dataset demonstrate the\neffectiveness of LEDA in accurately modeling and analyzing complex non-linear\ndeformations while maintaining inverse consistency. Additionally, we evaluate\nits ability to capture and incorporate clinical variables, enhancing its\nrelevance for clinical applications.\n","authors":["Krithika Iyer","Shireen Elhabian","Sarang Joshi"],"pdf_url":"https://arxiv.org/pdf/2412.16129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.01930v2","updated":"2024-12-20T18:18:41Z","published":"2023-07-04T21:35:49Z","title":"Learning ECG Signal Features Without Backpropagation Using Linear Laws","summary":"  This paper introduces LLT-ECG, a novel method for electrocardiogram (ECG)\nsignal classification that leverages concepts from theoretical physics to\nautomatically generate features from time series data. Unlike traditional deep\nlearning approaches, LLT-ECG operates in a forward manner, eliminating the need\nfor backpropagation and hyperparameter tuning. By identifying linear laws that\ncapture shared patterns within specific classes, the proposed method constructs\na compact and verifiable representation, enhancing the effectiveness of\ndownstream classifiers. We demonstrate LLT-ECG's state-of-the-art performance\non real-world ECG datasets from PhysioNet, underscoring its potential for\nmedical applications where speed and verifiability are crucial.\n","authors":["Péter Pósfay","Marcell T. Kurbucz","Péter Kovács","Antal Jakovác"],"pdf_url":"https://arxiv.org/pdf/2307.01930v2.pdf","comment":"35 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2412.16119v1","updated":"2024-12-20T18:05:22Z","published":"2024-12-20T18:05:22Z","title":"Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource\n  Scripts","summary":"  This study investigates the potential of Large Language Models (LLMs),\nparticularly GPT-4o, for Optical Character Recognition (OCR) in low-resource\nscripts such as Urdu, Albanian, and Tajik, with English serving as a benchmark.\nUsing a meticulously curated dataset of 2,520 images incorporating controlled\nvariations in text length, font size, background color, and blur, the research\nsimulates diverse real-world challenges. Results emphasize the limitations of\nzero-shot LLM-based OCR, particularly for linguistically complex scripts,\nhighlighting the need for annotated datasets and fine-tuned models. This work\nunderscores the urgency of addressing accessibility gaps in text digitization,\npaving the way for inclusive and robust OCR solutions for underserved\nlanguages.\n","authors":["Muhammad Abdullah Sohail","Salaar Masood","Hamza Iqbal"],"pdf_url":"https://arxiv.org/pdf/2412.16119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09583v5","updated":"2024-12-20T18:03:07Z","published":"2024-10-12T16:28:40Z","title":"POPoS: Improving Efficient and Robust Facial Landmark Detection with\n  Parallel Optimal Position Search","summary":"  Achieving a balance between accuracy and efficiency is a critical challenge\nin facial landmark detection (FLD). This paper introduces Parallel Optimal\nPosition Search (POPoS), a high-precision encoding-decoding framework designed\nto address the limitations of traditional FLD methods. POPoS employs three key\ncontributions: (1) Pseudo-range multilateration is utilized to correct heatmap\nerrors, improving landmark localization accuracy. By integrating multiple\nanchor points, it reduces the impact of individual heatmap inaccuracies,\nleading to robust overall positioning. (2) To enhance the pseudo-range accuracy\nof selected anchor points, a new loss function, named multilateration anchor\nloss, is proposed. This loss function enhances the accuracy of the distance\nmap, mitigates the risk of local optima, and ensures optimal solutions. (3) A\nsingle-step parallel computation algorithm is introduced, boosting\ncomputational efficiency and reducing processing time. Extensive evaluations\nacross five benchmark datasets demonstrate that POPoS consistently outperforms\nexisting methods, particularly excelling in low-resolution heatmaps scenarios\nwith minimal computational overhead. These advantages make POPoS a highly\nefficient and accurate tool for FLD, with broad applicability in real-world\nscenarios.\n","authors":["Chong-Yang Xiang","Jun-Yan He","Zhi-Qi Cheng","Xiao Wu","Xian-Sheng Hua"],"pdf_url":"https://arxiv.org/pdf/2410.09583v5.pdf","comment":"Accepted to AAAI 2025, 9 pages, 6 figures. Code:\n  https://github.com/teslatasy/POPoS"},{"id":"http://arxiv.org/abs/2412.16117v1","updated":"2024-12-20T18:01:58Z","published":"2024-12-20T18:01:58Z","title":"PruneVid: Visual Token Pruning for Efficient Video Large Language Models","summary":"  In this paper, we introduce PruneVid, a visual token pruning method designed\nto enhance the efficiency of multi-modal video understanding. Large Language\nModels (LLMs) have shown promising performance in video tasks due to their\nextended capabilities in comprehending visual modalities. However, the\nsubstantial redundancy in video data presents significant computational\nchallenges for LLMs. To address this issue, we introduce a training-free method\nthat 1) minimizes video redundancy by merging spatial-temporal tokens, and 2)\nleverages LLMs' reasoning capabilities to selectively prune visual features\nrelevant to question tokens, enhancing model efficiency. We validate our method\nacross multiple video benchmarks, which demonstrate that PruneVid can prune\nover 80% of tokens while maintaining competitive performance combined with\ndifferent model networks. This highlights its superior effectiveness and\nefficiency compared to existing pruning methods. Code:\nhttps://github.com/Visual-AI/PruneVid.\n","authors":["Xiaohu Huang","Hao Zhou","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2412.16117v1.pdf","comment":"Efficient Video Large Language Models"},{"id":"http://arxiv.org/abs/2412.16112v1","updated":"2024-12-20T17:57:09Z","published":"2024-12-20T17:57:09Z","title":"CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers\n  Up","summary":"  Diffusion Transformers (DiT) have become a leading architecture in image\ngeneration. However, the quadratic complexity of attention mechanisms, which\nare responsible for modeling token-wise relationships, results in significant\nlatency when generating high-resolution images. To address this issue, we aim\nat a linear attention mechanism in this paper that reduces the complexity of\npre-trained DiTs to linear. We begin our exploration with a comprehensive\nsummary of existing efficient attention mechanisms and identify four key\nfactors crucial for successful linearization of pre-trained DiTs: locality,\nformulation consistency, high-rank attention maps, and feature integrity. Based\non these insights, we introduce a convolution-like local attention strategy\ntermed CLEAR, which limits feature interactions to a local window around each\nquery token, and thus achieves linear complexity. Our experiments indicate\nthat, by fine-tuning the attention layer on merely 10K self-generated samples\nfor 10K iterations, we can effectively transfer knowledge from a pre-trained\nDiT to a student model with linear complexity, yielding results comparable to\nthe teacher model. Simultaneously, it reduces attention computations by 99.5%\nand accelerates generation by 6.3 times for generating 8K-resolution images.\nFurthermore, we investigate favorable properties in the distilled attention\nlayers, such as zero-shot generalization cross various models and plugins, and\nimproved support for multi-GPU parallel inference. Models and codes are\navailable here: https://github.com/Huage001/CLEAR.\n","authors":["Songhua Liu","Zhenxiong Tan","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2412.16112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16108v1","updated":"2024-12-20T17:49:22Z","published":"2024-12-20T17:49:22Z","title":"Demystifying the Potential of ChatGPT-4 Vision for Construction Progress\n  Monitoring","summary":"  The integration of Large Vision-Language Models (LVLMs) such as OpenAI's\nGPT-4 Vision into various sectors has marked a significant evolution in the\nfield of artificial intelligence, particularly in the analysis and\ninterpretation of visual data. This paper explores the practical application of\nGPT-4 Vision in the construction industry, focusing on its capabilities in\nmonitoring and tracking the progress of construction projects. Utilizing\nhigh-resolution aerial imagery of construction sites, the study examines how\nGPT-4 Vision performs detailed scene analysis and tracks developmental changes\nover time. The findings demonstrate that while GPT-4 Vision is proficient in\nidentifying construction stages, materials, and machinery, it faces challenges\nwith precise object localization and segmentation. Despite these limitations,\nthe potential for future advancements in this technology is considerable. This\nresearch not only highlights the current state and opportunities of using LVLMs\nin construction but also discusses future directions for enhancing the model's\nutility through domain-specific training and integration with other computer\nvision techniques and digital twins.\n","authors":["Ahmet Bahaddin Ersoz"],"pdf_url":"https://arxiv.org/pdf/2412.16108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16086v1","updated":"2024-12-20T17:33:50Z","published":"2024-12-20T17:33:50Z","title":"Towards Interpretable Radiology Report Generation via Concept\n  Bottlenecks using a Multi-Agentic RAG","summary":"  Deep learning has advanced medical image classification, but interpretability\nchallenges hinder its clinical adoption. This study enhances interpretability\nin Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)\nand a multi-agent Retrieval-Augmented Generation (RAG) system for report\ngeneration. By modeling relationships between visual features and clinical\nconcepts, we create interpretable concept vectors that guide a multi-agent RAG\nsystem to generate radiology reports, enhancing clinical relevance,\nexplainability, and transparency. Evaluation of the generated reports using an\nLLM-as-a-judge confirmed the interpretability and clinical utility of our\nmodel's outputs. On the COVID-QU dataset, our model achieved 81% classification\naccuracy and demonstrated robust report generation performance, with five key\nmetrics ranging between 84% and 90%. This interpretable multi-agent framework\nbridges the gap between high-performance AI and the explainability required for\nreliable AI-driven CXR analysis in clinical settings.\n","authors":["Hasan Md Tusfiqur Alam","Devansh Srivastav","Md Abdul Kadir","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2412.16086v1.pdf","comment":"Accepted in ECIR 2025"},{"id":"http://arxiv.org/abs/2412.16085v1","updated":"2024-12-20T17:33:35Z","published":"2024-12-20T17:33:35Z","title":"Efficient MedSAMs: Segment Anything in Medical Images on Laptop","summary":"  Promptable segmentation foundation models have emerged as a transformative\napproach to addressing the diverse needs in medical images, but most existing\nmodels require expensive computing, posing a big barrier to their adoption in\nclinical practice. In this work, we organized the first international\ncompetition dedicated to promptable medical image segmentation, featuring a\nlarge-scale dataset spanning nine common imaging modalities from over 20\ndifferent institutions. The top teams developed lightweight segmentation\nfoundation models and implemented an efficient inference pipeline that\nsubstantially reduced computational requirements while maintaining\nstate-of-the-art segmentation accuracy. Moreover, the post-challenge phase\nadvanced the algorithms through the design of performance booster and\nreproducibility tasks, resulting in improved algorithms and validated\nreproducibility of the winning solution. Furthermore, the best-performing\nalgorithms have been incorporated into the open-source software with a\nuser-friendly interface to facilitate clinical adoption. The data and code are\npublicly available to foster the further development of medical image\nsegmentation foundation models and pave the way for impactful real-world\napplications.\n","authors":["Jun Ma","Feifei Li","Sumin Kim","Reza Asakereh","Bao-Hiep Le","Dang-Khoa Nguyen-Vu","Alexander Pfefferle","Muxin Wei","Ruochen Gao","Donghang Lyu","Songxiao Yang","Lennart Purucker","Zdravko Marinov","Marius Staring","Haisheng Lu","Thuy Thanh Dao","Xincheng Ye","Zhi Li","Gianluca Brugnara","Philipp Vollmuth","Martha Foltyn-Dumitru","Jaeyoung Cho","Mustafa Ahmed Mahmutoglu","Martin Bendszus","Irada Pflüger","Aditya Rastogi","Dong Ni","Xin Yang","Guang-Quan Zhou","Kaini Wang","Nicholas Heller","Nikolaos Papanikolopoulos","Christopher Weight","Yubing Tong","Jayaram K Udupa","Cahill J. Patrick","Yaqi Wang","Yifan Zhang","Francisco Contijoch","Elliot McVeigh","Xin Ye","Shucheng He","Robert Haase","Thomas Pinetz","Alexander Radbruch","Inga Krause","Erich Kobler","Jian He","Yucheng Tang","Haichun Yang","Yuankai Huo","Gongning Luo","Kaisar Kushibar","Jandos Amankulov","Dias Toleshbayev","Amangeldi Mukhamejan","Jan Egger","Antonio Pepe","Christina Gsaxner","Gijs Luijten","Shohei Fujita","Tomohiro Kikuchi","Benedikt Wiestler","Jan S. Kirschke","Ezequiel de la Rosa","Federico Bolelli","Luca Lumetti","Costantino Grana","Kunpeng Xie","Guomin Wu","Behrus Puladi","Carlos Martín-Isla","Karim Lekadir","Victor M. Campello","Wei Shao","Wayne Brisbane","Hongxu Jiang","Hao Wei","Wu Yuan","Shuangle Li","Yuyin Zhou","Bo Wang"],"pdf_url":"https://arxiv.org/pdf/2412.16085v1.pdf","comment":"CVPR 2024 MedSAM on Laptop Competition Summary:\n  https://www.codabench.org/competitions/1847/"},{"id":"http://arxiv.org/abs/2406.07543v2","updated":"2024-12-20T17:24:44Z","published":"2024-06-11T17:59:35Z","title":"Vision Model Pre-training on Interleaved Image-Text Data via Latent\n  Compression Learning","summary":"  Recently, vision model pre-training has evolved from relying on manually\nannotated datasets to leveraging large-scale, web-crawled image-text data.\nDespite these advances, there is no pre-training method that effectively\nexploits the interleaved image-text data, which is very prevalent on the\nInternet. Inspired by the recent success of compression learning in natural\nlanguage processing, we propose a novel vision model pre-training method called\nLatent Compression Learning (LCL) for interleaved image-text data. This method\nperforms latent compression learning by maximizing the mutual information\nbetween the inputs and outputs of a causal attention model. The training\nobjective can be decomposed into two basic tasks: 1) contrastive learning\nbetween visual representation and preceding context, and 2) generating\nsubsequent text based on visual representation. Our experiments demonstrate\nthat our method not only matches the performance of CLIP on paired pre-training\ndatasets (e.g., LAION), but can also leverage interleaved pre-training data\n(e.g., MMC4) to learn robust visual representation from scratch, showcasing the\npotential of vision model pre-training with interleaved image-text data. Code\nis released at https://github.com/OpenGVLab/LCL.\n","authors":["Chenyu Yang","Xizhou Zhu","Jinguo Zhu","Weijie Su","Junjie Wang","Xuan Dong","Wenhai Wang","Lewei Lu","Bin Li","Jie Zhou","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2406.07543v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16079v1","updated":"2024-12-20T17:23:12Z","published":"2024-12-20T17:23:12Z","title":"Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg\n  Evolutionary Game","summary":"  Decentralised learning enables the training of deep learning algorithms\nwithout centralising data sets, resulting in benefits such as improved data\nprivacy, operational efficiency and the fostering of data ownership policies.\nHowever, significant data imbalances pose a challenge in this framework.\nParticipants with smaller datasets in distributed learning environments often\nachieve poorer results than participants with larger datasets. Data imbalances\nare particularly pronounced in medical fields and are caused by different\npatient populations, technological inequalities and divergent data collection\npractices.\n  In this paper, we consider distributed learning as an Stackelberg\nevolutionary game. We present two algorithms for setting the weights of each\nnode's contribution to the global model in each training round: the\nDeterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg\nWeighting Model (ASWM). We use three medical datasets to highlight the impact\nof dynamic weighting on underrepresented nodes in distributed learning. Our\nresults show that the ASWM significantly favours underrepresented nodes by\nimproving their performance by 2.713% in AUC. Meanwhile, nodes with larger\ndatasets experience only a modest average performance decrease of 0.441%.\n","authors":["Sebastian Niehaus","Ingo Roeder","Nico Scherf"],"pdf_url":"https://arxiv.org/pdf/2412.16079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16078v1","updated":"2024-12-20T17:21:05Z","published":"2024-12-20T17:21:05Z","title":"SegCol Challenge: Semantic Segmentation for Tools and Fold Edges in\n  Colonoscopy data","summary":"  Colorectal cancer (CRC) remains a leading cause of cancer-related deaths\nworldwide, with polyp removal being an effective early screening method.\nHowever, navigating the colon for thorough polyp detection poses significant\nchallenges. To advance camera navigation in colonoscopy, we propose the\nSemantic Segmentation for Tools and Fold Edges in Colonoscopy (SegCol)\nChallenge. This challenge introduces a dataset from the EndoMapper repository,\nfeaturing manually annotated, pixel-level semantic labels for colon folds and\nendoscopic tools across selected frames from 96 colonoscopy videos. By\nproviding fold edges as anatomical landmarks and depth discontinuity\ninformation from both fold and tool labels, the dataset is aimed to improve\ndepth perception and localization methods. Hosted as part of the Endovis\nChallenge at MICCAI 2024, SegCol aims to drive innovation in colonoscopy\nnavigation systems. Details are available at\nhttps://www.synapse.org/Synapse:syn54124209/wiki/626563, and code resources at\nhttps://github.com/surgical-vision/segcol_challenge .\n","authors":["Xinwei Ju","Rema Daher","Razvan Caramalau","Baoru Huang","Danail Stoyanov","Francisco Vasconcelos"],"pdf_url":"https://arxiv.org/pdf/2412.16078v1.pdf","comment":"4 pages, 1 figure. Dataset introduction for the SegCol Challenge at\n  MICCAI 2024. Full Challenge paper, including participant methods and\n  evaluation results, will be released soon"},{"id":"http://arxiv.org/abs/2412.10958v2","updated":"2024-12-20T16:59:40Z","published":"2024-12-14T20:29:29Z","title":"SoftVQ-VAE: Efficient 1-Dimensional Continuous Tokenizer","summary":"  Efficient image tokenization with high compression ratios remains a critical\nchallenge for training generative models. We present SoftVQ-VAE, a continuous\nimage tokenizer that leverages soft categorical posteriors to aggregate\nmultiple codewords into each latent token, substantially increasing the\nrepresentation capacity of the latent space. When applied to Transformer-based\narchitectures, our approach compresses 256x256 and 512x512 images using as few\nas 32 or 64 1-dimensional tokens. Not only does SoftVQ-VAE show consistent and\nhigh-quality reconstruction, more importantly, it also achieves\nstate-of-the-art and significantly faster image generation results across\ndifferent denoising-based generative models. Remarkably, SoftVQ-VAE improves\ninference throughput by up to 18x for generating 256x256 images and 55x for\n512x512 images while achieving competitive FID scores of 1.78 and 2.21 for\nSiT-XL. It also improves the training efficiency of the generative models by\nreducing the number of training iterations by 2.3x while maintaining comparable\nperformance. With its fully-differentiable design and semantic-rich latent\nspace, our experiment demonstrates that SoftVQ-VAE achieves efficient\ntokenization without compromising generation quality, paving the way for more\nefficient generative models. Code and model are released.\n","authors":["Hao Chen","Ze Wang","Xiang Li","Ximeng Sun","Fangyi Chen","Jiang Liu","Jindong Wang","Bhiksha Raj","Zicheng Liu","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2412.10958v2.pdf","comment":"Code and model: https://github.com/Hhhhhhao/continuous_tokenizer"},{"id":"http://arxiv.org/abs/2412.16050v1","updated":"2024-12-20T16:52:11Z","published":"2024-12-20T16:52:11Z","title":"Label-Efficient Data Augmentation with Video Diffusion Models for\n  Guidewire Segmentation in Cardiac Fluoroscopy","summary":"  The accurate segmentation of guidewires in interventional cardiac fluoroscopy\nvideos is crucial for computer-aided navigation tasks. Although deep learning\nmethods have demonstrated high accuracy and robustness in wire segmentation,\nthey require substantial annotated datasets for generalizability, underscoring\nthe need for extensive labeled data to enhance model performance. To address\nthis challenge, we propose the Segmentation-guided Frame-consistency Video\nDiffusion Model (SF-VD) to generate large collections of labeled fluoroscopy\nvideos, augmenting the training data for wire segmentation networks. SF-VD\nleverages videos with limited annotations by independently modeling scene\ndistribution and motion distribution. It first samples the scene distribution\nby generating 2D fluoroscopy images with wires positioned according to a\nspecified input mask, and then samples the motion distribution by progressively\ngenerating subsequent frames, ensuring frame-to-frame coherence through a\nframe-consistency strategy. A segmentation-guided mechanism further refines the\nprocess by adjusting wire contrast, ensuring a diverse range of visibility in\nthe synthesized image. Evaluation on a fluoroscopy dataset confirms the\nsuperior quality of the generated videos and shows significant improvements in\nguidewire segmentation.\n","authors":["Shaoyan Pan","Yikang Liu","Lin Zhao","Eric Z. Chen","Xiao Chen","Terrence Chen","Shanhui Sun"],"pdf_url":"https://arxiv.org/pdf/2412.16050v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.16046v1","updated":"2024-12-20T16:48:52Z","published":"2024-12-20T16:48:52Z","title":"Segmentation of arbitrary features in very high resolution remote\n  sensing imagery","summary":"  Very high resolution (VHR) mapping through remote sensing (RS) imagery\npresents a new opportunity to inform decision-making and sustainable practices\nin countless domains. Efficient processing of big VHR data requires automated\ntools applicable to numerous geographic regions and features. Contemporary RS\nstudies address this challenge by employing deep learning (DL) models for\nspecific datasets or features, which limits their applicability across\ncontexts.\n  The present research aims to overcome this limitation by introducing\nEcoMapper, a scalable solution to segment arbitrary features in VHR RS imagery.\nEcoMapper fully automates processing of geospatial data, DL model training, and\ninference. Models trained with EcoMapper successfully segmented two distinct\nfeatures in a real-world UAV dataset, achieving scores competitive with prior\nstudies which employed context-specific models.\n  To evaluate EcoMapper, many additional models were trained on permutations of\nprincipal field survey characteristics (FSCs). A relationship was discovered\nallowing derivation of optimal ground sampling distance from feature size,\ntermed Cording Index (CI). A comprehensive methodology for field surveys was\ndeveloped to ensure DL methods can be applied effectively to collected data.\n  The EcoMapper code accompanying this work is available at\nhttps://github.com/hcording/ecomapper .\n","authors":["Henry Cording","Yves Plancherel","Pablo Brito-Parada"],"pdf_url":"https://arxiv.org/pdf/2412.16046v1.pdf","comment":"Main article: 18 pages, 9 figures; appendix: 17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.16039v1","updated":"2024-12-20T16:40:11Z","published":"2024-12-20T16:40:11Z","title":"SafeCFG: Redirecting Harmful Classifier-Free Guidance for Safe\n  Generation","summary":"  Diffusion models (DMs) have demonstrated exceptional performance in\ntext-to-image (T2I) tasks, leading to their widespread use. With the\nintroduction of classifier-free guidance (CFG), the quality of images generated\nby DMs is improved. However, DMs can generate more harmful images by\nmaliciously guiding the image generation process through CFG. Some safe\nguidance methods aim to mitigate the risk of generating harmful images but\noften reduce the quality of clean image generation. To address this issue, we\nintroduce the Harmful Guidance Redirector (HGR), which redirects harmful CFG\ndirection while preserving clean CFG direction during image generation,\ntransforming CFG into SafeCFG and achieving high safety and quality generation.\nWe train HGR to redirect multiple harmful CFG directions simultaneously,\ndemonstrating its ability to eliminate various harmful elements while\npreserving high-quality generation. Additionally, we find that HGR can detect\nimage harmfulness, allowing for unsupervised fine-tuning of safe diffusion\nmodels without pre-defined clean or harmful labels. Experimental results show\nthat by incorporating HGR, images generated by diffusion models achieve both\nhigh quality and strong safety, and safe DMs trained through unsupervised\nmethods according to the harmfulness detected by HGR also exhibit good safety\nperformance. The codes will be publicly available.\n","authors":["Jiadong Pan","Hongcheng Gao","Liang Li","Zheng-Jun Zha","Qingming Huang","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2412.16039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16028v1","updated":"2024-12-20T16:25:20Z","published":"2024-12-20T16:25:20Z","title":"CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from\n  Defocused Images","summary":"  3D Gaussian Splatting (3DGS) has attracted significant attention for its\nhigh-quality novel view rendering, inspiring research to address real-world\nchallenges. While conventional methods depend on sharp images for accurate\nscene reconstruction, real-world scenarios are often affected by defocus blur\ndue to finite depth of field, making it essential to account for realistic 3D\nscene representation. In this study, we propose CoCoGaussian, a Circle of\nConfusion-aware Gaussian Splatting that enables precise 3D scene representation\nusing only defocused images. CoCoGaussian addresses the challenge of defocus\nblur by modeling the Circle of Confusion (CoC) through a physically grounded\napproach based on the principles of photographic defocus. Exploiting 3D\nGaussians, we compute the CoC diameter from depth and learnable aperture\ninformation, generating multiple Gaussians to precisely capture the CoC shape.\nFurthermore, we introduce a learnable scaling factor to enhance robustness and\nprovide more flexibility in handling unreliable depth in scenes with reflective\nor refractive surfaces. Experiments on both synthetic and real-world datasets\ndemonstrate that CoCoGaussian achieves state-of-the-art performance across\nmultiple benchmarks.\n","authors":["Jungho Lee","Suhwan Cho","Taeoh Kim","Ho-Deok Jang","Minhyeok Lee","Geonho Cha","Dongyoon Wee","Dogyoon Lee","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2412.16028v1.pdf","comment":"Project Page: https://Jho-Yonsei.github.io/CoCoGaussian/"},{"id":"http://arxiv.org/abs/2405.09404v3","updated":"2024-12-20T16:19:53Z","published":"2024-05-15T15:00:46Z","title":"Learning Temporally Equivariance for Degenerative Disease Progression in\n  OCT by Predicting Future Representations","summary":"  Contrastive pretraining provides robust representations by ensuring their\ninvariance to different image transformations while simultaneously preventing\nrepresentational collapse. Equivariant contrastive learning, on the other hand,\nprovides representations sensitive to specific image transformations while\nremaining invariant to others. By introducing equivariance to time-induced\ntransformations, such as disease-related anatomical changes in longitudinal\nimaging, the model can effectively capture such changes in the representation\nspace. In this work, we propose a Time-equivariant Contrastive Learning (TC)\nmethod. First, an encoder embeds two unlabeled scans from different time points\nof the same patient into the representation space. Next, a temporal\nequivariance module is trained to predict the representation of a later visit\nbased on the representation from one of the previous visits and the\ncorresponding time interval with a novel regularization loss term while\npreserving the invariance property to irrelevant image transformations. On a\nlarge longitudinal dataset, our model clearly outperforms existing equivariant\ncontrastive methods in predicting progression from intermediate age-related\nmacular degeneration (AMD) to advanced wet-AMD within a specified time-window.\n","authors":["Taha Emre","Arunava Chakravarty","Dmitrii Lachinov","Antoine Rivail","Ursula Schmidt-Erfurth","Hrvoje Bogunović"],"pdf_url":"https://arxiv.org/pdf/2405.09404v3.pdf","comment":"Accepted at MICCAI 2024 (early accept, top 11%)"},{"id":"http://arxiv.org/abs/2409.02035v2","updated":"2024-12-20T16:19:40Z","published":"2024-09-03T16:30:48Z","title":"A Modern Take on Visual Relationship Reasoning for Grasp Planning","summary":"  Interacting with real-world cluttered scenes pose several challenges to\nrobotic agents that need to understand complex spatial dependencies among the\nobserved objects to determine optimal pick sequences or efficient object\nretrieval strategies. Existing solutions typically manage simplified scenarios\nand focus on predicting pairwise object relationships following an initial\nobject detection phase, but often overlook the global context or struggle with\nhandling redundant and missing object relations. In this work, we present a\nmodern take on visual relational reasoning for grasp planning. We introduce\nD3GD, a novel testbed that includes bin picking scenes with up to 35 objects\nfrom 97 distinct categories. Additionally, we propose D3G, a new end-to-end\ntransformer-based dependency graph generation model that simultaneously detects\nobjects and produces an adjacency matrix representing their spatial\nrelationships. Recognizing the limitations of standard metrics, we employ the\nAverage Precision of Relationships for the first time to evaluate model\nperformance, conducting an extensive experimental benchmark. The obtained\nresults establish our approach as the new state-of-the-art for this task,\nlaying the foundation for future research in robotic manipulation. We publicly\nrelease the code and dataset at https://paolotron.github.io/d3g.github.io.\n","authors":["Paolo Rabino","Tatiana Tommasi"],"pdf_url":"https://arxiv.org/pdf/2409.02035v2.pdf","comment":"Accepted at IEEE RAL - in press"},{"id":"http://arxiv.org/abs/2410.19732v2","updated":"2024-12-20T16:19:17Z","published":"2024-10-25T17:59:09Z","title":"Rethinking Visual Dependency in Long-Context Reasoning for Large\n  Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) excel in cross-model tasks but\nexperience performance declines in long-context reasoning due to overreliance\non textual information and reduced visual dependency. In this study, we\nempirically analyze LVLMs in long-context reasoning, revealing that increased\ncontext length leads to a higher dependence on language at the expense of\nvisual dependency. To address this issue, we propose a novel training-free\ncontext pruning method that selectively removes less critical textual\ninformation. Our approach enhances visual dependency and reduces textual noise,\nthereby improving LVLM performance in long-context reasoning. We validate our\nmethod by constructing a long-context dataset, demonstrating its effectiveness\nacross various LVLMs. Moreover, further analysis confirms the robustness of\ndifferent token pruning strategies and preliminary explores scaling laws\nbetween pruning rates and context length.\n","authors":["Yucheng Zhou","Zhi Rao","Jun Wan","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2410.19732v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19224v4","updated":"2024-12-20T16:04:16Z","published":"2024-05-29T16:04:03Z","title":"A study on the adequacy of common IQA measures for medical images","summary":"  Image quality assessment (IQA) is standard practice in the development stage\nof novel machine learning algorithms that operate on images. The most commonly\nused IQA measures have been developed and tested for natural images, but not in\nthe medical setting. Reported inconsistencies arising in medical images are not\nsurprising, as they have different properties than natural images. In this\nstudy, we test the applicability of common IQA measures for medical image data\nby comparing their assessment to manually rated chest X-ray (5 experts) and\nphotoacoustic image data (2 experts). Moreover, we include supplementary\nstudies on grayscale natural images and accelerated brain MRI data. The results\nof all experiments show a similar outcome in line with previous findings for\nmedical images: PSNR and SSIM in the default setting are in the lower range of\nthe result list and HaarPSI outperforms the other tested measures in the\noverall performance. Also among the top performers in our experiments are the\nfull reference measures FSIM, LPIPS and MS-SSIM. Generally, the results on\nnatural images yield considerably higher correlations, suggesting that\nadditional employment of tailored IQA measures for medical imaging algorithms\nis needed.\n","authors":["Anna Breger","Clemens Karner","Ian Selby","Janek Gröhl","Sören Dittmer","Edward Lilley","Judith Babar","Jake Beckford","Thomas R Else","Timothy J Sadler","Shahab Shahipasand","Arthikkaa Thavakumar","Michael Roberts","Carola-Bibiane Schönlieb"],"pdf_url":"https://arxiv.org/pdf/2405.19224v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10184v2","updated":"2024-12-20T15:49:47Z","published":"2024-12-13T14:55:24Z","title":"Sims: An Interactive Tool for Geospatial Matching and Clustering","summary":"  Acquiring, processing, and visualizing geospatial data requires significant\ncomputing resources, especially for large spatio-temporal domains. This\nchallenge hinders the rapid discovery of predictive features, which is\nessential for advancing geospatial modeling. To address this, we developed\nSimilarity Search (Sims), a no-code web tool that allows users to perform\nclustering and similarity search over defined regions of interest using Google\nEarth Engine as a backend. Sims is designed to complement existing modeling\ntools by focusing on feature exploration rather than model creation. We\ndemonstrate the utility of Sims through a case study analyzing simulated maize\nyield data in Rwanda, where we evaluate how different combinations of soil,\nweather, and agronomic features affect the clustering of yield response zones.\nSims is open source and available at https://github.com/microsoft/Sims\n","authors":["Akram Zaytar","Girmaw Abebe Tadesse","Caleb Robinson","Eduardo G. Bendito","Medha Devare","Meklit Chernet","Gilles Q. Hacheme","Rahul Dodhia","Juan M. Lavista Ferres"],"pdf_url":"https://arxiv.org/pdf/2412.10184v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15979v1","updated":"2024-12-20T15:22:51Z","published":"2024-12-20T15:22:51Z","title":"MR-GDINO: Efficient Open-World Continual Object Detection","summary":"  Open-world (OW) recognition and detection models show strong zero- and\nfew-shot adaptation abilities, inspiring their use as initializations in\ncontinual learning methods to improve performance. Despite promising results on\nseen classes, such OW abilities on unseen classes are largely degenerated due\nto catastrophic forgetting. To tackle this challenge, we propose an open-world\ncontinual object detection task, requiring detectors to generalize to old, new,\nand unseen categories in continual learning scenarios. Based on this task, we\npresent a challenging yet practical OW-COD benchmark to assess detection\nabilities. The goal is to motivate OW detectors to simultaneously preserve\nlearned classes, adapt to new classes, and maintain open-world capabilities\nunder few-shot adaptations. To mitigate forgetting in unseen categories, we\npropose MR-GDINO, a strong, efficient and scalable baseline via memory and\nretrieval mechanisms within a highly scalable memory pool. Experimental results\nshow that existing continual detectors suffer from severe forgetting for both\nseen and unseen categories. In contrast, MR-GDINO largely mitigates forgetting\nwith only 0.1% activated extra parameters, achieving state-of-the-art\nperformance for old, new, and unseen categories.\n","authors":["Bowen Dong","Zitong Huang","Guanglei Yang","Lei Zhang","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2412.15979v1.pdf","comment":"Website: https://m1saka.moe/owcod/ . Code is available at:\n  https://github.com/DongSky/MR-GDINO"},{"id":"http://arxiv.org/abs/2412.15967v1","updated":"2024-12-20T15:07:55Z","published":"2024-12-20T15:07:55Z","title":"Self-Supervised Radiograph Anatomical Region Classification -- How Clean\n  Is Your Real-World Data?","summary":"  Modern deep learning-based clinical imaging workflows rely on accurate labels\nof the examined anatomical region. Knowing the anatomical region is required to\nselect applicable downstream models and to effectively generate cohorts of high\nquality data for future medical and machine learning research efforts. However,\nthis information may not be available in externally sourced data or generally\ncontain data entry errors. To address this problem, we show the effectiveness\nof self-supervised methods such as SimCLR and BYOL as well as supervised\ncontrastive deep learning methods in assigning one of 14 anatomical region\nclasses in our in-house dataset of 48,434 skeletal radiographs. We achieve a\nstrong linear evaluation accuracy of 96.6% with a single model and 97.7% using\nan ensemble approach. Furthermore, only a few labeled instances (1% of the\ntraining set) suffice to achieve an accuracy of 92.2%, enabling usage in\nlow-label and thus low-resource scenarios. Our model can be used to correct\ndata entry mistakes: a follow-up analysis of the test set errors of our\nbest-performing single model by an expert radiologist identified 35% incorrect\nlabels and 11% out-of-domain images. When accounted for, the radiograph\nanatomical region labelling performance increased -- without and with an\nensemble, respectively -- to a theoretical accuracy of 98.0% and 98.8%.\n","authors":["Simon Langer","Jessica Ritter","Rickmer Braren","Daniel Rueckert","Paul Hager"],"pdf_url":"https://arxiv.org/pdf/2412.15967v1.pdf","comment":"12 pages, 4 figures, 2 supplementary figures"},{"id":"http://arxiv.org/abs/2412.15966v1","updated":"2024-12-20T15:06:15Z","published":"2024-12-20T15:06:15Z","title":"Monkey Transfer Learning Can Improve Human Pose Estimation","summary":"  In this study, we investigated whether transfer learning from macaque monkeys\ncould improve human pose estimation. Current state-of-the-art pose estimation\ntechniques, often employing deep neural networks, can match human annotation in\nnon-clinical datasets. However, they underperform in novel situations, limiting\ntheir generalisability to clinical populations with pathological movement\npatterns. Clinical datasets are not widely available for AI training due to\nethical challenges and a lack of data collection. We observe that data from\nother species may be able to bridge this gap by exposing the network to a\nbroader range of motion cues. We found that utilising data from other species\nand undertaking transfer learning improved human pose estimation in terms of\nprecision and recall compared to the benchmark, which was trained on humans\nonly. Compared to the benchmark, fewer human training examples were needed for\nthe transfer learning approach (1,000 vs 19,185). These results suggest that\nmacaque pose estimation can improve human pose estimation in clinical\nsituations. Future work should further explore the utility of pose estimation\ntrained with monkey data in clinical populations.\n","authors":["Bradley Scott","Clarisse de Vries","Aiden Durrant","Nir Oren","Edward Chadwick","Dimitra Blana"],"pdf_url":"https://arxiv.org/pdf/2412.15966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14622v2","updated":"2024-12-20T15:06:14Z","published":"2024-03-21T17:59:35Z","title":"Language Repository for Long Video Understanding","summary":"  Language has become a prominent modality in computer vision with the rise of\nLLMs. Despite supporting long context-lengths, their effectiveness in handling\nlong-term information gradually declines with input length. This becomes\ncritical, especially in applications such as long-form video understanding. In\nthis paper, we introduce a Language Repository (LangRepo) for LLMs, that\nmaintains concise and structured information as an interpretable (i.e.,\nall-textual) representation. Our repository is updated iteratively based on\nmulti-scale video chunks. We introduce write and read operations that focus on\npruning redundancies in text, and extracting information at various temporal\nscales. The proposed framework is evaluated on zero-shot visual\nquestion-answering benchmarks including EgoSchema, NExT-QA, IntentQA and\nNExT-GQA, showing state-of-the-art performance at its scale. Our code is\navailable at https://github.com/kkahatapitiya/LangRepo.\n","authors":["Kumara Kahatapitiya","Kanchana Ranasinghe","Jongwoo Park","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2403.14622v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15939v1","updated":"2024-12-20T14:32:56Z","published":"2024-12-20T14:32:56Z","title":"Reframing Image Difference Captioning with BLIP2IDC and Synthetic\n  Augmentation","summary":"  The rise of the generative models quality during the past years enabled the\ngeneration of edited variations of images at an important scale. To counter the\nharmful effects of such technology, the Image Difference Captioning (IDC) task\naims to describe the differences between two images. While this task is\nsuccessfully handled for simple 3D rendered images, it struggles on real-world\nimages. The reason is twofold: the training data-scarcity, and the difficulty\nto capture fine-grained differences between complex images. To address those\nissues, we propose in this paper a simple yet effective framework to both adapt\nexisting image captioning models to the IDC task and augment IDC datasets. We\nintroduce BLIP2IDC, an adaptation of BLIP2 to the IDC task at low computational\ncost, and show it outperforms two-streams approaches by a significant margin on\nreal-world IDC datasets. We also propose to use synthetic augmentation to\nimprove the performance of IDC models in an agnostic fashion. We show that our\nsynthetic augmentation strategy provides high quality data, leading to a\nchallenging new dataset well-suited for IDC named Syned1.\n","authors":["Gautier Evennou","Antoine Chaffin","Vivien Chappelier","Ewa Kijak"],"pdf_url":"https://arxiv.org/pdf/2412.15939v1.pdf","comment":"This paper has been accepted for the IEEE/CVF Winter Conference on\n  Applications of Computer Vision (WACV) 2025"},{"id":"http://arxiv.org/abs/2411.10257v2","updated":"2024-12-20T14:24:30Z","published":"2024-11-15T15:04:04Z","title":"The Unreasonable Effectiveness of Guidance for Diffusion Models","summary":"  Guidance is an error-correcting technique used to improve the perceptual\nquality of images generated by diffusion models. Typically, the correction is\nachieved by linear extrapolation, using an auxiliary diffusion model that has\nlower performance than the primary model. Using a 2D toy example, we show that\nit is highly beneficial when the auxiliary model exhibits similar errors as the\nprimary one but stronger. We verify this finding in higher dimensions, where we\nshow that competitive generative performance to state-of-the-art guidance\nmethods can be achieved when the auxiliary model differs from the primary one\nonly by having stronger weight regularization. As an independent contribution,\nwe investigate whether upweighting long-range spatial dependencies improves\nvisual fidelity. The result is a novel guidance method, which we call sliding\nwindow guidance (SWG), that guides the primary model with itself by\nconstraining its receptive field. Intriguingly, SWG aligns better with human\npreferences than state-of-the-art guidance methods while requiring neither\ntraining, architectural modifications, nor class conditioning. The code will be\nreleased.\n","authors":["Tim Kaiser","Nikolas Adaloglou","Markus Kollmann"],"pdf_url":"https://arxiv.org/pdf/2411.10257v2.pdf","comment":"Preprint. 30 pages, 19 figures in total, including appendix"},{"id":"http://arxiv.org/abs/2412.15925v1","updated":"2024-12-20T14:18:16Z","published":"2024-12-20T14:18:16Z","title":"MiniGPT-Pancreas: Multimodal Large Language Model for Pancreas Cancer\n  Classification and Detection","summary":"  Problem: Pancreas radiological imaging is challenging due to the small size,\nblurred boundaries, and variability of shape and position of the organ among\npatients. Goal: In this work we present MiniGPT-Pancreas, a Multimodal Large\nLanguage Model (MLLM), as an interactive chatbot to support clinicians in\npancreas cancer diagnosis by integrating visual and textual information.\nMethods: MiniGPT-v2, a general-purpose MLLM, was fine-tuned in a cascaded way\nfor pancreas detection, tumor classification, and tumor detection with\nmultimodal prompts combining questions and computed tomography scans from the\nNational Institute of Health (NIH), and Medical Segmentation Decathlon (MSD)\ndatasets. The AbdomenCT-1k dataset was used to detect the liver, spleen,\nkidney, and pancreas. Results: MiniGPT-Pancreas achieved an Intersection over\nUnion (IoU) of 0.595 and 0.550 for the detection of pancreas on NIH and MSD\ndatasets, respectively. For the pancreas cancer classification task on the MSD\ndataset, accuracy, precision, and recall were 0.876, 0.874, and 0.878,\nrespectively. When evaluating MiniGPT-Pancreas on the AbdomenCT-1k dataset for\nmulti-organ detection, the IoU was 0.8399 for the liver, 0.722 for the kidney,\n0.705 for the spleen, and 0.497 for the pancreas. For the pancreas tumor\ndetection task, the IoU score was 0.168 on the MSD dataset. Conclusions:\nMiniGPT-Pancreas represents a promising solution to support clinicians in the\nclassification of pancreas images with pancreas tumors. Future research is\nneeded to improve the score on the detection task, especially for pancreas\ntumors.\n","authors":["Andrea Moglia","Elia Clement Nastasio","Luca Mainardi","Pietro Cerveri"],"pdf_url":"https://arxiv.org/pdf/2412.15925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15924v1","updated":"2024-12-20T14:17:03Z","published":"2024-12-20T14:17:03Z","title":"Watertox: The Art of Simplicity in Universal Attacks A Cross-Model\n  Framework for Robust Adversarial Generation","summary":"  Contemporary adversarial attack methods face significant limitations in\ncross-model transferability and practical applicability. We present Watertox,\nan elegant adversarial attack framework achieving remarkable effectiveness\nthrough architectural diversity and precision-controlled perturbations. Our\ntwo-stage Fast Gradient Sign Method combines uniform baseline perturbations\n($\\epsilon_1 = 0.1$) with targeted enhancements ($\\epsilon_2 = 0.4$). The\nframework leverages an ensemble of complementary architectures, from VGG to\nConvNeXt, synthesizing diverse perspectives through an innovative voting\nmechanism. Against state-of-the-art architectures, Watertox reduces model\naccuracy from 70.6% to 16.0%, with zero-shot attacks achieving up to 98.8%\naccuracy reduction against unseen architectures. These results establish\nWatertox as a significant advancement in adversarial methodologies, with\npromising applications in visual security systems and CAPTCHA generation.\n","authors":["Zhenghao Gao","Shengjie Xu","Meixi Chen","Fangyao Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.15924v1.pdf","comment":"18 pages, 4 figures, 3 tables. Advances a novel method for generating\n  cross-model transferable adversarial perturbations through a two-stage FGSM\n  process and architectural ensemble voting mechanism"},{"id":"http://arxiv.org/abs/2412.14925v2","updated":"2024-12-20T14:01:54Z","published":"2024-12-19T15:02:50Z","title":"Automatic Spectral Calibration of Hyperspectral Images:Method, Dataset\n  and Benchmark","summary":"  Hyperspectral image (HSI) densely samples the world in both the space and\nfrequency domain and therefore is more distinctive than RGB images. Usually,\nHSI needs to be calibrated to minimize the impact of various illumination\nconditions. The traditional way to calibrate HSI utilizes a physical reference,\nwhich involves manual operations, occlusions, and/or limits camera mobility.\nThese limitations inspire this paper to automatically calibrate HSIs using a\nlearning-based method. Towards this goal, a large-scale HSI calibration dataset\nis created, which has 765 high-quality HSI pairs covering diversified natural\nscenes and illuminations. The dataset is further expanded to 7650 pairs by\ncombining with 10 different physically measured illuminations. A spectral\nillumination transformer (SIT) together with an illumination attention module\nis proposed. Extensive benchmarks demonstrate the SoTA performance of the\nproposed SIT. The benchmarks also indicate that low-light conditions are more\nchallenging than normal conditions. The dataset and codes are available\nonline:https://github.com/duranze/Automatic-spectral-calibration-of-HSI\n","authors":["Zhuoran Du","Shaodi You","Cheng Cheng","Shikui Wei"],"pdf_url":"https://arxiv.org/pdf/2412.14925v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15909v1","updated":"2024-12-20T13:59:27Z","published":"2024-12-20T13:59:27Z","title":"CCNDF: Curvature Constrained Neural Distance Fields from 3D LiDAR\n  Sequences","summary":"  Neural distance fields (NDF) have emerged as a powerful tool for addressing\nchallenges in 3D computer vision and graphics downstream problems. While\nsignificant progress has been made to learn NDF from various kind of sensor\ndata, a crucial aspect that demands attention is the supervision of neural\nfields during training as the ground-truth NDFs are not available for\nlarge-scale outdoor scenes. Previous works have utilized various forms of\nexpected signed distance to guide model learning. Yet, these approaches often\nneed to pay more attention to critical considerations of surface geometry and\nare limited to small-scale implementations. To this end, we propose a novel\nmethodology leveraging second-order derivatives of the signed distance field\nfor improved neural field learning. Our approach addresses limitations by\naccurately estimating signed distance, offering a more comprehensive\nunderstanding of underlying geometry. To assess the efficacy of our\nmethodology, we conducted comparative evaluations against prevalent methods for\nmapping and localization tasks, which are primary application areas of NDF. Our\nresults demonstrate the superiority of the proposed approach, highlighting its\npotential for advancing the capabilities of neural distance fields in computer\nvision and graphics applications.\n","authors":["Akshit Singh","Karan Bhakuni","Rajendra Nagar"],"pdf_url":"https://arxiv.org/pdf/2412.15909v1.pdf","comment":"ACCV 2024, Oral Presentation"},{"id":"http://arxiv.org/abs/2412.15890v1","updated":"2024-12-20T13:40:28Z","published":"2024-12-20T13:40:28Z","title":"NeuroPump: Simultaneous Geometric and Color Rectification for Underwater\n  Images","summary":"  Underwater image restoration aims to remove geometric and color distortions\ndue to water refraction, absorption and scattering. Previous studies focus on\nrestoring either color or the geometry, but to our best knowledge, not both.\nHowever, in practice it may be cumbersome to address the two rectifications\none-by-one. In this paper, we propose NeuroPump, a self-supervised method to\nsimultaneously optimize and rectify underwater geometry and color as if water\nwere pumped out. The key idea is to explicitly model refraction, absorption and\nscattering in Neural Radiance Field (NeRF) pipeline, such that it not only\nperforms simultaneous geometric and color rectification, but also enables to\nsynthesize novel views and optical effects by controlling the decoupled\nparameters. In addition, to address issue of lack of real paired ground truth\nimages, we propose an underwater 360 benchmark dataset that has real paired\n(i.e., with and without water) images. Our method clearly outperforms other\nbaselines both quantitatively and qualitatively.\n","authors":["Yue Guo","Haoxiang Liao","Haibin Ling","Bingyao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.15890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15856v3","updated":"2024-12-20T13:26:32Z","published":"2023-11-27T14:23:36Z","title":"Joint Supervised and Self-supervised Learning for MRI Reconstruction","summary":"  Magnetic Resonance Imaging (MRI) represents an important diagnostic modality;\nhowever, its inherently slow acquisition process poses challenges in obtaining\nfully-sampled $k$-space data under motion. In the absence of fully-sampled\nacquisitions, serving as ground truths, training deep learning algorithms in a\nsupervised manner to predict the underlying ground truth image becomes\nchallenging. To address this limitation, self-supervised methods have emerged\nas a viable alternative, leveraging available subsampled $k$-space data to\ntrain deep neural networks for MRI reconstruction. Nevertheless, these\napproaches often fall short when compared to supervised methods. We propose\nJoint Supervised and Self-supervised Learning (JSSL), a novel training approach\nfor deep learning-based MRI reconstruction algorithms aimed at enhancing\nreconstruction quality in cases where target datasets containing fully-sampled\n$k$-space measurements are unavailable. JSSL operates by simultaneously\ntraining a model in a self-supervised learning setting, using subsampled data\nfrom the target dataset(s), and in a supervised learning manner, utilizing\ndatasets with fully-sampled $k$-space data, referred to as proxy datasets. We\ndemonstrate JSSL's efficacy using subsampled prostate or cardiac MRI data as\nthe target datasets, with fully-sampled brain and knee, or brain, knee and\nprostate $k$-space acquisitions, respectively, as proxy datasets. Our results\nshowcase substantial improvements over conventional self-supervised methods,\nvalidated using common image quality metrics. Furthermore, we provide\ntheoretical motivations for JSSL and establish \"rule-of-thumb\" guidelines for\ntraining MRI reconstruction models. JSSL effectively enhances MRI\nreconstruction quality in scenarios where fully-sampled $k$-space data is not\navailable, leveraging the strengths of supervised learning by incorporating\nproxy datasets.\n","authors":["George Yiasemis","Nikita Moriakov","Clara I. Sánchez","Jan-Jakob Sonke","Jonas Teuwen"],"pdf_url":"https://arxiv.org/pdf/2311.15856v3.pdf","comment":"pages, 14 figures, 6 tables"},{"id":"http://arxiv.org/abs/2412.15867v1","updated":"2024-12-20T13:10:43Z","published":"2024-12-20T13:10:43Z","title":"IRGS: Inter-Reflective Gaussian Splatting with 2D Gaussian Ray Tracing","summary":"  In inverse rendering, accurately modeling visibility and indirect radiance\nfor incident light is essential for capturing secondary effects. Due to the\nabsence of a powerful Gaussian ray tracer, previous 3DGS-based methods have\neither adopted a simplified rendering equation or used learnable parameters to\napproximate incident light, resulting in inaccurate material and lighting\nestimations. To this end, we introduce inter-reflective Gaussian splatting\n(IRGS) for inverse rendering. To capture inter-reflection, we apply the full\nrendering equation without simplification and compute incident radiance on the\nfly using the proposed differentiable 2D Gaussian ray tracing. Additionally, we\npresent an efficient optimization scheme to handle the computational demands of\nMonte Carlo sampling for rendering equation evaluation. Furthermore, we\nintroduce a novel strategy for querying the indirect radiance of incident light\nwhen relighting the optimized scenes. Extensive experiments on multiple\nstandard benchmarks validate the effectiveness of IRGS, demonstrating its\ncapability to accurately model complex inter-reflection effects.\n","authors":["Chun Gu","Xiaofei Wei","Zixuan Zeng","Yuxuan Yao","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.15867v1.pdf","comment":"Project page: https://fudan-zvg.github.io/IRGS"},{"id":"http://arxiv.org/abs/2412.15853v1","updated":"2024-12-20T12:48:58Z","published":"2024-12-20T12:48:58Z","title":"Semi-Supervised Adaptation of Diffusion Models for Handwritten Text\n  Generation","summary":"  The generation of images of realistic looking, readable handwritten text is a\nchallenging task which is referred to as handwritten text generation (HTG).\nGiven a string and examples from a writer, the goal is to synthesize an image\ndepicting the correctly spelled word in handwriting with the calligraphic style\nof the desired writer. An important application of HTG is the generation of\ntraining images in order to adapt downstream models for new data sets. With\ntheir success in natural image generation, diffusion models (DMs) have become\nthe state-of-the-art approach in HTG. In this work, we present an extension of\na latent DM for HTG to enable generation of writing styles not seen during\ntraining by learning style conditioning with a masked auto encoder. Our\nproposed content encoder allows for different ways of conditioning the DM on\ntextual and calligraphic features. Additionally, we employ classifier-free\nguidance and explore the influence on the quality of the generated training\nimages. For adapting the model to a new unlabeled data set, we propose a\nsemi-supervised training scheme. We evaluate our approach on the IAM-database\nand use the RIMES-database to examine the generation of data not seen during\ntraining achieving improvements in this particularly promising application of\nDMs for HTG.\n","authors":["Kai Brandenbusch"],"pdf_url":"https://arxiv.org/pdf/2412.15853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15847v1","updated":"2024-12-20T12:39:49Z","published":"2024-12-20T12:39:49Z","title":"Image Quality Assessment: Enhancing Perceptual Exploration and\n  Interpretation with Collaborative Feature Refinement and Hausdorff distance","summary":"  Current full-reference image quality assessment (FR-IQA) methods often fuse\nfeatures from reference and distorted images, overlooking that color and\nluminance distortions occur mainly at low frequencies, whereas edge and texture\ndistortions occur at high frequencies. This work introduces a pioneering\ntraining-free FR-IQA method that accurately predicts image quality in alignment\nwith the human visual system (HVS) by leveraging a novel perceptual degradation\nmodelling approach to address this limitation. First, a collaborative feature\nrefinement module employs a carefully designed wavelet transform to extract\nperceptually relevant features, capturing multiscale perceptual information and\nmimicking how the HVS analyses visual information at various scales and\norientations in the spatial and frequency domains. Second, a Hausdorff\ndistance-based distribution similarity measurement module robustly assesses the\ndiscrepancy between the feature distributions of the reference and distorted\nimages, effectively handling outliers and variations while mimicking the\nability of HVS to perceive and tolerate certain levels of distortion. The\nproposed method accurately captures perceptual quality differences without\nrequiring training data or subjective quality scores. Extensive experiments on\nmultiple benchmark datasets demonstrate superior performance compared with\nexisting state-of-the-art approaches, highlighting its ability to correlate\nstrongly with the HVS.\\footnote{The code is available at\n\\url{https://anonymous.4open.science/r/CVPR2025-F339}.}\n","authors":["Xuekai Wei","Junyu Zhang","Qinlin Hu","Mingliang Zhou\\\\Yong Feng","Weizhi Xian","Huayan Pu","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2412.15847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15845v1","updated":"2024-12-20T12:36:34Z","published":"2024-12-20T12:36:34Z","title":"Multi-dimensional Visual Prompt Enhanced Image Restoration via\n  Mamba-Transformer Aggregation","summary":"  Recent efforts on image restoration have focused on developing \"all-in-one\"\nmodels that can handle different degradation types and levels within single\nmodel. However, most of mainstream Transformer-based ones confronted with\ndilemma between model capabilities and computation burdens, since\nself-attention mechanism quadratically increase in computational complexity\nwith respect to image size, and has inadequacies in capturing long-range\ndependencies. Most of Mamba-related ones solely scanned feature map in spatial\ndimension for global modeling, failing to fully utilize information in channel\ndimension. To address aforementioned problems, this paper has proposed to fully\nutilize complementary advantages from Mamba and Transformer without sacrificing\ncomputation efficiency. Specifically, the selective scanning mechanism of Mamba\nis employed to focus on spatial modeling, enabling capture long-range spatial\ndependencies under linear complexity. The self-attention mechanism of\nTransformer is applied to focus on channel modeling, avoiding high computation\nburdens that are in quadratic growth with image's spatial dimensions. Moreover,\nto enrich informative prompts for effective image restoration,\nmulti-dimensional prompt learning modules are proposed to learn prompt-flows\nfrom multi-scale encoder/decoder layers, benefiting for revealing underlying\ncharacteristic of various degradations from both spatial and channel\nperspectives, therefore, enhancing the capabilities of \"all-in-one\" model to\nsolve various restoration tasks. Extensive experiment results on several image\nrestoration benchmark tasks such as image denoising, dehazing, and deraining,\nhave demonstrated that the proposed method can achieve new state-of-the-art\nperformance, compared with many popular mainstream methods. Related source\ncodes and pre-trained parameters will be public on github\nhttps://github.com/12138-chr/MTAIR.\n","authors":["Aiwen Jiang","Hourong Chen","Zhiwen Chen","Jihua Ye","Mingwen Wang"],"pdf_url":"https://arxiv.org/pdf/2412.15845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15844v1","updated":"2024-12-20T12:35:41Z","published":"2024-12-20T12:35:41Z","title":"Efficient Curation of Invertebrate Image Datasets Using Feature\n  Embeddings and Automatic Size Comparison","summary":"  The amount of image datasets collected for environmental monitoring purposes\nhas increased in the past years as computer vision assisted methods have gained\ninterest. Computer vision applications rely on high-quality datasets, making\ndata curation important. However, data curation is often done ad-hoc and the\nmethods used are rarely published. We present a method for curating large-scale\nimage datasets of invertebrates that contain multiple images of the same taxa\nand/or specimens and have relatively uniform background in the images. Our\napproach is based on extracting feature embeddings with pretrained deep neural\nnetworks, and using these embeddings to find visually most distinct images by\ncomparing their embeddings to the group prototype embedding. Also, we show that\na simple area-based size comparison approach is able to find a lot of common\nerroneous images, such as images containing detached body parts and\nmisclassified samples. In addition to the method, we propose using novel\nmetrics for evaluating human-in-the-loop outlier detection methods. The\nimplementations of the proposed curation methods, as well as a benchmark\ndataset containing annotated erroneous images, are publicly available in\nhttps://github.com/mikkoim/taxonomist-studio.\n","authors":["Mikko Impiö","Philipp M. Rehsen","Jenni Raitoharju"],"pdf_url":"https://arxiv.org/pdf/2412.15844v1.pdf","comment":"Accepted to IEEE CIETES 2025"},{"id":"http://arxiv.org/abs/2412.15835v1","updated":"2024-12-20T12:25:33Z","published":"2024-12-20T12:25:33Z","title":"Enhancing Generalized Few-Shot Semantic Segmentation via Effective\n  Knowledge Transfer","summary":"  Generalized few-shot semantic segmentation (GFSS) aims to segment objects of\nboth base and novel classes, using sufficient samples of base classes and few\nsamples of novel classes. Representative GFSS approaches typically employ a\ntwo-phase training scheme, involving base class pre-training followed by novel\nclass fine-tuning, to learn the classifiers for base and novel classes\nrespectively. Nevertheless, distribution gap exists between base and novel\nclasses in this process. To narrow this gap, we exploit effective knowledge\ntransfer from base to novel classes. First, a novel prototype modulation module\nis designed to modulate novel class prototypes by exploiting the correlations\nbetween base and novel classes. Second, a novel classifier calibration module\nis proposed to calibrate the weight distribution of the novel classifier\naccording to that of the base classifier. Furthermore, existing GFSS approaches\nsuffer from a lack of contextual information for novel classes due to their\nlimited samples, we thereby introduce a context consistency learning scheme to\ntransfer the contextual knowledge from base to novel classes. Extensive\nexperiments on PASCAL-5$^i$ and COCO-20$^i$ demonstrate that our approach\nsignificantly enhances the state of the art in the GFSS setting. The code is\navailable at: https://github.com/HHHHedy/GFSS-EKT.\n","authors":["Xinyue Chen","Miaojing Shi","Zijian Zhou","Lianghua He","Sophia Tsoka"],"pdf_url":"https://arxiv.org/pdf/2412.15835v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2409.04053v2","updated":"2024-12-20T12:14:37Z","published":"2024-09-06T06:49:55Z","title":"COLUMBUS: Evaluating COgnitive Lateral Understanding through\n  Multiple-choice reBUSes","summary":"  While visual question-answering (VQA) benchmarks have catalyzed the\ndevelopment of reasoning techniques, they have focused on vertical thinking.\nEffective problem-solving also necessitates lateral thinking, which remains\nunderstudied in AI and has not been used to test visual perception systems. To\nbridge this gap, we formulate visual lateral thinking as a multiple-choice\nquestion-answering task and describe a three-step taxonomy-driven methodology\nfor instantiating task examples. Then, we develop COLUMBUS, a synthetic\nbenchmark that applies the task pipeline to create QA sets with text and icon\nrebus puzzles based on publicly available collections of compounds and common\nphrases. COLUMBUS comprises over 1,000 puzzles, each with four answer\ncandidates. While the SotA vision-language models (VLMs) achieve decent\nperformance, our evaluation demonstrates a substantial gap between humans and\nmodels. VLMs benefit from human-curated descriptions but struggle to\nself-generate such representations at the right level of abstraction.\n","authors":["Koen Kraaijveld","Yifan Jiang","Kaixin Ma","Filip Ilievski"],"pdf_url":"https://arxiv.org/pdf/2409.04053v2.pdf","comment":"15 pages, 10 figures, accepted to AAAI-25"},{"id":"http://arxiv.org/abs/2411.13093v3","updated":"2024-12-20T12:09:50Z","published":"2024-11-20T07:44:34Z","title":"Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension","summary":"  Existing large video-language models (LVLMs) struggle to comprehend long\nvideos correctly due to limited context. To address this problem, fine-tuning\nlong-context LVLMs and employing GPT-based agents have emerged as promising\nsolutions. However, fine-tuning LVLMs would require extensive high-quality data\nand substantial GPU resources, while GPT-based agents would rely on proprietary\nmodels (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented\nGeneration (Video-RAG), a training-free and cost-effective pipeline that\nemploys visually-aligned auxiliary texts to help facilitate cross-modality\nalignment while providing additional information beyond the visual content.\nSpecifically, we leverage open-source external tools to extract\nvisually-aligned information from pure video data (e.g., audio, optical\ncharacter, and object detection), and incorporate the extracted information\ninto an existing LVLM as auxiliary texts, alongside video frames and queries,\nin a plug-and-play manner. Our Video-RAG offers several key advantages: (i)\nlightweight with low computing overhead due to single-turn retrieval; (ii) easy\nimplementation and compatibility with any LVLM; and (iii) significant,\nconsistent performance gains across long video understanding benchmarks,\nincluding Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates\nsuperior performance over proprietary models like Gemini-1.5-Pro and GPT-4o\nwhen utilized with a 72B model.\n","authors":["Yongdong Luo","Xiawu Zheng","Xiao Yang","Guilin Li","Haojia Lin","Jinfa Huang","Jiayi Ji","Fei Chao","Jiebo Luo","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2411.13093v3.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.15819v1","updated":"2024-12-20T12:01:01Z","published":"2024-12-20T12:01:01Z","title":"Robustness-enhanced Myoelectric Control with GAN-based Open-set\n  Recognition","summary":"  Electromyography (EMG) signals are widely used in human motion recognition\nand medical rehabilitation, yet their variability and susceptibility to noise\nsignificantly limit the reliability of myoelectric control systems. Existing\nrecognition algorithms often fail to handle unfamiliar actions effectively,\nleading to system instability and errors. This paper proposes a novel framework\nbased on Generative Adversarial Networks (GANs) to enhance the robustness and\nusability of myoelectric control systems by enabling open-set recognition. The\nmethod incorporates a GAN-based discriminator to identify and reject unknown\nactions, maintaining system stability by preventing misclassifications.\nExperimental evaluations on publicly available and self-collected datasets\ndemonstrate a recognition accuracy of 97.6\\% for known actions and a 23.6\\%\nimprovement in Active Error Rate (AER) after rejecting unknown actions. The\nproposed approach is computationally efficient and suitable for deployment on\nedge devices, making it practical for real-world applications.\n","authors":["Cheng Wang","Ziyang Feng","Pin Zhang","Manjiang Cao","Yiming Yuan","Tengfei Chang"],"pdf_url":"https://arxiv.org/pdf/2412.15819v1.pdf","comment":"11 pages, 14 figures"},{"id":"http://arxiv.org/abs/2409.09649v2","updated":"2024-12-20T12:00:48Z","published":"2024-09-15T07:46:18Z","title":"SparX: A Sparse Cross-Layer Connection Mechanism for Hierarchical Vision\n  Mamba and Transformer Networks","summary":"  Due to the capability of dynamic state space models (SSMs) in capturing\nlong-range dependencies with linear-time computational complexity, Mamba has\nshown notable performance in NLP tasks. This has inspired the rapid development\nof Mamba-based vision models, resulting in promising results in visual\nrecognition tasks. However, such models are not capable of distilling features\nacross layers through feature aggregation, interaction, and selection.\nMoreover, existing cross-layer feature aggregation methods designed for CNNs or\nViTs are not practical in Mamba-based models due to high computational costs.\nTherefore, this paper aims to introduce an efficient cross-layer feature\naggregation mechanism for vision backbone networks. Inspired by the Retinal\nGanglion Cells (RGCs) in the human visual system, we propose a new sparse\ncross-layer connection mechanism termed SparX to effectively improve\ncross-layer feature interaction and reuse. Specifically, we build two different\ntypes of network layers: ganglion layers and normal layers. The former has\nhigher connectivity and complexity, enabling multi-layer feature aggregation\nand interaction in an input-dependent manner. In contrast, the latter has lower\nconnectivity and complexity. By interleaving these two types of layers, we\ndesign a new family of vision backbone networks with sparsely cross-connected\nlayers, achieving an excellent trade-off among model size, computational cost,\nmemory cost, and accuracy in comparison to its counterparts. For instance, with\nfewer parameters, SparX-Mamba-T improves the top-1 accuracy of VMamba-T from\n82.5\\% to 83.5\\%, while SparX-Swin-T achieves a 1.3\\% increase in top-1\naccuracy compared to Swin-T. Extensive experimental results demonstrate that\nour new connection mechanism possesses both superior performance and\ngeneralization capabilities on various vision tasks.\n","authors":["Meng Lou","Yunxiang Fu","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2409.09649v2.pdf","comment":"Accepted by AAAI 2025. Code is available at\n  https://github.com/LMMMEng/SparX"},{"id":"http://arxiv.org/abs/2412.15818v1","updated":"2024-12-20T11:59:34Z","published":"2024-12-20T11:59:34Z","title":"Precision ICU Resource Planning: A Multimodal Model for Brain Surgery\n  Outcomes","summary":"  Although advances in brain surgery techniques have led to fewer postoperative\ncomplications requiring Intensive Care Unit (ICU) monitoring, the routine\ntransfer of patients to the ICU remains the clinical standard, despite its high\ncost. Predictive Gradient Boosted Trees based on clinical data have attempted\nto optimize ICU admission by identifying key risk factors pre-operatively;\nhowever, these approaches overlook valuable imaging data that could enhance\nprediction accuracy. In this work, we show that multimodal approaches that\ncombine clinical data with imaging data outperform the current clinical data\nonly baseline from 0.29 [F1] to 0.30 [F1], when only pre-operative clinical\ndata is used and from 0.37 [F1] to 0.41 [F1], for pre- and post-operative data.\nThis study demonstrates that effective ICU admission prediction benefits from\nmultimodal data fusion, especially in contexts of severe class imbalance.\n","authors":["Maximilian Fischer","Florian M. Hauptmann","Robin Peretzke","Paul Naser","Peter Neher","Jan-Oliver Neumann","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2412.15818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.02507v3","updated":"2024-12-20T11:57:09Z","published":"2022-05-05T08:33:13Z","title":"Explicit View-labels Matter: A Multifacet Complementarity Study of\n  Multi-view Clustering","summary":"  Consistency and complementarity are two key ingredients for boosting\nmulti-view clustering (MVC). Recently with the introduction of popular\ncontrastive learning, the consistency learning of views has been further\nenhanced in MVC, leading to promising performance. However, by contrast, the\ncomplementarity has not received sufficient attention except just in the\nfeature facet, where the Hilbert Schmidt Independence Criterion term or the\nindependent encoder-decoder network is usually adopted to capture view-specific\ninformation. This motivates us to reconsider the complementarity learning of\nviews comprehensively from multiple facets including the feature-, view-label-\nand contrast- facets, while maintaining the view consistency. We empirically\nfind that all the facets contribute to the complementarity learning, especially\nthe view-label facet, which is usually neglected by existing methods. Based on\nthis, a simple yet effective \\underline{M}ultifacet \\underline{C}omplementarity\nlearning framework for \\underline{M}ulti-\\underline{V}iew\n\\underline{C}lustering (MCMVC) is naturally developed, which fuses multifacet\ncomplementarity information, especially explicitly embedding the view-label\ninformation. To our best knowledge, it is the first time to use view-labels\nexplicitly to guide the complementarity learning of views. Compared with the\nSOTA baselines, MCMVC achieves remarkable improvements, e.g., by average\nmargins over $5.00\\%$ and $7.00\\%$ respectively in complete and incomplete MVC\nsettings on Caltech101-20 in terms of three evaluation metrics.\n","authors":["Chuanxing Geng","Aiyang Han","Songcan Chen"],"pdf_url":"https://arxiv.org/pdf/2205.02507v3.pdf","comment":"The first two authors contributed equally to this work. Accepted by\n  IEEE TPAMI2024"},{"id":"http://arxiv.org/abs/2412.15813v1","updated":"2024-12-20T11:42:41Z","published":"2024-12-20T11:42:41Z","title":"Cross-Modal Few-Shot Learning with Second-Order Neural Ordinary\n  Differential Equations","summary":"  We introduce SONO, a novel method leveraging Second-Order Neural Ordinary\nDifferential Equations (Second-Order NODEs) to enhance cross-modal few-shot\nlearning. By employing a simple yet effective architecture consisting of a\nSecond-Order NODEs model paired with a cross-modal classifier, SONO addresses\nthe significant challenge of overfitting, which is common in few-shot scenarios\ndue to limited training examples. Our second-order approach can approximate a\nbroader class of functions, enhancing the model's expressive power and feature\ngeneralization capabilities. We initialize our cross-modal classifier with text\nembeddings derived from class-relevant prompts, streamlining training\nefficiency by avoiding the need for frequent text encoder processing.\nAdditionally, we utilize text-based image augmentation, exploiting CLIP's\nrobust image-text correlation to enrich training data significantly. Extensive\nexperiments across multiple datasets demonstrate that SONO outperforms existing\nstate-of-the-art methods in few-shot learning performance.\n","authors":["Yi Zhang","Chun-Wun Cheng","Junyi He","Zhihai He","Carola-Bibiane Schönlieb","Yuyan Chen","Angelica I Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2412.15813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10783v2","updated":"2024-12-20T11:39:59Z","published":"2024-12-14T10:39:55Z","title":"Video Diffusion Transformers are In-Context Learners","summary":"  This paper investigates a solution for enabling in-context capabilities of\nvideo diffusion transformers, with minimal tuning required for activation.\nSpecifically, we propose a simple pipeline to leverage in-context generation:\n($\\textbf{i}$) concatenate videos along spacial or time dimension,\n($\\textbf{ii}$) jointly caption multi-scene video clips from one source, and\n($\\textbf{iii}$) apply task-specific fine-tuning using carefully curated small\ndatasets. Through a series of diverse controllable tasks, we demonstrate\nqualitatively that existing advanced text-to-video models can effectively\nperform in-context generation. Notably, it allows for the creation of\nconsistent multi-scene videos exceeding 30 seconds in duration, without\nadditional computational overhead. Importantly, this method requires no\nmodifications to the original models, results in high-fidelity video outputs\nthat better align with prompt specifications and maintain role consistency. Our\nframework presents a valuable tool for the research community and offers\ncritical insights for advancing product-level controllable video generation\nsystems. The data, code, and model weights are publicly available at:\n\\url{https://github.com/feizc/Video-In-Context}.\n","authors":["Zhengcong Fei","Di Qiu","Changqian Yu","Debang Li","Mingyuan Fan","Xiang Wen"],"pdf_url":"https://arxiv.org/pdf/2412.10783v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00587v2","updated":"2024-12-20T11:22:01Z","published":"2024-09-01T02:43:33Z","title":"FLUX that Plays Music","summary":"  This paper explores a simple extension of diffusion-based rectified flow\nTransformers for text-to-music generation, termed as FluxMusic. Generally,\nalong with design in advanced\nFlux\\footnote{https://github.com/black-forest-labs/flux} model, we transfers it\ninto a latent VAE space of mel-spectrum. It involves first applying a sequence\nof independent attention to the double text-music stream, followed by a stacked\nsingle music stream for denoised patch prediction. We employ multiple\npre-trained text encoders to sufficiently capture caption semantic information\nas well as inference flexibility. In between, coarse textual information, in\nconjunction with time step embeddings, is utilized in a modulation mechanism,\nwhile fine-grained textual details are concatenated with the music patch\nsequence as inputs. Through an in-depth study, we demonstrate that rectified\nflow training with an optimized architecture significantly outperforms\nestablished diffusion methods for the text-to-music task, as evidenced by\nvarious automatic metrics and human preference evaluations. Our experimental\ndata, code, and model weights are made publicly available at:\n\\url{https://github.com/feizc/FluxMusic}.\n","authors":["Zhengcong Fei","Mingyuan Fan","Changqian Yu","Junshi Huang"],"pdf_url":"https://arxiv.org/pdf/2409.00587v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15798v1","updated":"2024-12-20T11:15:31Z","published":"2024-12-20T11:15:31Z","title":"Diffusion-Based Conditional Image Editing through Optimized Inference\n  with Guidance","summary":"  We present a simple but effective training-free approach for text-driven\nimage-to-image translation based on a pretrained text-to-image diffusion model.\nOur goal is to generate an image that aligns with the target task while\npreserving the structure and background of a source image. To this end, we\nderive the representation guidance with a combination of two objectives:\nmaximizing the similarity to the target prompt based on the CLIP score and\nminimizing the structural distance to the source latent variable. This guidance\nimproves the fidelity of the generated target image to the given target prompt\nwhile maintaining the structure integrity of the source image. To incorporate\nthe representation guidance component, we optimize the target latent variable\nof diffusion model's reverse process with the guidance. Experimental results\ndemonstrate that our method achieves outstanding image-to-image translation\nperformance on various tasks when combined with the pretrained Stable Diffusion\nmodel.\n","authors":["Hyunsoo Lee","Minsoo Kang","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2412.15798v1.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2212.02014v3","updated":"2024-12-20T10:21:14Z","published":"2022-12-05T04:04:21Z","title":"Med-Query: Steerable Parsing of 9-DoF Medical Anatomies with Query\n  Embedding","summary":"  Automatic parsing of human anatomies at the instance-level from 3D computed\ntomography (CT) is a prerequisite step for many clinical applications. The\npresence of pathologies, broken structures or limited field-of-view (FOV) can\nall make anatomy parsing algorithms vulnerable. In this work, we explore how to\nleverage and implement the successful detection-then-segmentation paradigm for\n3D medical data, and propose a steerable, robust, and efficient computing\nframework for detection, identification, and segmentation of anatomies in CT\nscans. Considering the complicated shapes, sizes, and orientations of\nanatomies, without loss of generality, we present a nine degrees of freedom\n(9-DoF) pose estimation solution in full 3D space using a novel single-stage,\nnon-hierarchical representation. Our whole framework is executed in a steerable\nmanner where any anatomy of interest can be directly retrieved to further boost\ninference efficiency. We have validated our method on three medical imaging\nparsing tasks: ribs, spine, and abdominal organs. For rib parsing, CT scans\nhave been annotated at the rib instance-level for quantitative evaluation,\nsimilarly for spine vertebrae and abdominal organs. Extensive experiments on\n9-DoF box detection and rib instance segmentation demonstrate the high\nefficiency and effectiveness of our framework (with the identification rate of\n97.0% and the segmentation Dice score of 90.9%), compared favorably against\nseveral strong baselines (e.g., CenterNet, FCOS, and nnU-Net). For spine\nparsing and abdominal multi-organ segmentation, our method achieves competitive\nresults on par with state-of-the-art methods on the public CTSpine1K dataset\nand FLARE22 competition, respectively. Our annotations, code, and models are\navailable at: https://github.com/alibaba-damo-academy/Med_Query.\n","authors":["Heng Guo","Jianfeng Zhang","Ke Yan","Le Lu","Minfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2212.02014v3.pdf","comment":"Accepted by IEEE Journal of Biomedical and Health Informatics"},{"id":"http://arxiv.org/abs/2412.15752v1","updated":"2024-12-20T10:14:12Z","published":"2024-12-20T10:14:12Z","title":"Sparse Point Clouds Assisted Learned Image Compression","summary":"  In the field of autonomous driving, a variety of sensor data types exist,\neach representing different modalities of the same scene. Therefore, it is\nfeasible to utilize data from other sensors to facilitate image compression.\nHowever, few techniques have explored the potential benefits of utilizing\ninter-modality correlations to enhance the image compression performance. In\nthis paper, motivated by the recent success of learned image compression, we\npropose a new framework that uses sparse point clouds to assist in learned\nimage compression in the autonomous driving scenario. We first project the 3D\nsparse point cloud onto a 2D plane, resulting in a sparse depth map. Utilizing\nthis depth map, we proceed to predict camera images. Subsequently, we use these\npredicted images to extract multi-scale structural features. These features are\nthen incorporated into learned image compression pipeline as additional\ninformation to improve the compression performance. Our proposed framework is\ncompatible with various mainstream learned image compression models, and we\nvalidate our approach using different existing image compression methods. The\nexperimental results show that incorporating point cloud assistance into the\ncompression pipeline consistently enhances the performance.\n","authors":["Yiheng Jiang","Haotian Zhang","Li Li","Dong Liu","Zhu Li"],"pdf_url":"https://arxiv.org/pdf/2412.15752v1.pdf","comment":"Accepted by TCSVT"},{"id":"http://arxiv.org/abs/2412.14939v2","updated":"2024-12-20T10:02:01Z","published":"2024-12-19T15:15:03Z","title":"GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface\n  Reconstruction","summary":"  Neural surface representation has demonstrated remarkable success in the\nareas of novel view synthesis and 3D reconstruction. However, assessing the\ngeometric quality of 3D reconstructions in the absence of ground truth mesh\nremains a significant challenge, due to its rendering-based optimization\nprocess and entangled learning of appearance and geometry with photometric\nlosses. In this paper, we present a novel framework, i.e, GURecon, which\nestablishes a geometric uncertainty field for the neural surface based on\ngeometric consistency. Different from existing methods that rely on\nrendering-based measurement, GURecon models a continuous 3D uncertainty field\nfor the reconstructed surface, and is learned by an online distillation\napproach without introducing real geometric information for supervision.\nMoreover, in order to mitigate the interference of illumination on geometric\nconsistency, a decoupled field is learned and exploited to finetune the\nuncertainty field. Experiments on various datasets demonstrate the superiority\nof GURecon in modeling 3D geometric uncertainty, as well as its plug-and-play\nextension to various neural surface representations and improvement on\ndownstream tasks such as incremental reconstruction. The code and supplementary\nmaterial are available on the project website:\nhttps://zju3dv.github.io/GURecon/.\n","authors":["Zesong Yang","Ru Zhang","Jiale Shi","Zixiang Ai","Boming Zhao","Hujun Bao","Luwei Yang","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2412.14939v2.pdf","comment":"Accepted by AAAI 2025. Project page:\n  https://zju3dv.github.io/GURecon/"},{"id":"http://arxiv.org/abs/2412.15740v1","updated":"2024-12-20T10:00:36Z","published":"2024-12-20T10:00:36Z","title":"From Model Based to Learned Regularization in Medical Image\n  Registration: A Comprehensive Review","summary":"  Image registration is fundamental in medical imaging applications, such as\ndisease progression analysis or radiation therapy planning. The primary\nobjective of image registration is to precisely capture the deformation between\ntwo or more images, typically achieved by minimizing an optimization problem.\nDue to its inherent ill-posedness, regularization is a key component in driving\nthe solution toward anatomically meaningful deformations. A wide range of\nregularization methods has been proposed for both conventional and deep\nlearning-based registration. However, the appropriate application of\nregularization techniques often depends on the specific registration problem,\nand no one-fits-all method exists. Despite its importance, regularization is\noften overlooked or addressed with default approaches, assuming existing\nmethods are sufficient. A comprehensive and structured review remains missing.\nThis review addresses this gap by introducing a novel taxonomy that\nsystematically categorizes the diverse range of proposed regularization\nmethods. It highlights the emerging field of learned regularization, which\nleverages data-driven techniques to automatically derive deformation properties\nfrom the data. Moreover, this review examines the transfer of regularization\nmethods from conventional to learning-based registration, identifies open\nchallenges, and outlines future research directions. By emphasizing the\ncritical role of regularization in image registration, we hope to inspire the\nresearch community to reconsider regularization strategies in modern\nregistration algorithms and to explore this rapidly evolving field further.\n","authors":["Anna Reithmeir","Veronika Spieker","Vasiliki Sideri-Lampretsa","Daniel Rueckert","Julia A. Schnabel","Veronika A. Zimmer"],"pdf_url":"https://arxiv.org/pdf/2412.15740v1.pdf","comment":"Submitted to Medical Image Analysis"},{"id":"http://arxiv.org/abs/2412.15739v1","updated":"2024-12-20T10:00:26Z","published":"2024-12-20T10:00:26Z","title":"VORD: Visual Ordinal Calibration for Mitigating Object Hallucinations in\n  Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) have made remarkable developments along\nwith the recent surge of large language models. Despite their advancements,\nLVLMs have a tendency to generate plausible yet inaccurate or inconsistent\ninformation based on the provided source content. This phenomenon, also known\nas ``hallucinations\" can have serious downstream implications during the\ndeployment of LVLMs. To address this, we present VORD a simple and effective\nmethod that alleviates hallucinations by calibrating token predictions based on\nordinal relationships between modified image pairs. VORD is presented in two\nforms: 1.) a minimalist training-free variant which eliminates implausible\ntokens from modified image pairs, and 2.) a trainable objective function that\npenalizes unlikely tokens. Our experiments demonstrate that VORD delivers\nbetter calibration and effectively mitigates object hallucinations on a\nwide-range of LVLM benchmarks.\n","authors":["Dexter Neo","Tsuhan Chen"],"pdf_url":"https://arxiv.org/pdf/2412.15739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11237v2","updated":"2024-12-20T09:56:35Z","published":"2024-03-17T14:52:05Z","title":"FORCE: Physics-aware Human-object Interaction","summary":"  Interactions between human and objects are influenced not only by the\nobject's pose and shape, but also by physical attributes such as object mass\nand surface friction. They introduce important motion nuances that are\nessential for diversity and realism. Despite advancements in recent\nhuman-object interaction methods, this aspect has been overlooked. Generating\nnuanced human motion presents two challenges. First, it is non-trivial to learn\nfrom multi-modal human and object information derived from both the physical\nand non-physical attributes. Second, there exists no dataset capturing nuanced\nhuman interactions with objects of varying physical properties, hampering model\ndevelopment. This work addresses the gap by introducing the FORCE model, an\napproach for synthesizing diverse, nuanced human-object interactions by\nmodeling physical attributes. Our key insight is that human motion is dictated\nby the interrelation between the force exerted by the human and the perceived\nresistance. Guided by a novel intuitive physics encoding, the model captures\nthe interplay between human force and resistance. Experiments also demonstrate\nincorporating human force facilitates learning multi-class motion. Accompanying\nour model, we contribute a dataset, which features diverse, different-styled\nmotion through interactions with varying resistances.\n","authors":["Xiaohan Zhang","Bharat Lal Bhatnagar","Sebastian Starke","Ilya Petrov","Vladimir Guzov","Helisa Dhamo","Eduardo Pérez-Pellitero","Gerard Pons-Moll"],"pdf_url":"https://arxiv.org/pdf/2403.11237v2.pdf","comment":"24 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.15734v1","updated":"2024-12-20T09:55:37Z","published":"2024-12-20T09:55:37Z","title":"The Role of Recurrency in Image Segmentation for Noisy and Limited\n  Sample Settings","summary":"  The biological brain has inspired multiple advances in machine learning.\nHowever, most state-of-the-art models in computer vision do not operate like\nthe human brain, simply because they are not capable of changing or improving\ntheir decisions/outputs based on a deeper analysis. The brain is recurrent,\nwhile these models are not. It is therefore relevant to explore what would be\nthe impact of adding recurrent mechanisms to existing state-of-the-art\narchitectures and to answer the question of whether recurrency can improve\nexisting architectures. To this end, we build on a feed-forward segmentation\nmodel and explore multiple types of recurrency for image segmentation. We\nexplore self-organizing, relational, and memory retrieval types of recurrency\nthat minimize a specific energy function. In our experiments, we tested these\nmodels on artificial and medical imaging data, while analyzing the impact of\nhigh levels of noise and few-shot learning settings. Our results do not\nvalidate our initial hypothesis that recurrent models should perform better in\nthese settings, suggesting that these recurrent architectures, by themselves,\nare not sufficient to surpass state-of-the-art feed-forward versions and that\nadditional work needs to be done on the topic.\n","authors":["David Calhas","João Marques","Arlindo L. Oliveira"],"pdf_url":"https://arxiv.org/pdf/2412.15734v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2411.15723v3","updated":"2024-12-20T09:36:36Z","published":"2024-11-24T05:55:19Z","title":"GSurf: 3D Reconstruction via Signed Distance Fields with Direct Gaussian\n  Supervision","summary":"  Surface reconstruction from multi-view images is a core challenge in 3D\nvision. Recent studies have explored signed distance fields (SDF) within Neural\nRadiance Fields (NeRF) to achieve high-fidelity surface reconstructions.\nHowever, these approaches often suffer from slow training and rendering speeds\ncompared to 3D Gaussian splatting (3DGS). Current state-of-the-art techniques\nattempt to fuse depth information to extract geometry from 3DGS, but frequently\nresult in incomplete reconstructions and fragmented surfaces. In this paper, we\nintroduce GSurf, a novel end-to-end method for learning a signed distance field\ndirectly from Gaussian primitives. The continuous and smooth nature of SDF\naddresses common issues in the 3DGS family, such as holes resulting from noisy\nor missing depth data. By using Gaussian splatting for rendering, GSurf avoids\nthe redundant volume rendering typically required in other GS and SDF\nintegrations. Consequently, GSurf achieves faster training and rendering speeds\nwhile delivering 3D reconstruction quality comparable to neural implicit\nsurface methods, such as VolSDF and NeuS. Experimental results across various\nbenchmark datasets demonstrate the effectiveness of our method in producing\nhigh-fidelity 3D reconstructions.\n","authors":["Baixin Xu","Jiangbei Hu","Jiaze Li","Ying He"],"pdf_url":"https://arxiv.org/pdf/2411.15723v3.pdf","comment":"see https://github.com/xubaixinxbx/Gsurf"},{"id":"http://arxiv.org/abs/2412.15691v1","updated":"2024-12-20T09:10:17Z","published":"2024-12-20T09:10:17Z","title":"Exploiting Multimodal Spatial-temporal Patterns for Video Object\n  Tracking","summary":"  Multimodal tracking has garnered widespread attention as a result of its\nability to effectively address the inherent limitations of traditional RGB\ntracking. However, existing multimodal trackers mainly focus on the fusion and\nenhancement of spatial features or merely leverage the sparse temporal\nrelationships between video frames. These approaches do not fully exploit the\ntemporal correlations in multimodal videos, making it difficult to capture the\ndynamic changes and motion information of targets in complex scenarios. To\nalleviate this problem, we propose a unified multimodal spatial-temporal\ntracking approach named STTrack. In contrast to previous paradigms that solely\nrelied on updating reference information, we introduced a temporal state\ngenerator (TSG) that continuously generates a sequence of tokens containing\nmultimodal temporal information. These temporal information tokens are used to\nguide the localization of the target in the next time state, establish\nlong-range contextual relationships between video frames, and capture the\ntemporal trajectory of the target. Furthermore, at the spatial level, we\nintroduced the mamba fusion and background suppression interactive (BSI)\nmodules. These modules establish a dual-stage mechanism for coordinating\ninformation interaction and fusion between modalities. Extensive comparisons on\nfive benchmark datasets illustrate that STTrack achieves state-of-the-art\nperformance across various multimodal tracking scenarios. Code is available at:\nhttps://github.com/NJU-PCALab/STTrack.\n","authors":["Xiantao Hu","Ying Tai","Xu Zhao","Chen Zhao","Zhenyu Zhang","Jun Li","Bineng Zhong","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2412.15691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18723v3","updated":"2024-12-20T09:08:28Z","published":"2024-10-24T13:28:40Z","title":"VoxelKeypointFusion: Generalizable Multi-View Multi-Person Pose\n  Estimation","summary":"  In the rapidly evolving field of computer vision, the task of accurately\nestimating the poses of multiple individuals from various viewpoints presents a\nformidable challenge, especially if the estimations should be reliable as well.\nThis work presents an extensive evaluation of the generalization capabilities\nof multi-view multi-person pose estimators to unseen datasets and presents a\nnew algorithm with strong performance in this task. It also studies the\nimprovements by additionally using depth information. Since the new approach\ncan not only generalize well to unseen datasets, but also to different\nkeypoints, the first multi-view multi-person whole-body estimator is presented.\nTo support further research on those topics, all of the work is publicly\naccessible.\n","authors":["Daniel Bermuth","Alexander Poeppel","Wolfgang Reif"],"pdf_url":"https://arxiv.org/pdf/2410.18723v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15689v1","updated":"2024-12-20T09:07:36Z","published":"2024-12-20T09:07:36Z","title":"DOLLAR: Few-Step Video Generation via Distillation and Latent Reward\n  Optimization","summary":"  Diffusion probabilistic models have shown significant progress in video\ngeneration; however, their computational efficiency is limited by the large\nnumber of sampling steps required. Reducing sampling steps often compromises\nvideo quality or generation diversity. In this work, we introduce a\ndistillation method that combines variational score distillation and\nconsistency distillation to achieve few-step video generation, maintaining both\nhigh quality and diversity. We also propose a latent reward model fine-tuning\napproach to further enhance video generation performance according to any\nspecified reward metric. This approach reduces memory usage and does not\nrequire the reward to be differentiable. Our method demonstrates\nstate-of-the-art performance in few-step generation for 10-second videos (128\nframes at 12 FPS). The distilled student model achieves a score of 82.57 on\nVBench, surpassing the teacher model as well as baseline models Gen-3,\nT2V-Turbo, and Kling. One-step distillation accelerates the teacher model's\ndiffusion sampling by up to 278.6 times, enabling near real-time generation.\nHuman evaluations further validate the superior performance of our 4-step\nstudent models compared to teacher model using 50-step DDIM sampling.\n","authors":["Zihan Ding","Chi Jin","Difan Liu","Haitian Zheng","Krishna Kumar Singh","Qiang Zhang","Yan Kang","Zhe Lin","Yuchen Liu"],"pdf_url":"https://arxiv.org/pdf/2412.15689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15678v1","updated":"2024-12-20T08:50:11Z","published":"2024-12-20T08:50:11Z","title":"Multi-Pair Temporal Sentence Grounding via Multi-Thread Knowledge\n  Transfer Network","summary":"  Given some video-query pairs with untrimmed videos and sentence queries,\ntemporal sentence grounding (TSG) aims to locate query-relevant segments in\nthese videos. Although previous respectable TSG methods have achieved\nremarkable success, they train each video-query pair separately and ignore the\nrelationship between different pairs. We observe that the similar video/query\ncontent not only helps the TSG model better understand and generalize the\ncross-modal representation but also assists the model in locating some complex\nvideo-query pairs. Previous methods follow a single-thread framework that\ncannot co-train different pairs and usually spends much time re-obtaining\nredundant knowledge, limiting their real-world applications. To this end, in\nthis paper, we pose a brand-new setting: Multi-Pair TSG, which aims to co-train\nthese pairs. In particular, we propose a novel video-query co-training\napproach, Multi-Thread Knowledge Transfer Network, to locate a variety of\nvideo-query pairs effectively and efficiently. Firstly, we mine the spatial and\ntemporal semantics across different queries to cooperate with each other. To\nlearn intra- and inter-modal representations simultaneously, we design a\ncross-modal contrast module to explore the semantic consistency by a\nself-supervised strategy. To fully align visual and textual representations\nbetween different pairs, we design a prototype alignment strategy to 1) match\nobject prototypes and phrase prototypes for spatial alignment, and 2) align\nactivity prototypes and sentence prototypes for temporal alignment. Finally, we\ndevelop an adaptive negative selection module to adaptively generate a\nthreshold for cross-modal matching. Extensive experiments show the\neffectiveness and efficiency of our proposed method.\n","authors":["Xiang Fang","Wanlong Fang","Changshuo Wang","Daizong Liu","Keke Tang","Jianfeng Dong","Pan Zhou","Beibei Li"],"pdf_url":"https://arxiv.org/pdf/2412.15678v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.15677v1","updated":"2024-12-20T08:47:07Z","published":"2024-12-20T08:47:07Z","title":"AI-generated Image Quality Assessment in Visual Communication","summary":"  Assessing the quality of artificial intelligence-generated images (AIGIs)\nplays a crucial role in their application in real-world scenarios. However,\ntraditional image quality assessment (IQA) algorithms primarily focus on\nlow-level visual perception, while existing IQA works on AIGIs overemphasize\nthe generated content itself, neglecting its effectiveness in real-world\napplications. To bridge this gap, we propose AIGI-VC, a quality assessment\ndatabase for AI-Generated Images in Visual Communication, which studies the\ncommunicability of AIGIs in the advertising field from the perspectives of\ninformation clarity and emotional interaction. The dataset consists of 2,500\nimages spanning 14 advertisement topics and 8 emotion types. It provides\ncoarse-grained human preference annotations and fine-grained preference\ndescriptions, benchmarking the abilities of IQA methods in preference\nprediction, interpretation, and reasoning. We conduct an empirical study of\nexisting representative IQA methods and large multi-modal models on the AIGI-VC\ndataset, uncovering their strengths and weaknesses.\n","authors":["Yu Tian","Yixuan Li","Baoliang Chen","Hanwei Zhu","Shiqi Wang","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2412.15677v1.pdf","comment":"AAAI-2025; Project page: https://github.com/ytian73/AIGI-VC"},{"id":"http://arxiv.org/abs/2312.04793v2","updated":"2024-12-20T08:46:37Z","published":"2023-12-08T02:08:00Z","title":"User-Aware Prefix-Tuning is a Good Learner for Personalized Image\n  Captioning","summary":"  Image captioning bridges the gap between vision and language by automatically\ngenerating natural language descriptions for images. Traditional image\ncaptioning methods often overlook the preferences and characteristics of users.\nPersonalized image captioning solves this problem by incorporating user prior\nknowledge into the model, such as writing styles and preferred vocabularies.\nMost existing methods emphasize the user context fusion process by memory\nnetworks or transformers. However, these methods ignore the distinct domains of\neach dataset. Therefore, they need to update the entire caption model\nparameters when meeting new samples, which is time-consuming and\ncalculation-intensive. To address this challenge, we propose a novel\npersonalized image captioning framework that leverages user context to consider\npersonality factors. Additionally, our framework utilizes the prefix-tuning\nparadigm to extract knowledge from a frozen large language model, reducing the\ngap between different language domains. Specifically, we employ CLIP to extract\nthe visual features of an image and align the semantic space using a\nquery-guided mapping network. By incorporating the transformer layer, we merge\nthe visual features with the user's contextual prior knowledge to generate\ninformative prefixes. Moreover, we employ GPT-2 as the frozen large language\nmodel. With a small number of parameters to be trained, our model performs\nefficiently and effectively. Our model outperforms existing baseline models on\nInstagram and YFCC100M datasets across five evaluation metrics, demonstrating\nits superiority, including twofold improvements in metrics such as BLEU-4 and\nCIDEr.\n","authors":["Xuan Wang","Guanhong Wang","Wenhao Chai","Jiayu Zhou","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2312.04793v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15674v1","updated":"2024-12-20T08:41:25Z","published":"2024-12-20T08:41:25Z","title":"PersonaMagic: Stage-Regulated High-Fidelity Face Customization with\n  Tandem Equilibrium","summary":"  Personalized image generation has made significant strides in adapting\ncontent to novel concepts. However, a persistent challenge remains: balancing\nthe accurate reconstruction of unseen concepts with the need for editability\naccording to the prompt, especially when dealing with the complex nuances of\nfacial features. In this study, we delve into the temporal dynamics of the\ntext-to-image conditioning process, emphasizing the crucial role of stage\npartitioning in introducing new concepts. We present PersonaMagic, a\nstage-regulated generative technique designed for high-fidelity face\ncustomization. Using a simple MLP network, our method learns a series of\nembeddings within a specific timestep interval to capture face concepts.\nAdditionally, we develop a Tandem Equilibrium mechanism that adjusts\nself-attention responses in the text encoder, balancing text description and\nidentity preservation, improving both areas. Extensive experiments confirm the\nsuperiority of PersonaMagic over state-of-the-art methods in both qualitative\nand quantitative evaluations. Moreover, its robustness and flexibility are\nvalidated in non-facial domains, and it can also serve as a valuable plug-in\nfor enhancing the performance of pretrained personalization models.\n","authors":["Xinzhe Li","Jiahui Zhan","Shengfeng He","Yangyang Xu","Junyu Dong","Huaidong Zhang","Yong Du"],"pdf_url":"https://arxiv.org/pdf/2412.15674v1.pdf","comment":"This paper is accepted by AAAI 2025. The code is available at\n  https://github.com/xzhe-Vision/PersonaMagic"},{"id":"http://arxiv.org/abs/2412.15673v1","updated":"2024-12-20T08:38:26Z","published":"2024-12-20T08:38:26Z","title":"Learning Group Interactions and Semantic Intentions for Multi-Object\n  Trajectory Prediction","summary":"  Effective modeling of group interactions and dynamic semantic intentions is\ncrucial for forecasting behaviors like trajectories or movements. In complex\nscenarios like sports, agents' trajectories are influenced by group\ninteractions and intentions, including team strategies and opponent actions. To\nthis end, we propose a novel diffusion-based trajectory prediction framework\nthat integrates group-level interactions into a conditional diffusion model,\nenabling the generation of diverse trajectories aligned with specific group\nactivity. To capture dynamic semantic intentions, we frame group interaction\nprediction as a cooperative game, using Banzhaf interaction to model\ncooperation trends. We then fuse semantic intentions with enhanced agent\nembeddings, which are refined through both global and local aggregation.\nFurthermore, we expand the NBA SportVU dataset by adding human annotations of\nteam-level tactics for trajectory and tactic prediction tasks. Extensive\nexperiments on three widely-adopted datasets demonstrate that our model\noutperforms state-of-the-art methods. Our source code and data are available at\nhttps://github.com/aurora-xin/Group2Int-trajectory.\n","authors":["Mengshi Qi","Yuxin Yang","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2412.15673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15670v1","updated":"2024-12-20T08:36:17Z","published":"2024-12-20T08:36:17Z","title":"BS-LDM: Effective Bone Suppression in High-Resolution Chest X-Ray Images\n  with Conditional Latent Diffusion Models","summary":"  The interference of overlapping bones and pulmonary structures can reduce the\neffectiveness of Chest X-ray (CXR) examinations. Bone suppression techniques\nhave been developed to improve diagnostic accuracy. Dual-energy subtraction\n(DES) imaging, a common method for bone suppression, is costly and exposes\npatients to higher radiation levels. Deep learning-based image generation\nmethods have been proposed as alternatives, however, they often fail to produce\nhigh-quality and high-resolution images, resulting in the loss of critical\nlesion information and texture details. To address these issues, in this paper,\nwe introduce an end-to-end framework for bone suppression in high-resolution\nCXR images, termed BS-LDM. This framework employs a conditional latent\ndiffusion model to generate high-resolution soft tissue images with fine detail\nand critical lung pathology by performing bone suppression in the latent space.\nWe implement offset noise during the noise addition phase of the training\nprocess to better render low-frequency information in soft tissue images.\nAdditionally, we introduce a dynamic clipping strategy during the sampling\nprocess to refine pixel intensity in the generated soft tissue images. We\ncompiled a substantial and high-quality bone suppression dataset, SZCH-X-Rays,\nincluding high-resolution paired CXR and DES soft tissue images from 818\npatients, collected from our partner hospitals. Moreover, we pre-processed 241\npairs of CXR and DES soft tissue images from the JSRT dataset, the largest\npublicly available dataset. Comprehensive experimental and clinical evaluations\ndemonstrate that BS-LDM exhibits superior bone suppression capabilities,\nhighlighting its significant clinical potential.\n","authors":["Yifei Sun","Zhanghao Chen","Hao Zheng","Ruiquan Ge","Jin Liu","Wenwen Min","Ahmed Elazab","Xiang Wan","Changmiao Wang"],"pdf_url":"https://arxiv.org/pdf/2412.15670v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.15668v1","updated":"2024-12-20T08:32:02Z","published":"2024-12-20T08:32:02Z","title":"Adaptive Hierarchical Graph Cut for Multi-granularity\n  Out-of-distribution Detection","summary":"  This paper focuses on a significant yet challenging task: out-of-distribution\ndetection (OOD detection), which aims to distinguish and reject test samples\nwith semantic shifts, so as to prevent models trained on in-distribution (ID)\ndata from producing unreliable predictions. Although previous works have made\ndecent success, they are ineffective for real-world challenging applications\nsince these methods simply regard all unlabeled data as OOD data and ignore the\ncase that different datasets have different label granularity. For example,\n\"cat\" on CIFAR-10 and \"tabby cat\" on Tiny-ImageNet share the same semantics but\nhave different labels due to various label granularity. To this end, in this\npaper, we propose a novel Adaptive Hierarchical Graph Cut network (AHGC) to\ndeeply explore the semantic relationship between different images.\nSpecifically, we construct a hierarchical KNN graph to evaluate the\nsimilarities between different images based on the cosine similarity. Based on\nthe linkage and density information of the graph, we cut the graph into\nmultiple subgraphs to integrate these semantics-similar samples. If the labeled\npercentage in a subgraph is larger than a threshold, we will assign the label\nwith the highest percentage to unlabeled images. To further improve the model\ngeneralization, we augment each image into two augmentation versions, and\nmaximize the similarity between the two versions. Finally, we leverage the\nsimilarity score for OOD detection. Extensive experiments on two challenging\nbenchmarks (CIFAR- 10 and CIFAR-100) illustrate that in representative cases,\nAHGC outperforms state-of-the-art OOD detection methods by 81.24% on CIFAR-100\nand by 40.47% on CIFAR-10 in terms of \"FPR95\", which shows the effectiveness of\nour AHGC.\n","authors":["Xiang Fang","Arvind Easwaran","Blaise Genest","Ponnuthurai Nagaratnam Suganthan"],"pdf_url":"https://arxiv.org/pdf/2412.15668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15664v1","updated":"2024-12-20T08:25:15Z","published":"2024-12-20T08:25:15Z","title":"SCENIC: Scene-aware Semantic Navigation with Instruction-guided Control","summary":"  Synthesizing natural human motion that adapts to complex environments while\nallowing creative control remains a fundamental challenge in motion synthesis.\nExisting models often fall short, either by assuming flat terrain or lacking\nthe ability to control motion semantics through text. To address these\nlimitations, we introduce SCENIC, a diffusion model designed to generate human\nmotion that adapts to dynamic terrains within virtual scenes while enabling\nsemantic control through natural language. The key technical challenge lies in\nsimultaneously reasoning about complex scene geometry while maintaining text\ncontrol. This requires understanding both high-level navigation goals and\nfine-grained environmental constraints. The model must ensure physical\nplausibility and precise navigation across varied terrain, while also\npreserving user-specified text control, such as ``carefully stepping over\nobstacles\" or ``walking upstairs like a zombie.\" Our solution introduces a\nhierarchical scene reasoning approach. At its core is a novel scene-dependent,\ngoal-centric canonicalization that handles high-level goal constraint, and is\ncomplemented by an ego-centric distance field that captures local geometric\ndetails. This dual representation enables our model to generate physically\nplausible motion across diverse 3D scenes. By implementing frame-wise text\nalignment, our system achieves seamless transitions between different motion\nstyles while maintaining scene constraints. Experiments demonstrate our novel\ndiffusion model generates arbitrarily long human motions that both adapt to\ncomplex scenes with varying terrain surfaces and respond to textual prompts.\nAdditionally, we show SCENIC can generalize to four real-scene datasets. Our\ncode, dataset, and models will be released at\n\\url{https://virtualhumans.mpi-inf.mpg.de/scenic/}.\n","authors":["Xiaohan Zhang","Sebastian Starke","Vladimir Guzov","Zhensong Zhang","Eduardo Pérez Pellitero","Gerard Pons-Moll"],"pdf_url":"https://arxiv.org/pdf/2412.15664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15304v2","updated":"2024-12-20T08:23:46Z","published":"2024-05-24T07:47:36Z","title":"Unlearning Concepts in Diffusion Model via Concept Domain Correction and\n  Concept Preserving Gradient","summary":"  Text-to-image diffusion models have achieved remarkable success in generating\nphotorealistic images. However, the inclusion of sensitive information during\npre-training poses significant risks. Machine Unlearning (MU) offers a\npromising solution to eliminate sensitive concepts from these models. Despite\nits potential, existing MU methods face two main challenges: 1) limited\ngeneralization, where concept erasure is effective only within the unlearned\nset, failing to prevent sensitive concept generation from out-of-set prompts;\nand 2) utility degradation, where removing target concepts significantly\nimpacts the model's overall performance. To address these issues, we propose a\nnovel concept domain correction framework named \\textbf{DoCo} (\\textbf{Do}main\n\\textbf{Co}rrection). By aligning the output domains of sensitive and anchor\nconcepts through adversarial training, our approach ensures comprehensive\nunlearning of target concepts. Additionally, we introduce a concept-preserving\ngradient surgery technique that mitigates conflicting gradient components,\nthereby preserving the model's utility while unlearning specific concepts.\nExtensive experiments across various instances, styles, and offensive concepts\ndemonstrate the effectiveness of our method in unlearning targeted concepts\nwith minimal impact on related concepts, outperforming previous approaches even\nfor out-of-distribution prompts.\n","authors":["Yongliang Wu","Shiji Zhou","Mingzhuo Yang","Lianzhe Wang","Heng Chang","Wenbo Zhu","Xinting Hu","Xiao Zhou","Xu Yang"],"pdf_url":"https://arxiv.org/pdf/2405.15304v2.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12990v2","updated":"2024-12-20T08:19:03Z","published":"2024-12-17T15:07:50Z","title":"Future Aspects in Human Action Recognition: Exploring Emerging\n  Techniques and Ethical Influences","summary":"  Visual-based human action recognition can be found in various application\nfields, e.g., surveillance systems, sports analytics, medical assistive\ntechnologies, or human-robot interaction frameworks, and it concerns the\nidentification and classification of individuals' activities within a video.\nSince actions typically occur over a sequence of consecutive images, it is\nparticularly challenging due to the inclusion of temporal analysis, which\nintroduces an extra layer of complexity. However, although multiple approaches\ntry to handle temporal analysis, there are still difficulties because of their\ncomputational cost and lack of adaptability. Therefore, different types of\nvision data, containing transition information between consecutive images,\nprovided by next-generation hardware sensors will guide the robotics community\nin tackling the problem of human action recognition. On the other hand, while\nthere is a plethora of still-image datasets, that researchers can adopt to\ntrain new artificial intelligence models, videos representing human activities\nare of limited capabilities, e.g., small and unbalanced datasets or selected\nwithout control from multiple sources. To this end, generating new and\nrealistic synthetic videos is possible since labeling is performed throughout\nthe data creation process, while reinforcement learning techniques can permit\nthe avoidance of considerable dataset dependence. At the same time, human\nfactors' involvement raises ethical issues for the research community, as\ndoubts and concerns about new technologies already exist.\n","authors":["Antonios Gasteratos","Stavros N. Moutsis","Konstantinos A. Tsintotas","Yiannis Aloimonos"],"pdf_url":"https://arxiv.org/pdf/2412.12990v2.pdf","comment":"2 pages, 1 figure, conference"},{"id":"http://arxiv.org/abs/2412.05076v2","updated":"2024-12-20T08:17:53Z","published":"2024-12-06T14:34:32Z","title":"Improving analytical color and texture similarity estimation methods for\n  dataset-agnostic person reidentification","summary":"  This paper studies a combined person reidentification (re-id) method that\nuses human parsing, analytical feature extraction and similarity estimation\nschemes. One of its prominent features is its low computational requirements so\nit can be implemented on edge devices. The method allows direct comparison of\nspecific image regions using interpretable features which consist of color and\ntexture channels. It is proposed to analyze and compare colors in CIE-Lab color\nspace using histogram smoothing for noise reduction. A novel pre-configured\nlatent space (LS) supervised autoencoder (SAE) is proposed for texture analysis\nwhich encodes input textures as LS points. This allows to obtain more accurate\nsimilarity measures compared to simplistic label comparison. The proposed\nmethod also does not rely upon photos or other re-id data for training, which\nmakes it completely re-id dataset-agnostic. The viability of the proposed\nmethod is verified by computing rank-1, rank-10, and mAP re-id metrics on\nMarket1501 dataset. The results are comparable to those of conventional deep\nlearning methods and the potential ways to further improve the method are\ndiscussed.\n","authors":["Nikita Gabdullin"],"pdf_url":"https://arxiv.org/pdf/2412.05076v2.pdf","comment":"8 pages, 5 figures, 3 tables, 3 equations"},{"id":"http://arxiv.org/abs/2412.15646v1","updated":"2024-12-20T08:05:13Z","published":"2024-12-20T08:05:13Z","title":"CustomTTT: Motion and Appearance Customized Video Generation via\n  Test-Time Training","summary":"  Benefiting from large-scale pre-training of text-video pairs, current\ntext-to-video (T2V) diffusion models can generate high-quality videos from the\ntext description. Besides, given some reference images or videos, the\nparameter-efficient fine-tuning method, i.e. LoRA, can generate high-quality\ncustomized concepts, e.g., the specific subject or the motions from a reference\nvideo. However, combining the trained multiple concepts from different\nreferences into a single network shows obvious artifacts. To this end, we\npropose CustomTTT, where we can joint custom the appearance and the motion of\nthe given video easily. In detail, we first analyze the prompt influence in the\ncurrent video diffusion model and find the LoRAs are only needed for the\nspecific layers for appearance and motion customization. Besides, since each\nLoRA is trained individually, we propose a novel test-time training technique\nto update parameters after combination utilizing the trained customized models.\nWe conduct detailed experiments to verify the effectiveness of the proposed\nmethods. Our method outperforms several state-of-the-art works in both\nqualitative and quantitative evaluations.\n","authors":["Xiuli Bi","Jian Lu","Bo Liu","Xiaodong Cun","Yong Zhang","Weisheng Li","Bin Xiao"],"pdf_url":"https://arxiv.org/pdf/2412.15646v1.pdf","comment":"Accepted in AAAI 2025"},{"id":"http://arxiv.org/abs/2408.16450v2","updated":"2024-12-20T08:01:10Z","published":"2024-08-29T11:30:21Z","title":"What to Preserve and What to Transfer: Faithful, Identity-Preserving\n  Diffusion-based Hairstyle Transfer","summary":"  Hairstyle transfer is a challenging task in the image editing field that\nmodifies the hairstyle of a given face image while preserving its other\nappearance and background features. The existing hairstyle transfer approaches\nheavily rely on StyleGAN, which is pre-trained on cropped and aligned face\nimages. Hence, they struggle to generalize under challenging conditions such as\nextreme variations of head poses or focal lengths. To address this issue, we\npropose a one-stage hairstyle transfer diffusion model, HairFusion, that\napplies to real-world scenarios. Specifically, we carefully design a\nhair-agnostic representation as the input of the model, where the original hair\ninformation is thoroughly eliminated. Next, we introduce a hair align\ncross-attention (Align-CA) to accurately align the reference hairstyle with the\nface image while considering the difference in their head poses. To enhance the\npreservation of the face image's original features, we leverage adaptive hair\nblending during the inference, where the output's hair regions are estimated by\nthe cross-attention map in Align-CA and blended with non-hair areas of the face\nimage. Our experimental results show that our method achieves state-of-the-art\nperformance compared to the existing methods in preserving the integrity of\nboth the transferred hairstyle and the surrounding features. The codes are\navailable at https://github.com/cychungg/HairFusion\n","authors":["Chaeyeon Chung","Sunghyun Park","Jeongho Kim","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2408.16450v2.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2209.08996v4","updated":"2024-12-20T08:00:50Z","published":"2022-09-19T13:20:19Z","title":"EDO-Net: Learning Elastic Properties of Deformable Objects from Graph\n  Dynamics","summary":"  We study the problem of learning graph dynamics of deformable objects that\ngeneralizes to unknown physical properties. Our key insight is to leverage a\nlatent representation of elastic physical properties of cloth-like deformable\nobjects that can be extracted, for example, from a pulling interaction. In this\npaper we propose EDO-Net (Elastic Deformable Object - Net), a model of graph\ndynamics trained on a large variety of samples with different elastic\nproperties that does not rely on ground-truth labels of the properties. EDO-Net\njointly learns an adaptation module, and a forward-dynamics module. The former\nis responsible for extracting a latent representation of the physical\nproperties of the object, while the latter leverages the latent representation\nto predict future states of cloth-like objects represented as graphs. We\nevaluate EDO-Net both in simulation and real world, assessing its capabilities\nof: 1) generalizing to unknown physical properties, 2) transferring the learned\nrepresentation to new downstream tasks.\n","authors":["Alberta Longhini","Marco Moletta","Alfredo Reichlin","Michael C. Welle","David Held","Zackory Erickson","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2209.08996v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15637v1","updated":"2024-12-20T07:55:08Z","published":"2024-12-20T07:55:08Z","title":"CrackUDA: Incremental Unsupervised Domain Adaptation for Improved Crack\n  Segmentation in Civil Structures","summary":"  Crack segmentation plays a crucial role in ensuring the structural integrity\nand seismic safety of civil structures. However, existing crack segmentation\nalgorithms encounter challenges in maintaining accuracy with domain shifts\nacross datasets. To address this issue, we propose a novel deep network that\nemploys incremental training with unsupervised domain adaptation (UDA) using\nadversarial learning, without a significant drop in accuracy in the source\ndomain. Our approach leverages an encoder-decoder architecture, consisting of\nboth domain-invariant and domain-specific parameters. The encoder learns shared\ncrack features across all domains, ensuring robustness to domain variations.\nSimultaneously, the decoder's domain-specific parameters capture\ndomain-specific features unique to each domain. By combining these components,\nour model achieves improved crack segmentation performance. Furthermore, we\nintroduce BuildCrack, a new crack dataset comparable to sub-datasets of the\nwell-established CrackSeg9K dataset in terms of image count and crack\npercentage. We evaluate our proposed approach against state-of-the-art UDA\nmethods using different sub-datasets of CrackSeg9K and our custom dataset. Our\nexperimental results demonstrate a significant improvement in crack\nsegmentation accuracy and generalization across target domains compared to\nother UDA methods - specifically, an improvement of 0.65 and 2.7 mIoU on source\nand target domains respectively.\n","authors":["Kushagra Srivastava","Damodar Datta Kancharla","Rizvi Tahereen","Pradeep Kumar Ramancharla","Ravi Kiran Sarvadevabhatla","Harikumar Kandath"],"pdf_url":"https://arxiv.org/pdf/2412.15637v1.pdf","comment":"Accepted at ICPR 2024. Details and code can be accessed from\n  https://crackuda.github.io"},{"id":"http://arxiv.org/abs/2412.15632v1","updated":"2024-12-20T07:48:09Z","published":"2024-12-20T07:48:09Z","title":"A New Method to Capturing Compositional Knowledge in Linguistic Space","summary":"  Compositional understanding allows visual language models to interpret\ncomplex relationships between objects, attributes, and relations in images and\ntext. However, most existing methods often rely on hard negative examples and\nfine-tuning, which can overestimate improvements and are limited by the\ndifficulty of obtaining hard negatives. In this work, we introduce Zero-Shot\nCompositional Understanding (ZS-CU), a novel task that enhances compositional\nunderstanding without requiring hard negative training data. We propose YUKINO\n(Yielded Compositional Understanding Knowledge via Textual Inversion with NO),\nwhich uses textual inversion to map unlabeled images to pseudo-tokens in a\npre-trained CLIP model. We propose introducing \"no\" logical regularization to\naddress the issue of token interaction in inversion. Additionally, we suggest\nusing knowledge distillation to reduce the time complexity of textual\ninversion. Experimental results show that YUKINO outperforms the existing\nmulti-modal SOTA models by over 8% on the SugarCREPE benchmark, and also\nachieves significant improvements in image retrieval tasks.\n","authors":["Jiahe Wan"],"pdf_url":"https://arxiv.org/pdf/2412.15632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10634v3","updated":"2024-12-20T07:26:18Z","published":"2023-02-21T12:48:44Z","title":"A Deep Learning-Based Fully Automated Pipeline for Regurgitant Mitral\n  Valve Anatomy Analysis From 3D Echocardiography","summary":"  Three-dimensional transesophageal echocardiography (3DTEE) is the recommended\nimaging technique for the assessment of mitral valve (MV) morphology and\nlesions in case of mitral regurgitation (MR) requiring surgical or\ntranscatheter repair. Such assessment is key to thorough intervention planning\nand to intraprocedural guidance. However, it requires segmentation from 3DTEE\nimages, which is timeconsuming, operator-dependent, and often merely\nqualitative. In the present work, a novel workflow to quantify the\npatient-specific MV geometry from 3DTEE is proposed. The developed approach\nrelies on a 3D multi-decoder residual convolutional neural network (CNN) with a\nU-Net architecture for multi-class segmentation of MV annulus and leaflets. The\nCNN was trained and tested on a dataset comprising 55 3DTEE examinations of\nMR-affected patients. After training, the CNN is embedded into a fully\nautomatic, and hence fully repeatable, pipeline that refines the predicted\nsegmentation, detects MV anatomical landmarks and quantifies MV morphology. The\ntrained 3D CNN achieves an average Dice score of $0.82 \\pm 0.06$, mean surface\ndistance of $0.43 \\pm 0.14$ mm and 95% Hausdorff Distance (HD) of $3.57 \\pm\n1.56$ mm before segmentation refinement, outperforming a state-of-the-art\nbaseline residual U-Net architecture, and provides an unprecedented multi-class\nsegmentation of the annulus, anterior and posterior leaflet. The automatic 3D\nlinear morphological measurements of the annulus and leaflets, specifically\ndiameters and lengths, exhibit differences of less than 1.45 mm when compared\nto ground truth values. These measurements also demonstrate strong overall\nagreement with analyses conducted by semi-automated commercial software. The\nwhole process requires minimal user interaction and requires approximately 15\nseconds\n","authors":["Riccardo Munafò","Simone Saitta","Giacomo Ingallina","Paolo Denti","Francesco Maisano","Eustachio Agricola","Alberto Redaelli","Emiliano Votta"],"pdf_url":"https://arxiv.org/pdf/2302.10634v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15618v1","updated":"2024-12-20T07:22:41Z","published":"2024-12-20T07:22:41Z","title":"3D Shape Tokenization","summary":"  We introduce Shape Tokens, a 3D representation that is continuous, compact,\nand easy to incorporate into machine learning models. Shape Tokens act as\nconditioning vectors that represent shape information in a 3D flow-matching\nmodel. The flow-matching model is trained to approximate probability density\nfunctions corresponding to delta functions concentrated on the surfaces of\nshapes in 3D. By attaching Shape Tokens to various machine learning models, we\ncan generate new shapes, convert images to 3D, align 3D shapes with text and\nimages, and render shapes directly at variable, user specified, resolution.\nMoreover, Shape Tokens enable a systematic analysis of geometric properties\nsuch as normal, density, and deformation field. Across all tasks and\nexperiments, utilizing Shape Tokens demonstrate strong performance compared to\nexisting baselines.\n","authors":["Jen-Hao Rick Chang","Yuyang Wang","Miguel Angel Bautista Martin","Jiatao Gu","Josh Susskind","Oncel Tuzel"],"pdf_url":"https://arxiv.org/pdf/2412.15618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15614v1","updated":"2024-12-20T07:17:50Z","published":"2024-12-20T07:17:50Z","title":"Technical Report for ICML 2024 TiFA Workshop MLLM Attack Challenge:\n  Suffix Injection and Projected Gradient Descent Can Easily Fool An MLLM","summary":"  This technical report introduces our top-ranked solution that employs two\napproaches, \\ie suffix injection and projected gradient descent (PGD) , to\naddress the TiFA workshop MLLM attack challenge. Specifically, we first append\nthe text from an incorrectly labeled option (pseudo-labeled) to the original\nquery as a suffix. Using this modified query, our second approach applies the\nPGD method to add imperceptible perturbations to the image. Combining these two\ntechniques enables successful attacks on the LLaVA 1.5 model.\n","authors":["Yangyang Guo","Ziwei Xu","Xilie Xu","YongKang Wong","Liqiang Nie","Mohan Kankanhalli"],"pdf_url":"https://arxiv.org/pdf/2412.15614v1.pdf","comment":"ICML TiFA Challenge Technical Report"},{"id":"http://arxiv.org/abs/2411.13591v5","updated":"2024-12-20T07:16:32Z","published":"2024-11-18T05:47:12Z","title":"Improved GUI Grounding via Iterative Narrowing","summary":"  Graphical User Interface (GUI) grounding plays a crucial role in enhancing\nthe capabilities of Vision-Language Model (VLM) agents. While general VLMs,\nsuch as GPT-4V, demonstrate strong performance across various tasks, their\nproficiency in GUI grounding remains suboptimal. Recent studies have focused on\nfine-tuning these models specifically for zero-shot GUI grounding, yielding\nsignificant improvements over baseline performance. We introduce a visual\nprompting framework that employs an iterative narrowing mechanism to further\nimprove the performance of both general and fine-tuned models in GUI grounding.\nFor evaluation, we tested our method on a comprehensive benchmark comprising\nvarious UI platforms and provided the code to reproduce our results.\n","authors":["Anthony Nguyen"],"pdf_url":"https://arxiv.org/pdf/2411.13591v5.pdf","comment":"Code available at\n  https://github.com/ant-8/GUI-Grounding-via-Iterative-Narrowing"},{"id":"http://arxiv.org/abs/2406.16633v6","updated":"2024-12-20T07:14:58Z","published":"2024-06-24T13:30:55Z","title":"MLAAN: Scaling Supervised Local Learning with Multilaminar Leap\n  Augmented Auxiliary Network","summary":"  Deep neural networks (DNNs) typically employ an end-to-end (E2E) training\nparadigm which presents several challenges, including high GPU memory\nconsumption, inefficiency, and difficulties in model parallelization during\ntraining. Recent research has sought to address these issues, with one\npromising approach being local learning. This method involves partitioning the\nbackbone network into gradient-isolated modules and manually designing\nauxiliary networks to train these local modules. Existing methods often neglect\nthe interaction of information between local modules, leading to myopic issues\nand a performance gap compared to E2E training. To address these limitations,\nwe propose the Multilaminar Leap Augmented Auxiliary Network (MLAAN).\nSpecifically, MLAAN comprises Multilaminar Local Modules (MLM) and Leap\nAugmented Modules (LAM). MLM captures both local and global features through\nindependent and cascaded auxiliary networks, alleviating performance issues\ncaused by insufficient global features. However, overly simplistic auxiliary\nnetworks can impede MLM's ability to capture global information. To address\nthis, we further design LAM, an enhanced auxiliary network that uses the\nExponential Moving Average (EMA) method to facilitate information exchange\nbetween local modules, thereby mitigating the shortsightedness resulting from\ninadequate interaction. The synergy between MLM and LAM has demonstrated\nexcellent performance. Our experiments on the CIFAR-10, STL-10, SVHN, and\nImageNet datasets show that MLAAN can be seamlessly integrated into existing\nlocal learning frameworks, significantly enhancing their performance and even\nsurpassing end-to-end (E2E) training methods, while also reducing GPU memory\nconsumption.\n","authors":["Yuming Zhang","Shouxin Zhang","Peizhe Wang","Feiyu Zhu","Dongzhi Guan","Junhao Su","Jiabin Liu","Changpeng Cai"],"pdf_url":"https://arxiv.org/pdf/2406.16633v6.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.15606v1","updated":"2024-12-20T07:00:46Z","published":"2024-12-20T07:00:46Z","title":"Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool\n  Usage","summary":"  The advancement of large language models (LLMs) prompts the development of\nmulti-modal agents, which are used as a controller to call external tools,\nproviding a feasible way to solve practical tasks. In this paper, we propose a\nmulti-modal agent tuning method that automatically generates multi-modal\ntool-usage data and tunes a vision-language model (VLM) as the controller for\npowerful tool-usage reasoning. To preserve the data quality, we prompt the\nGPT-4o mini model to generate queries, files, and trajectories, followed by\nquery-file and trajectory verifiers. Based on the data synthesis pipeline, we\ncollect the MM-Traj dataset that contains 20K tasks with trajectories of tool\nusage. Then, we develop the T3-Agent via \\underline{T}rajectory\n\\underline{T}uning on VLMs for \\underline{T}ool usage using MM-Traj.\nEvaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently\nachieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B},\nwhich outperforms untrained VLMs by $20\\%$, showing the effectiveness of the\nproposed data synthesis pipeline, leading to high-quality data for tool-usage\ncapabilities.\n","authors":["Zhi Gao","Bofei Zhang","Pengxiang Li","Xiaojian Ma","Tao Yuan","Yue Fan","Yuwei Wu","Yunde Jia","Song-Chun Zhu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2412.15606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14868v3","updated":"2024-12-20T06:55:16Z","published":"2024-11-22T11:34:18Z","title":"Defective Edge Detection Using Cascaded Ensemble Canny Operator","summary":"  Edge detection has been one of the most difficult challenges in computer\nvision because of the difficulty in identifying the borders and edges from the\nreal-world images including objects of varying kinds and sizes. Methods based\non ensemble learning, which use a combination of backbones and attention\nmodules, outperformed more conventional approaches, such as Sobel and Canny\nedge detection. Nevertheless, these algorithms are still challenged when faced\nwith complicated scene photos. In addition, the identified edges utilizing the\ncurrent methods are not refined and often include incorrect edges. In this\nwork, we used a Cascaded Ensemble Canny operator to solve these problems and\ndetect the object edges. The most difficult Fresh and Rotten and Berkeley\ndatasets are used to test the suggested approach in Python. In terms of\nperformance metrics and output picture quality, the acquired results outperform\nthe specified edge detection networks\n","authors":["Anjali Nambiyar Rajkumar Kannan"],"pdf_url":"https://arxiv.org/pdf/2411.14868v3.pdf","comment":"2 Pages and 2 Figures"},{"id":"http://arxiv.org/abs/2412.15601v1","updated":"2024-12-20T06:48:17Z","published":"2024-12-20T06:48:17Z","title":"Gaze Label Alignment: Alleviating Domain Shift for Gaze Estimation","summary":"  Gaze estimation methods encounter significant performance deterioration when\nbeing evaluated across different domains, because of the domain gap between the\ntesting and training data. Existing methods try to solve this issue by reducing\nthe deviation of data distribution, however, they ignore the existence of label\ndeviation in the data due to the acquisition mechanism of the gaze label and\nthe individual physiological differences. In this paper, we first point out\nthat the influence brought by the label deviation cannot be ignored, and\npropose a gaze label alignment algorithm (GLA) to eliminate the label\ndistribution deviation. Specifically, we first train the feature extractor on\nall domains to get domain invariant features, and then select an anchor domain\nto train the gaze regressor. We predict the gaze label on remaining domains and\nuse a mapping function to align the labels. Finally, these aligned labels can\nbe used to train gaze estimation models. Therefore, our method can be combined\nwith any existing method. Experimental results show that our GLA method can\neffectively alleviate the label distribution shift, and SOTA gaze estimation\nmethods can be further improved obviously.\n","authors":["Guanzhong Zeng","Jingjing Wang","Zefu Xu","Pengwei Yin","Wenqi Ren","Di Xie","Jiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.15601v1.pdf","comment":"Camera Ready. Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.15595v1","updated":"2024-12-20T06:39:40Z","published":"2024-12-20T06:39:40Z","title":"Mask-RadarNet: Enhancing Transformer With Spatial-Temporal Semantic\n  Context for Radar Object Detection in Autonomous Driving","summary":"  As a cost-effective and robust technology, automotive radar has seen steady\nimprovement during the last years, making it an appealing complement to\ncommonly used sensors like camera and LiDAR in autonomous driving. Radio\nfrequency data with rich semantic information are attracting more and more\nattention. Most current radar-based models take radio frequency image sequences\nas the input. However, these models heavily rely on convolutional neural\nnetworks and leave out the spatial-temporal semantic context during the\nencoding stage. To solve these problems, we propose a model called\nMask-RadarNet to fully utilize the hierarchical semantic features from the\ninput radar data. Mask-RadarNet exploits the combination of interleaved\nconvolution and attention operations to replace the traditional architecture in\ntransformer-based models. In addition, patch shift is introduced to the\nMask-RadarNet for efficient spatial-temporal feature learning. By shifting part\nof patches with a specific mosaic pattern in the temporal dimension,\nMask-RadarNet achieves competitive performance while reducing the computational\nburden of the spatial-temporal modeling. In order to capture the\nspatial-temporal semantic contextual information, we design the class masking\nattention module (CMAM) in our encoder. Moreover, a lightweight auxiliary\ndecoder is added to our model to aggregate prior maps generated from the CMAM.\nExperiments on the CRUW dataset demonstrate the superiority of the proposed\nmethod to some state-of-the-art radar-based object detection algorithms. With\nrelatively lower computational complexity and fewer parameters, the proposed\nMask-RadarNet achieves higher recognition accuracy for object detection in\nautonomous driving.\n","authors":["Yuzhi Wu","Jun Liu","Guangfeng Jiang","Weijian Liu","Danilo Orlando"],"pdf_url":"https://arxiv.org/pdf/2412.15595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18605v3","updated":"2024-12-20T06:39:15Z","published":"2024-03-27T14:24:30Z","title":"FlexEdit: Flexible and Controllable Diffusion-based Object-centric Image\n  Editing","summary":"  Our work addresses limitations seen in previous approaches for object-centric\nediting problems, such as unrealistic results due to shape discrepancies and\nlimited control in object replacement or insertion. To this end, we introduce\nFlexEdit, a flexible and controllable editing framework for objects where we\niteratively adjust latents at each denoising step using our FlexEdit block.\nInitially, we optimize latents at test time to align with specified object\nconstraints. Then, our framework employs an adaptive mask, automatically\nextracted during denoising, to protect the background while seamlessly blending\nnew content into the target image. We demonstrate the versatility of FlexEdit\nin various object editing tasks and curate an evaluation test suite with\nsamples from both real and synthetic images, along with novel evaluation\nmetrics designed for object-centric editing. We conduct extensive experiments\non different editing scenarios, demonstrating the superiority of our editing\nframework over recent advanced text-guided image editing methods. Our project\npage is published at https://flex-edit.github.io/.\n","authors":["Trong-Tung Nguyen","Duc-Anh Nguyen","Anh Tran","Cuong Pham"],"pdf_url":"https://arxiv.org/pdf/2403.18605v3.pdf","comment":"Our project page: https://flex-edit.github.io/"},{"id":"http://arxiv.org/abs/2409.15092v4","updated":"2024-12-20T06:19:10Z","published":"2024-09-23T15:06:37Z","title":"M2OST: Many-to-one Regression for Predicting Spatial Transcriptomics\n  from Digital Pathology Images","summary":"  The advancement of Spatial Transcriptomics (ST) has facilitated the\nspatially-aware profiling of gene expressions based on histopathology images.\nAlthough ST data offers valuable insights into the micro-environment of tumors,\nits acquisition cost remains expensive. Therefore, directly predicting the ST\nexpressions from digital pathology images is desired. Current methods usually\nadopt existing regression backbones along with patch-sampling for this task,\nwhich ignores the inherent multi-scale information embedded in the pyramidal\ndata structure of digital pathology images, and wastes the inter-spot visual\ninformation crucial for accurate gene expression prediction. To address these\nlimitations, we propose M2OST, a many-to-one regression Transformer that can\naccommodate the hierarchical structure of the pathology images via a decoupled\nmulti-scale feature extractor. Unlike traditional models that are trained with\none-to-one image-label pairs, M2OST uses multiple images from different levels\nof the digital pathology image to jointly predict the gene expressions in their\ncommon corresponding spot. Built upon our many-to-one scheme, M2OST can be\neasily scaled to fit different numbers of inputs, and its network structure\ninherently incorporates nearby inter-spot features, enhancing regression\nperformance. We have tested M2OST on three public ST datasets and the\nexperimental results show that M2OST can achieve state-of-the-art performance\nwith fewer parameters and floating-point operations (FLOPs).\n","authors":["Hongyi Wang","Xiuju Du","Jing Liu","Shuyi Ouyang","Yen-Wei Chen","Lanfen Lin"],"pdf_url":"https://arxiv.org/pdf/2409.15092v4.pdf","comment":"Improved from our previous unpublished work arXiv:2401.10608. arXiv\n  admin note: substantial text overlap with arXiv:2401.10608"},{"id":"http://arxiv.org/abs/2412.15590v1","updated":"2024-12-20T06:00:59Z","published":"2024-12-20T06:00:59Z","title":"SemDP: Semantic-level Differential Privacy Protection for Face Datasets","summary":"  While large-scale face datasets have advanced deep learning-based face\nanalysis, they also raise privacy concerns due to the sensitive personal\ninformation they contain. Recent schemes have implemented differential privacy\nto protect face datasets. However, these schemes generally treat each image as\na separate database, which does not fully meet the core requirements of\ndifferential privacy. In this paper, we propose a semantic-level differential\nprivacy protection scheme that applies to the entire face dataset. Unlike\npixel-level differential privacy approaches, our scheme guarantees that\nsemantic privacy in faces is not compromised. The key idea is to convert\nunstructured data into structured data to enable the application of\ndifferential privacy. Specifically, we first extract semantic information from\nthe face dataset to build an attribute database, then apply differential\nperturbations to obscure this attribute data, and finally use an image\nsynthesis model to generate a protected face dataset. Extensive experimental\nresults show that our scheme can maintain visual naturalness and balance the\nprivacy-utility trade-off compared to the mainstream schemes.\n","authors":["Xiaoting Zhang","Tao Wang","Junhao Ji"],"pdf_url":"https://arxiv.org/pdf/2412.15590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09475v2","updated":"2024-12-20T05:34:30Z","published":"2024-07-12T17:57:00Z","title":"Adaptive Prediction Ensemble: Improving Out-of-Distribution\n  Generalization of Motion Forecasting","summary":"  Deep learning-based trajectory prediction models for autonomous driving often\nstruggle with generalization to out-of-distribution (OOD) scenarios, sometimes\nperforming worse than simple rule-based models. To address this limitation, we\npropose a novel framework, Adaptive Prediction Ensemble (APE), which integrates\ndeep learning and rule-based prediction experts. A learned routing function,\ntrained concurrently with the deep learning model, dynamically selects the most\nreliable prediction based on the input scenario. Our experiments on large-scale\ndatasets, including Waymo Open Motion Dataset (WOMD) and Argoverse, demonstrate\nimprovement in zero-shot generalization across datasets. We show that our\nmethod outperforms individual prediction models and other variants,\nparticularly in long-horizon prediction and scenarios with a high proportion of\nOOD data. This work highlights the potential of hybrid approaches for robust\nand generalizable motion prediction in autonomous driving. More details can be\nfound on the project page: https://sites.google.com/view/ape-generalization.\n","authors":["Jinning Li","Jiachen Li","Sangjae Bae","David Isele"],"pdf_url":"https://arxiv.org/pdf/2407.09475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07481v2","updated":"2024-12-20T05:29:17Z","published":"2024-12-10T13:03:42Z","title":"Manta: Enhancing Mamba for Few-Shot Action Recognition of Long\n  Sub-Sequence","summary":"  In few-shot action recognition (FSAR), long sub-sequences of video naturally\nexpress entire actions more effectively. However, the high computational\ncomplexity of mainstream Transformer-based methods limits their application.\nRecent Mamba demonstrates efficiency in modeling long sequences, but directly\napplying Mamba to FSAR overlooks the importance of local feature modeling and\nalignment. Moreover, long sub-sequences within the same class accumulate\nintra-class variance, which adversely impacts FSAR performance. To solve these\nchallenges, we propose a Matryoshka MAmba and CoNtrasTive LeArning framework\n(Manta). Firstly, the Matryoshka Mamba introduces multiple Inner Modules to\nenhance local feature representation, rather than directly modeling global\nfeatures. An Outer Module captures dependencies of timeline between these local\nfeatures for implicit temporal alignment. Secondly, a hybrid contrastive\nlearning paradigm, combining both supervised and unsupervised methods, is\ndesigned to mitigate the negative effects of intra-class variance accumulation.\nThe Matryoshka Mamba and the hybrid contrastive learning paradigm operate in\ntwo parallel branches within Manta, enhancing Mamba for FSAR of long\nsub-sequence. Manta achieves new state-of-the-art performance on prominent\nbenchmarks, including SSv2, Kinetics, UCF101, and HMDB51. Extensive empirical\nstudies prove that Manta significantly improves FSAR of long sub-sequence from\nmultiple perspectives.\n","authors":["Wenbo Huang","Jinghui Zhang","Guang Li","Lei Zhang","Shuoyuan Wang","Fang Dong","Jiahui Jin","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2412.07481v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2306.04940v4","updated":"2024-12-20T05:22:16Z","published":"2023-06-08T05:13:34Z","title":"LayerAct: Advanced Activation Mechanism for Robust Inference of CNNs","summary":"  In this work, we propose a novel activation mechanism called LayerAct for\nCNNs. This approach is motivated by our theoretical and experimental analyses,\nwhich demonstrate that Layer Normalization (LN) can mitigate a limitation of\nexisting activation functions regarding noise robustness. However, LN is known\nto be disadvantageous in CNNs due to its tendency to make activation outputs\nhomogeneous. The proposed method is designed to be more robust than existing\nactivation functions by reducing the upper bound of influence caused by input\nshifts without inheriting LN's limitation. We provide analyses and experiments\nshowing that LayerAct functions exhibit superior robustness compared to\nElementAct functions. Experimental results on three clean and noisy benchmark\ndatasets for image classification tasks indicate that LayerAct functions\noutperform other activation functions in handling noisy datasets while\nachieving superior performance on clean datasets in most cases.\n","authors":["Kihyuk Yoon","Chiehyeon Lim"],"pdf_url":"https://arxiv.org/pdf/2306.04940v4.pdf","comment":"7 pages, 5 figures, 4 tables except acknowledge, reference, and\n  appendix"},{"id":"http://arxiv.org/abs/2412.13211v2","updated":"2024-12-20T05:21:39Z","published":"2024-12-09T01:29:24Z","title":"ManiSkill-HAB: A Benchmark for Low-Level Manipulation in Home\n  Rearrangement Tasks","summary":"  High-quality benchmarks are the foundation for embodied AI research, enabling\nsignificant advancements in long-horizon navigation, manipulation and\nrearrangement tasks. However, as frontier tasks in robotics get more advanced,\nthey require faster simulation speed, more intricate test environments, and\nlarger demonstration datasets. To this end, we present MS-HAB, a holistic\nbenchmark for low-level manipulation and in-home object rearrangement. First,\nwe provide a GPU-accelerated implementation of the Home Assistant Benchmark\n(HAB). We support realistic low-level control and achieve over 3x the speed of\nprevious magical grasp implementations at similar GPU memory usage. Second, we\ntrain extensive reinforcement learning (RL) and imitation learning (IL)\nbaselines for future work to compare against. Finally, we develop a rule-based\ntrajectory filtering system to sample specific demonstrations from our RL\npolicies which match predefined criteria for robot behavior and safety.\nCombining demonstration filtering with our fast environments enables efficient,\ncontrolled data generation at scale.\n","authors":["Arth Shukla","Stone Tao","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2412.13211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15577v1","updated":"2024-12-20T05:20:10Z","published":"2024-12-20T05:20:10Z","title":"SaliencyI2PLoc: saliency-guided image-point cloud localization using\n  contrastive learning","summary":"  Image to point cloud global localization is crucial for robot navigation in\nGNSS-denied environments and has become increasingly important for multi-robot\nmap fusion and urban asset management. The modality gap between images and\npoint clouds poses significant challenges for cross-modality fusion. Current\ncross-modality global localization solutions either require modality\nunification, which leads to information loss, or rely on engineered training\nschemes to encode multi-modality features, which often lack feature alignment\nand relation consistency. To address these limitations, we propose,\nSaliencyI2PLoc, a novel contrastive learning based architecture that fuses the\nsaliency map into feature aggregation and maintains the feature relation\nconsistency on multi-manifold spaces. To alleviate the pre-process of data\nmining, the contrastive learning framework is applied which efficiently\nachieves cross-modality feature mapping. The context saliency-guided local\nfeature aggregation module is designed, which fully leverages the contribution\nof the stationary information in the scene generating a more representative\nglobal feature. Furthermore, to enhance the cross-modality feature alignment\nduring contrastive learning, the consistency of relative relationships between\nsamples in different manifold spaces is also taken into account. Experiments\nconducted on urban and highway scenario datasets demonstrate the effectiveness\nand robustness of our method. Specifically, our method achieves a Recall@1 of\n78.92% and a Recall@20 of 97.59% on the urban scenario evaluation dataset,\nshowing an improvement of 37.35% and 18.07%, compared to the baseline method.\nThis demonstrates that our architecture efficiently fuses images and point\nclouds and represents a significant step forward in cross-modality global\nlocalization. The project page and code will be released.\n","authors":["Yuhao Li","Jianping Li","Zhen Dong","Yuan Wang","Bisheng Yang"],"pdf_url":"https://arxiv.org/pdf/2412.15577v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2412.15576v1","updated":"2024-12-20T05:17:06Z","published":"2024-12-20T05:17:06Z","title":"QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped\n  Robot Learning","summary":"  This paper addresses the inherent inference latency challenges associated\nwith deploying multimodal large language models (MLLM) in quadruped\nvision-language-action (QUAR-VLA) tasks. Our investigation reveals that\nconventional parameter reduction techniques ultimately impair the performance\nof the language foundation model during the action instruction tuning phase,\nmaking them unsuitable for this purpose. We introduce a novel latency-free\nquadruped MLLM model, dubbed QUART-Online, designed to enhance inference\nefficiency without degrading the performance of the language foundation model.\nBy incorporating Action Chunk Discretization (ACD), we compress the original\naction representation space, mapping continuous action values onto a smaller\nset of discrete representative vectors while preserving critical information.\nSubsequently, we fine-tune the MLLM to integrate vision, language, and\ncompressed actions into a unified semantic space. Experimental results\ndemonstrate that QUART-Online operates in tandem with the existing MLLM system,\nachieving real-time inference in sync with the underlying controller frequency,\nsignificantly boosting the success rate across various tasks by 65\\%. Our\nproject page is\n\\href{https://quart-online.github.io}https://quart-online.github.io.\n","authors":["Xinyang Tong","Pengxiang Ding","Donglin Wang","Wenjie Zhang","Can Cui","Mingyang Sun","Yiguo Fan","Han Zhao","Hongyin Zhang","Yonghao Dang","Siteng Huang","Shangke Lyu"],"pdf_url":"https://arxiv.org/pdf/2412.15576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15574v1","updated":"2024-12-20T05:11:51Z","published":"2024-12-20T05:11:51Z","title":"J-EDI QA: Benchmark for deep-sea organism-specific multimodal LLM","summary":"  Japan Agency for Marine-Earth Science and Technology (JAMSTEC) has made\navailable the JAMSTEC Earth Deep-sea Image (J-EDI), a deep-sea video and image\narchive (https://www.godac.jamstec.go.jp/jedi/e/index.html). This archive\nserves as a valuable resource for researchers and scholars interested in\ndeep-sea imagery. The dataset comprises images and videos of deep-sea\nphenomena, predominantly of marine organisms, but also of the seafloor and\nphysical processes. In this study, we propose J-EDI QA, a benchmark for\nunderstanding images of deep-sea organisms using a multimodal large language\nmodel (LLM). The benchmark is comprised of 100 images, accompanied by questions\nand answers with four options by JAMSTEC researchers for each image. The QA\npairs are provided in Japanese, and the benchmark assesses the ability to\nunderstand deep-sea species in Japanese. In the evaluation presented in this\npaper, OpenAI o1 achieved a 50% correct response rate. This result indicates\nthat even with the capabilities of state-of-the-art models as of December 2024,\ndeep-sea species comprehension is not yet at an expert level. Further advances\nin deep-sea species-specific LLMs are therefore required.\n","authors":["Takero Yoshida","Yuikazu Ito","Yoshihiro Fujiwara","Shinji Tsuchida","Daisuke Sugiyama","Daisuke Matsuoka"],"pdf_url":"https://arxiv.org/pdf/2412.15574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15571v1","updated":"2024-12-20T05:09:18Z","published":"2024-12-20T05:09:18Z","title":"Continual Learning Using a Kernel-Based Method Over Foundation Models","summary":"  Continual learning (CL) learns a sequence of tasks incrementally. This paper\nstudies the challenging CL setting of class-incremental learning (CIL). CIL has\ntwo key challenges: catastrophic forgetting (CF) and inter-task class\nseparation (ICS). Despite numerous proposed methods, these issues remain\npersistent obstacles. This paper proposes a novel CIL method, called Kernel\nLinear Discriminant Analysis (KLDA), that can effectively avoid CF and ICS\nproblems. It leverages only the powerful features learned in a foundation model\n(FM). However, directly using these features proves suboptimal. To address\nthis, KLDA incorporates the Radial Basis Function (RBF) kernel and its Random\nFourier Features (RFF) to enhance the feature representations from the FM,\nleading to improved performance. When a new task arrives, KLDA computes only\nthe mean for each class in the task and updates a shared covariance matrix for\nall learned classes based on the kernelized features. Classification is\nperformed using Linear Discriminant Analysis. Our empirical evaluation using\ntext and image classification datasets demonstrates that KLDA significantly\noutperforms baselines. Remarkably, without relying on replay data, KLDA\nachieves accuracy comparable to joint training of all classes, which is\nconsidered the upper bound for CIL performance. The KLDA code is available at\nhttps://github.com/salehmomeni/klda.\n","authors":["Saleh Momeni","Sahisnu Mazumder","Bing Liu"],"pdf_url":"https://arxiv.org/pdf/2412.15571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15570v1","updated":"2024-12-20T05:08:42Z","published":"2024-12-20T05:08:42Z","title":"DefFiller: Mask-Conditioned Diffusion for Salient Steel Surface Defect\n  Generation","summary":"  Current saliency-based defect detection methods show promise in industrial\nsettings, but the unpredictability of defects in steel production environments\ncomplicates dataset creation, hampering model performance. Existing data\naugmentation approaches using generative models often require pixel-level\nannotations, which are time-consuming and resource-intensive. To address this,\nwe introduce DefFiller, a mask-conditioned defect generation method that\nleverages a layout-to-image diffusion model. DefFiller generates defect samples\npaired with mask conditions, eliminating the need for pixel-level annotations\nand enabling direct use in model training. We also develop an evaluation\nframework to assess the quality of generated samples and their impact on\ndetection performance. Experimental results on the SD-Saliency-900 dataset\ndemonstrate that DefFiller produces high-quality defect images that accurately\nmatch the provided mask conditions, significantly enhancing the performance of\nsaliency-based defect detection models trained on the augmented dataset.\n","authors":["Yichun Tai","Zhenzhen Huang","Tao Peng","Zhijiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.15570v1.pdf","comment":"20 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.05331v2","updated":"2024-12-20T04:59:53Z","published":"2024-12-05T07:44:40Z","title":"Deep Learning and Hybrid Approaches for Dynamic Scene Analysis, Object\n  Detection and Motion Tracking","summary":"  This project aims to develop a robust video surveillance system, which can\nsegment videos into smaller clips based on the detection of activities. It uses\nCCTV footage, for example, to record only major events-like the appearance of a\nperson or a thief-so that storage is optimized and digital searches are easier.\nIt utilizes the latest techniques in object detection and tracking, including\nConvolutional Neural Networks (CNNs) like YOLO, SSD, and Faster R-CNN, as well\nas Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks\n(LSTMs), to achieve high accuracy in detection and capture temporal\ndependencies. The approach incorporates adaptive background modeling through\nGaussian Mixture Models (GMM) and optical flow methods like Lucas-Kanade to\ndetect motions. Multi-scale and contextual analysis are used to improve\ndetection across different object sizes and environments. A hybrid motion\nsegmentation strategy combines statistical and deep learning models to manage\ncomplex movements, while optimizations for real-time processing ensure\nefficient computation. Tracking methods, such as Kalman Filters and Siamese\nnetworks, are employed to maintain smooth tracking even in cases of occlusion.\nDetection is improved on various-sized objects for multiple scenarios by\nmulti-scale and contextual analysis. Results demonstrate high precision and\nrecall in detecting and tracking objects, with significant improvements in\nprocessing times and accuracy due to real-time optimizations and\nillumination-invariant features. The impact of this research lies in its\npotential to transform video surveillance, reducing storage requirements and\nenhancing security through reliable and efficient object detection and\ntracking.\n","authors":["Shahran Rahman Alve"],"pdf_url":"https://arxiv.org/pdf/2412.05331v2.pdf","comment":"15 Pages, 7 Figures"},{"id":"http://arxiv.org/abs/2405.19547v2","updated":"2024-12-20T04:54:04Z","published":"2024-05-29T22:19:57Z","title":"CLIPLoss and Norm-Based Data Selection Methods for Multimodal\n  Contrastive Learning","summary":"  Data selection has emerged as a core issue for large-scale visual-language\nmodel pretaining (e.g., CLIP), particularly with noisy web-curated datasets.\nThree main data selection approaches are: (1) leveraging external non-CLIP\nmodels to aid data selection, (2) training new CLIP-style embedding models that\nare more effective at selecting high-quality data than the original OpenAI CLIP\nmodel, and (3) designing better metrics or strategies universally applicable to\nany CLIP embedding without requiring specific model properties (e.g., CLIPScore\nis one popular metric). While the first two approaches have been extensively\nstudied, the third remains under-explored. In this paper, we advance the third\napproach by proposing two new methods. Firstly, instead of classical CLIP\nscores that only consider the alignment between two modalities from a single\nsample, we introduce surrogate-CLIPLoss (s-CLIPLoss), a CLIP loss-inspired\nmethod that adds the alignment between one sample and its contrastive pairs as\nan extra normalization term for better quality measurement. Secondly, when\ndownstream tasks are known, we propose a new norm-based metric, NormSim, to\nmeasure the similarity between pretraining data and target data. We test our\nmethods on the data selection benchmark, DataComp~\\cite{gadre2023datacomp}.\nCompared to the best baseline using only OpenAI's CLIP-L/14, our methods\nachieve a 5.3\\% improvement on ImageNet-1k and a 2.8\\% improvement on 38\ndownstream evaluation tasks. Moreover, both s-CLIPLoss and NormSim are\ncompatible with existing techniques. By combining our methods with the current\nbest methods DFN and HYPE, we can boost average performance on downstream tasks\nby 0.9\\%, achieving a new state-of-the-art on the DataComp-medium benchmark.\n","authors":["Yiping Wang","Yifang Chen","Wendan Yan","Alex Fang","Wenjing Zhou","Kevin Jamieson","Simon Shaolei Du"],"pdf_url":"https://arxiv.org/pdf/2405.19547v2.pdf","comment":"This paper supercedes our previous VAS paper (arXiv:2402.02055). It's\n  accepted by NeurIPS2024 as spotlight paper. DataComp benchmark:\n  https://www.datacomp.ai/dcclip/leaderboard.html"},{"id":"http://arxiv.org/abs/2412.15550v1","updated":"2024-12-20T04:21:54Z","published":"2024-12-20T04:21:54Z","title":"EGSRAL: An Enhanced 3D Gaussian Splatting based Renderer with Automated\n  Labeling for Large-Scale Driving Scene","summary":"  3D Gaussian Splatting (3D GS) has gained popularity due to its faster\nrendering speed and high-quality novel view synthesis. Some researchers have\nexplored using 3D GS for reconstructing driving scenes. However, these methods\noften rely on various data types, such as depth maps, 3D boxes, and\ntrajectories of moving objects. Additionally, the lack of annotations for\nsynthesized images limits their direct application in downstream tasks. To\naddress these issues, we propose EGSRAL, a 3D GS-based method that relies\nsolely on training images without extra annotations. EGSRAL enhances 3D GS's\ncapability to model both dynamic objects and static backgrounds and introduces\na novel adaptor for auto labeling, generating corresponding annotations based\non existing annotations. We also propose a grouping strategy for vanilla 3D GS\nto address perspective issues in rendering large-scale, complex scenes. Our\nmethod achieves state-of-the-art performance on multiple datasets without any\nextra annotation. For example, the PSNR metric reaches 29.04 on the nuScenes\ndataset. Moreover, our automated labeling can significantly improve the\nperformance of 2D/3D detection tasks. Code is available at\nhttps://github.com/jiangxb98/EGSRAL.\n","authors":["Yixiong Huo","Guangfeng Jiang","Hongyang Wei","Ji Liu","Song Zhang","Han Liu","Xingliang Huang","Mingjie Lu","Jinzhang Peng","Dong Li","Lu Tian","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2412.15550v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.15544v1","updated":"2024-12-20T04:08:11Z","published":"2024-12-20T04:08:11Z","title":"VLM-RL: A Unified Vision Language Models and Reinforcement Learning\n  Framework for Safe Autonomous Driving","summary":"  In recent years, reinforcement learning (RL)-based methods for learning\ndriving policies have gained increasing attention in the autonomous driving\ncommunity and have achieved remarkable progress in various driving scenarios.\nHowever, traditional RL approaches rely on manually engineered rewards, which\nrequire extensive human effort and often lack generalizability. To address\nthese limitations, we propose \\textbf{VLM-RL}, a unified framework that\nintegrates pre-trained Vision-Language Models (VLMs) with RL to generate reward\nsignals using image observation and natural language goals. The core of VLM-RL\nis the contrasting language goal (CLG)-as-reward paradigm, which uses positive\nand negative language goals to generate semantic rewards. We further introduce\na hierarchical reward synthesis approach that combines CLG-based semantic\nrewards with vehicle state information, improving reward stability and offering\na more comprehensive reward signal. Additionally, a batch-processing technique\nis employed to optimize computational efficiency during training. Extensive\nexperiments in the CARLA simulator demonstrate that VLM-RL outperforms\nstate-of-the-art baselines, achieving a 10.5\\% reduction in collision rate, a\n104.6\\% increase in route completion rate, and robust generalization to unseen\ndriving scenarios. Furthermore, VLM-RL can seamlessly integrate almost any\nstandard RL algorithms, potentially revolutionizing the existing RL paradigm\nthat relies on manual reward engineering and enabling continuous performance\nimprovements. The demo video and code can be accessed at:\nhttps://zilin-huang.github.io/VLM-RL-website.\n","authors":["Zilin Huang","Zihao Sheng","Yansong Qu","Junwei You","Sikai Chen"],"pdf_url":"https://arxiv.org/pdf/2412.15544v1.pdf","comment":"28 pages, 16 figures"},{"id":"http://arxiv.org/abs/2412.15541v1","updated":"2024-12-20T03:58:28Z","published":"2024-12-20T03:58:28Z","title":"ChangeDiff: A Multi-Temporal Change Detection Data Generator with\n  Flexible Text Prompts via Diffusion Model","summary":"  Data-driven deep learning models have enabled tremendous progress in change\ndetection (CD) with the support of pixel-level annotations. However, collecting\ndiverse data and manually annotating them is costly, laborious, and\nknowledge-intensive. Existing generative methods for CD data synthesis show\ncompetitive potential in addressing this issue but still face the following\nlimitations: 1) difficulty in flexibly controlling change events, 2) dependence\non additional data to train the data generators, 3) focus on specific change\ndetection tasks. To this end, this paper focuses on the semantic CD (SCD) task\nand develops a multi-temporal SCD data generator ChangeDiff by exploring\npowerful diffusion models. ChangeDiff innovatively generates change data in two\nsteps: first, it uses text prompts and a text-to-layout (T2L) model to create\ncontinuous layouts, and then it employs layout-to-image (L2I) to convert these\nlayouts into images. Specifically, we propose multi-class distribution-guided\ntext prompts (MCDG-TP), allowing for layouts to be generated flexibly through\ncontrollable classes and their corresponding ratios. Subsequently, to\ngeneralize the T2L model to the proposed MCDG-TP, a class distribution\nrefinement loss is further designed as training supervision. %For the former, a\nmulti-classdistribution-guided text prompt (MCDG-TP) is proposed to complement\nvia controllable classes and ratios. To generalize the text-to-image diffusion\nmodel to the proposed MCDG-TP, a class distribution refinement loss is designed\nas training supervision. For the latter, MCDG-TP in three modes is proposed to\nsynthesize new layout masks from various texts. Our generated data shows\nsignificant progress in temporal continuity, spatial diversity, and quality\nrealism, empowering change detectors with accuracy and transferability. The\ncode is available at https://github.com/DZhaoXd/ChangeDiff\n","authors":["Qi Zang","Jiayi Yang","Shuang Wang","Dong Zhao","Wenjun Yi","Zhun Zhong"],"pdf_url":"https://arxiv.org/pdf/2412.15541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22306v2","updated":"2024-12-20T03:52:45Z","published":"2024-10-29T17:52:20Z","title":"Multi-Object 3D Grounding with Dynamic Modules and Language-Informed\n  Spatial Attention","summary":"  Multi-object 3D Grounding involves locating 3D boxes based on a given query\nphrase from a point cloud. It is a challenging and significant task with\nnumerous applications in visual understanding, human-computer interaction, and\nrobotics. To tackle this challenge, we introduce D-LISA, a two-stage approach\nincorporating three innovations. First, a dynamic vision module that enables a\nvariable and learnable number of box proposals. Second, a dynamic camera\npositioning that extracts features for each proposal. Third, a\nlanguage-informed spatial attention module that better reasons over the\nproposals to output the final prediction. Empirically, experiments show that\nour method outperforms the state-of-the-art methods on multi-object 3D\ngrounding by 12.8% (absolute) and is competitive in single-object 3D grounding.\n","authors":["Haomeng Zhang","Chiao-An Yang","Raymond A. Yeh"],"pdf_url":"https://arxiv.org/pdf/2410.22306v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.15533v1","updated":"2024-12-20T03:48:12Z","published":"2024-12-20T03:48:12Z","title":"From Galaxy Zoo DECaLS to BASS/MzLS: detailed galaxy morphology\n  classification with unsupervised domain adaption","summary":"  The DESI Legacy Imaging Surveys (DESI-LIS) comprise three distinct surveys:\nthe Dark Energy Camera Legacy Survey (DECaLS), the Beijing-Arizona Sky Survey\n(BASS), and the Mayall z-band Legacy Survey (MzLS).The citizen science project\nGalaxy Zoo DECaLS 5 (GZD-5) has provided extensive and detailed morphology\nlabels for a sample of 253,287 galaxies within the DECaLS survey. This dataset\nhas been foundational for numerous deep learning-based galaxy morphology\nclassification studies. However, due to differences in signal-to-noise ratios\nand resolutions between the DECaLS images and those from BASS and MzLS\n(collectively referred to as BMz), a neural network trained on DECaLS images\ncannot be directly applied to BMz images due to distributional mismatch.In this\nstudy, we explore an unsupervised domain adaptation (UDA) method that\nfine-tunes a source domain model trained on DECaLS images with GZD-5 labels to\nBMz images, aiming to reduce bias in galaxy morphology classification within\nthe BMz survey. Our source domain model, used as a starting point for UDA,\nachieves performance on the DECaLS galaxies' validation set comparable to the\nresults of related works. For BMz galaxies, the fine-tuned target domain model\nsignificantly improves performance compared to the direct application of the\nsource domain model, reaching a level comparable to that of the source domain.\nWe also release a catalogue of detailed morphology classifications for 248,088\ngalaxies within the BMz survey, accompanied by usage recommendations.\n","authors":["Renhao Ye","Shiyin Shen","Rafael S. de Souza","Quanfeng Xu","Mi Chen","Zhu Chen","Emille E. O. Ishida","Alberto Krone-Martins","Rupesh Durgesh"],"pdf_url":"https://arxiv.org/pdf/2412.15533v1.pdf","comment":"11 pages, 6 figures, accepted for publication in MNRAS"},{"id":"http://arxiv.org/abs/2412.11077v3","updated":"2024-12-20T03:42:24Z","published":"2024-12-15T06:22:20Z","title":"Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for\n  Training-Free Zero-Shot Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) aims to retrieve target images that closely\nresemble a reference image while integrating user-specified textual\nmodifications, thereby capturing user intent more precisely. Existing\ntraining-free zero-shot CIR (ZS-CIR) methods often employ a two-stage process:\nthey first generate a caption for the reference image and then use Large\nLanguage Models for reasoning to obtain a target description. However, these\nmethods suffer from missing critical visual details and limited reasoning\ncapabilities, leading to suboptimal retrieval performance. To address these\nchallenges, we propose a novel, training-free one-stage method, One-Stage\nReflective Chain-of-Thought Reasoning for ZS-CIR (OSrCIR), which employs\nMultimodal Large Language Models to retain essential visual information in a\nsingle-stage reasoning process, eliminating the information loss seen in\ntwo-stage methods. Our Reflective Chain-of-Thought framework further improves\ninterpretative accuracy by aligning manipulation intent with contextual cues\nfrom reference images. OSrCIR achieves performance gains of 1.80% to 6.44% over\nexisting training-free methods across multiple tasks, setting new\nstate-of-the-art results in ZS-CIR and enhancing its utility in vision-language\napplications. Our code will be available at\nhttps://github.com/Pter61/osrcir2024/.\n","authors":["Yuanmin Tang","Xiaoting Qin","Jue Zhang","Jing Yu","Gaopeng Gou","Gang Xiong","Qingwei Ling","Saravan Rajmohan","Dongmei Zhang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2412.11077v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15527v1","updated":"2024-12-20T03:31:45Z","published":"2024-12-20T03:31:45Z","title":"Underwater Image Quality Assessment: A Perceptual Framework Guided by\n  Physical Imaging","summary":"  In this paper, we propose a physically imaging-guided framework for\nunderwater image quality assessment (UIQA), called PIGUIQA. First, we formulate\nUIQA as a comprehensive problem that considers the combined effects of direct\ntransmission attenuation and backwards scattering on image perception. On this\nbasis, we incorporate advanced physics-based underwater imaging estimation into\nour method and define distortion metrics that measure the impact of direct\ntransmission attenuation and backwards scattering on image quality. Second,\nacknowledging the significant content differences across various regions of an\nimage and the varying perceptual sensitivity to distortions in these regions,\nwe design a local perceptual module on the basis of the neighborhood attention\nmechanism. This module effectively captures subtle features in images, thereby\nenhancing the adaptive perception of distortions on the basis of local\ninformation. Finally, by employing a global perceptual module to further\nintegrate the original image content with underwater image distortion\ninformation, the proposed model can accurately predict the image quality score.\nComprehensive experiments demonstrate that PIGUIQA achieves state-of-the-art\nperformance in underwater image quality prediction and exhibits strong\ngeneralizability. The code for PIGUIQA is available on\nhttps://anonymous.4open.science/r/PIGUIQA-A465/\n","authors":["Weizhi Xian","Mingliang Zhou","Leong Hou U","Lang Shujun","Bin Fang","Tao Xiang","Zhaowei Shang"],"pdf_url":"https://arxiv.org/pdf/2412.15527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15526v1","updated":"2024-12-20T03:31:33Z","published":"2024-12-20T03:31:33Z","title":"SGTC: Semantic-Guided Triplet Co-training for Sparsely Annotated\n  Semi-Supervised Medical Image Segmentation","summary":"  Although semi-supervised learning has made significant advances in the field\nof medical image segmentation, fully annotating a volumetric sample slice by\nslice remains a costly and time-consuming task. Even worse, most of the\nexisting approaches pay much attention to image-level information and ignore\nsemantic features, resulting in the inability to perceive weak boundaries. To\naddress these issues, we propose a novel Semantic-Guided Triplet Co-training\n(SGTC) framework, which achieves high-end medical image segmentation by only\nannotating three orthogonal slices of a few volumetric samples, significantly\nalleviating the burden of radiologists. Our method consist of two main\ncomponents. Specifically, to enable semantic-aware, fine-granular segmentation\nand enhance the quality of pseudo-labels, a novel semantic-guided auxiliary\nlearning mechanism is proposed based on the pretrained CLIP. In addition,\nfocusing on a more challenging but clinically realistic scenario, a new\ntriple-view disparity training strategy is proposed, which uses sparse\nannotations (i.e., only three labeled slices of a few volumes) to perform\nco-training between three sub-networks, significantly improving the robustness.\nExtensive experiments on three public medical datasets demonstrate that our\nmethod outperforms most state-of-the-art semi-supervised counterparts under\nsparse annotation settings. The source code is available at\nhttps://github.com/xmeimeimei/SGTC.\n","authors":["Ke Yan","Qing Cai","Fan Zhang","Ziyan Cao","Zhi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.15526v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2407.15894v3","updated":"2024-12-20T03:26:52Z","published":"2024-07-22T03:51:16Z","title":"Craft: Cross-modal Aligned Features Improve Robustness of Prompt Tuning","summary":"  Prompt Tuning has emerged as a prominent research paradigm for adapting\nvision-language models to various downstream tasks. However, recent research\nindicates that prompt tuning methods often lead to overfitting due to limited\ntraining samples. In this paper, we propose a Cross-modal Aligned Feature\nTuning (Craft) method to address this issue. Cross-modal alignment is conducted\nby first selecting anchors from the alternative domain and deriving relative\nrepresentations of the embeddings for the selected anchors. Optimizing for a\nfeature alignment loss over anchor-aligned text and image modalities creates a\nmore unified text-image common space. Overfitting in prompt tuning also\ndeteriorates model performance on out-of-distribution samples. To further\nimprove the prompt model's robustness, we propose minimizing Maximum Mean\nDiscrepancy (MMD) over the anchor-aligned feature spaces to mitigate domain\nshift. The experiment on four different prompt tuning structures consistently\nshows the improvement of our method, with increases of up to $6.1\\%$ in the\nBase-to-Novel generalization task, $5.8\\%$ in the group robustness task, and\n$2.7\\%$ in the out-of-distribution tasks. The code will be available at\nhttps://github.com/Jingchensun/Craft\n","authors":["Jingchen Sun","Rohan Sharma","Vishnu Suresh Lokhande","Changyou Chen"],"pdf_url":"https://arxiv.org/pdf/2407.15894v3.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2412.15523v1","updated":"2024-12-20T03:23:26Z","published":"2024-12-20T03:23:26Z","title":"InstructOCR: Instruction Boosting Scene Text Spotting","summary":"  In the field of scene text spotting, previous OCR methods primarily relied on\nimage encoders and pre-trained text information, but they often overlooked the\nadvantages of incorporating human language instructions. To address this gap,\nwe propose InstructOCR, an innovative instruction-based scene text spotting\nmodel that leverages human language instructions to enhance the understanding\nof text within images. Our framework employs both text and image encoders\nduring training and inference, along with instructions meticulously designed\nbased on text attributes. This approach enables the model to interpret text\nmore accurately and flexibly. Extensive experiments demonstrate the\neffectiveness of our model and we achieve state-of-the-art results on widely\nused benchmarks. Furthermore, the proposed framework can be seamlessly applied\nto scene text VQA tasks. By leveraging instruction strategies during\npre-training, the performance on downstream VQA tasks can be significantly\nimproved, with a 2.6% increase on the TextVQA dataset and a 2.1% increase on\nthe ST-VQA dataset. These experimental results provide insights into the\nbenefits of incorporating human language instructions for OCR-related tasks.\n","authors":["Chen Duan","Qianyi Jiang","Pei Fu","Jiamin Chen","Shengxi Li","Zining Wang","Shan Guo","Junfeng Luo"],"pdf_url":"https://arxiv.org/pdf/2412.15523v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2401.02041v2","updated":"2024-12-20T03:10:55Z","published":"2024-01-04T02:56:50Z","title":"Towards Efficient Object Re-Identification with A Novel Cloud-Edge\n  Collaborative Framework","summary":"  Object re-identification (ReID) is committed to searching for objects of the\nsame identity across cameras, and its real-world deployment is gradually\nincreasing. Current ReID methods assume that the deployed system follows the\ncentralized processing paradigm, i.e., all computations are conducted in the\ncloud server and edge devices are only used to capture images. As the number of\nvideos experiences a rapid escalation, this paradigm has become impractical due\nto the finite computational resources in the cloud server. Therefore, the ReID\nsystem should be converted to fit in the cloud-edge collaborative processing\nparadigm, which is crucial to boost its scalability and practicality. However,\ncurrent works lack relevant research on this important specific issue, making\nit difficult to adapt them into a cloud-edge framework effectively. In this\npaper, we propose a cloud-edge collaborative inference framework for ReID\nsystems, aiming to expedite the return of the desired image captured by the\ncamera to the cloud server by learning the spatial-temporal correlations among\nobjects. In the system, a Distribution-aware Correlation Modeling network\n(DaCM) is particularly proposed to embed the spatial-temporal correlations of\nthe camera network implicitly into a graph structure, and it can be applied 1)\nin the cloud to regulate the size of the upload window and 2) on the edge\ndevice to adjust the sequence of images, respectively. Notably, the proposed\nDaCM can be seamlessly combined with traditional ReID methods, enabling their\napplication within our proposed edge-cloud collaborative framework. Extensive\nexperiments demonstrate that our method obviously reduces transmission overhead\nand significantly improves performance.\n","authors":["Chuanming Wang","Yuxin Yang","Mengshi Qi","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2401.02041v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15515v1","updated":"2024-12-20T03:02:42Z","published":"2024-12-20T03:02:42Z","title":"Reconstruction of Contour Lines During the Digitization of Contour Maps\n  to Build a Digital Elevation Model","summary":"  Contour map has contour lines that are significant in building a Digital\nElevation Model (DEM). During the digitization and pre-processing of contour\nmaps, the contour line intersects with each other or break apart resulting in\nbroken contour segments. These broken segments impose a greater risk while\nbuilding DEM leading to a faulty model. In this project, a simple yet efficient\nmechanism is used to match and reconnect the endpoints of the broken segments\naccurately and efficiently. The matching of the endpoints is done using the\nconcept of minimum Euclidean distance and gradient direction while the Cubic\nHermite spline interpolation technique is used to reconnect the endpoints by\nestimating the values using a mathematical function that minimizes overall\nsurface curvature resulting in a smooth curve. The purpose of this work is to\nreconnect the broken contour lines generated during the digitization of the\ncontour map, to help build the most appropriate digital elevation model for the\ncorresponding contour map.\n","authors":["Aroj Subedi","Pradip Ganesh","Sandip Mishra"],"pdf_url":"https://arxiv.org/pdf/2412.15515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15514v1","updated":"2024-12-20T02:59:59Z","published":"2024-12-20T02:59:59Z","title":"PolySmart @ TRECVid 2024 Medical Video Question Answering","summary":"  Video Corpus Visual Answer Localization (VCVAL) includes question-related\nvideo retrieval and visual answer localization in the videos. Specifically, we\nuse text-to-text retrieval to find relevant videos for a medical question based\non the similarity of video transcript and answers generated by GPT4. For the\nvisual answer localization, the start and end timestamps of the answer are\npredicted by the alignments on both visual content and subtitles with queries.\nFor the Query-Focused Instructional Step Captioning (QFISC) task, the step\ncaptions are generated by GPT4. Specifically, we provide the video captions\ngenerated by the LLaVA-Next-Video model and the video subtitles with timestamps\nas context, and ask GPT4 to generate step captions for the given medical query.\nWe only submit one run for evaluation and it obtains a F-score of 11.92 and\nmean IoU of 9.6527.\n","authors":["Jiaxin Wu","Yiyang Jiang","Xiao-Yong Wei","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2412.15514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15511v1","updated":"2024-12-20T02:55:07Z","published":"2024-12-20T02:55:07Z","title":"RESQUE: Quantifying Estimator to Task and Distribution Shift for\n  Sustainable Model Reusability","summary":"  As a strategy for sustainability of deep learning, reusing an existing model\nby retraining it rather than training a new model from scratch is critical. In\nthis paper, we propose REpresentation Shift QUantifying Estimator (RESQUE), a\npredictive quantifier to estimate the retraining cost of a model to\ndistributional shifts or change of tasks. It provides a single concise index\nfor an estimate of resources required for retraining the model. Through\nextensive experiments, we show that RESQUE has a strong correlation with\nvarious retraining measures. Our results validate that RESQUE is an effective\nindicator in terms of epochs, gradient norms, changes of parameter magnitude,\nenergy, and carbon emissions. These measures align well with RESQUE for new\ntasks, multiple noise types, and varying noise intensities. As a result, RESQUE\nenables users to make informed decisions for retraining to different\ntasks/distribution shifts and determine the most cost-effective and sustainable\noption, allowing for the reuse of a model with a much smaller footprint in the\nenvironment. The code for this work is available here:\nhttps://github.com/JEKimLab/AAAI2025RESQUE\n","authors":["Vishwesh Sangarya","Jung-Eun Kim"],"pdf_url":"https://arxiv.org/pdf/2412.15511v1.pdf","comment":"The Annual AAAI Conference on Artificial Intelligence (AAAI), 2025"},{"id":"http://arxiv.org/abs/2412.15509v1","updated":"2024-12-20T02:45:37Z","published":"2024-12-20T02:45:37Z","title":"PolySmart @ TRECVid 2024 Video-To-Text","summary":"  In this paper, we present our methods and results for the Video-To-Text (VTT)\ntask at TRECVid 2024, exploring the capabilities of Vision-Language Models\n(VLMs) like LLaVA and LLaVA-NeXT-Video in generating natural language\ndescriptions for video content. We investigate the impact of fine-tuning VLMs\non VTT datasets to enhance description accuracy, contextual relevance, and\nlinguistic consistency. Our analysis reveals that fine-tuning substantially\nimproves the model's ability to produce more detailed and domain-aligned text,\nbridging the gap between generic VLM tasks and the specialized needs of VTT.\nExperimental results demonstrate that our fine-tuned model outperforms baseline\nVLMs across various evaluation metrics, underscoring the importance of\ndomain-specific tuning for complex VTT tasks.\n","authors":["Jiaxin Wu","Wengyu Zhang","Xiao-Yong Wei","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2412.15509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15507v1","updated":"2024-12-20T02:41:15Z","published":"2024-12-20T02:41:15Z","title":"Stylish and Functional: Guided Interpolation Subject to Physical\n  Constraints","summary":"  Generative AI is revolutionizing engineering design practices by enabling\nrapid prototyping and manipulation of designs. One example of design\nmanipulation involves taking two reference design images and using them as\nprompts to generate a design image that combines aspects of both. Real\nengineering designs have physical constraints and functional requirements in\naddition to aesthetic design considerations. Internet-scale foundation models\ncommonly used for image generation, however, are unable to take these physical\nconstraints and functional requirements into consideration as part of the\ngeneration process. We consider the problem of generating a design inspired by\ntwo input designs, and propose a zero-shot framework toward enforcing physical,\nfunctional requirements over the generation process by leveraging a pretrained\ndiffusion model as the backbone. As a case study, we consider the example of\nrotational symmetry in generation of wheel designs. Automotive wheels are\nrequired to be rotationally symmetric for physical stability. We formulate the\nrequirement of rotational symmetry by the use of a symmetrizer, and we use this\nsymmetrizer to guide the diffusion process towards symmetric wheel generations.\nOur experimental results find that the proposed approach makes generated\ninterpolations with higher realism than methods in related work, as evaluated\nby Fr\\'echet inception distance (FID). We also find that our approach generates\ndesigns that more closely satisfy physical and functional requirements than\ngenerating without the symmetry guidance.\n","authors":["Yan-Ying Chen","Nikos Arechiga","Chenyang Yuan","Matthew Hong","Matt Klenk","Charlene Wu"],"pdf_url":"https://arxiv.org/pdf/2412.15507v1.pdf","comment":"Accepted by Foundation Models for Science Workshop, 38th Conference\n  on Neural Information Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2411.01988v3","updated":"2024-12-20T02:40:28Z","published":"2024-11-04T11:20:17Z","title":"QCS:Feature Refining from Quadruplet Cross Similarity for Facial\n  Expression Recognition","summary":"  Facial expression recognition faces challenges where labeled significant\nfeatures in datasets are mixed with unlabeled redundant ones. In this paper, we\nintroduce Cross Similarity Attention (CSA) to mine richer intrinsic information\nfrom image pairs, overcoming a limitation when the Scaled Dot-Product Attention\nof ViT is directly applied to calculate the similarity between two different\nimages. Based on CSA, we simultaneously minimize intra-class differences and\nmaximize inter-class differences at the fine-grained feature level through\ninteractions among multiple branches. Contrastive residual distillation is\nutilized to transfer the information learned in the cross module back to the\nbase network. We ingeniously design a four-branch centrally symmetric network,\nnamed Quadruplet Cross Similarity (QCS), which alleviates gradient conflicts\narising from the cross module and achieves balanced and stable training. It can\nadaptively extract discriminative features while isolating redundant ones. The\ncross-attention modules exist during training, and only one base branch is\nretained during inference, resulting in no increase in inference time.\nExtensive experiments show that our proposed method achieves state-of-the-art\nperformance on several FER datasets.\n","authors":["Chengpeng Wang","Li Chen","Lili Wang","Zhaofan Li","Xuebin Lv"],"pdf_url":"https://arxiv.org/pdf/2411.01988v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15499v1","updated":"2024-12-20T02:25:31Z","published":"2024-12-20T02:25:31Z","title":"A Robust Prototype-Based Network with Interpretable RBF Classifier\n  Foundations","summary":"  Prototype-based classification learning methods are known to be inherently\ninterpretable. However, this paradigm suffers from major limitations compared\nto deep models, such as lower performance. This led to the development of the\nso-called deep Prototype-Based Networks (PBNs), also known as prototypical\nparts models. In this work, we analyze these models with respect to different\nproperties, including interpretability. In particular, we focus on the\nClassification-by-Components (CBC) approach, which uses a probabilistic model\nto ensure interpretability and can be used as a shallow or deep architecture.\nWe show that this model has several shortcomings, like creating contradicting\nexplanations. Based on these findings, we propose an extension of CBC that\nsolves these issues. Moreover, we prove that this extension has robustness\nguarantees and derive a loss that optimizes robustness. Additionally, our\nanalysis shows that most (deep) PBNs are related to (deep) RBF classifiers,\nwhich implies that our robustness guarantees generalize to shallow RBF\nclassifiers. The empirical evaluation demonstrates that our deep PBN yields\nstate-of-the-art classification accuracy on different benchmarks while\nresolving the interpretability shortcomings of other approaches. Further, our\nshallow PBN variant outperforms other shallow PBNs while being inherently\ninterpretable and exhibiting provable robustness guarantees.\n","authors":["Sascha Saralajew","Ashish Rana","Thomas Villmann","Ammar Shaker"],"pdf_url":"https://arxiv.org/pdf/2412.15499v1.pdf","comment":"To appear at AAAI 2025. Includes the Appendix"},{"id":"http://arxiv.org/abs/2412.15491v1","updated":"2024-12-20T02:13:11Z","published":"2024-12-20T02:13:11Z","title":"GCA-3D: Towards Generalized and Consistent Domain Adaptation of 3D\n  Generators","summary":"  Recently, 3D generative domain adaptation has emerged to adapt the\npre-trained generator to other domains without collecting massive datasets and\ncamera pose distributions. Typically, they leverage large-scale pre-trained\ntext-to-image diffusion models to synthesize images for the target domain and\nthen fine-tune the 3D model. However, they suffer from the tedious pipeline of\ndata generation, which inevitably introduces pose bias between the source\ndomain and synthetic dataset. Furthermore, they are not generalized to support\none-shot image-guided domain adaptation, which is more challenging due to the\nmore severe pose bias and additional identity bias introduced by the single\nimage reference. To address these issues, we propose GCA-3D, a generalized and\nconsistent 3D domain adaptation method without the intricate pipeline of data\ngeneration. Different from previous pipeline methods, we introduce multi-modal\ndepth-aware score distillation sampling loss to efficiently adapt 3D generative\nmodels in a non-adversarial manner. This multi-modal loss enables GCA-3D in\nboth text prompt and one-shot image prompt adaptation. Besides, it leverages\nper-instance depth maps from the volume rendering module to mitigate the\noverfitting problem and retain the diversity of results. To enhance the pose\nand identity consistency, we further propose a hierarchical spatial consistency\nloss to align the spatial structure between the generated images in the source\nand target domain. Experiments demonstrate that GCA-3D outperforms previous\nmethods in terms of efficiency, generalization, pose accuracy, and identity\nconsistency.\n","authors":["Hengjia Li","Yang Liu","Yibo Zhao","Haoran Cheng","Yang Yang","Linxuan Xia","Zekai Luo","Qibo Qiu","Boxi Wu","Tu Zheng","Zheng Yang","Deng Cai"],"pdf_url":"https://arxiv.org/pdf/2412.15491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19391v3","updated":"2024-12-20T02:12:06Z","published":"2024-06-27T17:59:40Z","title":"Fibottention: Inceptive Visual Representation Learning with Diverse\n  Attention Across Heads","summary":"  Transformer architectures such as Vision Transformers (ViT) have proven\neffective for solving visual perception tasks. However, they suffer from two\nmajor limitations; first, the quadratic complexity of self-attention limits the\nnumber of tokens that can be processed, and second, Transformers often require\nlarge amounts of training data to attain state-of-the-art performance. In this\npaper, we propose a new multi-head self-attention (MHSA) variant named\nFibottention, which can replace MHSA in Transformer architectures. Fibottention\nis data-efficient and computationally more suitable for processing large\nnumbers of tokens than the standard MHSA. It employs structured sparse\nattention based on dilated Fibonacci sequences, which, uniquely, differ across\nattention heads, resulting in inception-like diverse features across heads. The\nspacing of the Fibonacci sequences follows the Wythoff array, which minimizes\nthe redundancy of token interactions aggregated across different attention\nheads, while still capturing sufficient complementary information through token\npair interactions. These sparse attention patterns are unique among the\nexisting sparse attention and lead to an $O(N \\log N)$ complexity, where $N$ is\nthe number of tokens. Leveraging only 2-6% of the elements in the\nself-attention heads, Fibottention embedded into popular, state-of-the-art\nTransformer architectures can achieve significantly improved predictive\nperformance for domains with limited data such as image classification, video\nunderstanding, and robot learning tasks, and render reduced computational\ncomplexity. We further validated the improved diversity of feature\nrepresentations resulting from different self-attention heads, and our model\ndesign against other sparse attention mechanisms.\n","authors":["Ali Khaleghi Rahimian","Manish Kumar Govind","Subhajit Maity","Dominick Reilly","Christian Kümmerle","Srijan Das","Aritra Dutta"],"pdf_url":"https://arxiv.org/pdf/2406.19391v3.pdf","comment":"The complete implementation, including source code and evaluation\n  scripts, is publicly available at:\n  https://github.com/Charlotte-CharMLab/Fibottention"},{"id":"http://arxiv.org/abs/2412.15486v1","updated":"2024-12-20T01:48:37Z","published":"2024-12-20T01:48:37Z","title":"Toward Appearance-based Autonomous Landing Site Identification for\n  Multirotor Drones in Unstructured Environments","summary":"  A remaining challenge in multirotor drone flight is the autonomous\nidentification of viable landing sites in unstructured environments. One\napproach to solve this problem is to create lightweight, appearance-based\nterrain classifiers that can segment a drone's RGB images into safe and unsafe\nregions. However, such classifiers require data sets of images and masks that\ncan be prohibitively expensive to create. We propose a pipeline to\nautomatically generate synthetic data sets to train these classifiers,\nleveraging modern drones' ability to survey terrain automatically and the\nability to automatically calculate landing safety masks from terrain models\nderived from such surveys. We then train a U-Net on the synthetic data set,\ntest it on real-world data for validation, and demonstrate it on our drone\nplatform in real-time.\n","authors":["Joshua Springer","Gylfi Þór Guðmundsson","Marcel Kyas"],"pdf_url":"https://arxiv.org/pdf/2412.15486v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.15484v1","updated":"2024-12-20T01:37:22Z","published":"2024-12-20T01:37:22Z","title":"Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and\n  Dual Evaluation Metrics for Factuality and Coverage","summary":"  Multimodal large language models (MLLMs) excel at generating highly detailed\ncaptions but often produce hallucinations. Our analysis reveals that existing\nhallucination detection methods struggle with detailed captions. We attribute\nthis to the increasing reliance of MLLMs on their generated text, rather than\nthe input image, as the sequence length grows. To address this issue, we\npropose a multiagent approach that leverages LLM-MLLM collaboration to correct\ngiven captions. Additionally, we introduce an evaluation framework and a\nbenchmark dataset to facilitate the systematic analysis of detailed captions.\nOur experiments demonstrate that our proposed evaluation method better aligns\nwith human judgments of factuality than existing metrics and that existing\napproaches to improve the MLLM factuality may fall short in hyper-detailed\nimage captioning tasks. In contrast, our proposed method significantly enhances\nthe factual accuracy of captions, even improving those generated by GPT-4V.\nFinally, we highlight a limitation of VQA-centric benchmarking by demonstrating\nthat an MLLM's performance on VQA benchmarks may not correlate with its ability\nto generate detailed image captions.\n","authors":["Saehyung Lee","Seunghyun Yoon","Trung Bui","Jing Shi","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2412.15484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15483v1","updated":"2024-12-20T01:33:43Z","published":"2024-12-20T01:33:43Z","title":"Task-Specific Preconditioner for Cross-Domain Few-Shot Learning","summary":"  Cross-Domain Few-Shot Learning~(CDFSL) methods typically parameterize models\nwith task-agnostic and task-specific parameters. To adapt task-specific\nparameters, recent approaches have utilized fixed optimization strategies,\ndespite their potential sub-optimality across varying domains or target tasks.\nTo address this issue, we propose a novel adaptation mechanism called\nTask-Specific Preconditioned gradient descent~(TSP). Our method first\nmeta-learns Domain-Specific Preconditioners~(DSPs) that capture the\ncharacteristics of each meta-training domain, which are then linearly combined\nusing task-coefficients to form the Task-Specific Preconditioner. The\npreconditioner is applied to gradient descent, making the optimization adaptive\nto the target task. We constrain our preconditioners to be positive definite,\nguiding the preconditioned gradient toward the direction of steepest descent.\nEmpirical evaluations on the Meta-Dataset show that TSP achieves\nstate-of-the-art performance across diverse experimental scenarios.\n","authors":["Suhyun Kang","Jungwon Park","Wonseok Lee","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2412.15483v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2406.15334v3","updated":"2024-12-20T01:24:51Z","published":"2024-06-21T17:50:02Z","title":"Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning","summary":"  The recent success of interleaved Large Multimodal Models (LMMs) in few-shot\nlearning suggests that in-context learning (ICL) with many examples can be\npromising for learning new tasks. However, this many-shot multimodal ICL\nsetting has one crucial problem: it is fundamentally limited by the model's\ncontext length set at pretraining. The problem is especially prominent in the\nmultimodal domain, which processes both text and images, requiring additional\ntokens. This motivates the need for a multimodal method to compress many shots\ninto fewer tokens without finetuning. In this work, we enable LMMs to perform\nmultimodal, many-shot in-context learning by leveraging Multimodal Task Vectors\n(MTV) -- compact implicit representations of in-context examples compressed in\nthe model's attention heads. Specifically, we first demonstrate the existence\nof such MTV in LMMs and then leverage these extracted MTV to enable many-shot\nin-context learning for various vision-and-language tasks. Our experiments\nsuggest that MTV can scale in performance with the number of compressed shots\nand generalize to similar out-of-domain tasks without additional context length\nfor inference. Code: https://github.com/Brandon3964/MultiModal-Task-Vector\n","authors":["Brandon Huang","Chancharik Mitra","Assaf Arbelle","Leonid Karlinsky","Trevor Darrell","Roei Herzig"],"pdf_url":"https://arxiv.org/pdf/2406.15334v3.pdf","comment":"Published in NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.15477v1","updated":"2024-12-20T01:11:30Z","published":"2024-12-20T01:11:30Z","title":"Difficulty-aware Balancing Margin Loss for Long-tailed Recognition","summary":"  When trained with severely imbalanced data, deep neural networks often\nstruggle to accurately recognize classes with only a few samples. Previous\nstudies in long-tailed recognition have attempted to rebalance biased learning\nusing known sample distributions, primarily addressing different classification\ndifficulties at the class level. However, these approaches often overlook the\ninstance difficulty variation within each class. In this paper, we propose a\ndifficulty-aware balancing margin (DBM) loss, which considers both class\nimbalance and instance difficulty. DBM loss comprises two components: a\nclass-wise margin to mitigate learning bias caused by imbalanced class\nfrequencies, and an instance-wise margin assigned to hard positive samples\nbased on their individual difficulty. DBM loss improves class discriminativity\nby assigning larger margins to more difficult samples. Our method seamlessly\ncombines with existing approaches and consistently improves performance across\nvarious long-tailed recognition benchmarks.\n","authors":["Minseok Son","Inyong Koo","Jinyoung Park","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2412.15477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12172v2","updated":"2024-12-20T00:32:57Z","published":"2023-09-21T15:28:04Z","title":"SANPO: A Scene Understanding, Accessibility and Human Navigation Dataset","summary":"  Vision is essential for human navigation. The World Health Organization (WHO)\nestimates that 43.3 million people were blind in 2020, and this number is\nprojected to reach 61 million by 2050. Modern scene understanding models could\nempower these people by assisting them with navigation, obstacle avoidance and\nvisual recognition capabilities. The research community needs high quality\ndatasets for both training and evaluation to build these systems. While\ndatasets for autonomous vehicles are abundant, there is a critical gap in\ndatasets tailored for outdoor human navigation. This gap poses a major obstacle\nto the development of computer vision based Assistive Technologies. To overcome\nthis obstacle, we present SANPO, a large-scale egocentric video dataset\ndesigned for dense prediction in outdoor human navigation environments. SANPO\ncontains 701 stereo videos of 30+ seconds captured in diverse real-world\noutdoor environments across four geographic locations in the USA. Every frame\nhas a high resolution depth map and 112K frames were annotated with temporally\nconsistent dense video panoptic segmentation labels. The dataset also includes\n1961 high-quality synthetic videos with pixel accurate depth and panoptic\nsegmentation annotations to balance the noisy real world annotations with the\nhigh precision synthetic annotations.\n  SANPO is already publicly available and is being used by mobile applications\nlike Project Guideline to train mobile models that help low-vision users go\nrunning outdoors independently. To preserve anonymization during peer review,\nwe will provide a link to our dataset upon acceptance.\n  SANPO is available here:\nhttps://google-research-datasets.github.io/sanpo_dataset/\n","authors":["Sagar M. Waghmare","Kimberly Wilber","Dave Hawkey","Xuan Yang","Matthew Wilson","Stephanie Debats","Cattalyya Nuengsigkapian","Astuti Sharma","Lars Pandikow","Huisheng Wang","Hartwig Adam","Mikhail Sirotenko"],"pdf_url":"https://arxiv.org/pdf/2309.12172v2.pdf","comment":"WACV2025 submission version. 8 pages, plus supplementary material"}]},"2024-12-23T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.17812v1","updated":"2024-12-23T18:59:49Z","published":"2024-12-23T18:59:49Z","title":"FaceLift: Single Image to 3D Head with View Generation and GS-LRM","summary":"  We present FaceLift, a feed-forward approach for rapid, high-quality,\n360-degree head reconstruction from a single image. Our pipeline begins by\nemploying a multi-view latent diffusion model that generates consistent side\nand back views of the head from a single facial input. These generated views\nthen serve as input to a GS-LRM reconstructor, which produces a comprehensive\n3D representation using Gaussian splats. To train our system, we develop a\ndataset of multi-view renderings using synthetic 3D human head as-sets. The\ndiffusion-based multi-view generator is trained exclusively on synthetic head\nimages, while the GS-LRM reconstructor undergoes initial training on Objaverse\nfollowed by fine-tuning on synthetic head data. FaceLift excels at preserving\nidentity and maintaining view consistency across views. Despite being trained\nsolely on synthetic data, FaceLift demonstrates remarkable generalization to\nreal-world images. Through extensive qualitative and quantitative evaluations,\nwe show that FaceLift outperforms state-of-the-art methods in 3D head\nreconstruction, highlighting its practical applicability and robust performance\non real-world images. In addition to single image reconstruction, FaceLift\nsupports video inputs for 4D novel view synthesis and seamlessly integrates\nwith 2D reanimation techniques to enable 3D facial animation. Project page:\nhttps://weijielyu.github.io/FaceLift.\n","authors":["Weijie Lyu","Yi Zhou","Ming-Hsuan Yang","Zhixin Shu"],"pdf_url":"https://arxiv.org/pdf/2412.17812v1.pdf","comment":"Project page: https://weijielyu.github.io/FaceLift"},{"id":"http://arxiv.org/abs/2412.17811v1","updated":"2024-12-23T18:59:28Z","published":"2024-12-23T18:59:28Z","title":"ChatGarment: Garment Estimation, Generation and Editing via Large\n  Language Models","summary":"  We introduce ChatGarment, a novel approach that leverages large\nvision-language models (VLMs) to automate the estimation, generation, and\nediting of 3D garments from images or text descriptions. Unlike previous\nmethods that struggle in real-world scenarios or lack interactive editing\ncapabilities, ChatGarment can estimate sewing patterns from in-the-wild images\nor sketches, generate them from text descriptions, and edit garments based on\nuser instructions, all within an interactive dialogue. These sewing patterns\ncan then be draped into 3D garments, which are easily animatable and\nsimulatable. This is achieved by finetuning a VLM to directly generate a JSON\nfile that includes both textual descriptions of garment types and styles, as\nwell as continuous numerical attributes. This JSON file is then used to create\nsewing patterns through a programming parametric model. To support this, we\nrefine the existing programming model, GarmentCode, by expanding its garment\ntype coverage and simplifying its structure for efficient VLM fine-tuning.\nAdditionally, we construct a large-scale dataset of image-to-sewing-pattern and\ntext-to-sewing-pattern pairs through an automated data pipeline. Extensive\nevaluations demonstrate ChatGarment's ability to accurately reconstruct,\ngenerate, and edit garments from multimodal inputs, highlighting its potential\nto revolutionize workflows in fashion and gaming applications. Code and data\nwill be available at https://chatgarment.github.io/.\n","authors":["Siyuan Bian","Chenghao Xu","Yuliang Xiu","Artur Grigorev","Zhen Liu","Cewu Lu","Michael J. Black","Yao Feng"],"pdf_url":"https://arxiv.org/pdf/2412.17811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17808v1","updated":"2024-12-23T18:59:06Z","published":"2024-12-23T18:59:06Z","title":"Dora: Sampling and Benchmarking for 3D Shape Variational Auto-Encoders","summary":"  Recent 3D content generation pipelines commonly employ Variational\nAutoencoders (VAEs) to encode shapes into compact latent representations for\ndiffusion-based generation. However, the widely adopted uniform point sampling\nstrategy in Shape VAE training often leads to a significant loss of geometric\ndetails, limiting the quality of shape reconstruction and downstream generation\ntasks. We present Dora-VAE, a novel approach that enhances VAE reconstruction\nthrough our proposed sharp edge sampling strategy and a dual cross-attention\nmechanism. By identifying and prioritizing regions with high geometric\ncomplexity during training, our method significantly improves the preservation\nof fine-grained shape features. Such sampling strategy and the dual attention\nmechanism enable the VAE to focus on crucial geometric details that are\ntypically missed by uniform sampling approaches. To systematically evaluate VAE\nreconstruction quality, we additionally propose Dora-bench, a benchmark that\nquantifies shape complexity through the density of sharp edges, introducing a\nnew metric focused on reconstruction accuracy at these salient geometric\nfeatures. Extensive experiments on the Dora-bench demonstrate that Dora-VAE\nachieves comparable reconstruction quality to the state-of-the-art dense\nXCube-VAE while requiring a latent space at least 8$\\times$ smaller (1,280 vs.\n> 10,000 codes). We will release our code and benchmark dataset to facilitate\nfuture research in 3D shape modeling.\n","authors":["Rui Chen","Jianfeng Zhang","Yixun Liang","Guan Luo","Weiyu Li","Jiarui Liu","Xiu Li","Xiaoxiao Long","Jiashi Feng","Ping Tan"],"pdf_url":"https://arxiv.org/pdf/2412.17808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17807v1","updated":"2024-12-23T18:58:39Z","published":"2024-12-23T18:58:39Z","title":"Cross-View Referring Multi-Object Tracking","summary":"  Referring Multi-Object Tracking (RMOT) is an important topic in the current\ntracking field. Its task form is to guide the tracker to track objects that\nmatch the language description. Current research mainly focuses on referring\nmulti-object tracking under single-view, which refers to a view sequence or\nmultiple unrelated view sequences. However, in the single-view, some\nappearances of objects are easily invisible, resulting in incorrect matching of\nobjects with the language description. In this work, we propose a new task,\ncalled Cross-view Referring Multi-Object Tracking (CRMOT). It introduces the\ncross-view to obtain the appearances of objects from multiple views, avoiding\nthe problem of the invisible appearances of objects in RMOT task. CRMOT is a\nmore challenging task of accurately tracking the objects that match the\nlanguage description and maintaining the identity consistency of objects in\neach cross-view. To advance CRMOT task, we construct a cross-view referring\nmulti-object tracking benchmark based on CAMPUS and DIVOTrack datasets, named\nCRTrack. Specifically, it provides 13 different scenes and 221 language\ndescriptions. Furthermore, we propose an end-to-end cross-view referring\nmulti-object tracking method, named CRTracker. Extensive experiments on the\nCRTrack benchmark verify the effectiveness of our method. The dataset and code\nare available at https://github.com/chen-si-jia/CRMOT.\n","authors":["Sijia Chen","En Yu","Wenbing Tao"],"pdf_url":"https://arxiv.org/pdf/2412.17807v1.pdf","comment":"Accepted by AAAI 2025!"},{"id":"http://arxiv.org/abs/2412.17806v1","updated":"2024-12-23T18:58:34Z","published":"2024-12-23T18:58:34Z","title":"Reconstructing People, Places, and Cameras","summary":"  We present \"Humans and Structure from Motion\" (HSfM), a method for jointly\nreconstructing multiple human meshes, scene point clouds, and camera parameters\nin a metric world coordinate system from a sparse set of uncalibrated\nmulti-view images featuring people. Our approach combines data-driven scene\nreconstruction with the traditional Structure-from-Motion (SfM) framework to\nachieve more accurate scene reconstruction and camera estimation, while\nsimultaneously recovering human meshes. In contrast to existing scene\nreconstruction and SfM methods that lack metric scale information, our method\nestimates approximate metric scale by leveraging a human statistical model.\nFurthermore, it reconstructs multiple human meshes within the same world\ncoordinate system alongside the scene point cloud, effectively capturing\nspatial relationships among individuals and their positions in the environment.\nWe initialize the reconstruction of humans, scenes, and cameras using robust\nfoundational models and jointly optimize these elements. This joint\noptimization synergistically improves the accuracy of each component. We\ncompare our method to existing approaches on two challenging benchmarks,\nEgoHumans and EgoExo4D, demonstrating significant improvements in human\nlocalization accuracy within the world coordinate frame (reducing error from\n3.51m to 1.04m in EgoHumans and from 2.9m to 0.56m in EgoExo4D). Notably, our\nresults show that incorporating human data into the SfM pipeline improves\ncamera pose estimation (e.g., increasing RRA@15 by 20.3% on EgoHumans).\nAdditionally, qualitative results show that our approach improves overall scene\nreconstruction quality. Our code is available at: muelea.github.io/hsfm.\n","authors":["Lea Müller","Hongsuk Choi","Anthony Zhang","Brent Yi","Jitendra Malik","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2412.17806v1.pdf","comment":"Project website: muelea.github.io/hsfm"},{"id":"http://arxiv.org/abs/2412.17805v1","updated":"2024-12-23T18:58:24Z","published":"2024-12-23T18:58:24Z","title":"Large Motion Video Autoencoding with Cross-modal Video VAE","summary":"  Learning a robust video Variational Autoencoder (VAE) is essential for\nreducing video redundancy and facilitating efficient video generation. Directly\napplying image VAEs to individual frames in isolation can result in temporal\ninconsistencies and suboptimal compression rates due to a lack of temporal\ncompression. Existing Video VAEs have begun to address temporal compression;\nhowever, they often suffer from inadequate reconstruction performance. In this\npaper, we present a novel and powerful video autoencoder capable of\nhigh-fidelity video encoding. First, we observe that entangling spatial and\ntemporal compression by merely extending the image VAE to a 3D VAE can\nintroduce motion blur and detail distortion artifacts. Thus, we propose\ntemporal-aware spatial compression to better encode and decode the spatial\ninformation. Additionally, we integrate a lightweight motion compression model\nfor further temporal compression. Second, we propose to leverage the textual\ninformation inherent in text-to-video datasets and incorporate text guidance\ninto our model. This significantly enhances reconstruction quality,\nparticularly in terms of detail preservation and temporal stability. Third, we\nfurther improve the versatility of our model through joint training on both\nimages and videos, which not only enhances reconstruction quality but also\nenables the model to perform both image and video autoencoding. Extensive\nevaluations against strong recent baselines demonstrate the superior\nperformance of our method. The project website can be found\nat~\\href{https://yzxing87.github.io/vae/}{https://yzxing87.github.io/vae/}.\n","authors":["Yazhou Xing","Yang Fei","Yingqing He","Jingye Chen","Jiaxin Xie","Xiaowei Chi","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2412.17805v1.pdf","comment":"Project Website: https://yzxing87.github.io/vae/"},{"id":"http://arxiv.org/abs/2412.17804v1","updated":"2024-12-23T18:58:17Z","published":"2024-12-23T18:58:17Z","title":"GauSim: Registering Elastic Objects into Digital World by Gaussian\n  Simulator","summary":"  In this work, we introduce GauSim, a novel neural network-based simulator\ndesigned to capture the dynamic behaviors of real-world elastic objects\nrepresented through Gaussian kernels. Unlike traditional methods that treat\nkernels as particles within particle-based simulations, we leverage continuum\nmechanics, modeling each kernel as a continuous piece of matter to account for\nrealistic deformations without idealized assumptions. To improve computational\nefficiency and fidelity, we employ a hierarchical structure that organizes\nkernels into Center of Mass Systems (CMS) with explicit formulations, enabling\na coarse-to-fine simulation approach. This structure significantly reduces\ncomputational overhead while preserving detailed dynamics. In addition, GauSim\nincorporates explicit physics constraints, such as mass and momentum\nconservation, ensuring interpretable results and robust, physically plausible\nsimulations. To validate our approach, we present a new dataset, READY,\ncontaining multi-view videos of real-world elastic deformations. Experimental\nresults demonstrate that GauSim achieves superior performance compared to\nexisting physics-driven baselines, offering a practical and accurate solution\nfor simulating complex dynamic behaviors. Code and model will be released.\nProject page: https://www.mmlab-ntu.com/project/gausim/index.html .\n","authors":["Yidi Shao","Mu Huang","Chen Change Loy","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2412.17804v1.pdf","comment":"Project page: https://www.mmlab-ntu.com/project/gausim/index.html"},{"id":"http://arxiv.org/abs/2412.17800v1","updated":"2024-12-23T18:57:43Z","published":"2024-12-23T18:57:43Z","title":"Comprehensive Multi-Modal Prototypes are Simple and Effective\n  Classifiers for Vast-Vocabulary Object Detection","summary":"  Enabling models to recognize vast open-world categories has been a\nlongstanding pursuit in object detection. By leveraging the generalization\ncapabilities of vision-language models, current open-world detectors can\nrecognize a broader range of vocabularies, despite being trained on limited\ncategories. However, when the scale of the category vocabularies during\ntraining expands to a real-world level, previous classifiers aligned with\ncoarse class names significantly reduce the recognition performance of these\ndetectors. In this paper, we introduce Prova, a multi-modal prototype\nclassifier for vast-vocabulary object detection. Prova extracts comprehensive\nmulti-modal prototypes as initialization of alignment classifiers to tackle the\nvast-vocabulary object recognition failure problem. On V3Det, this simple\nmethod greatly enhances the performance among one-stage, two-stage, and\nDETR-based detectors with only additional projection layers in both supervised\nand open-vocabulary settings. In particular, Prova improves Faster R-CNN, FCOS,\nand DINO by 3.3, 6.2, and 2.9 AP respectively in the supervised setting of\nV3Det. For the open-vocabulary setting, Prova achieves a new state-of-the-art\nperformance with 32.8 base AP and 11.0 novel AP, which is of 2.6 and 4.3 gain\nover the previous methods.\n","authors":["Yitong Chen","Wenhao Yao","Lingchen Meng","Sihong Wu","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.17800v1.pdf","comment":"Code is available at https://github.com/Row11n/Prova/tree/main"},{"id":"http://arxiv.org/abs/2412.17787v1","updated":"2024-12-23T18:48:04Z","published":"2024-12-23T18:48:04Z","title":"Cross-Lingual Text-Rich Visual Comprehension: An Information Theory\n  Perspective","summary":"  Recent Large Vision-Language Models (LVLMs) have shown promising reasoning\ncapabilities on text-rich images from charts, tables, and documents. However,\nthe abundant text within such images may increase the model's sensitivity to\nlanguage. This raises the need to evaluate LVLM performance on cross-lingual\ntext-rich visual inputs, where the language in the image differs from the\nlanguage of the instructions. To address this, we introduce XT-VQA\n(Cross-Lingual Text-Rich Visual Question Answering), a benchmark designed to\nassess how LVLMs handle language inconsistency between image text and\nquestions. XT-VQA integrates five existing text-rich VQA datasets and a newly\ncollected dataset, XPaperQA, covering diverse scenarios that require faithful\nrecognition and comprehension of visual information despite language\ninconsistency. Our evaluation of prominent LVLMs on XT-VQA reveals a\nsignificant drop in performance for cross-lingual scenarios, even for models\nwith multilingual capabilities. A mutual information analysis suggests that\nthis performance gap stems from cross-lingual questions failing to adequately\nactivate relevant visual information. To mitigate this issue, we propose\nMVCL-MI (Maximization of Vision-Language Cross-Lingual Mutual Information),\nwhere a visual-text cross-lingual alignment is built by maximizing mutual\ninformation between the model's outputs and visual information. This is\nachieved by distilling knowledge from monolingual to cross-lingual settings\nthrough KL divergence minimization, where monolingual output logits serve as a\nteacher. Experimental results on the XT-VQA demonstrate that MVCL-MI\neffectively reduces the visual-text cross-lingual performance disparity while\npreserving the inherent capabilities of LVLMs, shedding new light on the\npotential practice for improving LVLMs. Codes are available at:\nhttps://github.com/Stardust-y/XTVQA.git\n","authors":["Xinmiao Yu","Xiaocheng Feng","Yun Li","Minghui Liao","Ya-Qi Yu","Xiachong Feng","Weihong Zhong","Ruihan Chen","Mengkang Hu","Jihao Wu","Dandan Tu","Duyu Tang","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2412.17787v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17769v1","updated":"2024-12-23T18:29:03Z","published":"2024-12-23T18:29:03Z","title":"ActiveGS: Active Scene Reconstruction using Gaussian Splatting","summary":"  Robotics applications often rely on scene reconstructions to enable\ndownstream tasks. In this work, we tackle the challenge of actively building an\naccurate map of an unknown scene using an on-board RGB-D camera. We propose a\nhybrid map representation that combines a Gaussian splatting map with a coarse\nvoxel map, leveraging the strengths of both representations: the high-fidelity\nscene reconstruction capabilities of Gaussian splatting and the spatial\nmodelling strengths of the voxel map. The core of our framework is an effective\nconfidence modelling technique for the Gaussian splatting map to identify\nunder-reconstructed areas, while utilising spatial information from the voxel\nmap to target unexplored areas and assist in collision-free path planning. By\nactively collecting scene information in under-reconstructed and unexplored\nareas for map updates, our approach achieves superior Gaussian splatting\nreconstruction results compared to state-of-the-art approaches. Additionally,\nwe demonstrate the applicability of our active scene reconstruction framework\nin the real world using an unmanned aerial vehicle.\n","authors":["Liren Jin","Xingguang Zhong","Yue Pan","Jens Behley","Cyrill Stachniss","Marija Popović"],"pdf_url":"https://arxiv.org/pdf/2412.17769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17759v1","updated":"2024-12-23T18:15:19Z","published":"2024-12-23T18:15:19Z","title":"Survey of Large Multimodal Model Datasets, Application Categories and\n  Taxonomy","summary":"  Multimodal learning, a rapidly evolving field in artificial intelligence,\nseeks to construct more versatile and robust systems by integrating and\nanalyzing diverse types of data, including text, images, audio, and video.\nInspired by the human ability to assimilate information through many senses,\nthis method enables applications such as text-to-video conversion, visual\nquestion answering, and image captioning. Recent developments in datasets that\nsupport multimodal language models (MLLMs) are highlighted in this overview.\nLarge-scale multimodal datasets are essential because they allow for thorough\ntesting and training of these models. With an emphasis on their contributions\nto the discipline, the study examines a variety of datasets, including those\nfor training, domain-specific tasks, and real-world applications. It also\nemphasizes how crucial benchmark datasets are for assessing models' performance\nin a range of scenarios, scalability, and applicability. Since multimodal\nlearning is always changing, overcoming these obstacles will help AI research\nand applications reach new heights.\n","authors":["Priyaranjan Pattnayak","Hitesh Laxmichand Patel","Bhargava Kumar","Amit Agarwal","Ishan Banerjee","Srikant Panda","Tejaswini Kumar"],"pdf_url":"https://arxiv.org/pdf/2412.17759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17741v1","updated":"2024-12-23T17:44:05Z","published":"2024-12-23T17:44:05Z","title":"Reasoning to Attend: Try to Understand How <SEG> Token Works","summary":"  Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ token as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specified model\n(\\eg, SAM). However, we observe that little research has looked into how it\nworks.In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map,which\nreveals that what $\\texttt{<SEG>}$ token contributes to is the semantic\nsimilarity within image-text pairs. Specifically, $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion.Also, extensive experiments have been conducted on the ReasonSeg and\nRefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+/g) dataset. All codes and\nmodels are publicly available at https://github.com/rui-qian/READ.\n","authors":["Rui Qian","Xin Yin","Dejing Dou"],"pdf_url":"https://arxiv.org/pdf/2412.17741v1.pdf","comment":"https://github.com/rui-qian/READ"},{"id":"http://arxiv.org/abs/2403.17827v2","updated":"2024-12-23T17:36:22Z","published":"2024-03-26T16:06:42Z","title":"DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from\n  Textual Descriptions","summary":"  Generating natural hand-object interactions in 3D is challenging as the\nresulting hand and object motions are expected to be physically plausible and\nsemantically meaningful. Furthermore, generalization to unseen objects is\nhindered by the limited scale of available hand-object interaction datasets. In\nthis paper, we propose a novel method, dubbed DiffH2O, which can synthesize\nrealistic, one or two-handed object interactions from provided text prompts and\ngeometry of the object. The method introduces three techniques that enable\neffective learning from limited data. First, we decompose the task into a\ngrasping stage and an text-based manipulation stage and use separate diffusion\nmodels for each. In the grasping stage, the model only generates hand motions,\nwhereas in the manipulation phase both hand and object poses are synthesized.\nSecond, we propose a compact representation that tightly couples hand and\nobject poses and helps in generating realistic hand-object interactions. Third,\nwe propose two different guidance schemes to allow more control of the\ngenerated motions: grasp guidance and detailed textual guidance. Grasp guidance\ntakes a single target grasping pose and guides the diffusion model to reach\nthis grasp at the end of the grasping stage, which provides control over the\ngrasping pose. Given a grasping motion from this stage, multiple different\nactions can be prompted in the manipulation phase. For the textual guidance, we\ncontribute comprehensive text descriptions to the GRAB dataset and show that\nthey enable our method to have more fine-grained control over hand-object\ninteractions. Our quantitative and qualitative evaluation demonstrates that the\nproposed method outperforms baseline methods and leads to natural hand-object\nmotions.\n","authors":["Sammy Christen","Shreyas Hampali","Fadime Sener","Edoardo Remelli","Tomas Hodan","Eric Sauser","Shugao Ma","Bugra Tekin"],"pdf_url":"https://arxiv.org/pdf/2403.17827v2.pdf","comment":"Project Page: https://diffh2o.github.io/"},{"id":"http://arxiv.org/abs/2412.17730v1","updated":"2024-12-23T17:27:30Z","published":"2024-12-23T17:27:30Z","title":"Mimicking-Bench: A Benchmark for Generalizable Humanoid-Scene\n  Interaction Learning via Human Mimicking","summary":"  Learning generic skills for humanoid robots interacting with 3D scenes by\nmimicking human data is a key research challenge with significant implications\nfor robotics and real-world applications. However, existing methodologies and\nbenchmarks are constrained by the use of small-scale, manually collected\ndemonstrations, lacking the general dataset and benchmark support necessary to\nexplore scene geometry generalization effectively. To address this gap, we\nintroduce Mimicking-Bench, the first comprehensive benchmark designed for\ngeneralizable humanoid-scene interaction learning through mimicking large-scale\nhuman animation references. Mimicking-Bench includes six household full-body\nhumanoid-scene interaction tasks, covering 11K diverse object shapes, along\nwith 20K synthetic and 3K real-world human interaction skill references. We\nconstruct a complete humanoid skill learning pipeline and benchmark approaches\nfor motion retargeting, motion tracking, imitation learning, and their various\ncombinations. Extensive experiments highlight the value of human mimicking for\nskill learning, revealing key challenges and research directions.\n","authors":["Yun Liu","Bowen Yang","Licheng Zhong","He Wang","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2412.17730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17726v1","updated":"2024-12-23T17:16:58Z","published":"2024-12-23T17:16:58Z","title":"VidTwin: Video VAE with Decoupled Structure and Dynamics","summary":"  Recent advancements in video autoencoders (Video AEs) have significantly\nimproved the quality and efficiency of video generation. In this paper, we\npropose a novel and compact video autoencoder, VidTwin, that decouples video\ninto two distinct latent spaces: Structure latent vectors, which capture\noverall content and global movement, and Dynamics latent vectors, which\nrepresent fine-grained details and rapid movements. Specifically, our approach\nleverages an Encoder-Decoder backbone, augmented with two submodules for\nextracting these latent spaces, respectively. The first submodule employs a\nQ-Former to extract low-frequency motion trends, followed by downsampling\nblocks to remove redundant content details. The second averages the latent\nvectors along the spatial dimension to capture rapid motion. Extensive\nexperiments show that VidTwin achieves a high compression rate of 0.20% with\nhigh reconstruction quality (PSNR of 28.14 on the MCL-JCV dataset), and\nperforms efficiently and effectively in downstream generative tasks. Moreover,\nour model demonstrates explainability and scalability, paving the way for\nfuture research in video latent representation and generation. Our code has\nbeen released at https://github.com/microsoft/VidTok/tree/main/vidtwin.\n","authors":["Yuchi Wang","Junliang Guo","Xinyi Xie","Tianyu He","Xu Sun","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2412.17726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08101v3","updated":"2024-12-23T17:06:20Z","published":"2024-07-11T00:10:45Z","title":"What to Say and When to Say it: Live Fitness Coaching as a Testbed for\n  Situated Interaction","summary":"  Vision-language models have shown impressive progress in recent years.\nHowever, existing models are largely limited to turn-based interactions, where\neach turn must be stepped (i.e., prompted) by the user. Open-ended,\nasynchronous interactions, where an AI model may proactively deliver timely\nresponses or feedback based on the unfolding situation in real-time, are an\nopen challenge. In this work, we present the QEVD benchmark and dataset, which\nexplores human-AI interaction in the challenging, yet controlled, real-world\ndomain of fitness coaching -- a task which intrinsically requires monitoring\nlive user activity and providing immediate feedback. The benchmark requires\nvision-language models to recognize complex human actions, identify possible\nmistakes, and provide appropriate feedback in real-time. Our experiments reveal\nthe limitations of existing state-of-the-art vision-language models for such\nasynchronous situated interactions. Motivated by this, we propose a simple\nend-to-end streaming baseline that can respond asynchronously to human actions\nwith appropriate feedback at the appropriate time.\n","authors":["Sunny Panchal","Apratim Bhattacharyya","Guillaume Berger","Antoine Mercier","Cornelius Bohm","Florian Dietrichkeit","Reza Pourreza","Xuanlin Li","Pulkit Madan","Mingu Lee","Mark Todorovich","Ingo Bax","Roland Memisevic"],"pdf_url":"https://arxiv.org/pdf/2407.08101v3.pdf","comment":"Accepted to the 2024 NeurIPS Datasets and Benchmarks track; data and\n  code are available at:\n  https://www.qualcomm.com/developer/software/qevd-dataset and\n  https://github.com/Qualcomm-AI-research/FitCoach"},{"id":"http://arxiv.org/abs/2412.15979v2","updated":"2024-12-23T16:55:57Z","published":"2024-12-20T15:22:51Z","title":"MR-GDINO: Efficient Open-World Continual Object Detection","summary":"  Open-world (OW) recognition and detection models show strong zero- and\nfew-shot adaptation abilities, inspiring their use as initializations in\ncontinual learning methods to improve performance. Despite promising results on\nseen classes, such OW abilities on unseen classes are largely degenerated due\nto catastrophic forgetting. To tackle this challenge, we propose an open-world\ncontinual object detection task, requiring detectors to generalize to old, new,\nand unseen categories in continual learning scenarios. Based on this task, we\npresent a challenging yet practical OW-COD benchmark to assess detection\nabilities. The goal is to motivate OW detectors to simultaneously preserve\nlearned classes, adapt to new classes, and maintain open-world capabilities\nunder few-shot adaptations. To mitigate forgetting in unseen categories, we\npropose MR-GDINO, a strong, efficient and scalable baseline via memory and\nretrieval mechanisms within a highly scalable memory pool. Experimental results\nshow that existing continual detectors suffer from severe forgetting for both\nseen and unseen categories. In contrast, MR-GDINO largely mitigates forgetting\nwith only 0.1% activated extra parameters, achieving state-of-the-art\nperformance for old, new, and unseen categories.\n","authors":["Bowen Dong","Zitong Huang","Guanglei Yang","Lei Zhang","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2412.15979v2.pdf","comment":"Website: https://m1saka.moe/owcod/ . Code is available at:\n  https://github.com/DongSky/MR-GDINO"},{"id":"http://arxiv.org/abs/2411.15921v2","updated":"2024-12-23T16:50:54Z","published":"2024-11-24T17:08:43Z","title":"A Tunable Despeckling Neural Network Stabilized via Diffusion Equation","summary":"  The removal of multiplicative Gamma noise is a critical research area in the\napplication of synthetic aperture radar (SAR) imaging, where neural networks\nserve as a potent tool. However, real-world data often diverges from\ntheoretical models, exhibiting various disturbances, which makes the neural\nnetwork less effective. Adversarial attacks can be used as a criterion for\njudging the adaptability of neural networks to real data, since adversarial\nattacks can find the most extreme perturbations that make neural networks\nineffective. In this work, the diffusion equation is designed as a\nregularization block to provide sufficient regularity to the whole neural\nnetwork, due to its spontaneous dissipative nature. We propose a tunable,\nregularized neural network framework that unrolls a shallow denoising neural\nnetwork block and a diffusion regularity block into a single network for\nend-to-end training. The linear heat equation, known for its inherent\nsmoothness and low-pass filtering properties, is adopted as the diffusion\nregularization block. In our model, a single time step hyperparameter governs\nthe smoothness of the outputs and can be adjusted dynamically, significantly\nenhancing flexibility. The stability and convergence of our model are\ntheoretically proven. Experimental results demonstrate that the proposed model\neffectively eliminates high-frequency oscillations induced by adversarial\nattacks. Finally, the proposed model is benchmarked against several\nstate-of-the-art denoising methods on simulated images, adversarial samples,\nand real SAR images, achieving superior performance in both quantitative and\nvisual evaluations.\n","authors":["Yi Ran","Zhichang Guo","Jia Li","Yao Li","Martin Burger","Boying Wu"],"pdf_url":"https://arxiv.org/pdf/2411.15921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00314v2","updated":"2024-12-23T16:48:28Z","published":"2023-01-01T00:47:03Z","title":"Causal Deep Learning","summary":"  We derive a set of causal deep neural networks whose architectures are a\nconsequence of tensor (multilinear) factor analysis, a framework that\nfacilitates forward and inverse causal inference. Forward causal questions are\naddressed with a neural architecture composed of causal capsules and a tensor\ntransformer. Causal capsules compute a set of invariant causal factor\nrepresentations, whose interactions are governed by a tensor transformation.\nInverse causal questions are addressed with a neural network that implements\nthe multilinear projection algorithm. The architecture reverses the order of\nthe operations of a forward neural network and estimates the causes of effects.\nAs an alternative to aggressive bottleneck dimension reduction or regularized\nregression that may camouflage an inherently underdetermined inverse problem,\nwe prescribe modeling different aspects of the mechanism of data formation with\npiecewise tensor models whose multilinear projections produce multiple\ncandidate solutions. Our forward and inverse questions may be addressed with\nshallow architectures, but for computationally scalable solutions, we derive a\nset of deep neural networks by taking advantage of block algebra. An\ninterleaved kernel hierarchy results in a doubly non-linear tensor factor\nmodels. The causal neural networks that are a consequence of tensor factor\nanalysis are data agnostic, but are illustrated with facial images. Sequential,\nparallel and asynchronous parallel computation strategies are described.\n","authors":["M. Alex O. Vasilescu"],"pdf_url":"https://arxiv.org/pdf/2301.00314v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17715v1","updated":"2024-12-23T16:45:37Z","published":"2024-12-23T16:45:37Z","title":"GaussianPainter: Painting Point Cloud into 3D Gaussians with Normal\n  Guidance","summary":"  In this paper, we present GaussianPainter, the first method to paint a point\ncloud into 3D Gaussians given a reference image. GaussianPainter introduces an\ninnovative feed-forward approach to overcome the limitations of time-consuming\ntest-time optimization in 3D Gaussian splatting. Our method addresses a\ncritical challenge in the field: the non-uniqueness problem inherent in the\nlarge parameter space of 3D Gaussian splatting. This space, encompassing\nrotation, anisotropic scales, and spherical harmonic coefficients, introduces\nthe challenge of rendering similar images from substantially different Gaussian\nfields. As a result, feed-forward networks face instability when attempting to\ndirectly predict high-quality Gaussian fields, struggling to converge on\nconsistent parameters for a given output. To address this issue, we propose to\nestimate a surface normal for each point to determine its Gaussian rotation.\nThis strategy enables the network to effectively predict the remaining Gaussian\nparameters in the constrained space. We further enhance our approach with an\nappearance injection module, incorporating reference image appearance into\nGaussian fields via a multiscale triplane representation. Our method\nsuccessfully balances efficiency and fidelity in 3D Gaussian generation,\nachieving high-quality, diverse, and robust 3D content creation from point\nclouds in a single forward pass.\n","authors":["Jingqiu Zhou","Lue Fan","Xuesong Chen","Linjiang Huang","Si Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2412.17715v1.pdf","comment":"To appear in AAAI 2025"},{"id":"http://arxiv.org/abs/2411.15596v3","updated":"2024-12-23T16:40:32Z","published":"2024-11-23T16:13:40Z","title":"Comparative Analysis of Resource-Efficient CNN Architectures for Brain\n  Tumor Classification","summary":"  Accurate brain tumor classification in MRI images is critical for timely\ndiagnosis and treatment planning. While deep learning models like ResNet-18,\nVGG-16 have shown high accuracy, they often come with increased complexity and\ncomputational demands. This study presents a comparative analysis of effective\nyet simple Convolutional Neural Network (CNN) architecture and pre-trained\nResNet18, and VGG16 model for brain tumor classification using two publicly\navailable datasets: Br35H:: Brain Tumor Detection 2020 and Brain Tumor MRI\nDataset. The custom CNN architecture, despite its lower complexity,\ndemonstrates competitive performance with the pre-trained ResNet18 and VGG16\nmodels. In binary classification tasks, the custom CNN achieved an accuracy of\n98.67% on the Br35H dataset and 99.62% on the Brain Tumor MRI Dataset. For\nmulti-class classification, the custom CNN, with a slight architectural\nmodification, achieved an accuracy of 98.09%, on the Brain Tumor MRI Dataset.\nComparatively, ResNet18 and VGG16 maintained high performance levels, but the\ncustom CNNs provided a more computationally efficient alternative.\nAdditionally,the custom CNNs were evaluated using few-shot learning (0, 5, 10,\n15, 20, 40, and 80 shots) to assess their robustness, achieving notable\naccuracy improvements with increased shots. This study highlights the potential\nof well-designed, less complex CNN architectures as effective and\ncomputationally efficient alternatives to deeper, pre-trained models for\nmedical imaging tasks, including brain tumor classification. This study\nunderscores the potential of custom CNNs in medical imaging tasks and\nencourages further exploration in this direction.\n","authors":["Md Ashik Khan","Rafath Bin Zafar Auvee"],"pdf_url":"https://arxiv.org/pdf/2411.15596v3.pdf","comment":"A revised and extended version of this paper has been accepted at the\n  27th International Conference on Computer and Information Technology (ICCIT\n  2024). It spans 8 pages and includes 6 figures"},{"id":"http://arxiv.org/abs/2412.17700v1","updated":"2024-12-23T16:31:45Z","published":"2024-12-23T16:31:45Z","title":"MRANet: A Modified Residual Attention Networks for Lung and Colon Cancer\n  Classification","summary":"  Lung and colon cancers are predominant contributors to cancer mortality.\nEarly and accurate diagnosis is crucial for effective treatment. By utilizing\nimaging technology in different image detection, learning models have shown\npromise in automating cancer classification from histopathological images. This\nincludes the histopathological diagnosis, an important factor in cancer type\nidentification. This research focuses on creating a high-efficiency\ndeep-learning model for identifying lung and colon cancer from\nhistopathological images. We proposed a novel approach based on a modified\nresidual attention network architecture. The model was trained on a dataset of\n25,000 high-resolution histopathological images across several classes. Our\nproposed model achieved an exceptional accuracy of 99.30%, 96.63%, and 97.56%\nfor two, three, and five classes, respectively; those are outperforming other\nstate-of-the-art architectures. This study presents a highly accurate deep\nlearning model for lung and colon cancer classification. The superior\nperformance of our proposed model addresses a critical need in medical AI\napplications.\n","authors":["Diponkor Bala","S M Rakib Ul Karim","Rownak Ara Rasul"],"pdf_url":"https://arxiv.org/pdf/2412.17700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17699v1","updated":"2024-12-23T16:31:29Z","published":"2024-12-23T16:31:29Z","title":"Establishing Reality-Virtuality Interconnections in Urban Digital Twins\n  for Superior Intelligent Road Inspection","summary":"  Road inspection is essential for ensuring road maintenance and traffic\nsafety, as road defects gradually emerge and compromise road functionality.\nTraditional methods, which rely on manual evaluations, are labor-intensive,\ncostly, and time-consuming. Although data-driven approaches are gaining\ntraction, the scarcity and spatial sparsity of road defects in the real world\npose significant challenges in acquiring high-quality datasets. Existing\nsimulators designed to generate detailed synthetic driving scenes, however,\nlack models for road defects. Furthermore, advanced driving tasks involving\ninteractions with road surfaces, such as planning and control in defective\nareas, remain underexplored. To address these limitations, we propose a system\nbased on Urban Digital Twin (UDT) technology for intelligent road inspection.\nFirst, hierarchical road models are constructed from real-world driving data,\ncreating highly detailed representations of road defect structures and surface\nelevations. Next, digital road twins are generated to create simulation\nenvironments for comprehensive analysis and evaluation. These scenarios are\nsubsequently imported into a simulator to enable both data acquisition and\nphysical simulation. Experimental results demonstrate that driving tasks,\nincluding perception and decision-making, can be significantly improved using\nthe high-fidelity road defect scenes generated by our system.\n","authors":["Yikang Zhang","Chuang-Wei Liu","Jiahang Li","Yingbing Chen","Jie Cheng","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2412.17699v1.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.17677v1","updated":"2024-12-23T16:01:12Z","published":"2024-12-23T16:01:12Z","title":"EPE-P: Evidence-based Parameter-efficient Prompting for Multimodal\n  Learning with Missing Modalities","summary":"  Missing modalities are a common challenge in real-world multimodal learning\nscenarios, occurring during both training and testing. Existing methods for\nmanaging missing modalities often require the design of separate prompts for\neach modality or missing case, leading to complex designs and a substantial\nincrease in the number of parameters to be learned. As the number of modalities\ngrows, these methods become increasingly inefficient due to parameter\nredundancy. To address these issues, we propose Evidence-based\nParameter-Efficient Prompting (EPE-P), a novel and parameter-efficient method\nfor pretrained multimodal networks. Our approach introduces a streamlined\ndesign that integrates prompting information across different modalities,\nreducing complexity and mitigating redundant parameters. Furthermore, we\npropose an Evidence-based Loss function to better handle the uncertainty\nassociated with missing modalities, improving the model's decision-making. Our\nexperiments demonstrate that EPE-P outperforms existing prompting-based methods\nin terms of both effectiveness and efficiency. The code is released at\nhttps://github.com/Boris-Jobs/EPE-P_MLLMs-Robustness.\n","authors":["Zhe Chen","Xun Lin","Yawen Cui","Zitong Yu"],"pdf_url":"https://arxiv.org/pdf/2412.17677v1.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2412.17671v1","updated":"2024-12-23T15:54:32Z","published":"2024-12-23T15:54:32Z","title":"A Bias-Free Training Paradigm for More General AI-generated Image\n  Detection","summary":"  Successful forensic detectors can produce excellent results in supervised\nlearning benchmarks but struggle to transfer to real-world applications. We\nbelieve this limitation is largely due to inadequate training data quality.\nWhile most research focuses on developing new algorithms, less attention is\ngiven to training data selection, despite evidence that performance can be\nstrongly impacted by spurious correlations such as content, format, or\nresolution. A well-designed forensic detector should detect generator specific\nartifacts rather than reflect data biases. To this end, we propose B-Free, a\nbias-free training paradigm, where fake images are generated from real ones\nusing the conditioning procedure of stable diffusion models. This ensures\nsemantic alignment between real and fake images, allowing any differences to\nstem solely from the subtle artifacts introduced by AI generation. Through\ncontent-based augmentation, we show significant improvements in both\ngeneralization and robustness over state-of-the-art detectors and more\ncalibrated results across 27 different generative models, including recent\nreleases, like FLUX and Stable Diffusion 3.5. Our findings emphasize the\nimportance of a careful dataset curation, highlighting the need for further\nresearch in dataset design. Code and data will be publicly available at\nhttps://grip-unina.github.io/B-Free/\n","authors":["Fabrizio Guillaro","Giada Zingarini","Ben Usman","Avneesh Sud","Davide Cozzolino","Luisa Verdoliva"],"pdf_url":"https://arxiv.org/pdf/2412.17671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07499v2","updated":"2024-12-23T15:34:09Z","published":"2024-12-10T13:27:58Z","title":"EDGE: Unknown-aware Multi-label Learning by Energy Distribution Gap\n  Expansion","summary":"  Multi-label Out-Of-Distribution (OOD) detection aims to discriminate the OOD\nsamples from the multi-label In-Distribution (ID) ones. Compared with its\nmulticlass counterpart, it is crucial to model the joint information among\nclasses. To this end, JointEnergy, which is a representative multi-label OOD\ninference criterion, summarizes the logits of all the classes. However, we find\nthat JointEnergy can produce an imbalance problem in OOD detection, especially\nwhen the model lacks enough discrimination ability. Specifically, we find that\nthe samples only related to minority classes tend to be classified as OOD\nsamples due to the ambiguous energy decision boundary. Besides, imbalanced\nmulti-label learning methods, originally designed for ID ones, would not be\nsuitable for OOD detection scenarios, even producing a serious negative\ntransfer effect. In this paper, we resort to auxiliary outlier exposure (OE)\nand propose an unknown-aware multi-label learning framework to reshape the\nuncertainty energy space layout. In this framework, the energy score is\nseparately optimized for tail ID samples and unknown samples, and the energy\ndistribution gap between them is expanded, such that the tail ID samples can\nhave a significantly larger energy score than the OOD ones. What's more, a\nsimple yet effective measure is designed to select more informative OE\ndatasets. Finally, comprehensive experimental results on multiple multi-label\nand OOD datasets reveal the effectiveness of the proposed method.\n","authors":["Yuchen Sun","Qianqian Xu","Zitai Wang","Zhiyong Yang","Junwei He"],"pdf_url":"https://arxiv.org/pdf/2412.07499v2.pdf","comment":"9 pages, 5 figures, accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.17654v1","updated":"2024-12-23T15:32:26Z","published":"2024-12-23T15:32:26Z","title":"Enhanced Temporal Processing in Spiking Neural Networks for Static\n  Object Detection Using 3D Convolutions","summary":"  Spiking Neural Networks (SNNs) are a class of network models capable of\nprocessing spatiotemporal information, with event-driven characteristics and\nenergy efficiency advantages. Recently, directly trained SNNs have shown\npotential to match or surpass the performance of traditional Artificial Neural\nNetworks (ANNs) in classification tasks. However, in object detection tasks,\ndirectly trained SNNs still exhibit a significant performance gap compared to\nANNs when tested on frame-based static object datasets (such as COCO2017).\nTherefore, bridging this performance gap and enabling directly trained SNNs to\nachieve performance comparable to ANNs on these static datasets has become one\nof the key challenges in the development of SNNs.To address this challenge,\nthis paper focuses on enhancing the SNN's unique ability to process\nspatiotemporal information. Spiking neurons, as the core components of SNNs,\nfacilitate the exchange of information between different temporal channels\nduring the process of converting input floating-point data into binary spike\nsignals. However, existing neuron models still have certain limitations in the\ncommunication of temporal information. Some studies have even suggested that\ndisabling the backpropagation in the time dimension during SNN training can\nstill yield good training results. To improve the SNN handling of temporal\ninformation, this paper proposes replacing traditional 2D convolutions with 3D\nconvolutions, thus directly incorporating temporal information into the\nconvolutional process. Additionally, temporal information recurrence mechanism\nis introduced within the neurons to further enhance the neurons' efficiency in\nutilizing temporal information.Experimental results show that the proposed\nmethod enables directly trained SNNs to achieve performance levels comparable\nto ANNs on the COCO2017 and VOC datasets.\n","authors":["Huaxu He"],"pdf_url":"https://arxiv.org/pdf/2412.17654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17644v1","updated":"2024-12-23T15:21:28Z","published":"2024-12-23T15:21:28Z","title":"DreamFit: Garment-Centric Human Generation via a Lightweight\n  Anything-Dressing Encoder","summary":"  Diffusion models for garment-centric human generation from text or image\nprompts have garnered emerging attention for their great application potential.\nHowever, existing methods often face a dilemma: lightweight approaches, such as\nadapters, are prone to generate inconsistent textures; while finetune-based\nmethods involve high training costs and struggle to maintain the generalization\ncapabilities of pretrained diffusion models, limiting their performance across\ndiverse scenarios. To address these challenges, we propose DreamFit, which\nincorporates a lightweight Anything-Dressing Encoder specifically tailored for\nthe garment-centric human generation. DreamFit has three key advantages: (1)\n\\textbf{Lightweight training}: with the proposed adaptive attention and LoRA\nmodules, DreamFit significantly minimizes the model complexity to 83.4M\ntrainable parameters. (2)\\textbf{Anything-Dressing}: Our model generalizes\nsurprisingly well to a wide range of (non-)garments, creative styles, and\nprompt instructions, consistently delivering high-quality results across\ndiverse scenarios. (3) \\textbf{Plug-and-play}: DreamFit is engineered for\nsmooth integration with any community control plugins for diffusion models,\nensuring easy compatibility and minimizing adoption barriers. To further\nenhance generation quality, DreamFit leverages pretrained large multi-modal\nmodels (LMMs) to enrich the prompt with fine-grained garment descriptions,\nthereby reducing the prompt gap between training and inference. We conduct\ncomprehensive experiments on both $768 \\times 512$ high-resolution benchmarks\nand in-the-wild images. DreamFit surpasses all existing methods, highlighting\nits state-of-the-art capabilities of garment-centric human generation.\n","authors":["Ente Lin","Xujie Zhang","Fuwei Zhao","Yuxuan Luo","Xin Dong","Long Zeng","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2412.17644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17640v1","updated":"2024-12-23T15:18:24Z","published":"2024-12-23T15:18:24Z","title":"Hierarchical Vector Quantization for Unsupervised Action Segmentation","summary":"  In this work, we address unsupervised temporal action segmentation, which\nsegments a set of long, untrimmed videos into semantically meaningful segments\nthat are consistent across videos. While recent approaches combine\nrepresentation learning and clustering in a single step for this task, they do\nnot cope with large variations within temporal segments of the same class. To\naddress this limitation, we propose a novel method, termed Hierarchical Vector\nQuantization (\\ours), that consists of two subsequent vector quantization\nmodules. This results in a hierarchical clustering where the additional\nsubclusters cover the variations within a cluster. We demonstrate that our\napproach captures the distribution of segment lengths much better than the\nstate of the art. To this end, we introduce a new metric based on the\nJensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We\nevaluate our approach on three public datasets, namely Breakfast, YouTube\nInstructional and IKEA ASM. Our approach outperforms the state of the art in\nterms of F1 score, recall and JSD.\n","authors":["Federico Spurio","Emad Bahrami","Gianpiero Francesca","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2412.17640v1.pdf","comment":"To be published in Conference on Artificial Intelligence (AAAI) 2025"},{"id":"http://arxiv.org/abs/2412.17637v1","updated":"2024-12-23T15:13:56Z","published":"2024-12-23T15:13:56Z","title":"SCBench: A Sports Commentary Benchmark for Video LLMs","summary":"  Recently, significant advances have been made in Video Large Language Models\n(Video LLMs) in both academia and industry. However, methods to evaluate and\nbenchmark the performance of different Video LLMs, especially their\nfine-grained, temporal visual capabilities, remain very limited. On one hand,\ncurrent benchmarks use relatively simple videos (e.g., subtitled movie clips)\nwhere the model can understand the entire video by processing just a few\nframes. On the other hand, their datasets lack diversity in task format,\ncomprising only QA or multi-choice QA, which overlooks the models' capacity for\ngenerating in-depth and precise texts. Sports videos, which feature intricate\nvisual information, sequential events, and emotionally charged commentary,\npresent a critical challenge for Video LLMs, making sports commentary an ideal\nbenchmarking task. Inspired by these challenges, we propose a novel task:\nsports video commentary generation, developed $\\textbf{SCBench}$ for Video\nLLMs. To construct such a benchmark, we introduce (1) $\\textbf{SCORES}$, a\nsix-dimensional metric specifically designed for our task, upon which we\npropose a GPT-based evaluation method, and (2) $\\textbf{CommentarySet}$, a\ndataset consisting of 5,775 annotated video clips and ground-truth labels\ntailored to our metric. Based on SCBench, we conduct comprehensive evaluations\non multiple Video LLMs (e.g. VILA, Video-LLaVA, etc.) and chain-of-thought\nbaseline methods. Our results found that InternVL-Chat-2 achieves the best\nperformance with 5.44, surpassing the second-best by 1.04. Our work provides a\nfresh perspective for future research, aiming to enhance models' overall\ncapabilities in complex visual understanding tasks. Our dataset will be\nreleased soon.\n","authors":["Kuangzhi Ge","Lingjun Chen","Kevin Zhang","Yulin Luo","Tianyu Shi","Liaoyuan Fan","Xiang Li","Guanqun Wang","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.17637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17635v1","updated":"2024-12-23T15:12:20Z","published":"2024-12-23T15:12:20Z","title":"LangSurf: Language-Embedded Surface Gaussians for 3D Scene Understanding","summary":"  Applying Gaussian Splatting to perception tasks for 3D scene understanding is\nbecoming increasingly popular. Most existing works primarily focus on rendering\n2D feature maps from novel viewpoints, which leads to an imprecise 3D language\nfield with outlier languages, ultimately failing to align objects in 3D space.\nBy utilizing masked images for feature extraction, these approaches also lack\nessential contextual information, leading to inaccurate feature representation.\nTo this end, we propose a Language-Embedded Surface Field (LangSurf), which\naccurately aligns the 3D language fields with the surface of objects,\nfacilitating precise 2D and 3D segmentation with text query, widely expanding\nthe downstream tasks such as removal and editing. The core of LangSurf is a\njoint training strategy that flattens the language Gaussian on the object\nsurfaces using geometry supervision and contrastive losses to assign accurate\nlanguage features to the Gaussians of objects. In addition, we also introduce\nthe Hierarchical-Context Awareness Module to extract features at the image\nlevel for contextual information then perform hierarchical mask pooling using\nmasks segmented by SAM to obtain fine-grained language features in different\nhierarchies. Extensive experiments on open-vocabulary 2D and 3D semantic\nsegmentation demonstrate that LangSurf outperforms the previous\nstate-of-the-art method LangSplat by a large margin. As shown in\nFig.~\\ref{fig:teaser}, our method is capable of segmenting objects in 3D space,\nthus boosting the effectiveness of our approach in instance recognition,\nremoval, and editing, which is also supported by comprehensive experiments.\n\\url{https://langsurf.github.io}{Project Page}.\n","authors":["Hao Li","Roy Qin","Zhengyu Zou","Diqi He","Bohan Li","Bingquan Dai","Dingewn Zhang","Junwei Han"],"pdf_url":"https://arxiv.org/pdf/2412.17635v1.pdf","comment":"\\url{https://langsurf.github.io}{Project Page}"},{"id":"http://arxiv.org/abs/2409.12784v6","updated":"2024-12-23T15:08:35Z","published":"2024-09-19T13:51:21Z","title":"Evaluating Image Hallucination in Text-to-Image Generation with\n  Question-Answering","summary":"  Despite the impressive success of text-to-image (TTI) generation models,\nexisting studies overlook the issue of whether these models accurately convey\nfactual information. In this paper, we focus on the problem of image\nhallucination, where images created by generation models fail to faithfully\ndepict factual content. To address this, we introduce I-HallA (Image\nHallucination evaluation with Question Answering), a novel automated evaluation\nmetric that measures the factuality of generated images through visual question\nanswering (VQA). We also introduce I-HallA v1.0, a curated benchmark dataset\nfor this purpose. As part of this process, we develop a pipeline that generates\nhigh-quality question-answer pairs using multiple GPT-4 Omni-based agents, with\nhuman judgments to ensure accuracy. Our evaluation protocols measure image\nhallucination by testing if images from existing TTI models can correctly\nrespond to these questions. The I-HallA v1.0 dataset comprises 1.2K diverse\nimage-text pairs across nine categories with 1,000 rigorously curated questions\ncovering various compositional challenges. We evaluate five TTI models using\nI-HallA and reveal that these state-of-the-art models often fail to accurately\nconvey factual information. Moreover, we validate the reliability of our metric\nby demonstrating a strong Spearman correlation ($\\rho$=0.95) with human\njudgments. We believe our benchmark dataset and metric can serve as a\nfoundation for developing factually accurate TTI generation models. Additional\nresources can be found on our project page: https://sgt-lim.github.io/I-HallA/.\n","authors":["Youngsun Lim","Hojun Choi","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2409.12784v6.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2412.17632v1","updated":"2024-12-23T15:08:08Z","published":"2024-12-23T15:08:08Z","title":"ANID: How Far Are We? Evaluating the Discrepancies Between\n  AI-synthesized Images and Natural Images through Multimodal Guidance","summary":"  In the rapidly evolving field of Artificial Intelligence Generated Content\n(AIGC), one of the key challenges is distinguishing AI-synthesized images from\nnatural images. Despite the remarkable capabilities of advanced AI generative\nmodels in producing visually compelling images, significant discrepancies\nremain when these images are compared to natural ones. To systematically\ninvestigate and quantify these discrepancies, we introduce an AI-Natural Image\nDiscrepancy Evaluation benchmark aimed at addressing the critical question:\n\\textit{how far are AI-generated images (AIGIs) from truly realistic images?}\nWe have constructed a large-scale multimodal dataset, the Distinguishing\nNatural and AI-generated Images (DNAI) dataset, which includes over 440,000\nAIGI samples generated by 8 representative models using both unimodal and\nmultimodal prompts, such as Text-to-Image (T2I), Image-to-Image (I2I), and Text\n\\textit{vs.} Image-to-Image (TI2I). Our fine-grained assessment framework\nprovides a comprehensive evaluation of the DNAI dataset across five key\ndimensions: naive visual feature quality, semantic alignment in multimodal\ngeneration, aesthetic appeal, downstream task applicability, and coordinated\nhuman validation. Extensive evaluation results highlight significant\ndiscrepancies across these dimensions, underscoring the necessity of aligning\nquantitative metrics with human judgment to achieve a holistic understanding of\nAI-generated image quality. Code is available at\n\\href{https://github.com/ryliu68/ANID}{https://github.com/ryliu68/ANID}.\n","authors":["Renyang Liu","Ziyu Lyu","Wei Zhou","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2412.17632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17630v1","updated":"2024-12-23T15:06:46Z","published":"2024-12-23T15:06:46Z","title":"Detail-Preserving Latent Diffusion for Stable Shadow Removal","summary":"  Achieving high-quality shadow removal with strong generalizability is\nchallenging in scenes with complex global illumination. Due to the limited\ndiversity in shadow removal datasets, current methods are prone to overfitting\ntraining data, often leading to reduced performance on unseen cases. To address\nthis, we leverage the rich visual priors of a pre-trained Stable Diffusion (SD)\nmodel and propose a two-stage fine-tuning pipeline to adapt the SD model for\nstable and efficient shadow removal. In the first stage, we fix the VAE and\nfine-tune the denoiser in latent space, which yields substantial shadow removal\nbut may lose some high-frequency details. To resolve this, we introduce a\nsecond stage, called the detail injection stage. This stage selectively\nextracts features from the VAE encoder to modulate the decoder, injecting fine\ndetails into the final results. Experimental results show that our method\noutperforms state-of-the-art shadow removal techniques. The cross-dataset\nevaluation further demonstrates that our method generalizes effectively to\nunseen data, enhancing the applicability of shadow removal methods.\n","authors":["Jiamin Xu","Yuxin Zheng","Zelong Li","Chi Wang","Renshu Gu","Weiwei Xu","Gang Xu"],"pdf_url":"https://arxiv.org/pdf/2412.17630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17628v1","updated":"2024-12-23T14:59:46Z","published":"2024-12-23T14:59:46Z","title":"Editing Implicit and Explicit Representations of Radiance Fields: A\n  Survey","summary":"  Neural Radiance Fields (NeRF) revolutionized novel view synthesis in recent\nyears by offering a new volumetric representation, which is compact and\nprovides high-quality image rendering. However, the methods to edit those\nradiance fields developed slower than the many improvements to other aspects of\nNeRF. With the recent development of alternative radiance field-based\nrepresentations inspired by NeRF as well as the worldwide rise in popularity of\ntext-to-image models, many new opportunities and strategies have emerged to\nprovide radiance field editing. In this paper, we deliver a comprehensive\nsurvey of the different editing methods present in the literature for NeRF and\nother similar radiance field representations. We propose a new taxonomy for\nclassifying existing works based on their editing methodologies, review\npioneering models, reflect on current and potential new applications of\nradiance field editing, and compare state-of-the-art approaches in terms of\nediting options and performance.\n","authors":["Arthur Hubert","Gamal Elghazaly","Raphael Frank"],"pdf_url":"https://arxiv.org/pdf/2412.17628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14058v2","updated":"2024-12-23T14:46:27Z","published":"2024-12-18T17:07:20Z","title":"Towards Generalist Robot Policies: What Matters in Building\n  Vision-Language-Action Models","summary":"  Foundation Vision Language Models (VLMs) exhibit strong capabilities in\nmulti-modal representation learning, comprehension, and reasoning. By injecting\naction components into the VLMs, Vision-Language-Action Models (VLAs) can be\nnaturally formed and also show promising performance. Existing work has\ndemonstrated the effectiveness and generalization of VLAs in multiple scenarios\nand tasks. Nevertheless, the transfer from VLMs to VLAs is not trivial since\nexisting VLAs differ in their backbones, action-prediction formulations, data\ndistributions, and training recipes. This leads to a missing piece for a\nsystematic understanding of the design choices of VLAs. In this work, we\ndisclose the key factors that significantly influence the performance of VLA\nand focus on answering three essential design choices: which backbone to\nselect, how to formulate the VLA architectures, and when to add\ncross-embodiment data. The obtained results convince us firmly to explain why\nwe need VLA and develop a new family of VLAs, RoboVLMs, which require very few\nmanual designs and achieve a new state-of-the-art performance in three\nsimulation tasks and real-world experiments. Through our extensive experiments,\nwhich include over 8 VLM backbones, 4 policy architectures, and over 600\ndistinct designed experiments, we provide a detailed guidebook for the future\ndesign of VLAs. In addition to the study, the highly flexible RoboVLMs\nframework, which supports easy integrations of new VLMs and free combinations\nof various design choices, is made public to facilitate future research. We\nopen-source all details, including codes, models, datasets, and toolkits, along\nwith detailed training and evaluation recipes at: robovlms.github.io.\n","authors":["Xinghang Li","Peiyan Li","Minghuan Liu","Dong Wang","Jirong Liu","Bingyi Kang","Xiao Ma","Tao Kong","Hanbo Zhang","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14058v2.pdf","comment":"Project page: robovlms.github.io. Added limitations and future works"},{"id":"http://arxiv.org/abs/2412.17619v1","updated":"2024-12-23T14:43:51Z","published":"2024-12-23T14:43:51Z","title":"Kernel-Aware Graph Prompt Learning for Few-Shot Anomaly Detection","summary":"  Few-shot anomaly detection (FSAD) aims to detect unseen anomaly regions with\nthe guidance of very few normal support images from the same class. Existing\nFSAD methods usually find anomalies by directly designing complex text prompts\nto align them with visual features under the prevailing large vision-language\nmodel paradigm. However, these methods, almost always, neglect intrinsic\ncontextual information in visual features, e.g., the interaction relationships\nbetween different vision layers, which is an important clue for detecting\nanomalies comprehensively. To this end, we propose a kernel-aware graph prompt\nlearning framework, termed as KAG-prompt, by reasoning the cross-layer\nrelations among visual features for FSAD. Specifically, a kernel-aware\nhierarchical graph is built by taking the different layer features focusing on\nanomalous regions of different sizes as nodes, meanwhile, the relationships\nbetween arbitrary pairs of nodes stand for the edges of the graph. By message\npassing over this graph, KAG-prompt can capture cross-layer contextual\ninformation, thus leading to more accurate anomaly prediction. Moreover, to\nintegrate the information of multiple important anomaly signals in the\nprediction map, we propose a novel image-level scoring method based on\nmulti-level information fusion. Extensive experiments on MVTecAD and VisA\ndatasets show that KAG-prompt achieves state-of-the-art FSAD results for\nimage-level/pixel-level anomaly detection. Code is available at\nhttps://github.com/CVL-hub/KAG-prompt.git.\n","authors":["Fenfang Tao","Guo-Sen Xie","Fang Zhao","Xiangbo Shu"],"pdf_url":"https://arxiv.org/pdf/2412.17619v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.09692v2","updated":"2024-12-23T14:36:35Z","published":"2024-12-12T19:14:52Z","title":"Three-in-One: Robust Enhanced Universal Transferable Anti-Facial\n  Retrieval in Online Social Networks","summary":"  Deep hash-based retrieval techniques are widely used in facial retrieval\nsystems to improve the efficiency of facial matching. However, it also carries\nthe danger of exposing private information. Deep hash models are easily\ninfluenced by adversarial examples, which can be leveraged to protect private\nimages from malicious retrieval. The existing adversarial example methods\nagainst deep hash models focus on universality and transferability, lacking the\nresearch on its robustness in online social networks (OSNs), which leads to\ntheir failure in anti-retrieval after post-processing. Therefore, we provide\nthe first in-depth discussion on robustness adversarial perturbation in\nuniversal transferable anti-facial retrieval and propose Three-in-One\nAdversarial Perturbation (TOAP). Specifically, we construct a local and global\nCompression Generator (CG) to simulate complex post-processing scenarios, which\ncan be used to mitigate perturbation. Then, we propose robust optimization\nobjectives based on the discovery of the variation patterns of model's\ndistribution after post-processing, and generate adversarial examples using\nthese objectives and meta-learning. Finally, we iteratively optimize\nperturbation by alternately generating adversarial examples and fine-tuning the\nCG, balancing the performance of perturbation while enhancing CG's ability to\nmitigate them. Numerous experiments demonstrate that, in addition to its\nadvantages in universality and transferability, TOAP significantly outperforms\ncurrent state-of-the-art methods in multiple robustness metrics. It further\nimproves universality and transferability by 5% to 28%, and achieves up to\nabout 33% significant improvement in several simulated post-processing\nscenarios as well as mainstream OSNs, demonstrating that TOAP can effectively\nprotect private images from malicious retrieval in real-world scenarios.\n","authors":["Yunna Lv","Long Tang","Dengpan Ye","Caiyun Xie","Jiacheng Deng","Yiheng He"],"pdf_url":"https://arxiv.org/pdf/2412.09692v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01200v3","updated":"2024-12-23T14:33:11Z","published":"2024-11-02T10:09:08Z","title":"GarmentLab: A Unified Simulation and Benchmark for Garment Manipulation","summary":"  Manipulating garments and fabrics has long been a critical endeavor in the\ndevelopment of home-assistant robots. However, due to complex dynamics and\ntopological structures, garment manipulations pose significant challenges.\nRecent successes in reinforcement learning and vision-based methods offer\npromising avenues for learning garment manipulation. Nevertheless, these\napproaches are severely constrained by current benchmarks, which offer limited\ndiversity of tasks and unrealistic simulation behavior. Therefore, we present\nGarmentLab, a content-rich benchmark and realistic simulation designed for\ndeformable object and garment manipulation. Our benchmark encompasses a diverse\nrange of garment types, robotic systems and manipulators. The abundant tasks in\nthe benchmark further explores of the interactions between garments, deformable\nobjects, rigid bodies, fluids, and human body. Moreover, by incorporating\nmultiple simulation methods such as FEM and PBD, along with our proposed\nsim-to-real algorithms and real-world benchmark, we aim to significantly narrow\nthe sim-to-real gap. We evaluate state-of-the-art vision methods, reinforcement\nlearning, and imitation learning approaches on these tasks, highlighting the\nchallenges faced by current algorithms, notably their limited generalization\ncapabilities. Our proposed open-source environments and comprehensive analysis\nshow promising boost to future research in garment manipulation by unlocking\nthe full potential of these methods. We guarantee that we will open-source our\ncode as soon as possible. You can watch the videos in supplementary files to\nlearn more about the details of our work. Our project page is available at:\nhttps://garmentlab.github.io/\n","authors":["Haoran Lu","Ruihai Wu","Yitong Li","Sijie Li","Ziyu Zhu","Chuanruo Ning","Yan Shen","Longzan Luo","Yuanpei Chen","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2411.01200v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.17612v1","updated":"2024-12-23T14:31:15Z","published":"2024-12-23T14:31:15Z","title":"CoSurfGS:Collaborative 3D Surface Gaussian Splatting with Distributed\n  Learning for Large Scene Reconstruction","summary":"  3D Gaussian Splatting (3DGS) has demonstrated impressive performance in scene\nreconstruction. However, most existing GS-based surface reconstruction methods\nfocus on 3D objects or limited scenes. Directly applying these methods to\nlarge-scale scene reconstruction will pose challenges such as high memory\ncosts, excessive time consumption, and lack of geometric detail, which makes it\ndifficult to implement in practical applications. To address these issues, we\npropose a multi-agent collaborative fast 3DGS surface reconstruction framework\nbased on distributed learning for large-scale surface reconstruction.\nSpecifically, we develop local model compression (LMC) and model aggregation\nschemes (MAS) to achieve high-quality surface representation of large scenes\nwhile reducing GPU memory consumption. Extensive experiments on Urban3d,\nMegaNeRF, and BlendedMVS demonstrate that our proposed method can achieve fast\nand scalable high-fidelity surface reconstruction and photorealistic rendering.\nOur project page is available at \\url{https://gyy456.github.io/CoSurfGS}.\n","authors":["Yuanyuan Gao","Yalun Dai","Hao Li","Weicai Ye","Junyi Chen","Danpeng Chen","Dingwen Zhang","Tong He","Guofeng Zhang","Junwei Han"],"pdf_url":"https://arxiv.org/pdf/2412.17612v1.pdf","comment":"Our project page is available at\n  \\url{https://gyy456.github.io/CoSurfGS}"},{"id":"http://arxiv.org/abs/2412.17610v1","updated":"2024-12-23T14:29:41Z","published":"2024-12-23T14:29:41Z","title":"Personalized Large Vision-Language Models","summary":"  The personalization model has gained significant attention in image\ngeneration yet remains underexplored for large vision-language models (LVLMs).\nBeyond generic ones, with personalization, LVLMs handle interactive dialogues\nusing referential concepts (e.g., ``Mike and Susan are talking.'') instead of\nthe generic form (e.g., ``a boy and a girl are talking.''), making the\nconversation more customizable and referentially friendly. In addition, PLVM is\nequipped to continuously add new concepts during a dialogue without incurring\nadditional costs, which significantly enhances the practicality. PLVM proposes\nAligner, a pre-trained visual encoder to align referential concepts with the\nqueried images. During the dialogues, it extracts features of reference images\nwith these corresponding concepts and recognizes them in the queried image,\nenabling personalization. We note that the computational cost and parameter\ncount of the Aligner are negligible within the entire framework. With\ncomprehensive qualitative and quantitative analyses, we reveal the\neffectiveness and superiority of PLVM.\n","authors":["Chau Pham","Hoang Phan","David Doermann","Yunjie Tian"],"pdf_url":"https://arxiv.org/pdf/2412.17610v1.pdf","comment":"A simple way to personalize your LLM"},{"id":"http://arxiv.org/abs/2412.17606v1","updated":"2024-12-23T14:25:33Z","published":"2024-12-23T14:25:33Z","title":"SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized\n  Images","summary":"  Building a large-scale figure QA dataset requires a considerable amount of\nwork, from gathering and selecting figures to extracting attributes like text,\nnumbers, and colors, and generating QAs. Although recent developments in LLMs\nhave led to efforts to synthesize figures, most of these focus primarily on QA\ngeneration. Additionally, creating figures directly using LLMs often encounters\nissues such as code errors, similar-looking figures, and repetitive content in\nfigures. To address this issue, we present SBSFigures (Stage-by-Stage Synthetic\nFigures), a dataset for pre-training figure QA. Our proposed pipeline enables\nthe creation of chart figures with complete annotations of the visualized data\nand dense QA annotations without any manual annotation process. Our\nstage-by-stage pipeline makes it possible to create diverse topic and\nappearance figures efficiently while minimizing code errors. Our SBSFigures\ndemonstrate a strong pre-training effect, making it possible to achieve\nefficient training with a limited amount of real-world chart data starting from\nour pre-trained weights.\n","authors":["Risa Shinoda","Kuniaki Saito","Shohei Tanaka","Tosho Hirasawa","Yoshitaka Ushiku"],"pdf_url":"https://arxiv.org/pdf/2412.17606v1.pdf","comment":"AAAI-25 Workshop on Document Understanding and Intelligence. Dataset\n  and code: https://github.com/omron-sinicx/SBSFigures"},{"id":"http://arxiv.org/abs/2412.09319v3","updated":"2024-12-23T14:23:30Z","published":"2024-12-12T14:44:05Z","title":"FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot\n  Medical Image Segmentation","summary":"  Existing few-shot medical image segmentation (FSMIS) models fail to address a\npractical issue in medical imaging: the domain shift caused by different\nimaging techniques, which limits the applicability to current FSMIS tasks. To\novercome this limitation, we focus on the cross-domain few-shot medical image\nsegmentation (CD-FSMIS) task, aiming to develop a generalized model capable of\nadapting to a broader range of medical image segmentation scenarios with\nlimited labeled data from the novel target domain. Inspired by the\ncharacteristics of frequency domain similarity across different domains, we\npropose a Frequency-aware Matching Network (FAMNet), which includes two key\ncomponents: a Frequency-aware Matching (FAM) module and a Multi-Spectral Fusion\n(MSF) module. The FAM module tackles two problems during the meta-learning\nphase: 1) intra-domain variance caused by the inherent support-query bias, due\nto the different appearances of organs and lesions, and 2) inter-domain\nvariance caused by different medical imaging techniques. Additionally, we\ndesign an MSF module to integrate the different frequency features decoupled by\nthe FAM module, and further mitigate the impact of inter-domain variance on the\nmodel's segmentation performance. Combining these two modules, our FAMNet\nsurpasses existing FSMIS models and Cross-domain Few-shot Semantic Segmentation\nmodels on three cross-domain datasets, achieving state-of-the-art performance\nin the CD-FSMIS task.\n","authors":["Yuntian Bo","Yazhou Zhu","Lunbo Li","Haofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09319v3.pdf","comment":"Accepted by the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.17601v1","updated":"2024-12-23T14:20:07Z","published":"2024-12-23T14:20:07Z","title":"AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot\n  Semantic Segmentation","summary":"  Few-shot learning aims to recognize novel concepts by leveraging prior\nknowledge learned from a few samples. However, for visually intensive tasks\nsuch as few-shot semantic segmentation, pixel-level annotations are\ntime-consuming and costly. Therefore, in this paper, we utilize the more\nchallenging image-level annotations and propose an adaptive frequency-aware\nnetwork (AFANet) for weakly-supervised few-shot semantic segmentation (WFSS).\nSpecifically, we first propose a cross-granularity frequency-aware module (CFM)\nthat decouples RGB images into high-frequency and low-frequency distributions\nand further optimizes semantic structural information by realigning them.\nUnlike most existing WFSS methods using the textual information from the\nmulti-modal language-vision model, e.g., CLIP, in an offline learning manner,\nwe further propose a CLIP-guided spatial-adapter module (CSM), which performs\nspatial domain adaptive transformation on textual information through online\nlearning, thus providing enriched cross-modal semantic information for CFM.\nExtensive experiments on the Pascal-5\\textsuperscript{i} and\nCOCO-20\\textsuperscript{i} datasets demonstrate that AFANet has achieved\nstate-of-the-art performance. The code is available at\nhttps://github.com/jarch-ma/AFANet.\n","authors":["Jiaqi Ma","Guo-Sen Xie","Fang Zhao","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2412.17601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11535v2","updated":"2024-12-23T14:17:30Z","published":"2024-12-16T08:13:05Z","title":"Relative Distance Guided Dynamic Partition Learning for Scale-Invariant\n  UAV-View Geo-Localization","summary":"  UAV-view Geo-Localization~(UVGL) presents substantial challenges,\nparticularly due to the disparity in visual appearance between drone-captured\nimagery and satellite perspectives. Existing methods usually assume consistent\nscaling factor across different views. Therefore, they adopt predefined\npartition alignment and extract viewpoint-invariant representation by\nconstructing a variety of part-level features. However, the scaling assumption\nis not always hold in the real-world scenarios that variations of UAV flight\nstate leads to the scale mismatch of cross-views, resulting in serious\nperformance degradation. To overcome this issue, we propose a partition\nlearning framework based on relative distance, which alleviates the dependence\non scale consistency while mining fine-grained features. Specifically, we\npropose a distance guided dynamic partition learning strategy~(DGDPL),\nconsisting of a square partition strategy and a distance-guided adjustment\nstrategy. The former is utilized to extract fine-grained features and global\nfeatures in a simple manner. The latter calculates the relative distance ratio\nbetween drone- and satellite-view to adjust the partition size, thereby\nexplicitly aligning the semantic information between partition pairs.\nFurthermore, we propose a saliency-guided refinement strategy to refine\npart-level features, so as to further improve the retrieval accuracy. Extensive\nexperiments show that our approach achieves superior geo-localization accuracy\nacross various scale-inconsistent scenarios, and exhibits remarkable robustness\nagainst scale variations. The code will be released.\n","authors":["Quan Chen","Tingyu Wang","Rongfeng Lu","Bolun Zheng","Zhedong Zheng","Chenggang Yan"],"pdf_url":"https://arxiv.org/pdf/2412.11535v2.pdf","comment":"In Peer Review"},{"id":"http://arxiv.org/abs/2411.19525v2","updated":"2024-12-23T14:15:50Z","published":"2024-11-29T07:49:44Z","title":"LokiTalk: Learning Fine-Grained and Generalizable Correspondences to\n  Enhance NeRF-based Talking Head Synthesis","summary":"  Despite significant progress in talking head synthesis since the introduction\nof Neural Radiance Fields (NeRF), visual artifacts and high training costs\npersist as major obstacles to large-scale commercial adoption. We propose that\nidentifying and establishing fine-grained and generalizable correspondences\nbetween driving signals and generated results can simultaneously resolve both\nproblems. Here we present LokiTalk, a novel framework designed to enhance\nNeRF-based talking heads with lifelike facial dynamics and improved training\nefficiency. To achieve fine-grained correspondences, we introduce\nRegion-Specific Deformation Fields, which decompose the overall portrait motion\ninto lip movements, eye blinking, head pose, and torso movements. By\nhierarchically modeling the driving signals and their associated regions\nthrough two cascaded deformation fields, we significantly improve dynamic\naccuracy and minimize synthetic artifacts. Furthermore, we propose ID-Aware\nKnowledge Transfer, a plug-and-play module that learns generalizable dynamic\nand static correspondences from multi-identity videos, while simultaneously\nextracting ID-specific dynamic and static features to refine the depiction of\nindividual characters. Comprehensive evaluations demonstrate that LokiTalk\ndelivers superior high-fidelity results and training efficiency compared to\nprevious methods. The code will be released upon acceptance.\n","authors":["Tianqi Li","Ruobing Zheng","Bonan Li","Zicheng Zhang","Meng Wang","Jingdong Chen","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2411.19525v2.pdf","comment":"Project Page: https://digital-avatar.github.io/ai/LokiTalk/"},{"id":"http://arxiv.org/abs/2412.17595v1","updated":"2024-12-23T14:11:30Z","published":"2024-12-23T14:11:30Z","title":"V$^2$-SfMLearner: Learning Monocular Depth and Ego-motion for Multimodal\n  Wireless Capsule Endoscopy","summary":"  Deep learning can predict depth maps and capsule ego-motion from capsule\nendoscopy videos, aiding in 3D scene reconstruction and lesion localization.\nHowever, the collisions of the capsule endoscopies within the gastrointestinal\ntract cause vibration perturbations in the training data. Existing solutions\nfocus solely on vision-based processing, neglecting other auxiliary signals\nlike vibrations that could reduce noise and improve performance. Therefore, we\npropose V$^2$-SfMLearner, a multimodal approach integrating vibration signals\ninto vision-based depth and capsule motion estimation for monocular capsule\nendoscopy. We construct a multimodal capsule endoscopy dataset containing\nvibration and visual signals, and our artificial intelligence solution develops\nan unsupervised method using vision-vibration signals, effectively eliminating\nvibration perturbations through multimodal learning. Specifically, we carefully\ndesign a vibration network branch and a Fourier fusion module, to detect and\nmitigate vibration noises. The fusion framework is compatible with popular\nvision-only algorithms. Extensive validation on the multimodal dataset\ndemonstrates superior performance and robustness against vision-only\nalgorithms. Without the need for large external equipment, our V$^2$-SfMLearner\nhas the potential for integration into clinical capsule robots, providing\nreal-time and dependable digestive examination tools. The findings show promise\nfor practical implementation in clinical settings, enhancing the diagnostic\ncapabilities of doctors.\n","authors":["Long Bai","Beilei Cui","Liangyu Wang","Yanheng Li","Shilong Yao","Sishen Yuan","Yanan Wu","Yang Zhang","Max Q. -H. Meng","Zhen Li","Weiping Ding","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2412.17595v1.pdf","comment":"To appear in IEEE Transactions on Automation Science and Engineering\n  (IEEE TASE)"},{"id":"http://arxiv.org/abs/2411.19509v2","updated":"2024-12-23T14:04:09Z","published":"2024-11-29T07:01:31Z","title":"Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head\n  Synthesis","summary":"  Recent advances in diffusion models have revolutionized audio-driven talking\nhead synthesis. Beyond precise lip synchronization, diffusion-based methods\nexcel in generating subtle expressions and natural head movements that are\nwell-aligned with the audio signal. However, these methods are confronted by\nslow inference speed, insufficient fine-grained control over facial motions,\nand occasional visual artifacts largely due to an implicit latent space derived\nfrom Variational Auto-Encoders (VAE), which prevent their adoption in realtime\ninteraction applications. To address these issues, we introduce Ditto, a\ndiffusion-based framework that enables controllable realtime talking head\nsynthesis. Our key innovation lies in bridging motion generation and\nphotorealistic neural rendering through an explicit identity-agnostic motion\nspace, replacing conventional VAE representations. This design substantially\nreduces the complexity of diffusion learning while enabling precise control\nover the synthesized talking heads. We further propose an inference strategy\nthat jointly optimizes three key components: audio feature extraction, motion\ngeneration, and video synthesis. This optimization enables streaming\nprocessing, realtime inference, and low first-frame delay, which are the\nfunctionalities crucial for interactive applications such as AI assistants.\nExtensive experimental results demonstrate that Ditto generates compelling\ntalking head videos and substantially outperforms existing methods in both\nmotion control and realtime performance.\n","authors":["Tianqi Li","Ruobing Zheng","Minghui Yang","Jingdong Chen","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2411.19509v2.pdf","comment":"Project Page: https://digital-avatar.github.io/ai/Ditto/"},{"id":"http://arxiv.org/abs/2412.17587v1","updated":"2024-12-23T14:01:10Z","published":"2024-12-23T14:01:10Z","title":"Improved Cotton Leaf Disease Classification Using Parameter-Efficient\n  Deep Learning Framework","summary":"  Cotton crops, often called \"white gold,\" face significant production\nchallenges, primarily due to various leaf-affecting diseases. As a major global\nsource of fiber, timely and accurate disease identification is crucial to\nensure optimal yields and maintain crop health. While deep learning and machine\nlearning techniques have been explored to address this challenge, there remains\na gap in developing lightweight models with fewer parameters which could be\ncomputationally effective for agricultural practitioners. To address this, we\npropose an innovative deep learning framework integrating a subset of trainable\nlayers from MobileNet, transfer learning, data augmentation, a learning rate\ndecay schedule, model checkpoints, and early stopping mechanisms. Our model\ndemonstrates exceptional performance, accurately classifying seven cotton\ndisease types with an overall accuracy of 98.42% and class-wise precision\nranging from 96% to 100%. This results in significantly enhanced efficiency,\nsurpassing recent approaches in accuracy and model complexity. The existing\nmodels in the literature have yet to attain such high accuracy, even when\ntested on data sets with fewer disease types. The substantial performance\nimprovement, combined with the lightweight nature of the model, makes it\npractically suitable for real-world applications in smart farming. By offering\na high-performing and efficient solution, our framework can potentially address\nchallenges in cotton cultivation, contributing to sustainable agricultural\npractices.\n","authors":["Aswini Kumar Patra","Tejashwini Gajurel"],"pdf_url":"https://arxiv.org/pdf/2412.17587v1.pdf","comment":"4 figures, 3 Tables"},{"id":"http://arxiv.org/abs/2412.17586v1","updated":"2024-12-23T13:58:52Z","published":"2024-12-23T13:58:52Z","title":"Enhancing Reconstruction-Based Out-of-Distribution Detection in Brain\n  MRI with Model and Metric Ensembles","summary":"  Out-of-distribution (OOD) detection is crucial for safely deploying automated\nmedical image analysis systems, as abnormal patterns in images could hamper\ntheir performance. However, OOD detection in medical imaging remains an open\nchallenge, and we address three gaps: the underexplored potential of a simple\nOOD detection model, the lack of optimization of deep learning strategies\nspecifically for OOD detection, and the selection of appropriate reconstruction\nmetrics. In this study, we investigated the effectiveness of a\nreconstruction-based autoencoder for unsupervised detection of synthetic\nartifacts in brain MRI. We evaluated the general reconstruction capability of\nthe model, analyzed the impact of the selected training epoch and\nreconstruction metrics, assessed the potential of model and/or metric\nensembles, and tested the model on a dataset containing a diverse range of\nartifacts. Among the metrics assessed, the contrast component of SSIM and LPIPS\nconsistently outperformed others in detecting homogeneous circular anomalies.\nBy combining two well-converged models and using LPIPS and contrast as\nreconstruction metrics, we achieved a pixel-level area under the\nPrecision-Recall curve of 0.66. Furthermore, with the more realistic OOD\ndataset, we observed that the detection performance varied between artifact\ntypes; local artifacts were more difficult to detect, while global artifacts\nshowed better detection results. These findings underscore the importance of\ncarefully selecting metrics and model configurations, and highlight the need\nfor tailored approaches, as standard deep learning approaches do not always\nalign with the unique needs of OOD detection.\n","authors":["Evi M. C. Huijben","Sina Amirrajab","Josien P. W. Pluim"],"pdf_url":"https://arxiv.org/pdf/2412.17586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05897v2","updated":"2024-12-23T13:50:44Z","published":"2024-06-09T19:33:32Z","title":"InfoGaussian: Structure-Aware Dynamic Gaussians through Lightweight\n  Information Shaping","summary":"  3D Gaussians, as a low-level scene representation, typically involve\nthousands to millions of Gaussians. This makes it difficult to control the\nscene in ways that reflect the underlying dynamic structure, where the number\nof independent entities is typically much smaller. In particular, it can be\nchallenging to animate and move objects in the scene, which requires\ncoordination among many Gaussians. To address this issue, we develop a mutual\ninformation shaping technique that enforces movement resonance between\ncorrelated Gaussians in a motion network. Such correlations can be learned from\nputative 2D object masks in different views. By approximating the mutual\ninformation with the Jacobians of the motions, our method ensures consistent\nmovements of the Gaussians composing different objects under various\nperturbations. In particular, we develop an efficient contrastive training\npipeline with lightweight optimization to shape the motion network, avoiding\nthe need for re-shaping throughout the motion sequence. Notably, our training\nonly touches a small fraction of all Gaussians in the scene yet attains the\ndesired compositional behavior according to the underlying dynamic structure.\nThe proposed technique is evaluated on challenging scenes and demonstrates\nsignificant performance improvement in promoting consistent movements and 3D\nobject segmentation while inducing low computation and memory requirements.\n","authors":["Yunchao Zhang","Guandao Yang","Leonidas Guibas","Yanchao Yang"],"pdf_url":"https://arxiv.org/pdf/2406.05897v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17574v1","updated":"2024-12-23T13:45:56Z","published":"2024-12-23T13:45:56Z","title":"HumanVBench: Exploring Human-Centric Video Understanding Capabilities of\n  MLLMs with Synthetic Benchmark Data","summary":"  In the domain of Multimodal Large Language Models (MLLMs), achieving\nhuman-centric video understanding remains a formidable challenge. Existing\nbenchmarks primarily emphasize object and action recognition, often neglecting\nthe intricate nuances of human emotions, behaviors, and speech visual alignment\nwithin video content. We present HumanVBench, an innovative benchmark\nmeticulously crafted to bridge these gaps in the evaluation of video MLLMs.\nHumanVBench comprises 17 carefully designed tasks that explore two primary\ndimensions: inner emotion and outer manifestations, spanning static and\ndynamic, basic and complex, as well as single-modal and cross-modal aspects.\nWith two advanced automated pipelines for video annotation and\ndistractor-included QA generation, HumanVBench utilizes diverse\nstate-of-the-art (SOTA) techniques to streamline benchmark data synthesis and\nquality assessment, minimizing human annotation dependency tailored to\nhuman-centric multimodal attributes. A comprehensive evaluation across 16 SOTA\nvideo MLLMs reveals notable limitations in current performance, especially in\ncross-modal and temporal alignment, underscoring the necessity for further\nrefinement toward achieving more human-like understanding. HumanVBench is\nopen-sourced to facilitate future advancements and real-world applications in\nvideo MLLMs.\n","authors":["Ting Zhou","Daoyuan Chen","Qirui Jiao","Bolin Ding","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2412.17574v1.pdf","comment":"22 pages, 24 figures, 4 tables"},{"id":"http://arxiv.org/abs/2412.17573v1","updated":"2024-12-23T13:45:29Z","published":"2024-12-23T13:45:29Z","title":"URoadNet: Dual Sparse Attentive U-Net for Multiscale Road Network\n  Extraction","summary":"  The challenges of road network segmentation demand an algorithm capable of\nadapting to the sparse and irregular shapes, as well as the diverse context,\nwhich often leads traditional encoding-decoding methods and simple Transformer\nembeddings to failure. We introduce a computationally efficient and powerful\nframework for elegant road-aware segmentation. Our method, called URoadNet,\neffectively encodes fine-grained local road connectivity and holistic global\ntopological semantics while decoding multiscale road network information.\nURoadNet offers a novel alternative to the U-Net architecture by integrating\nconnectivity attention, which can exploit intra-road interactions across\nmulti-level sampling features with reduced computational complexity. This local\ninteraction serves as valuable prior information for learning global\ninteractions between road networks and the background through another\nintegrality attention mechanism. The two forms of sparse attention are arranged\nalternatively and complementarily, and trained jointly, resulting in\nperformance improvements without significant increases in computational\ncomplexity. Extensive experiments on various datasets with different\nresolutions, including Massachusetts, DeepGlobe, SpaceNet, and Large-Scale\nremote sensing images, demonstrate that URoadNet outperforms state-of-the-art\ntechniques. Our approach represents a significant advancement in the field of\nroad network extraction, providing a computationally feasible solution that\nachieves high-quality segmentation results.\n","authors":["Jie Song","Yue Sun","Ziyun Cai","Liang Xiao","Yawen Huang","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.17573v1.pdf","comment":"12 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.17572v1","updated":"2024-12-23T13:44:51Z","published":"2024-12-23T13:44:51Z","title":"Empathetic Response in Audio-Visual Conversations Using Emotion\n  Preference Optimization and MambaCompressor","summary":"  Chatbot research is advancing with the growing importance of chatbots in\nfields that require human interactions, such as customer support and mental\nhealth care. Despite these advancements, chatbots still face significant\nchallenges in understanding subtle nuances and managing long conversation\nhistories. To address these issues, our study introduces a dual approach:\nfirstly, we employ Emotional Preference Optimization (EPO) to train chatbots\nnot only with correct responses but also with counter-emotional responses-those\nthat are contextually similar but emotionally divergent. This training enables\nthe model to discern fine nuance distinctions between correct and\ncounter-emotional responses, thereby enhancing the quality of its responses.\nSecondly, we introduce MambaCompressor to effectively compress and manage\nextensive conversation histories, significantly reducing time and memory\ncomplexities while improving the chatbot's contextual understanding. Our\ncomprehensive experiments across multiple datasets demonstrate that our model\nsignificantly outperforms existing models in generating empathetic responses\nand efficiently managing lengthy dialogues.\n","authors":["Yeonju Kim","Se Jin Park","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2412.17572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17566v1","updated":"2024-12-23T13:37:26Z","published":"2024-12-23T13:37:26Z","title":"The Dynamic Duo of Collaborative Masking and Target for Advanced Masked\n  Autoencoder Learning","summary":"  Masked autoencoders (MAE) have recently succeeded in self-supervised vision\nrepresentation learning. Previous work mainly applied custom-designed (e.g.,\nrandom, block-wise) masking or teacher (e.g., CLIP)-guided masking and targets.\nHowever, they ignore the potential role of the self-training (student) model in\ngiving feedback to the teacher for masking and targets. In this work, we\npresent to integrate Collaborative Masking and Targets for boosting Masked\nAutoEncoders, namely CMT-MAE. Specifically, CMT-MAE leverages a simple\ncollaborative masking mechanism through linear aggregation across attentions\nfrom both teacher and student models. We further propose using the output\nfeatures from those two models as the collaborative target of the decoder. Our\nsimple and effective framework pre-trained on ImageNet-1K achieves\nstate-of-the-art linear probing and fine-tuning performance. In particular,\nusing ViT-base, we improve the fine-tuning results of the vanilla MAE from\n83.6% to 85.7%.\n","authors":["Shentong Mo"],"pdf_url":"https://arxiv.org/pdf/2412.17566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17561v1","updated":"2024-12-23T13:29:35Z","published":"2024-12-23T13:29:35Z","title":"S-INF: Towards Realistic Indoor Scene Synthesis via Scene Implicit\n  Neural Field","summary":"  Learning-based methods have become increasingly popular in 3D indoor scene\nsynthesis (ISS), showing superior performance over traditional\noptimization-based approaches. These learning-based methods typically model\ndistributions on simple yet explicit scene representations using generative\nmodels. However, due to the oversimplified explicit representations that\noverlook detailed information and the lack of guidance from multimodal\nrelationships within the scene, most learning-based methods struggle to\ngenerate indoor scenes with realistic object arrangements and styles. In this\npaper, we introduce a new method, Scene Implicit Neural Field (S-INF), for\nindoor scene synthesis, aiming to learn meaningful representations of\nmultimodal relationships, to enhance the realism of indoor scene synthesis.\nS-INF assumes that the scene layout is often related to the object-detailed\ninformation. It disentangles the multimodal relationships into scene layout\nrelationships and detailed object relationships, fusing them later through\nimplicit neural fields (INFs). By learning specialized scene layout\nrelationships and projecting them into S-INF, we achieve a realistic generation\nof scene layout. Additionally, S-INF captures dense and detailed object\nrelationships through differentiable rendering, ensuring stylistic consistency\nacross objects. Through extensive experiments on the benchmark 3D-FRONT\ndataset, we demonstrate that our method consistently achieves state-of-the-art\nperformance under different types of ISS.\n","authors":["Zixi Liang","Guowei Xu","Haifeng Wu","Ye Huang","Wen Li","Lixin Duan"],"pdf_url":"https://arxiv.org/pdf/2412.17561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17541v1","updated":"2024-12-23T13:03:51Z","published":"2024-12-23T13:03:51Z","title":"Concept Discovery in Deep Neural Networks for Explainable Face\n  Anti-Spoofing","summary":"  With the rapid growth usage of face recognition in people's daily life, face\nanti-spoofing becomes increasingly important to avoid malicious attacks. Recent\nface anti-spoofing models can reach a high classification accuracy on multiple\ndatasets but these models can only tell people ``this face is fake'' while\nlacking the explanation to answer ``why it is fake''. Such a system undermines\ntrustworthiness and causes user confusion, as it denies their requests without\nproviding any explanations. In this paper, we incorporate XAI into face\nanti-spoofing and propose a new problem termed X-FAS (eXplainable Face\nAnti-Spoofing) empowering face anti-spoofing models to provide an explanation.\nWe propose SPED (SPoofing Evidence Discovery), an X-FAS method which can\ndiscover spoof concepts and provide reliable explanations on the basis of\ndiscovered concepts. To evaluate the quality of X-FAS methods, we propose an\nX-FAS benchmark with annotated spoofing evidence by experts. We analyze SPED\nexplanations on face anti-spoofing dataset and compare SPED quantitatively and\nqualitatively with previous XAI methods on proposed X-FAS benchmark.\nExperimental results demonstrate SPED's ability to generate reliable\nexplanations.\n","authors":["Haoyuan Zhang","Xiangyu Zhu","Li Gao","Jiawei Pan","Kai Pang","Guoying Zhao","Stan Z. Li","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2412.17541v1.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.17540v1","updated":"2024-12-23T13:02:45Z","published":"2024-12-23T13:02:45Z","title":"WildPPG: A Real-World PPG Dataset of Long Continuous Recordings","summary":"  Reflective photoplethysmography (PPG) has become the default sensing\ntechnique in wearable devices to monitor cardiac activity via a person's heart\nrate (HR). However, PPG-based HR estimates can be substantially impacted by\nfactors such as the wearer's activities, sensor placement and resulting motion\nartifacts, as well as environmental characteristics such as temperature and\nambient light. These and other factors can significantly impact and decrease HR\nprediction reliability. In this paper, we show that state-of-the-art HR\nestimation methods struggle when processing \\emph{representative} data from\neveryday activities in outdoor environments, likely because they rely on\nexisting datasets that captured controlled conditions. We introduce a novel\nmultimodal dataset and benchmark results for continuous PPG recordings during\noutdoor activities from 16 participants over 13.5 hours, captured from four\nwearable sensors, each worn at a different location on the body, totaling\n216\\,hours. Our recordings include accelerometer, temperature, and altitude\ndata, as well as a synchronized Lead I-based electrocardiogram for ground-truth\nHR references. Participants completed a round trip from Zurich to Jungfraujoch,\na tall mountain in Switzerland over the course of one day. The trip included\noutdoor and indoor activities such as walking, hiking, stair climbing, eating,\ndrinking, and resting at various temperatures and altitudes (up to 3,571\\,m\nabove sea level) as well as using cars, trains, cable cars, and lifts for\ntransport -- all of which impacted participants' physiological dynamics. We\nalso present a novel method that estimates HR values more robustly in such\nreal-world scenarios than existing baselines.\n","authors":["Manuel Meier","Berken Utku Demirel","Christian Holz"],"pdf_url":"https://arxiv.org/pdf/2412.17540v1.pdf","comment":"Accepted at NeurIPS2024"},{"id":"http://arxiv.org/abs/2412.17532v1","updated":"2024-12-23T12:57:34Z","published":"2024-12-23T12:57:34Z","title":"Exploring Dynamic Novel View Synthesis Technologies for Cinematography","summary":"  Novel view synthesis (NVS) has shown significant promise for applications in\ncinematographic production, particularly through the exploitation of Neural\nRadiance Fields (NeRF) and Gaussian Splatting (GS). These methods model real 3D\nscenes, enabling the creation of new shots that are challenging to capture in\nthe real world due to set topology or expensive equipment requirement. This\ninnovation also offers cinematographic advantages such as smooth camera\nmovements, virtual re-shoots, slow-motion effects, etc. This paper explores\ndynamic NVS with the aim of facilitating the model selection process. We\nshowcase its potential through a short montage filmed using various NVS models.\n","authors":["Adrian Azzarelli","Nantheera Anantrasirichai","David R Bull"],"pdf_url":"https://arxiv.org/pdf/2412.17532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17523v1","updated":"2024-12-23T12:47:04Z","published":"2024-12-23T12:47:04Z","title":"Constructing Fair Latent Space for Intersection of Fairness and\n  Explainability","summary":"  As the use of machine learning models has increased, numerous studies have\naimed to enhance fairness. However, research on the intersection of fairness\nand explainability remains insufficient, leading to potential issues in gaining\nthe trust of actual users. Here, we propose a novel module that constructs a\nfair latent space, enabling faithful explanation while ensuring fairness. The\nfair latent space is constructed by disentangling and redistributing labels and\nsensitive attributes, allowing the generation of counterfactual explanations\nfor each type of information. Our module is attached to a pretrained generative\nmodel, transforming its biased latent space into a fair latent space.\nAdditionally, since only the module needs to be trained, there are advantages\nin terms of time and cost savings, without the need to train the entire\ngenerative model. We validate the fair latent space with various fairness\nmetrics and demonstrate that our approach can effectively provide explanations\nfor biased decisions and assurances of fairness.\n","authors":["Hyungjun Joo","Hyeonggeun Han","Sehwan Kim","Sangwoo Hong","Jungwoo Lee"],"pdf_url":"https://arxiv.org/pdf/2412.17523v1.pdf","comment":"14 pages, 5 figures, accepted in AAAI 2025"},{"id":"http://arxiv.org/abs/2403.07408v2","updated":"2024-12-23T12:36:55Z","published":"2024-03-12T08:35:42Z","title":"NightHaze: Nighttime Image Dehazing via Self-Prior Learning","summary":"  Masked autoencoder (MAE) shows that severe augmentation during training\nproduces robust representations for high-level tasks. This paper brings the\nMAE-like framework to nighttime image enhancement, demonstrating that severe\naugmentation during training produces strong network priors that are resilient\nto real-world night haze degradations. We propose a novel nighttime image\ndehazing method with self-prior learning. Our main novelty lies in the design\nof severe augmentation, which allows our model to learn robust priors. Unlike\nMAE that uses masking, we leverage two key challenging factors of nighttime\nimages as augmentation: light effects and noise. During training, we\nintentionally degrade clear images by blending them with light effects as well\nas by adding noise, and subsequently restore the clear images. This enables our\nmodel to learn clear background priors. By increasing the noise values to\napproach as high as the pixel intensity values of the glow and light effect\nblended images, our augmentation becomes severe, resulting in stronger priors.\nWhile our self-prior learning is considerably effective in suppressing glow and\nrevealing details of background scenes, in some cases, there are still some\nundesired artifacts that remain, particularly in the forms of over-suppression.\nTo address these artifacts, we propose a self-refinement module based on the\nsemi-supervised teacher-student framework. Our NightHaze, especially our\nMAE-like self-prior learning, shows that models trained with severe\naugmentation effectively improve the visibility of input haze images,\napproaching the clarity of clear nighttime images. Extensive experiments\ndemonstrate that our NightHaze achieves state-of-the-art performance,\noutperforming existing nighttime image dehazing methods by a substantial margin\nof 15.5% for MUSIQ and 23.5% for ClipIQA.\n","authors":["Beibei Lin","Yeying Jin","Wending Yan","Wei Ye","Yuan Yuan","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2403.07408v2.pdf","comment":"Accepted by AAAI 2025. Project page:\n  https://bb12346.github.io/NightHaze/"},{"id":"http://arxiv.org/abs/2412.17517v1","updated":"2024-12-23T12:30:28Z","published":"2024-12-23T12:30:28Z","title":"Dataset for Real-World Human Action Detection Using FMCW mmWave Radar","summary":"  Human action detection using privacy-preserving mmWave radar sensors is\nstudied for its applications in healthcare and home automation. Unlike existing\nresearch, limited to simulations in controlled environments, we present a\nreal-world mmWave radar dataset with baseline results for human action\ndetection.\n","authors":["Dylan jayabahu","Parthipan Siva"],"pdf_url":"https://arxiv.org/pdf/2412.17517v1.pdf","comment":"To be published in JCVIS (proceedings of 10th Annual Conference on\n  Vision and Intelligent Systems)"},{"id":"http://arxiv.org/abs/2410.01517v2","updated":"2024-12-23T12:28:05Z","published":"2024-10-02T13:08:56Z","title":"UW-GS: Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater\n  Scene Reconstruction","summary":"  3D Gaussian splatting (3DGS) offers the capability to achieve real-time high\nquality 3D scene rendering. However, 3DGS assumes that the scene is in a clear\nmedium environment and struggles to generate satisfactory representations in\nunderwater scenes, where light absorption and scattering are prevalent and\nmoving objects are involved. To overcome these, we introduce a novel Gaussian\nSplatting-based method, UW-GS, designed specifically for underwater\napplications. It introduces a color appearance that models distance-dependent\ncolor variation, employs a new physics-based density control strategy to\nenhance clarity for distant objects, and uses a binary motion mask to handle\ndynamic content. Optimized with a well-designed loss function supporting for\nscattering media and strengthened by pseudo-depth maps, UW-GS outperforms\nexisting methods with PSNR gains up to 1.26dB. To fully verify the\neffectiveness of the model, we also developed a new underwater dataset, S-UW,\nwith dynamic object masks.\n","authors":["Haoran Wang","Nantheera Anantrasirichai","Fan Zhang","David Bull"],"pdf_url":"https://arxiv.org/pdf/2410.01517v2.pdf","comment":"Accepted at IEEE/CVF WACV 2025"},{"id":"http://arxiv.org/abs/2412.08464v2","updated":"2024-12-23T12:23:08Z","published":"2024-12-11T15:30:06Z","title":"CC-Diff: Enhancing Contextual Coherence in Remote Sensing Image\n  Synthesis","summary":"  Accurately depicting real-world landscapes in remote sensing (RS) images\nrequires precise alignment between objects and their environment. However, most\nexisting synthesis methods for natural images prioritize foreground control,\noften reducing the background to plain textures. This neglects the interaction\nbetween foreground and background, which can lead to incoherence in RS\nscenarios. In this paper, we introduce CC-Diff, a Diffusion Model-based\napproach for RS image generation with enhanced Context Coherence. To capture\nspatial interdependence, we propose a sequential pipeline where background\ngeneration is conditioned on synthesized foreground instances. Distinct\nlearnable queries are also employed to model both the complex background\ntexture and its semantic relation to the foreground. Extensive experiments\ndemonstrate that CC-Diff outperforms state-of-the-art methods in visual\nfidelity, semantic accuracy, and positional precision, excelling in both RS and\nnatural image domains. CC-Diff also shows strong trainability, improving\ndetection accuracy by 2.04 mAP on DOTA and 2.25 mAP on the COCO benchmark.\n","authors":["Mu Zhang","Yunfan Liu","Yue Liu","Hongtian Yu","Qixiang Ye"],"pdf_url":"https://arxiv.org/pdf/2412.08464v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17512v1","updated":"2024-12-23T12:19:03Z","published":"2024-12-23T12:19:03Z","title":"BEE: Metric-Adapted Explanations via Baseline Exploration-Exploitation","summary":"  Two prominent challenges in explainability research involve 1) the nuanced\nevaluation of explanations and 2) the modeling of missing information through\nbaseline representations. The existing literature introduces diverse evaluation\nmetrics, each scrutinizing the quality of explanations through distinct lenses.\nAdditionally, various baseline representations have been proposed, each\nmodeling the notion of missingness differently. Yet, a consensus on the\nultimate evaluation metric and baseline representation remains elusive. This\nwork acknowledges the diversity in explanation metrics and baselines,\ndemonstrating that different metrics exhibit preferences for distinct\nexplanation maps resulting from the utilization of different baseline\nrepresentations and distributions. To address the diversity in metrics and\naccommodate the variety of baseline representations in a unified manner, we\npropose Baseline Exploration-Exploitation (BEE) - a path-integration method\nthat introduces randomness to the integration process by modeling the baseline\nas a learned random tensor. This tensor follows a learned mixture of baseline\ndistributions optimized through a contextual exploration-exploitation procedure\nto enhance performance on the specific metric of interest. By resampling the\nbaseline from the learned distribution, BEE generates a comprehensive set of\nexplanation maps, facilitating the selection of the best-performing explanation\nmap in this broad set for the given metric. Extensive evaluations across\nvarious model architectures showcase the superior performance of BEE in\ncomparison to state-of-the-art explanation methods on a variety of objective\nevaluation metrics.\n","authors":["Oren Barkan","Yehonatan Elisha","Jonathan Weill","Noam Koenigstein"],"pdf_url":"https://arxiv.org/pdf/2412.17512v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.17504v1","updated":"2024-12-23T12:03:35Z","published":"2024-12-23T12:03:35Z","title":"An Evaluation Framework for Product Images Background Inpainting based\n  on Human Feedback and Product Consistency","summary":"  In product advertising applications, the automated inpainting of backgrounds\nutilizing AI techniques in product images has emerged as a significant task.\nHowever, the techniques still suffer from issues such as inappropriate\nbackground and inconsistent product in generated product images, and existing\napproaches for evaluating the quality of generated product images are mostly\ninconsistent with human feedback causing the evaluation for this task to depend\non manual annotation. To relieve the issues above, this paper proposes Human\nFeedback and Product Consistency (HFPC), which can automatically assess the\ngenerated product images based on two modules. Firstly, to solve inappropriate\nbackgrounds, human feedback on 44,000 automated inpainting product images is\ncollected to train a reward model based on multi-modal features extracted from\nBLIP and comparative learning. Secondly, to filter generated product images\ncontaining inconsistent products, a fine-tuned segmentation model is employed\nto segment the product of the original and generated product images and then\ncompare the differences between the above two. Extensive experiments have\ndemonstrated that HFPC can effectively evaluate the quality of generated\nproduct images and significantly reduce the expense of manual annotation.\nMoreover, HFPC achieves state-of-the-art(96.4% in precision) in comparison to\nother open-source visual-quality-assessment models. Dataset and code are\navailable at: https://github.com/created-Bi/background inpainting products\ndataset/.\n","authors":["Yuqi Liang","Jun Luo","Xiaoxi Guo","Jianqi Bi"],"pdf_url":"https://arxiv.org/pdf/2412.17504v1.pdf","comment":"accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2312.02218v4","updated":"2024-12-23T11:53:49Z","published":"2023-12-03T15:19:08Z","title":"WavePlanes: Compact Hex Planes for Dynamic Novel View Synthesis","summary":"  Dynamic Novel View Synthesis (Dynamic NVS) enhances NVS technologies to model\nmoving 3-D scenes. However, current methods are resource intensive and\nchallenging to compress. To address this, we present WavePlanes, a fast and\nmore compact hex plane representation, applicable to both Neural Radiance\nFields and Gaussian Splatting methods. Rather than modeling many feature scales\nseparately (as done previously), we use the inverse discrete wavelet transform\nto reconstruct features at varying scales. This leads to a more compact\nrepresentation and allows us to explore wavelet-based compression schemes for\nfurther gains. The proposed compression scheme exploits the sparsity of wavelet\ncoefficients, by applying hard thresholding to the wavelet planes and storing\nnonzero coefficients and their locations on each plane in a Hash Map. Compared\nto the state-of-the-art (SotA), WavePlanes is significantly smaller, less\nresource demanding and competitive in reconstruction quality. Compared to small\nSotA models, WavePlanes outperforms methods in both model size and quality of\nnovel views.\n","authors":["Adrian Azzarelli","Nantheera Anantrasirichai","David R Bull"],"pdf_url":"https://arxiv.org/pdf/2312.02218v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17496v1","updated":"2024-12-23T11:53:06Z","published":"2024-12-23T11:53:06Z","title":"Guided Real Image Dehazing using YCbCr Color Space","summary":"  Image dehazing, particularly with learning-based methods, has gained\nsignificant attention due to its importance in real-world applications.\nHowever, relying solely on the RGB color space often fall short, frequently\nleaving residual haze. This arises from two main issues: the difficulty in\nobtaining clear textural features from hazy RGB images and the complexity of\nacquiring real haze/clean image pairs outside controlled environments like\nsmoke-filled scenes. To address these issues, we first propose a novel\nStructure Guided Dehazing Network (SGDN) that leverages the superior structural\nproperties of YCbCr features over RGB. It comprises two key modules: Bi-Color\nGuidance Bridge (BGB) and Color Enhancement Module (CEM). BGB integrates a\nphase integration module and an interactive attention module, utilizing the\nrich texture features of the YCbCr space to guide the RGB space, thereby\nrecovering clearer features in both frequency and spatial domains. To maintain\ntonal consistency, CEM further enhances the color perception of RGB features by\naggregating YCbCr channel information. Furthermore, for effective supervised\nlearning, we introduce a Real-World Well-Aligned Haze (RW$^2$AH) dataset, which\nincludes a diverse range of scenes from various geographical regions and\nclimate conditions. Experimental results demonstrate that our method surpasses\nexisting state-of-the-art methods across multiple real-world smoke/haze\ndatasets. Code and Dataset:\n\\textcolor{blue}{\\url{https://github.com/fiwy0527/AAAI25_SGDN.}}\n","authors":["Wenxuan Fang","Jankai Fan","Yu Zheng","Jiangwei Weng","Ying Tai","Jun Li"],"pdf_url":"https://arxiv.org/pdf/2412.17496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05780v3","updated":"2024-12-23T11:42:18Z","published":"2024-12-08T02:23:40Z","title":"BudgetFusion: Perceptually-Guided Adaptive Diffusion Models","summary":"  Diffusion models have shown unprecedented success in the task of\ntext-to-image generation. While these models are capable of generating\nhigh-quality and realistic images, the complexity of sequential denoising has\nraised societal concerns regarding high computational demands and energy\nconsumption. In response, various efforts have been made to improve inference\nefficiency. However, most of the existing efforts have taken a fixed approach\nwith neural network simplification or text prompt optimization. Are the quality\nimprovements from all denoising computations equally perceivable to humans? We\nobserved that images from different text prompts may require different\ncomputational efforts given the desired content. The observation motivates us\nto present BudgetFusion, a novel model that suggests the most perceptually\nefficient number of diffusion steps before a diffusion model starts to generate\nan image. This is achieved by predicting multi-level perceptual metrics\nrelative to diffusion steps. With the popular Stable Diffusion as an example,\nwe conduct both numerical analyses and user studies. Our experiments show that\nBudgetFusion saves up to five seconds per prompt without compromising\nperceptual similarity. We hope this work can initiate efforts toward answering\na core question: how much do humans perceptually gain from images created by a\ngenerative model, per watt of energy?\n","authors":["Qinchan Li","Kenneth Chen","Changyue Su","Qi Sun"],"pdf_url":"https://arxiv.org/pdf/2412.05780v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14603v2","updated":"2024-12-23T11:40:38Z","published":"2024-12-19T07:49:40Z","title":"Successive optimization of optics and post-processing with\n  differentiable coherent PSF operator and field information","summary":"  Recently, the joint design of optical systems and downstream algorithms is\nshowing significant potential. However, existing rays-described methods are\nlimited to optimizing geometric degradation, making it difficult to fully\nrepresent the optical characteristics of complex, miniaturized lenses\nconstrained by wavefront aberration or diffraction effects. In this work, we\nintroduce a precise optical simulation model, and every operation in pipeline\nis differentiable. This model employs a novel initial value strategy to enhance\nthe reliability of intersection calculation on high aspherics. Moreover, it\nutilizes a differential operator to reduce memory consumption during coherent\npoint spread function calculations. To efficiently address various degradation,\nwe design a joint optimization procedure that leverages field information.\nGuided by a general restoration network, the proposed method not only enhances\nthe image quality, but also successively improves the optical performance\nacross multiple lenses that are already in professional level. This joint\noptimization pipeline offers innovative insights into the practical design of\nsophisticated optical systems and post-processing algorithms. The source code\nwill be made publicly available at\nhttps://github.com/Zrr-ZJU/Successive-optimization\n","authors":["Zheng Ren","Jingwen Zhou","Wenguan Zhang","Jiapu Yan","Bingkun Chen","Huajun Feng","Shiqi Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14603v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05039v2","updated":"2024-12-23T11:38:01Z","published":"2024-05-08T13:13:02Z","title":"Reviewing Intelligent Cinematography: AI research for camera-based video\n  production","summary":"  This paper offers the first comprehensive review of artificial intelligence\n(AI) research in the context of real camera content acquisition for\nentertainment purposes and is aimed at both researchers and cinematographers.\nAddressing the lack of review papers in the field of intelligent\ncinematography} (IC) and the breadth of related computer vision research, we\npresent a holistic view of the IC landscape while providing technical insight,\nimportant for experts across disciplines. We provide technical background on\ngenerative AI, object detection, automated camera calibration and 3-D content\nacquisition, with references to assist non-technical readers. The application\nsections categorize work in terms of four production types: General Production,\nVirtual Production, Live Production and Aerial Production. Within each\napplication section, we (1) sub-classify work according to research topic and\n(2) describe the trends and challenges relevant to each type of production. In\nthe final chapter, we address the greater scope of IC research and summarize\nthe significant potential of this area to influence the creative industries\nsector. We suggest that work relating to virtual production has the greatest\npotential to impact other mediums of production, driven by the growing interest\nin LED volumes/stages for in-camera virtual effects (ICVFX) and automated 3-D\ncapture for virtual modeling of real world scenes and actors. We also address\nethical and legal concerns regarding the use of creative AI that impact on\nartists, actors, technologists and the general public.\n","authors":["Adrian Azzarelli","Nantheera Anantrasirichai","David R Bull"],"pdf_url":"https://arxiv.org/pdf/2405.05039v2.pdf","comment":"For researchers and cinematographers. 43 pages including Table of\n  Contents, List of Figures and Tables. We obtained permission to use Figures 5\n  and 11. All other Figures have been drawn by us"},{"id":"http://arxiv.org/abs/2304.07527v2","updated":"2024-12-23T11:30:51Z","published":"2023-04-15T10:24:51Z","title":"Align-DETR: Enhancing End-to-end Object Detection with Aligned Loss","summary":"  DETR has set up a simple end-to-end pipeline for object detection by\nformulating this task as a set prediction problem, showing promising potential.\nDespite its notable advancements, this paper identifies two key forms of\nmisalignment within the model: classification-regression misalignment and\ncross-layer target misalignment. Both issues impede DETR's convergence and\ndegrade its overall performance. To tackle both issues simultaneously, we\nintroduce a novel loss function, termed as Align Loss, designed to resolve the\ndiscrepancy between the two tasks. Align Loss guides the optimization of DETR\nthrough a joint quality metric, strengthening the connection between\nclassification and regression. Furthermore, it incorporates an exponential\ndown-weighting term to facilitate a smooth transition from positive to negative\nsamples. Align-DETR also employs many-to-one matching for supervision of\nintermediate layers, akin to the design of H-DETR, which enhances robustness\nagainst instability. We conducted extensive experiments, yielding highly\ncompetitive results. Notably, our method achieves a 49.3% (+0.6) AP on the\nH-DETR baseline with the ResNet-50 backbone. It also sets a new\nstate-of-the-art performance, reaching 50.5% AP in the 1x setting and 51.7% AP\nin the 2x setting, surpassing several strong competitors. Our code is available\nat https://github.com/FelixCaae/AlignDETR.\n","authors":["Zhi Cai","Songtao Liu","Guodong Wang","Zheng Ge","Xiangyu Zhang","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2304.07527v2.pdf","comment":"Accepted by BMVC2024"},{"id":"http://arxiv.org/abs/2412.17477v1","updated":"2024-12-23T11:09:30Z","published":"2024-12-23T11:09:30Z","title":"Predicting Satisfied User and Machine Ratio for Compressed Images: A\n  Unified Approach","summary":"  Nowadays, high-quality images are pursued by both humans for better viewing\nexperience and by machines for more accurate visual analysis. However, images\nare usually compressed before being consumed, decreasing their quality. It is\nmeaningful to predict the perceptual quality of compressed images for both\nhumans and machines, which guides the optimization for compression. In this\npaper, we propose a unified approach to address this. Specifically, we create a\ndeep learning-based model to predict Satisfied User Ratio (SUR) and Satisfied\nMachine Ratio (SMR) of compressed images simultaneously. We first pre-train a\nfeature extractor network on a large-scale SMR-annotated dataset with human\nperception-related quality labels generated by diverse image quality models,\nwhich simulates the acquisition of SUR labels. Then, we propose an\nMLP-Mixer-based network to predict SUR and SMR by leveraging and fusing the\nextracted multi-layer features. We introduce a Difference Feature Residual\nLearning (DFRL) module to learn more discriminative difference features. We\nfurther use a Multi-Head Attention Aggregation and Pooling (MHAAP) layer to\naggregate difference features and reduce their redundancy. Experimental results\nindicate that the proposed model significantly outperforms state-of-the-art SUR\nand SMR prediction methods. Moreover, our joint learning scheme of human and\nmachine perceptual quality prediction tasks is effective at improving the\nperformance of both.\n","authors":["Qi Zhang","Shanshe Wang","Xinfeng Zhang","Siwei Ma","Jingshan Pan","Wen Gao"],"pdf_url":"https://arxiv.org/pdf/2412.17477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14280v3","updated":"2024-12-23T10:58:37Z","published":"2024-11-21T16:33:35Z","title":"EasyHOI: Unleashing the Power of Large Models for Reconstructing\n  Hand-Object Interactions in the Wild","summary":"  Our work aims to reconstruct hand-object interactions from a single-view\nimage, which is a fundamental but ill-posed task. Unlike methods that\nreconstruct from videos, multi-view images, or predefined 3D templates,\nsingle-view reconstruction faces significant challenges due to inherent\nambiguities and occlusions. These challenges are further amplified by the\ndiverse nature of hand poses and the vast variety of object shapes and sizes.\nOur key insight is that current foundational models for segmentation,\ninpainting, and 3D reconstruction robustly generalize to in-the-wild images,\nwhich could provide strong visual and geometric priors for reconstructing\nhand-object interactions. Specifically, given a single image, we first design a\nnovel pipeline to estimate the underlying hand pose and object shape using\noff-the-shelf large models. Furthermore, with the initial reconstruction, we\nemploy a prior-guided optimization scheme, which optimizes hand pose to comply\nwith 3D physical constraints and the 2D input image content. We perform\nexperiments across several datasets and show that our method consistently\noutperforms baselines and faithfully reconstructs a diverse set of hand-object\ninteractions. Here is the link of our project page:\nhttps://lym29.github.io/EasyHOI-page/\n","authors":["Yumeng Liu","Xiaoxiao Long","Zemin Yang","Yuan Liu","Marc Habermann","Christian Theobalt","Yuexin Ma","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14280v3.pdf","comment":"Project page: https://lym29.github.io/EasyHOI-page/"},{"id":"http://arxiv.org/abs/2410.18057v2","updated":"2024-12-23T10:48:15Z","published":"2024-10-23T17:30:50Z","title":"CLEAR: Character Unlearning in Textual and Visual Modalities","summary":"  Machine Unlearning (MU) is critical for enhancing privacy and security in\ndeep learning models, particularly in large multimodal language models (MLLMs),\nby removing specific private or hazardous information. While MU has made\nsignificant progress in textual and visual modalities, multimodal unlearning\n(MMU) remains significantly underexplored, partially due to the absence of a\nsuitable open-source benchmark. To address this, we introduce CLEAR, a new\nbenchmark designed to evaluate MMU methods. CLEAR contains 200 fictitious\nindividuals and 3,700 images linked with corresponding question-answer pairs,\nenabling a thorough evaluation across modalities. We assess 10 MU methods,\nadapting them for MMU, and highlight new challenges specific to multimodal\nforgetting. The dataset is available at\nhttps://huggingface.co/datasets/therem/CLEAR\n","authors":["Alexey Dontsov","Dmitrii Korzh","Alexey Zhavoronkin","Boris Mikheev","Denis Bobkov","Aibek Alanov","Oleg Y. Rogov","Ivan Oseledets","Elena Tutubalina"],"pdf_url":"https://arxiv.org/pdf/2410.18057v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17464v1","updated":"2024-12-23T10:41:18Z","published":"2024-12-23T10:41:18Z","title":"CALLIC: Content Adaptive Learning for Lossless Image Compression","summary":"  Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.\n","authors":["Daxin Li","Yuanchao Bai","Kai Wang","Junjun Jiang","Xianming Liu","Wen Gao"],"pdf_url":"https://arxiv.org/pdf/2412.17464v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.17458v1","updated":"2024-12-23T10:26:26Z","published":"2024-12-23T10:26:26Z","title":"Progressive Boundary Guided Anomaly Synthesis for Industrial Anomaly\n  Detection","summary":"  Unsupervised anomaly detection methods can identify surface defects in\nindustrial images by leveraging only normal samples for training. Due to the\nrisk of overfitting when learning from a single class, anomaly synthesis\nstrategies are introduced to enhance detection capability by generating\nartificial anomalies. However, existing strategies heavily rely on anomalous\ntextures from auxiliary datasets. Moreover, their limitations in the coverage\nand directionality of anomaly synthesis may result in a failure to capture\nuseful information and lead to significant redundancy. To address these issues,\nwe propose a novel Progressive Boundary-guided Anomaly Synthesis (PBAS)\nstrategy, which can directionally synthesize crucial feature-level anomalies\nwithout auxiliary textures. It consists of three core components: Approximate\nBoundary Learning (ABL), Anomaly Feature Synthesis (AFS), and Refined Boundary\nOptimization (RBO). To make the distribution of normal samples more compact,\nABL first learns an approximate decision boundary by center constraint, which\nimproves the center initialization through feature alignment. AFS then\ndirectionally synthesizes anomalies with more flexible scales guided by the\nhypersphere distribution of normal features. Since the boundary is so loose\nthat it may contain real anomalies, RBO refines the decision boundary through\nthe binary classification of artificial anomalies and normal features.\nExperimental results show that our method achieves state-of-the-art performance\nand the fastest detection speed on three widely used industrial datasets,\nincluding MVTec AD, VisA, and MPDD. The code will be available at:\nhttps://github.com/cqylunlun/PBAS.\n","authors":["Qiyu Chen","Huiyuan Luo","Han Gao","Chengkan Lv","Zhengtao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.17458v1.pdf","comment":"Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology"},{"id":"http://arxiv.org/abs/2412.17451v1","updated":"2024-12-23T10:18:41Z","published":"2024-12-23T10:18:41Z","title":"Diving into Self-Evolving Training for Multimodal Reasoning","summary":"  Reasoning ability is essential for Large Multimodal Models (LMMs). In the\nabsence of multimodal chain-of-thought annotated data, self-evolving training,\nwhere the model learns from its own outputs, has emerged as an effective and\nscalable approach for enhancing reasoning abilities. Despite its growing usage,\na comprehensive understanding of self-evolving training, particularly in the\ncontext of multimodal reasoning, remains limited. In this paper, we delve into\nthe intricacies of self-evolving training for multimodal reasoning, pinpointing\nthree key factors: Training Method, Reward Model, and Prompt Variation. We\nsystematically examine each factor and explore how various configurations\naffect the training's effectiveness. Our analysis leads to a set of best\npractices for each factor, aimed at optimizing multimodal reasoning.\nFurthermore, we explore the Self-Evolution Dynamics during training and the\nimpact of automatic balancing mechanisms in boosting performance. After all the\ninvestigations, we present a final recipe for self-evolving training in\nmultimodal reasoning, encapsulating these design choices into a framework we\ncall MSTaR (Multimodal Self-evolving Training for Reasoning), which is\nuniversally effective for models with different sizes on various benchmarks,\ne.g., surpassing the pre-evolved model significantly on 5 multimodal reasoning\nbenchmarks without using additional human annotations, as demonstrated on\nMiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B) and InternVL2 (2B). We believe this\nstudy fills a significant gap in the understanding of self-evolving training\nfor multimodal reasoning and offers a robust framework for future research. Our\npolicy and reward models, as well as the collected data, is released to\nfacilitate further investigation in multimodal reasoning.\n","authors":["Wei Liu","Junlong Li","Xiwen Zhang","Fan Zhou","Yu Cheng","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2412.17451v1.pdf","comment":"Project Page: https://mstar-lmm.github.io"},{"id":"http://arxiv.org/abs/2409.06002v3","updated":"2024-12-23T09:57:21Z","published":"2024-09-09T19:01:14Z","title":"Enhanced Generative Data Augmentation for Semantic Segmentation via\n  Stronger Guidance","summary":"  Data augmentation is crucial for pixel-wise annotation tasks like semantic\nsegmentation, where labeling requires significant effort and intensive labor.\nTraditional methods, involving simple transformations such as rotations and\nflips, create new images but often lack diversity along key semantic dimensions\nand fail to alter high-level semantic properties. To address this issue,\ngenerative models have emerged as an effective solution for augmenting data by\ngenerating synthetic images. Controllable Generative models offer data\naugmentation methods for semantic segmentation tasks by using prompts and\nvisual references from the original image. However, these models face\nchallenges in generating synthetic images that accurately reflect the content\nand structure of the original image due to difficulties in creating effective\nprompts and visual references. In this work, we introduce an effective data\naugmentation pipeline for semantic segmentation using Controllable Diffusion\nmodel. Our proposed method includes efficient prompt generation using\n\\textit{Class-Prompt Appending} and \\textit{Visual Prior Blending} to enhance\nattention to labeled classes in real images, allowing the pipeline to generate\na precise number of augmented images while preserving the structure of\nsegmentation-labeled classes. In addition, we implement a \\textit{class\nbalancing algorithm} to ensure a balanced training dataset when merging the\nsynthetic and original images. Evaluation on PASCAL VOC datasets, our pipeline\ndemonstrates its effectiveness in generating high-quality synthetic images for\nsemantic segmentation. Our code is available at\n\\href{https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance}{this\nhttps URL}.\n","authors":["Quang-Huy Che","Duc-Tri Le","Bich-Nga Pham","Duc-Khai Lam","Vinh-Tiep Nguyen"],"pdf_url":"https://arxiv.org/pdf/2409.06002v3.pdf","comment":"Accepted to ICPRAM 2025"},{"id":"http://arxiv.org/abs/2412.17417v1","updated":"2024-12-23T09:29:40Z","published":"2024-12-23T09:29:40Z","title":"Multimodal Preference Data Synthetic Alignment with Reward Model","summary":"  Multimodal large language models (MLLMs) have significantly advanced tasks\nlike caption generation and visual question answering by integrating visual and\ntextual data. However, they sometimes produce misleading or hallucinate content\ndue to discrepancies between their pre-training data and real user prompts.\nExisting approaches using Direct Preference Optimization (DPO) in\nvision-language tasks often rely on strong models like GPT-4 or CLIP to\ndetermine positive and negative responses. Here, we propose a new framework in\ngenerating synthetic data using a reward model as a proxy of human preference\nfor effective multimodal alignment with DPO training. The resulting DPO dataset\nranges from 2K to 9K image-text pairs, was evaluated on LLaVA-v1.5-7B, where\nour approach demonstrated substantial improvements in both the trustworthiness\nand reasoning capabilities of the base model across multiple hallucination and\nvision-language benchmark. The experiment results indicate that integrating\nselected synthetic data, such as from generative and rewards models can\neffectively reduce reliance on human-annotated data while enhancing MLLMs'\nalignment capability, offering a scalable solution for safer deployment.\n","authors":["Robert Wijaya","Ngoc-Bao Nguyen","Ngai-Man Cheung"],"pdf_url":"https://arxiv.org/pdf/2412.17417v1.pdf","comment":"Project Page: https://pds-dpo.github.io/"},{"id":"http://arxiv.org/abs/2412.17415v1","updated":"2024-12-23T09:26:38Z","published":"2024-12-23T09:26:38Z","title":"VidCtx: Context-aware Video Question Answering with Image Models","summary":"  To address computational and memory limitations of Large Multimodal Models in\nthe Video Question-Answering task, several recent methods extract textual\nrepresentations per frame (e.g., by captioning) and feed them to a Large\nLanguage Model (LLM) that processes them to produce the final response.\nHowever, in this way, the LLM does not have access to visual information and\noften has to process repetitive textual descriptions of nearby frames. To\naddress those shortcomings, in this paper, we introduce VidCtx, a novel\ntraining-free VideoQA framework which integrates both modalities, i.e. both\nvisual information from input frames and textual descriptions of others frames\nthat give the appropriate context. More specifically, in the proposed framework\na pre-trained Large Multimodal Model (LMM) is prompted to extract at regular\nintervals, question-aware textual descriptions (captions) of video frames.\nThose will be used as context when the same LMM will be prompted to answer the\nquestion at hand given as input a) a certain frame, b) the question and c) the\ncontext/caption of an appropriate frame. To avoid redundant information, we\nchose as context the descriptions of distant frames. Finally, a simple yet\neffective max pooling mechanism is used to aggregate the frame-level decisions.\nThis methodology enables the model to focus on the relevant segments of the\nvideo and scale to a high number of frames. Experiments show that VidCtx\nachieves competitive performance among approaches that rely on open models on\nthree public Video QA benchmarks, NExT-QA, IntentQA and STAR.\n","authors":["Andreas Goulas","Vasileios Mezaris","Ioannis Patras"],"pdf_url":"https://arxiv.org/pdf/2412.17415v1.pdf","comment":"Submitted for publication"},{"id":"http://arxiv.org/abs/2412.17405v1","updated":"2024-12-23T09:16:10Z","published":"2024-12-23T09:16:10Z","title":"Impact of Evidence Theory Uncertainty on Training Object Detection\n  Models","summary":"  This paper investigates the use of Evidence Theory to enhance the training\nefficiency of object detection models by incorporating uncertainty into the\nfeedback loop. In each training iteration, during the validation phase,\nEvidence Theory is applied to establish a relationship between ground truth\nlabels and predictions. The Dempster-Shafer rule of combination is used to\nquantify uncertainty based on the evidence from these predictions. This\nuncertainty measure is then utilized to weight the feedback loss for the\nsubsequent iteration, allowing the model to adjust its learning dynamically. By\nexperimenting with various uncertainty-weighting strategies, this study aims to\ndetermine the most effective method for optimizing feedback to accelerate the\ntraining process. The results demonstrate that using uncertainty-based feedback\nnot only reduces training time but can also enhance model performance compared\nto traditional approaches. This research offers insights into the role of\nuncertainty in improving machine learning workflows, particularly in object\ndetection, and suggests broader applications for uncertainty-driven training\nacross other AI disciplines.\n","authors":["M. Tahasanul Ibrahim","Rifshu Hussain Shaik","Andreas Schwung"],"pdf_url":"https://arxiv.org/pdf/2412.17405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13222v3","updated":"2024-12-23T09:14:23Z","published":"2024-09-20T05:16:06Z","title":"3D-GSW: 3D Gaussian Splatting for Robust Watermarking","summary":"  As 3D Gaussian Splatting~(3D-GS) gains significant attention and its\ncommercial usage increases, the need for watermarking technologies to prevent\nunauthorized use of the 3D-GS models and rendered images has become\nincreasingly important. In this paper, we introduce a robust watermarking\nmethod for 3D-GS that secures ownership of both the model and its rendered\nimages. Our proposed method remains robust against distortions in rendered\nimages and model attacks while maintaining high rendering quality. To achieve\nthese objectives, we present Frequency-Guided Densification~(FGD), which\nremoves 3D Gaussians based on their contribution to rendering quality,\nenhancing real-time rendering and the robustness of the message. FGD utilizes\nDiscrete Fourier Transform to split 3D Gaussians in high-frequency areas,\nimproving rendering quality. Furthermore, we employ a gradient mask for 3D\nGaussians and design a wavelet-subband loss to enhance rendering quality. Our\nexperiments show that our method embeds the message in the rendered images\ninvisibly and robustly against various attacks, including model distortion. Our\nmethod achieves state-of-the-art performance. Project page:\nhttps://kuai-lab.github.io/3dgsw2024/\n","authors":["Youngdong Jang","Hyunje Park","Feng Yang","Heeju Ko","Euijin Choo","Sangpil Kim"],"pdf_url":"https://arxiv.org/pdf/2409.13222v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14535v2","updated":"2024-12-23T09:14:13Z","published":"2024-12-19T05:23:49Z","title":"DAMPER: A Dual-Stage Medical Report Generation Framework with\n  Coarse-Grained MeSH Alignment and Fine-Grained Hypergraph Matching","summary":"  Medical report generation is crucial for clinical diagnosis and patient\nmanagement, summarizing diagnoses and recommendations based on medical imaging.\nHowever, existing work often overlook the clinical pipeline involved in report\nwriting, where physicians typically conduct an initial quick review followed by\na detailed examination. Moreover, current alignment methods may lead to\nmisaligned relationships. To address these issues, we propose DAMPER, a\ndual-stage framework for medical report generation that mimics the clinical\npipeline of report writing in two stages. In the first stage, a MeSH-Guided\nCoarse-Grained Alignment (MCG) stage that aligns chest X-ray (CXR) image\nfeatures with medical subject headings (MeSH) features to generate a rough\nkeyphrase representation of the overall impression. In the second stage, a\nHypergraph-Enhanced Fine-Grained Alignment (HFG) stage that constructs\nhypergraphs for image patches and report annotations, modeling high-order\nrelationships within each modality and performing hypergraph matching to\ncapture semantic correlations between image regions and textual phrases.\nFinally,the coarse-grained visual features, generated MeSH representations, and\nvisual hypergraph features are fed into a report decoder to produce the final\nmedical report. Extensive experiments on public datasets demonstrate the\neffectiveness of DAMPER in generating comprehensive and accurate medical\nreports, outperforming state-of-the-art methods across various evaluation\nmetrics.\n","authors":["Xiaofei Huang","Wenting Chen","Jie Liu","Qisheng Lu","Xiaoling Luo","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2412.14535v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17401v1","updated":"2024-12-23T09:06:27Z","published":"2024-12-23T09:06:27Z","title":"Learning Dynamic Local Context Representations for Infrared Small Target\n  Detection","summary":"  Infrared small target detection (ISTD) is challenging due to complex\nbackgrounds, low signal-to-clutter ratios, and varying target sizes and shapes.\nEffective detection relies on capturing local contextual information at the\nappropriate scale. However, small-kernel CNNs have limited receptive fields,\nleading to false alarms, while transformer models, with global receptive\nfields, often treat small targets as noise, resulting in miss-detections.\nHybrid models struggle to bridge the semantic gap between CNNs and\ntransformers, causing high complexity.To address these challenges, we propose\nLCRNet, a novel method that learns dynamic local context representations for\nISTD. The model consists of three components: (1) C2FBlock, inspired by PDE\nsolvers, for efficient small target information capture; (2) DLC-Attention, a\nlarge-kernel attention mechanism that dynamically builds context and reduces\nfeature redundancy; and (3) HLKConv, a hierarchical convolution operator based\non large-kernel decomposition that preserves sparsity and mitigates the\ndrawbacks of dilated convolutions. Despite its simplicity, with only 1.65M\nparameters, LCRNet achieves state-of-the-art (SOTA) performance.Experiments on\nmultiple datasets, comparing LCRNet with 33 SOTA methods, demonstrate its\nsuperior performance and efficiency.\n","authors":["Guoyi Zhang","Guangsheng Xu","Han Wang","Siyang Chen","Yunxiao Shan","Xiaohu Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.17401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03001v2","updated":"2024-12-23T09:03:02Z","published":"2024-08-06T07:19:51Z","title":"One Framework to Rule Them All: Unifying Multimodal Tasks with LLM\n  Neural-Tuning","summary":"  Large-scale models have exhibited remarkable capabilities across diverse\ndomains, including automated medical services and intelligent customer support.\nHowever, as most large models are trained on single-modality corpora, enabling\nthem to effectively process and understand multimodal signals remains a\nsignificant challenge. Current research often focuses on designing\ntask-specific or scenario-specific tuning strategies, which limits the\nscalability and versatility. To address this limitation, we propose a unified\nframework that concurrently handles multiple tasks and modalities. In this\nframework, all modalities and tasks are represented as unified tokens and\ntrained using a single, consistent approach. To enable efficient multitask\nprocessing, we introduce a novel tuning strategy termed neural tuning, inspired\nby the concept of sparse distributed representation in the human brain, where\nonly specific subsets of neurons are activated for each task. Furthermore, to\nadvance research in multimodal and multitask learning, we present a new\nbenchmark, MMUD, which includes samples annotated with multiple task labels\nspanning reasoning segmentation, referring segmentation, image captioning, and\ntext-to-image generation. By applying neural tuning to pretrained large models\non the MMUD benchmark, we demonstrate the ability to handle multiple tasks\nsimultaneously in a streamlined and efficient manner. All models, code, and\ndatasets will be released publicly upon publication, fostering further research\nand innovation in this field.\n","authors":["Hao Sun","Yu Song","Jiaqing Liu","Jihong Hu","Yen-Wei Chen","Lanfen Lin"],"pdf_url":"https://arxiv.org/pdf/2408.03001v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15976v2","updated":"2024-12-23T09:01:29Z","published":"2024-11-24T20:35:04Z","title":"DRIVE: Dual-Robustness via Information Variability and Entropic\n  Consistency in Source-Free Unsupervised Domain Adaptation","summary":"  Adapting machine learning models to new domains without labeled data,\nespecially when source data is inaccessible, is a critical challenge in\napplications like medical imaging, autonomous driving, and remote sensing. This\ntask, known as Source-Free Unsupervised Domain Adaptation (SFUDA), involves\nadapting a pre-trained model to a target domain using only unlabeled target\ndata, which can lead to issues such as overfitting, underfitting, and poor\ngeneralization due to domain discrepancies and noise. Existing SFUDA methods\noften rely on single-model architectures, struggling with uncertainty and\nvariability in the target domain. To address these challenges, we propose DRIVE\n(Dual-Robustness through Information Variability and Entropy), a novel SFUDA\nframework leveraging a dual-model architecture. The two models, initialized\nwith identical weights, work in parallel to capture diverse target domain\ncharacteristics. One model is exposed to perturbations via projection gradient\ndescent (PGD) guided by mutual information, focusing on high-uncertainty\nregions. We also introduce an entropy-aware pseudo-labeling strategy that\nadjusts label weights based on prediction uncertainty, ensuring the model\nfocuses on reliable data while avoiding noisy regions. The adaptation process\nhas two stages: the first aligns the models on stable features using a mutual\ninformation consistency loss, and the second dynamically adjusts the\nperturbation level based on the loss from the first stage, encouraging the\nmodel to explore a broader range of the target domain while preserving existing\nperformance. This enhances generalization capabilities and robustness against\ninterference. Evaluations on standard SFUDA benchmarks show that DRIVE\nconsistently outperforms previous methods, delivering improved adaptation\naccuracy and stability across complex target domains.\n","authors":["Ruiqiang Xiao","Songning Lai","Yijun Yang","Jiemin Wu","Yutao Yue","Lei Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.15976v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17397v1","updated":"2024-12-23T08:51:48Z","published":"2024-12-23T08:51:48Z","title":"Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search\n  Boosted Reasoning via Iterative Preference Learning","summary":"  With current state-of-the-art approaches aimed at enhancing the reasoning\ncapabilities of Large Language Models(LLMs) through iterative preference\nlearning inspired by AlphaZero, we propose to further enhance the step-wise\nreasoning capabilities through intrinsic self-correction to some extent. Our\nwork leverages step-wise preference learning to enhance self-verification via\nreinforcement learning. We initially conduct our work through a two-stage\ntraining procedure. At the first stage, the self-correction reasoning ability\nof an LLM is enhanced through its own predictions, relying entirely on\nself-generated data within the intrinsic self-correction to some extent. At the\nsecond stage, the baseline step-wise preference learning is leveraged via the\napplication of the enhanced self-correct policy achieved at the first stage. In\nthe evaluation of arithmetic reasoning tasks, our approach outperforms\nOpenMath2-Llama3.1-8B, dart-math-mistral-7b-uniform on MATH with increases in\naccuracy to 71.34%(+4.18%) and 48.06%(+4.94%) and LLama-3.1-8B-Instruct,\nMistral-7B-Instruct-v0.1 on GSM8K with increases in accuracy to 86.76%(+2.00%)\nand 38.06%(+2.28%).\n","authors":["Huchen Jiang","Yangyang Ma","Chaofan Ding","Kexin Luan","Xinhan Di"],"pdf_url":"https://arxiv.org/pdf/2412.17397v1.pdf","comment":"6 Pages,3 figures, accepted by AAAI 2025 Workshop NeurMAD"},{"id":"http://arxiv.org/abs/2412.17390v1","updated":"2024-12-23T08:43:39Z","published":"2024-12-23T08:43:39Z","title":"PointVoxelFormer -- Reviving point cloud networks for 3D medical imaging","summary":"  Point clouds are a very efficient way to represent volumetric data in medical\nimaging. First, they do not occupy resources for empty spaces and therefore can\navoid trade-offs between resolution and field-of-view for voxel-based 3D\nconvolutional networks (CNNs) - leading to smaller and robust models. Second,\nthey provide a modality agnostic representation of anatomical surfaces and\nshapes to avoid domain gaps for generic geometric models. Third, they remove\nidentifiable patient-specific information and may increase privacy preservation\nwhen publicly sharing data. Despite their benefits, point clouds are still\nunderexplored in medical imaging compared to volumetric 3D CNNs and vision\ntransformers. To date both datasets and stringent studies on comparative\nstrengths and weaknesses of methodological choices are missing. Interactions\nand information exchange of spatially close points - e.g. through k-nearest\nneighbour graphs in edge convolutions or point transformations - within points\nclouds are crucial for learning geometrically meaningful features but may incur\ncomputational bottlenecks. This work presents a hybrid approach that combines\npoint-wise operations with intermediate differentiable rasterisation and dense\nlocalised CNNs. For deformable point cloud registration, we devise an early\nfusion scheme for coordinate features that joins both clouds within a common\nreference frame and is coupled with an inverse consistent, two-step alignment\narchitecture. Our extensive experiments on three different datasets for\nsegmentation and registration demonstrate that our method, PointVoxelFormer,\nenables very compact models that excel with threefold speed-ups, fivefold\nmemory reduction and over 30% registration error reduction against edge\nconvolutions and other state-of-the-art models in geometric deep learning.\n","authors":["Mattias Paul Heinrich"],"pdf_url":"https://arxiv.org/pdf/2412.17390v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.17387v1","updated":"2024-12-23T08:40:08Z","published":"2024-12-23T08:40:08Z","title":"Singular Value Scaling: Efficient Generative Model Compression via\n  Pruned Weights Refinement","summary":"  While pruning methods effectively maintain model performance without extra\ntraining costs, they often focus solely on preserving crucial connections,\noverlooking the impact of pruned weights on subsequent fine-tuning or\ndistillation, leading to inefficiencies. Moreover, most compression techniques\nfor generative models have been developed primarily for GANs, tailored to\nspecific architectures like StyleGAN, and research into compressing Diffusion\nmodels has just begun. Even more, these methods are often applicable only to\nGANs or Diffusion models, highlighting the need for approaches that work across\nboth model types. In this paper, we introduce Singular Value Scaling (SVS), a\nversatile technique for refining pruned weights, applicable to both model\ntypes. Our analysis reveals that pruned weights often exhibit dominant singular\nvectors, hindering fine-tuning efficiency and leading to suboptimal performance\ncompared to random initialization. Our method enhances weight initialization by\nminimizing the disparities between singular values of pruned weights, thereby\nimproving the fine-tuning process. This approach not only guides the compressed\nmodel toward superior solutions but also significantly speeds up fine-tuning.\nExtensive experiments on StyleGAN2, StyleGAN3 and DDPM demonstrate that SVS\nimproves compression performance across model types without additional training\ncosts. Our code is available at:\nhttps://github.com/LAIT-CVLab/Singular_Value_Scaling.\n","authors":["Hyeonjin Kim","Jaejun Yoo"],"pdf_url":"https://arxiv.org/pdf/2412.17387v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.17378v1","updated":"2024-12-23T08:26:30Z","published":"2024-12-23T08:26:30Z","title":"Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained\n  Tiling","summary":"  3D Gaussian Splatting (3DGS) is increasingly attracting attention in both\nacademia and industry owing to its superior visual quality and rendering speed.\nHowever, training a 3DGS model remains a time-intensive task, especially in\nload imbalance scenarios where workload diversity among pixels and Gaussian\nspheres causes poor renderCUDA kernel performance. We introduce Balanced 3DGS,\na Gaussian-wise parallelism rendering with fine-grained tiling approach in 3DGS\ntraining process, perfectly solving load-imbalance issues. First, we\ninnovatively introduce the inter-block dynamic workload distribution technique\nto map workloads to Streaming Multiprocessor(SM) resources within a single GPU\ndynamically, which constitutes the foundation of load balancing. Second, we are\nthe first to propose the Gaussian-wise parallel rendering technique to\nsignificantly reduce workload divergence inside a warp, which serves as a\ncritical component in addressing load imbalance. Based on the above two\nmethods, we further creatively put forward the fine-grained combined load\nbalancing technique to uniformly distribute workload across all SMs, which\nboosts the forward renderCUDA kernel performance by up to 7.52x. Besides, we\npresent a self-adaptive render kernel selection strategy during the 3DGS\ntraining process based on different load-balance situations, which effectively\nimproves training efficiency.\n","authors":["Hao Gui","Lin Hu","Rui Chen","Mingxiao Huang","Yuxin Yin","Jin Yang","Yong Wu"],"pdf_url":"https://arxiv.org/pdf/2412.17378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17377v1","updated":"2024-12-23T08:26:00Z","published":"2024-12-23T08:26:00Z","title":"A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild\n  High-Difficulty Motions","summary":"  Extracting physically plausible 3D human motion from videos is a critical\ntask. Although existing simulation-based motion imitation methods can enhance\nthe physical quality of daily motions estimated from monocular video capture,\nextending this capability to high-difficulty motions remains an open challenge.\nThis can be attributed to some flawed motion clips in video-based motion\ncapture results and the inherent complexity in modeling high-difficulty\nmotions. Therefore, sensing the advantage of segmentation in localizing human\nbody, we introduce a mask-based motion correction module (MCM) that leverages\nmotion context and video mask to repair flawed motions, producing\nimitation-friendly motions; and propose a physics-based motion transfer module\n(PTM), which employs a pretrain and adapt approach for motion imitation,\nimproving physical plausibility with the ability to handle in-the-wild and\nchallenging motions. Our approach is designed as a plug-and-play module to\nphysically refine the video motion capture results, including high-difficulty\nin-the-wild motions. Finally, to validate our approach, we collected a\nchallenging in-the-wild test set to establish a benchmark, and our method has\ndemonstrated effectiveness on both the new benchmark and existing public\ndatasets.https://physicalmotionrestoration.github.io\n","authors":["Youliang Zhang","Ronghui Li","Yachao Zhang","Liang Pan","Jingbo Wang","Yebin Liu","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2412.17377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15876v2","updated":"2024-12-23T08:10:30Z","published":"2024-08-28T15:47:32Z","title":"Unleashing the Temporal-Spatial Reasoning Capacity of GPT for\n  Training-Free Audio and Language Referenced Video Object Segmentation","summary":"  In this paper, we propose an Audio-Language-Referenced SAM 2 (AL-Ref-SAM 2)\npipeline to explore the training-free paradigm for audio and\nlanguage-referenced video object segmentation, namely AVS and RVOS tasks. The\nintuitive solution leverages GroundingDINO to identify the target object from a\nsingle frame and SAM 2 to segment the identified object throughout the video,\nwhich is less robust to spatiotemporal variations due to a lack of video\ncontext exploration. Thus, in our AL-Ref-SAM 2 pipeline, we propose a novel\nGPT-assisted Pivot Selection (GPT-PS) module to instruct GPT-4 to perform\ntwo-step temporal-spatial reasoning for sequentially selecting pivot frames and\npivot boxes, thereby providing SAM 2 with a high-quality initial object prompt.\nWithin GPT-PS, two task-specific Chain-of-Thought prompts are designed to\nunleash GPT's temporal-spatial reasoning capacity by guiding GPT to make\nselections based on a comprehensive understanding of video and reference\ninformation. Furthermore, we propose a Language-Binded Reference Unification\n(LBRU) module to convert audio signals into language-formatted references,\nthereby unifying the formats of AVS and RVOS tasks in the same pipeline.\nExtensive experiments on both tasks show that our training-free AL-Ref-SAM 2\npipeline achieves performances comparable to or even better than\nfully-supervised fine-tuning methods. The code is available at:\nhttps://github.com/appletea233/AL-Ref-SAM2.\n","authors":["Shaofei Huang","Rui Ling","Hongyu Li","Tianrui Hui","Zongheng Tang","Xiaoming Wei","Jizhong Han","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2408.15876v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14598v2","updated":"2024-12-23T08:08:19Z","published":"2024-12-19T07:39:06Z","title":"Can We Get Rid of Handcrafted Feature Extractors? SparseViT:\n  Nonsemantics-Centered, Parameter-Efficient Image Manipulation Localization\n  through Spare-Coding Transformer","summary":"  Non-semantic features or semantic-agnostic features, which are irrelevant to\nimage context but sensitive to image manipulations, are recognized as\nevidential to Image Manipulation Localization (IML). Since manual labels are\nimpossible, existing works rely on handcrafted methods to extract non-semantic\nfeatures. Handcrafted non-semantic features jeopardize IML model's\ngeneralization ability in unseen or complex scenarios. Therefore, for IML, the\nelephant in the room is: How to adaptively extract non-semantic features?\nNon-semantic features are context-irrelevant and manipulation-sensitive. That\nis, within an image, they are consistent across patches unless manipulation\noccurs. Then, spare and discrete interactions among image patches are\nsufficient for extracting non-semantic features. However, image semantics vary\ndrastically on different patches, requiring dense and continuous interactions\namong image patches for learning semantic representations. Hence, in this\npaper, we propose a Sparse Vision Transformer (SparseViT), which reformulates\nthe dense, global self-attention in ViT into a sparse, discrete manner. Such\nsparse self-attention breaks image semantics and forces SparseViT to adaptively\nextract non-semantic features for images. Besides, compared with existing IML\nmodels, the sparse self-attention mechanism largely reduced the model size (max\n80% in FLOPs), achieving stunning parameter efficiency and computation\nreduction. Extensive experiments demonstrate that, without any handcrafted\nfeature extractors, SparseViT is superior in both generalization and efficiency\nacross benchmark datasets.\n","authors":["Lei Su","Xiaochen Ma","Xuekang Zhu","Chaoqun Niu","Zeyu Lei","Ji-Zhe Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14598v2.pdf","comment":"published to AAAI2025"},{"id":"http://arxiv.org/abs/2401.15947v5","updated":"2024-12-23T08:05:14Z","published":"2024-01-29T08:13:40Z","title":"MoE-LLaVA: Mixture of Experts for Large Vision-Language Models","summary":"  Recent advances demonstrate that scaling Large Vision-Language Models (LVLMs)\neffectively improves downstream task performances. However, existing scaling\nmethods enable all model parameters to be active for each token in the\ncalculation, which brings massive training and inferring costs. In this work,\nwe propose a simple yet effective training strategy MoE-Tuning for LVLMs. This\nstrategy innovatively addresses the common issue of performance degradation in\nmulti-modal sparsity learning, consequently constructing a sparse model with an\noutrageous number of parameters but a constant computational cost. Furthermore,\nwe present the MoE-LLaVA, a MoE-based sparse LVLM architecture, which uniquely\nactivates only the top-k experts through routers during deployment, keeping the\nremaining experts inactive. Extensive experiments show the significant\nperformance of MoE-LLaVA in a variety of visual understanding and object\nhallucination benchmarks. Remarkably, with only approximately 3B sparsely\nactivated parameters, MoE-LLaVA demonstrates performance comparable to the\nLLaVA-1.5-7B on various visual understanding datasets and even surpasses the\nLLaVA-1.5-13B in object hallucination benchmark. Through MoE-LLaVA, we aim to\nestablish a baseline for sparse LVLMs and provide valuable insights for future\nresearch in developing more efficient and effective multi-modal learning\nsystems. Code is released at https://github.com/PKU-YuanGroup/MoE-LLaVA.\n","authors":["Bin Lin","Zhenyu Tang","Yang Ye","Jinfa Huang","Junwu Zhang","Yatian Pang","Peng Jin","Munan Ning","Jiebo Luo","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2401.15947v5.pdf","comment":"update author"},{"id":"http://arxiv.org/abs/2412.17366v1","updated":"2024-12-23T08:03:59Z","published":"2024-12-23T08:03:59Z","title":"FlowMamba: Learning Point Cloud Scene Flow with Global Motion\n  Propagation","summary":"  Scene flow methods based on deep learning have achieved impressive\nperformance. However, current top-performing methods still struggle with\nill-posed regions, such as extensive flat regions or occlusions, due to\ninsufficient local evidence. In this paper, we propose a novel global-aware\nscene flow estimation network with global motion propagation, named FlowMamba.\nThe core idea of FlowMamba is a novel Iterative Unit based on the State Space\nModel (ISU), which first propagates global motion patterns and then adaptively\nintegrates the global motion information with previously hidden states. As the\nirregular nature of point clouds limits the performance of ISU in global motion\npropagation, we propose a feature-induced ordering strategy (FIO). The FIO\nleverages semantic-related and motion-related features to order points into a\nsequence characterized by spatial continuity. Extensive experiments demonstrate\nthe effectiveness of FlowMamba, with 21.9\\% and 20.5\\% EPE3D reduction from the\nbest published results on FlyingThings3D and KITTI datasets. Specifically, our\nFlowMamba is the first method to achieve millimeter-level prediction accuracy\nin FlyingThings3D and KITTI. Furthermore, the proposed ISU can be seamlessly\nembedded into existing iterative networks as a plug-and-play module, improving\ntheir estimation accuracy significantly.\n","authors":["Min Lin","Gangwei Xu","Yun Wang","Xianqi Wang","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2412.17366v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2410.01341v2","updated":"2024-12-23T07:55:56Z","published":"2024-10-02T08:58:34Z","title":"Cognition Transferring and Decoupling for Text-supervised Egocentric\n  Semantic Segmentation","summary":"  In this paper, we explore a novel Text-supervised Egocentic Semantic\nSegmentation (TESS) task that aims to assign pixel-level categories to\negocentric images weakly supervised by texts from image-level labels. In this\ntask with prospective potential, the egocentric scenes contain dense\nwearer-object relations and inter-object interference. However, most recent\nthird-view methods leverage the frozen Contrastive Language-Image Pre-training\n(CLIP) model, which is pre-trained on the semantic-oriented third-view data and\nlapses in the egocentric view due to the ``relation insensitive\" problem.\nHence, we propose a Cognition Transferring and Decoupling Network (CTDN) that\nfirst learns the egocentric wearer-object relations via correlating the image\nand text. Besides, a Cognition Transferring Module (CTM) is developed to\ndistill the cognitive knowledge from the large-scale pre-trained model to our\nmodel for recognizing egocentric objects with various semantics. Based on the\ntransferred cognition, the Foreground-background Decoupling Module (FDM)\ndisentangles the visual representations to explicitly discriminate the\nforeground and background regions to mitigate false activation areas caused by\nforeground-background interferential objects during egocentric relation\nlearning. Extensive experiments on four TESS benchmarks demonstrate the\neffectiveness of our approach, which outperforms many recent related methods by\na large margin. Code will be available at https://github.com/ZhaofengSHI/CTDN.\n","authors":["Zhaofeng Shi","Heqian Qiu","Lanxiao Wang","Fanman Meng","Qingbo Wu","Hongliang Li"],"pdf_url":"https://arxiv.org/pdf/2410.01341v2.pdf","comment":"Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)"},{"id":"http://arxiv.org/abs/2408.17005v2","updated":"2024-12-23T07:44:13Z","published":"2024-08-30T04:37:52Z","title":"Efficient Camera Exposure Control for Visual Odometry via Deep\n  Reinforcement Learning","summary":"  The stability of visual odometry (VO) systems is undermined by degraded image\nquality, especially in environments with significant illumination changes. This\nstudy employs a deep reinforcement learning (DRL) framework to train agents for\nexposure control, aiming to enhance imaging performance in challenging\nconditions. A lightweight image simulator is developed to facilitate the\ntraining process, enabling the diversification of image exposure and sequence\ntrajectory. This setup enables completely offline training, eliminating the\nneed for direct interaction with camera hardware and the real environments.\nDifferent levels of reward functions are crafted to enhance the VO systems,\nequipping the DRL agents with varying intelligence. Extensive experiments have\nshown that our exposure control agents achieve superior efficiency-with an\naverage inference duration of 1.58 ms per frame on a CPU-and respond more\nquickly than traditional feedback control schemes. By choosing an appropriate\nreward function, agents acquire an intelligent understanding of motion trends\nand anticipate future illumination changes. This predictive capability allows\nVO systems to deliver more stable and precise odometry results. The codes and\ndatasets are available at https://github.com/ShuyangUni/drl_exposure_ctrl.\n","authors":["Shuyang Zhang","Jinhao He","Yilong Zhu","Jin Wu","Jie Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.17005v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.17267v2","updated":"2024-12-23T07:25:51Z","published":"2024-08-30T13:13:35Z","title":"UrBench: A Comprehensive Benchmark for Evaluating Large Multimodal\n  Models in Multi-View Urban Scenarios","summary":"  Recent evaluations of Large Multimodal Models (LMMs) have explored their\ncapabilities in various domains, with only few benchmarks specifically focusing\non urban environments. Moreover, existing urban benchmarks have been limited to\nevaluating LMMs with basic region-level urban tasks under singular views,\nleading to incomplete evaluations of LMMs' abilities in urban environments. To\naddress these issues, we present UrBench, a comprehensive benchmark designed\nfor evaluating LMMs in complex multi-view urban scenarios. UrBench contains\n11.6K meticulously curated questions at both region-level and role-level that\ncover 4 task dimensions: Geo-Localization, Scene Reasoning, Scene\nUnderstanding, and Object Understanding, totaling 14 task types. In\nconstructing UrBench, we utilize data from existing datasets and additionally\ncollect data from 11 cities, creating new annotations using a cross-view\ndetection-matching method. With these images and annotations, we then integrate\nLMM-based, rule-based, and human-based methods to construct large-scale\nhigh-quality questions. Our evaluations on 21 LMMs show that current LMMs\nstruggle in the urban environments in several aspects. Even the best performing\nGPT-4o lags behind humans in most tasks, ranging from simple tasks such as\ncounting to complex tasks such as orientation, localization and object\nattribute recognition, with an average performance gap of 17.4%. Our benchmark\nalso reveals that LMMs exhibit inconsistent behaviors with different urban\nviews, especially with respect to understanding cross-view relations. UrBench\ndatasets and benchmark results will be publicly available at\nhttps://opendatalab.github.io/UrBench/.\n","authors":["Baichuan Zhou","Haote Yang","Dairong Chen","Junyan Ye","Tianyi Bai","Jinhua Yu","Songyang Zhang","Dahua Lin","Conghui He","Weijia Li"],"pdf_url":"https://arxiv.org/pdf/2408.17267v2.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.17350v1","updated":"2024-12-23T07:21:41Z","published":"2024-12-23T07:21:41Z","title":"DiffFormer: a Differential Spatial-Spectral Transformer for\n  Hyperspectral Image Classification","summary":"  Hyperspectral image classification (HSIC) has gained significant attention\nbecause of its potential in analyzing high-dimensional data with rich spectral\nand spatial information. In this work, we propose the Differential\nSpatial-Spectral Transformer (DiffFormer), a novel framework designed to\naddress the inherent challenges of HSIC, such as spectral redundancy and\nspatial discontinuity. The DiffFormer leverages a Differential Multi-Head\nSelf-Attention (DMHSA) mechanism, which enhances local feature discrimination\nby introducing differential attention to accentuate subtle variations across\nneighboring spectral-spatial patches. The architecture integrates\nSpectral-Spatial Tokenization through three-dimensional (3D) convolution-based\npatch embeddings, positional encoding, and a stack of transformer layers\nequipped with the SWiGLU activation function for efficient feature extraction\n(SwiGLU is a variant of the Gated Linear Unit (GLU) activation function). A\ntoken-based classification head further ensures robust representation learning,\nenabling precise labeling of hyperspectral pixels. Extensive experiments on\nbenchmark hyperspectral datasets demonstrate the superiority of DiffFormer in\nterms of classification accuracy, computational efficiency, and\ngeneralizability, compared to existing state-of-the-art (SOTA) methods. In\naddition, this work provides a detailed analysis of computational complexity,\nshowcasing the scalability of the model for large-scale remote sensing\napplications. The source code will be made available at\n\\url{https://github.com/mahmad000/DiffFormer} after the first round of\nrevision.\n","authors":["Muhammad Ahmad","Manuel Mazzara","Salvatore Distefano","Adil Mehmood Khan","Silvia Liberata Ullo"],"pdf_url":"https://arxiv.org/pdf/2412.17350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13190v2","updated":"2024-12-23T07:19:04Z","published":"2024-12-17T18:59:33Z","title":"MotionBridge: Dynamic Video Inbetweening with Flexible Controls","summary":"  By generating plausible and smooth transitions between two image frames,\nvideo inbetweening is an essential tool for video editing and long video\nsynthesis. Traditional works lack the capability to generate complex large\nmotions. While recent video generation techniques are powerful in creating\nhigh-quality results, they often lack fine control over the details of\nintermediate frames, which can lead to results that do not align with the\ncreative mind. We introduce MotionBridge, a unified video inbetweening\nframework that allows flexible controls, including trajectory strokes,\nkeyframes, masks, guide pixels, and text. However, learning such multi-modal\ncontrols in a unified framework is a challenging task. We thus design two\ngenerators to extract the control signal faithfully and encode feature through\ndual-branch embedders to resolve ambiguities. We further introduce a curriculum\ntraining strategy to smoothly learn various controls. Extensive qualitative and\nquantitative experiments have demonstrated that such multi-modal controls\nenable a more dynamic, customizable, and contextually accurate visual\nnarrative.\n","authors":["Maham Tanveer","Yang Zhou","Simon Niklaus","Ali Mahdavi Amiri","Hao Zhang","Krishna Kumar Singh","Nanxuan Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.13190v2.pdf","comment":"Project website: [https://motionbridge.github.io/]"},{"id":"http://arxiv.org/abs/2412.17346v1","updated":"2024-12-23T07:18:13Z","published":"2024-12-23T07:18:13Z","title":"FFA Sora, video generation as fundus fluorescein angiography simulator","summary":"  Fundus fluorescein angiography (FFA) is critical for diagnosing retinal\nvascular diseases, but beginners often struggle with image interpretation. This\nstudy develops FFA Sora, a text-to-video model that converts FFA reports into\ndynamic videos via a Wavelet-Flow Variational Autoencoder (WF-VAE) and a\ndiffusion transformer (DiT). Trained on an anonymized dataset, FFA Sora\naccurately simulates disease features from the input text, as confirmed by\nobjective metrics: Frechet Video Distance (FVD) = 329.78, Learned Perceptual\nImage Patch Similarity (LPIPS) = 0.48, and Visual-question-answering Score\n(VQAScore) = 0.61. Specific evaluations showed acceptable alignment between the\ngenerated videos and textual prompts, with BERTScore of 0.35. Additionally, the\nmodel demonstrated strong privacy-preserving performance in retrieval\nevaluations, achieving an average Recall@K of 0.073. Human assessments\nindicated satisfactory visual quality, with an average score of 1.570(scale: 1\n= best, 5 = worst). This model addresses privacy concerns associated with\nsharing large-scale FFA data and enhances medical education.\n","authors":["Xinyuan Wu","Lili Wang","Ruoyu Chen","Bowen Liu","Weiyi Zhang","Xi Yang","Yifan Feng","Mingguang He","Danli Shi"],"pdf_url":"https://arxiv.org/pdf/2412.17346v1.pdf","comment":"24 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.02664v3","updated":"2024-12-23T07:12:37Z","published":"2024-09-04T12:46:30Z","title":"Standing on the Shoulders of Giants: Reprogramming Visual-Language Model\n  for General Deepfake Detection","summary":"  The proliferation of deepfake faces poses huge potential negative impacts on\nour daily lives. Despite substantial advancements in deepfake detection over\nthese years, the generalizability of existing methods against forgeries from\nunseen datasets or created by emerging generative models remains constrained.\nIn this paper, inspired by the zero-shot advantages of Vision-Language Models\n(VLMs), we propose a novel approach that repurposes a well-trained VLM for\ngeneral deepfake detection. Motivated by the model reprogramming paradigm that\nmanipulates the model prediction via input perturbations, our method can\nreprogram a pre-trained VLM model (e.g., CLIP) solely based on manipulating its\ninput without tuning the inner parameters. First, learnable visual\nperturbations are used to refine feature extraction for deepfake detection.\nThen, we exploit information of face embedding to create sample-level\nadaptative text prompts, improving the performance. Extensive experiments on\nseveral popular benchmark datasets demonstrate that (1) the cross-dataset and\ncross-manipulation performances of deepfake detection can be significantly and\nconsistently improved (e.g., over 88\\% AUC in cross-dataset setting from FF++\nto WildDeepfake); (2) the superior performances are achieved with fewer\ntrainable parameters, making it a promising approach for real-world\napplications.\n","authors":["Kaiqing Lin","Yuzhen Lin","Weixiang Li","Taiping Yao","Bin Li"],"pdf_url":"https://arxiv.org/pdf/2409.02664v3.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.17337v1","updated":"2024-12-23T07:02:44Z","published":"2024-12-23T07:02:44Z","title":"Neural-MCRL: Neural Multimodal Contrastive Representation Learning for\n  EEG-based Visual Decoding","summary":"  Decoding neural visual representations from electroencephalogram (EEG)-based\nbrain activity is crucial for advancing brain-machine interfaces (BMI) and has\ntransformative potential for neural sensory rehabilitation. While multimodal\ncontrastive representation learning (MCRL) has shown promise in neural\ndecoding, existing methods often overlook semantic consistency and completeness\nwithin modalities and lack effective semantic alignment across modalities. This\nlimits their ability to capture the complex representations of visual neural\nresponses. We propose Neural-MCRL, a novel framework that achieves multimodal\nalignment through semantic bridging and cross-attention mechanisms, while\nensuring completeness within modalities and consistency across modalities. Our\nframework also features the Neural Encoder with Spectral-Temporal Adaptation\n(NESTA), a EEG encoder that adaptively captures spectral patterns and learns\nsubject-specific transformations. Experimental results demonstrate significant\nimprovements in visual decoding accuracy and model generalization compared to\nstate-of-the-art methods, advancing the field of EEG-based neural visual\nrepresentation decoding in BMI. Codes will be available at:\nhttps://github.com/NZWANG/Neural-MCRL.\n","authors":["Yueyang Li","Zijian Kang","Shengyu Gong","Wenhao Dong","Weiming Zeng","Hongjie Yan","Wai Ting Siok","Nizhuan Wang"],"pdf_url":"https://arxiv.org/pdf/2412.17337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15646v2","updated":"2024-12-23T06:52:45Z","published":"2024-12-20T08:05:13Z","title":"CustomTTT: Motion and Appearance Customized Video Generation via\n  Test-Time Training","summary":"  Benefiting from large-scale pre-training of text-video pairs, current\ntext-to-video (T2V) diffusion models can generate high-quality videos from the\ntext description. Besides, given some reference images or videos, the\nparameter-efficient fine-tuning method, i.e. LoRA, can generate high-quality\ncustomized concepts, e.g., the specific subject or the motions from a reference\nvideo. However, combining the trained multiple concepts from different\nreferences into a single network shows obvious artifacts. To this end, we\npropose CustomTTT, where we can joint custom the appearance and the motion of\nthe given video easily. In detail, we first analyze the prompt influence in the\ncurrent video diffusion model and find the LoRAs are only needed for the\nspecific layers for appearance and motion customization. Besides, since each\nLoRA is trained individually, we propose a novel test-time training technique\nto update parameters after combination utilizing the trained customized models.\nWe conduct detailed experiments to verify the effectiveness of the proposed\nmethods. Our method outperforms several state-of-the-art works in both\nqualitative and quantitative evaluations.\n","authors":["Xiuli Bi","Jian Lu","Bo Liu","Xiaodong Cun","Yong Zhang","Weisheng Li","Bin Xiao"],"pdf_url":"https://arxiv.org/pdf/2412.15646v2.pdf","comment":"Accepted in AAAI 2025. Project Page: https://customttt.github.io/\n  Code: https://github.com/RongPiKing/CustomTTT"},{"id":"http://arxiv.org/abs/2412.17331v1","updated":"2024-12-23T06:49:59Z","published":"2024-12-23T06:49:59Z","title":"Uncertainty-Participation Context Consistency Learning for\n  Semi-supervised Semantic Segmentation","summary":"  Semi-supervised semantic segmentation has attracted considerable attention\nfor its ability to mitigate the reliance on extensive labeled data. However,\nexisting consistency regularization methods only utilize high certain pixels\nwith prediction confidence surpassing a fixed threshold for training, failing\nto fully leverage the potential supervisory information within the network.\nTherefore, this paper proposes the Uncertainty-participation Context\nConsistency Learning (UCCL) method to explore richer supervisory signals.\nSpecifically, we first design the semantic backpropagation update (SBU)\nstrategy to fully exploit the knowledge from uncertain pixel regions, enabling\nthe model to learn consistent pixel-level semantic information from those\nareas. Furthermore, we propose the class-aware knowledge regulation (CKR)\nmodule to facilitate the regulation of class-level semantic features across\ndifferent augmented views, promoting consistent learning of class-level\nsemantic information within the encoder. Experimental results on two public\nbenchmarks demonstrate that our proposed method achieves state-of-the-art\nperformance. Our code is available at https://github.com/YUKEKEJAN/UCCL.\n","authors":["Jianjian Yin","Yi Chen","Zhichao Zheng","Junsheng Zhou","Yanhui Gu"],"pdf_url":"https://arxiv.org/pdf/2412.17331v1.pdf","comment":"To be published in ICASSP"},{"id":"http://arxiv.org/abs/2412.10116v2","updated":"2024-12-23T06:49:13Z","published":"2024-12-13T12:59:12Z","title":"HS-FPN: High Frequency and Spatial Perception FPN for Tiny Object\n  Detection","summary":"  The introduction of Feature Pyramid Network (FPN) has significantly improved\nobject detection performance. However, substantial challenges remain in\ndetecting tiny objects, as their features occupy only a very small proportion\nof the feature maps. Although FPN integrates multi-scale features, it does not\ndirectly enhance or enrich the features of tiny objects. Furthermore, FPN lacks\nspatial perception ability. To address these issues, we propose a novel High\nFrequency and Spatial Perception Feature Pyramid Network (HS-FPN) with two\ninnovative modules. First, we designed a high frequency perception module (HFP)\nthat generates high frequency responses through high pass filters. These high\nfrequency responses are used as mask weights from both spatial and channel\nperspectives to enrich and highlight the features of tiny objects in the\noriginal feature maps. Second, we developed a spatial dependency perception\nmodule (SDP) to capture the spatial dependencies that FPN lacks. Our\nexperiments demonstrate that detectors based on HS-FPN exhibit competitive\nadvantages over state-of-the-art models on the AI-TOD dataset for tiny object\ndetection.\n","authors":["Zican Shi","Jing Hu","Jie Ren","Hengkang Ye","Xuyang Yuan","Yan Ouyang","Jia He","Bo Ji","Junyu Guo"],"pdf_url":"https://arxiv.org/pdf/2412.10116v2.pdf","comment":"13 pages,12 figures,7 tables"},{"id":"http://arxiv.org/abs/2412.17325v1","updated":"2024-12-23T06:34:23Z","published":"2024-12-23T06:34:23Z","title":"Feature Based Methods Domain Adaptation for Object Detection: A Review\n  Paper","summary":"  Domain adaptation, a pivotal branch of transfer learning, aims to enhance the\nperformance of machine learning models when deployed in target domains with\ndistinct data distributions. This is particularly critical for object detection\ntasks, where domain shifts (caused by factors such as lighting conditions,\nviewing angles, and environmental variations) can lead to significant\nperformance degradation. This review delves into advanced methodologies for\ndomain adaptation, including adversarial learning, discrepancy-based,\nmulti-domain, teacher-student, ensemble, and VLM techniques, emphasizing their\nefficacy in reducing domain gaps and enhancing model robustness. Feature-based\nmethods have emerged as powerful tools for addressing these challenges by\nharmonizing feature representations across domains. These techniques, such as\nFeature Alignment, Feature Augmentation/Reconstruction, and Feature\nTransformation, are employed alongside or as integral parts of other domain\nadaptation strategies to minimize domain gaps and improve model performance.\nSpecial attention is given to strategies that minimize the reliance on\nextensive labeled data and using unlabeled data, particularly in scenarios\ninvolving synthetic-to-real domain shifts. Applications in fields such as\nautonomous driving and medical imaging are explored, showcasing the potential\nof these methods to ensure reliable object detection in diverse and complex\nsettings. By providing a thorough analysis of state-of-the-art techniques,\nchallenges, and future directions, this work offers a valuable reference for\nresearchers striving to develop resilient and adaptable object detection\nframeworks, advancing the seamless deployment of artificial intelligence in\ndynamic environments.\n","authors":["Helia Mohamadi","Mohammad Ali Keyvanrad","Mohammad Reza Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2412.17325v1.pdf","comment":"46 pages, 13 figures, It will be submitted to a journal"},{"id":"http://arxiv.org/abs/2412.15576v2","updated":"2024-12-23T06:06:17Z","published":"2024-12-20T05:17:06Z","title":"QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped\n  Robot Learning","summary":"  This paper addresses the inherent inference latency challenges associated\nwith deploying multimodal large language models (MLLM) in quadruped\nvision-language-action (QUAR-VLA) tasks. Our investigation reveals that\nconventional parameter reduction techniques ultimately impair the performance\nof the language foundation model during the action instruction tuning phase,\nmaking them unsuitable for this purpose. We introduce a novel latency-free\nquadruped MLLM model, dubbed QUART-Online, designed to enhance inference\nefficiency without degrading the performance of the language foundation model.\nBy incorporating Action Chunk Discretization (ACD), we compress the original\naction representation space, mapping continuous action values onto a smaller\nset of discrete representative vectors while preserving critical information.\nSubsequently, we fine-tune the MLLM to integrate vision, language, and\ncompressed actions into a unified semantic space. Experimental results\ndemonstrate that QUART-Online operates in tandem with the existing MLLM system,\nachieving real-time inference in sync with the underlying controller frequency,\nsignificantly boosting the success rate across various tasks by 65%. Our\nproject page is https://quart-online.github.io.\n","authors":["Xinyang Tong","Pengxiang Ding","Donglin Wang","Wenjie Zhang","Can Cui","Mingyang Sun","Yiguo Fan","Han Zhao","Hongyin Zhang","Yonghao Dang","Siteng Huang","Shangke Lyu"],"pdf_url":"https://arxiv.org/pdf/2412.15576v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17306v1","updated":"2024-12-23T05:53:52Z","published":"2024-12-23T05:53:52Z","title":"Multiple Consistency-guided Test-Time Adaptation for Contrastive\n  Audio-Language Models with Unlabeled Audio","summary":"  One fascinating aspect of pre-trained Audio-Language Models (ALMs) learning\nis their impressive zero-shot generalization capability and test-time\nadaptation (TTA) methods aiming to improve domain performance without\nannotations. However, previous test time adaptation (TTA) methods for ALMs in\nzero-shot classification tend to be stuck in incorrect model predictions. In\norder to further boost the performance, we propose multiple guidance on prompt\nlearning without annotated labels. First, guidance of consistency on both\ncontext tokens and domain tokens of ALMs is set. Second, guidance of both\nconsistency across multiple augmented views of each single test sample and\ncontrastive learning across different test samples is set. Third, we propose a\ncorresponding end-end learning framework for the proposed test-time adaptation\nmethod without annotated labels. We extensively evaluate our approach on 12\ndownstream tasks across domains, our proposed adaptation method leads to 4.41%\n(max 7.50%) average zero-shot performance improvement in comparison with the\nstate-of-the-art models.\n","authors":["Gongyu Chen","Haomin Zhang","Chaofan Ding","Zihao Chen","Xinhan Di"],"pdf_url":"https://arxiv.org/pdf/2412.17306v1.pdf","comment":"6 pages, 1 figure, accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2412.17305v1","updated":"2024-12-23T05:52:32Z","published":"2024-12-23T05:52:32Z","title":"FedLEC: Effective Federated Learning Algorithm with Spiking Neural\n  Networks Under Label Skews","summary":"  With the advancement of neuromorphic chips, implementing Federated Learning\n(FL) with Spiking Neural Networks (SNNs) potentially offers a more\nenergy-efficient schema for collaborative learning across various\nresource-constrained edge devices. However, one significant challenge in the FL\nsystems is that the data from different clients are often non-independently and\nidentically distributed (non-IID), with label skews presenting substantial\ndifficulties in various federated SNN learning tasks. In this study, we propose\na practical post-hoc framework named FedLEC to address the challenge. This\nframework penalizes the corresponding local logits for locally missing labels\nto enhance each local model's generalization ability. Additionally, it\nleverages the pertinent label distribution information distilled from the\nglobal model to mitigate label bias. Extensive experiments with three different\nstructured SNNs across five datasets (i.e., three non-neuromorphic and two\nneuromorphic datasets) demonstrate the efficiency of FedLEC. Compared to seven\nstate-of-the-art FL algorithms, FedLEC achieves an average accuracy improvement\nof approximately 11.59\\% under various label skew distribution settings.\n","authors":["Di Yu","Xin Du","Linshan Jiang","Shunwen Bai","Wentao Tong","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2412.17305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17302v1","updated":"2024-12-23T05:46:08Z","published":"2024-12-23T05:46:08Z","title":"Neural Spatial-Temporal Tensor Representation for Infrared Small Target\n  Detection","summary":"  Optimization-based approaches dominate infrared small target detection as\nthey leverage infrared imagery's intrinsic low-rankness and sparsity. While\neffective for single-frame images, they struggle with dynamic changes in\nmulti-frame scenarios as traditional spatial-temporal representations often\nfail to adapt. To address these challenges, we introduce a Neural-represented\nSpatial-Temporal Tensor (NeurSTT) model. This framework employs nonlinear\nnetworks to enhance spatial-temporal feature correlations in background\napproximation, thereby supporting target detection in an unsupervised manner.\nSpecifically, we employ neural layers to approximate sequential backgrounds\nwithin a low-rank informed deep scheme. A neural three-dimensional total\nvariation is developed to refine background smoothness while reducing static\ntarget-like clusters in sequences. Traditional sparsity constraints are\nincorporated into the loss functions to preserve potential targets. By\nreplacing complex solvers with a deep updating strategy, NeurSTT simplifies the\noptimization process in a domain-awareness way. Visual and numerical results\nacross various datasets demonstrate that our method outperforms detection\nchallenges. Notably, it has 16.6$\\times$ fewer parameters and averaged 19.19\\%\nhigher in $IoU$ compared to the suboptimal method on $256 \\times 256$\nsequences.\n","authors":["Fengyi Wu","Simin Liu","Haoan Wang","Bingjie Tao","Junhai Luo","Zhenming Peng"],"pdf_url":"https://arxiv.org/pdf/2412.17302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17297v1","updated":"2024-12-23T05:38:49Z","published":"2024-12-23T05:38:49Z","title":"Revisiting Multimodal Fusion for 3D Anomaly Detection from an\n  Architectural Perspective","summary":"  Existing efforts to boost multimodal fusion of 3D anomaly detection (3D-AD)\nprimarily concentrate on devising more effective multimodal fusion strategies.\nHowever, little attention was devoted to analyzing the role of multimodal\nfusion architecture (topology) design in contributing to 3D-AD. In this paper,\nwe aim to bridge this gap and present a systematic study on the impact of\nmultimodal fusion architecture design on 3D-AD. This work considers the\nmultimodal fusion architecture design at the intra-module fusion level, i.e.,\nindependent modality-specific modules, involving early, middle or late\nmultimodal features with specific fusion operations, and also at the\ninter-module fusion level, i.e., the strategies to fuse those modules. In both\ncases, we first derive insights through theoretically and experimentally\nexploring how architectural designs influence 3D-AD. Then, we extend SOTA\nneural architecture search (NAS) paradigm and propose 3D-ADNAS to\nsimultaneously search across multimodal fusion strategies and modality-specific\nmodules for the first time.Extensive experiments show that 3D-ADNAS obtains\nconsistent improvements in 3D-AD across various model capacities in terms of\naccuracy, frame rate, and memory usage, and it exhibits great potential in\ndealing with few-shot 3D-AD tasks.\n","authors":["Kaifang Long","Guoyang Xie","Lianbo Ma","Jiaqi Liu","Zhichao Lu"],"pdf_url":"https://arxiv.org/pdf/2412.17297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17292v1","updated":"2024-12-23T05:24:26Z","published":"2024-12-23T05:24:26Z","title":"AV-EmoDialog: Chat with Audio-Visual Users Leveraging Emotional Cues","summary":"  In human communication, both verbal and non-verbal cues play a crucial role\nin conveying emotions, intentions, and meaning beyond words alone. These\nnon-linguistic information, such as facial expressions, eye contact, voice\ntone, and pitch, are fundamental elements of effective interactions, enriching\nconversations by adding emotional and contextual depth. Recognizing the\nimportance of non-linguistic content in communication, we present AV-EmoDialog,\na dialogue system designed to exploit verbal and non-verbal information from\nusers' audio-visual inputs to generate more responsive and empathetic\ninteractions. AV-EmoDialog systematically exploits the emotional cues in\naudio-visual dialogues; extracting speech content and emotional tones from\nspeech, analyzing fine-grained facial expressions from visuals, and integrating\nthese cues to generate emotionally aware responses in an end-to-end manner.\nThrough extensive experiments, we validate that the proposed AV-EmoDialog\noutperforms existing multimodal LLMs in generating not only emotionally\nappropriate but also contextually appropriate responses.\n","authors":["Se Jin Park","Yeonju Kim","Hyeongseop Rha","Bella Godiva","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2412.17292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17290v1","updated":"2024-12-23T05:22:44Z","published":"2024-12-23T05:22:44Z","title":"Free-viewpoint Human Animation with Pose-correlated Reference Selection","summary":"  Diffusion-based human animation aims to animate a human character based on a\nsource human image as well as driving signals such as a sequence of poses.\nLeveraging the generative capacity of diffusion model, existing approaches are\nable to generate high-fidelity poses, but struggle with significant viewpoint\nchanges, especially in zoom-in/zoom-out scenarios where camera-character\ndistance varies. This limits the applications such as cinematic shot type plan\nor camera control. We propose a pose-correlated reference selection diffusion\nnetwork, supporting substantial viewpoint variations in human animation. Our\nkey idea is to enable the network to utilize multiple reference images as\ninput, since significant viewpoint changes often lead to missing appearance\ndetails on the human body. To eliminate the computational cost, we first\nintroduce a novel pose correlation module to compute similarities between\nnon-aligned target and source poses, and then propose an adaptive reference\nselection strategy, utilizing the attention map to identify key regions for\nanimation generation. To train our model, we curated a large dataset from\npublic TED talks featuring varied shots of the same character, helping the\nmodel learn synthesis for different perspectives. Our experimental results show\nthat with the same number of reference images, our model performs favorably\ncompared to the current SOTA methods under large viewpoint change. We further\nshow that the adaptive reference selection is able to choose the most relevant\nreference regions to generate humans under free viewpoints.\n","authors":["Fa-Ting Hong","Zhan Xu","Haiyang Liu","Qinjie Lin","Luchuan Song","Zhixin Shu","Yang Zhou","Duygu Ceylan","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2412.17290v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2412.17284v1","updated":"2024-12-23T05:06:26Z","published":"2024-12-23T05:06:26Z","title":"Towards Unsupervised Model Selection for Domain Adaptive Object\n  Detection","summary":"  Evaluating the performance of deep models in new scenarios has drawn\nincreasing attention in recent years. However, while it is possible to collect\ndata from new scenarios, the annotations are not always available. Existing\nDAOD methods often rely on validation or test sets on the target domain for\nmodel selection, which is impractical in real-world applications. In this\npaper, we propose a novel unsupervised model selection approach for domain\nadaptive object detection, which is able to select almost the optimal model for\nthe target domain without using any target labels. Our approach is based on the\nflat minima principle, i,e., models located in the flat minima region in the\nparameter space usually exhibit excellent generalization ability. However,\ntraditional methods require labeled data to evaluate how well a model is\nlocated in the flat minima region, which is unrealistic for the DAOD task.\nTherefore, we design a Detection Adaptation Score (DAS) approach to\napproximately measure the flat minima without using target labels. We show via\na generalization bound that the flatness can be deemed as model variance, while\nthe minima depend on the domain distribution distance for the DAOD task.\nAccordingly, we propose a Flatness Index Score (FIS) to assess the flatness by\nmeasuring the classification and localization fluctuation before and after\nperturbations of model parameters and a Prototypical Distance Ratio (PDR) score\nto seek the minima by measuring the transferability and discriminability of the\nmodels. In this way, the proposed DAS approach can effectively evaluate the\nmodel generalization ability on the target domain. We have conducted extensive\nexperiments on various DAOD benchmarks and approaches, and the experimental\nresults show that the proposed DAS correlates well with the performance of DAOD\nmodels and can be used as an effective tool for model selection after training.\n","authors":["Hengfu Yu","Jinhong Deng","Wen Li","Lixin Duan"],"pdf_url":"https://arxiv.org/pdf/2412.17284v1.pdf","comment":"16 pages, 5 figures, Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2408.08604v3","updated":"2024-12-23T05:06:15Z","published":"2024-08-16T08:45:25Z","title":"Bi-Directional Deep Contextual Video Compression","summary":"  Deep video compression has made remarkable process in recent years, with the\nmajority of advancements concentrated on P-frame coding. Although efforts to\nenhance B-frame coding are ongoing, their compression performance is still far\nbehind that of traditional bi-directional video codecs. In this paper, we\nintroduce a bi-directional deep contextual video compression scheme tailored\nfor B-frames, termed DCVC-B, to improve the compression performance of deep\nB-frame coding. Our scheme mainly has three key innovations. First, we develop\na bi-directional motion difference context propagation method for effective\nmotion difference coding, which significantly reduces the bit cost of\nbi-directional motions. Second, we propose a bi-directional contextual\ncompression model and a corresponding bi-directional temporal entropy model, to\nmake better use of the multi-scale temporal contexts. Third, we propose a\nhierarchical quality structure-based training strategy, leading to an effective\nbit allocation across large groups of pictures (GOP). Experimental results show\nthat our DCVC-B achieves an average reduction of 26.6% in BD-Rate compared to\nthe reference software for H.265/HEVC under random access conditions.\nRemarkably, it surpasses the performance of the H.266/VVC reference software on\ncertain test datasets under the same configuration. We anticipate our work can\nprovide valuable insights and bring up deep B-frame coding to the next level.\n","authors":["Xihua Sheng","Li Li","Dong Liu","Shiqi Wang"],"pdf_url":"https://arxiv.org/pdf/2408.08604v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19363v2","updated":"2024-12-23T04:32:45Z","published":"2024-10-25T08:01:35Z","title":"Capsule Endoscopy Multi-classification via Gated Attention and Wavelet\n  Transformations","summary":"  Abnormalities in the gastrointestinal tract significantly influence the\npatient's health and require a timely diagnosis for effective treatment. With\nsuch consideration, an effective automatic classification of these\nabnormalities from a video capsule endoscopy (VCE) frame is crucial for\nimprovement in diagnostic workflows.\n  The work presents the process of developing and evaluating a novel model\ndesigned to classify gastrointestinal anomalies from a VCE video frame.\nIntegration of Omni Dimensional Gated Attention (OGA) mechanism and Wavelet\ntransformation techniques into the model's architecture allowed the model to\nfocus on the most critical areas in the endoscopy images, reducing noise and\nirrelevant features. This is particularly advantageous in capsule endoscopy,\nwhere images often contain a high degree of variability in texture and color.\nWavelet transformations contributed by efficiently capturing spatial and\nfrequency-domain information, improving feature extraction, especially for\ndetecting subtle features from the VCE frames. Furthermore, the features\nextracted from the Stationary Wavelet Transform and Discrete Wavelet Transform\nare concatenated channel-wise to capture multiscale features, which are\nessential for detecting polyps, ulcerations, and bleeding. This approach\nimproves classification accuracy on imbalanced capsule endoscopy datasets. The\nproposed model achieved 92.76% and 91.19% as training and validation accuracies\nrespectively. At the same time, Training and Validation losses are 0.2057 and\n0.2700. The proposed model achieved a Balanced Accuracy of 94.81%, AUC of\n87.49%, F1-score of 91.11%, precision of 91.17%, recall of 91.19% and\nspecificity of 98.44%. Additionally, the model's performance is benchmarked\nagainst two base models, VGG16 and ResNet50, demonstrating its enhanced ability\nto identify and classify a range of gastrointestinal abnormalities accurately.\n","authors":["Lakshmi Srinivas Panchananam","Praveen Kumar Chandaliya","Kishor Upla","Kiran Raja"],"pdf_url":"https://arxiv.org/pdf/2410.19363v2.pdf","comment":"Capsule Vision 2024 Challenge"},{"id":"http://arxiv.org/abs/2412.07481v3","updated":"2024-12-23T04:25:42Z","published":"2024-12-10T13:03:42Z","title":"Manta: Enhancing Mamba for Few-Shot Action Recognition of Long\n  Sub-Sequence","summary":"  In few-shot action recognition (FSAR), long sub-sequences of video naturally\nexpress entire actions more effectively. However, the high computational\ncomplexity of mainstream Transformer-based methods limits their application.\nRecent Mamba demonstrates efficiency in modeling long sequences, but directly\napplying Mamba to FSAR overlooks the importance of local feature modeling and\nalignment. Moreover, long sub-sequences within the same class accumulate\nintra-class variance, which adversely impacts FSAR performance. To solve these\nchallenges, we propose a Matryoshka MAmba and CoNtrasTive LeArning framework\n(Manta). Firstly, the Matryoshka Mamba introduces multiple Inner Modules to\nenhance local feature representation, rather than directly modeling global\nfeatures. An Outer Module captures dependencies of timeline between these local\nfeatures for implicit temporal alignment. Secondly, a hybrid contrastive\nlearning paradigm, combining both supervised and unsupervised methods, is\ndesigned to mitigate the negative effects of intra-class variance accumulation.\nThe Matryoshka Mamba and the hybrid contrastive learning paradigm operate in\ntwo parallel branches within Manta, enhancing Mamba for FSAR of long\nsub-sequence. Manta achieves new state-of-the-art performance on prominent\nbenchmarks, including SSv2, Kinetics, UCF101, and HMDB51. Extensive empirical\nstudies prove that Manta significantly improves FSAR of long sub-sequence from\nmultiple perspectives.\n","authors":["Wenbo Huang","Jinghui Zhang","Guang Li","Lei Zhang","Shuoyuan Wang","Fang Dong","Jiahui Jin","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2412.07481v3.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.17263v1","updated":"2024-12-23T04:16:44Z","published":"2024-12-23T04:16:44Z","title":"VarAD: Lightweight High-Resolution Image Anomaly Detection via Visual\n  Autoregressive Modeling","summary":"  This paper addresses a practical task: High-Resolution Image Anomaly\nDetection (HRIAD). In comparison to conventional image anomaly detection for\nlow-resolution images, HRIAD imposes a heavier computational burden and\nnecessitates superior global information capture capacity. To tackle HRIAD,\nthis paper translates image anomaly detection into visual token prediction and\nproposes VarAD based on visual autoregressive modeling for token prediction.\nSpecifically, VarAD first extracts multi-hierarchy and multi-directional visual\ntoken sequences, and then employs an advanced model, Mamba, for visual\nautoregressive modeling and token prediction. During the prediction process,\nVarAD effectively exploits information from all preceding tokens to predict the\ntarget token. Finally, the discrepancies between predicted tokens and original\ntokens are utilized to score anomalies. Comprehensive experiments on four\npublicly available datasets and a real-world button inspection dataset\ndemonstrate that the proposed VarAD achieves superior high-resolution image\nanomaly detection performance while maintaining lightweight, rendering VarAD a\nviable solution for HRIAD. Code is available at\n\\href{https://github.com/caoyunkang/VarAD}{\\url{https://github.com/caoyunkang/VarAD}}.\n","authors":["Yunkang Cao","Haiming Yao","Wei Luo","Weiming Shen"],"pdf_url":"https://arxiv.org/pdf/2412.17263v1.pdf","comment":"Accepted by IEEE TII"},{"id":"http://arxiv.org/abs/2412.03910v2","updated":"2024-12-23T04:04:39Z","published":"2024-12-05T06:28:08Z","title":"DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for\n  Monocular Dynamic 3D Reconstruction","summary":"  Dynamic scene reconstruction from monocular video is critical for real-world\napplications. This paper tackles the dual challenges of dynamic novel-view\nsynthesis and 3D geometry reconstruction by introducing a hybrid framework:\nDeformable Gaussian Splatting and Dynamic Neural Surfaces (DGNS), in which both\nmodules can leverage each other for both tasks. During training, depth maps\ngenerated by the deformable Gaussian splatting module guide the ray sampling\nfor faster processing and provide depth supervision within the dynamic neural\nsurface module to improve geometry reconstruction. Simultaneously, the dynamic\nneural surface directs the distribution of Gaussian primitives around the\nsurface, enhancing rendering quality. To further refine depth supervision, we\nintroduce a depth-filtering process on depth maps derived from Gaussian\nrasterization. Extensive experiments on public datasets demonstrate that DGNS\nachieves state-of-the-art performance in both novel-view synthesis and 3D\nreconstruction.\n","authors":["Xuesong Li","Jinguang Tong","Jie Hong","Vivien Rolland","Lars Petersson"],"pdf_url":"https://arxiv.org/pdf/2412.03910v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21534v5","updated":"2024-12-23T04:03:44Z","published":"2024-07-31T11:40:29Z","title":"ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large\n  Language Models","summary":"  In this work, we propose a training-free method to inject visual referring\ninto Multimodal Large Language Models (MLLMs) through learnable visual token\noptimization. We observe the relationship between text prompt tokens and visual\ntokens in MLLMs, where attention layers model the connection between them. Our\napproach involves adjusting visual tokens from the MLP output during inference,\ncontrolling which text prompt tokens attend to which visual tokens. We optimize\na learnable visual token based on an energy function, enhancing the strength of\nreferential regions in the attention map. This enables detailed region\ndescription and reasoning without the need for substantial training costs or\nmodel retraining. Our method offers a promising direction for integrating\nreferential abilities into MLLMs. Our method support referring with box, mask,\nscribble and point. The results demonstrate that our method exhibits\ncontrollability and interpretability.\n","authors":["Mingrui Wu","Xinyue Cai","Jiayi Ji","Jiale Li","Oucheng Huang","Gen Luo","Hao Fei","Guannan Jiang","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2407.21534v5.pdf","comment":"Accepted to NeurIPS 2024;\n  Code:https://github.com/mrwu-mac/ControlMLLM"},{"id":"http://arxiv.org/abs/2412.17258v1","updated":"2024-12-23T04:01:44Z","published":"2024-12-23T04:01:44Z","title":"An Intrinsically Explainable Approach to Detecting Vertebral Compression\n  Fractures in CT Scans via Neurosymbolic Modeling","summary":"  Vertebral compression fractures (VCFs) are a common and potentially serious\nconsequence of osteoporosis. Yet, they often remain undiagnosed. Opportunistic\nscreening, which involves automated analysis of medical imaging data acquired\nprimarily for other purposes, is a cost-effective method to identify\nundiagnosed VCFs. In high-stakes scenarios like opportunistic medical\ndiagnosis, model interpretability is a key factor for the adoption of AI\nrecommendations. Rule-based methods are inherently explainable and closely\nalign with clinical guidelines, but they are not immediately applicable to\nhigh-dimensional data such as CT scans. To address this gap, we introduce a\nneurosymbolic approach for VCF detection in CT volumes. The proposed model\ncombines deep learning (DL) for vertebral segmentation with a shape-based\nalgorithm (SBA) that analyzes vertebral height distributions in salient\nanatomical regions. This allows for the definition of a rule set over the\nheight distributions to detect VCFs. Evaluation of VerSe19 dataset shows that\nour method achieves an accuracy of 96% and a sensitivity of 91% in VCF\ndetection. In comparison, a black box model, DenseNet, achieved an accuracy of\n95% and sensitivity of 91% in the same dataset. Our results demonstrate that\nour intrinsically explainable approach can match or surpass the performance of\nblack box deep neural networks while providing additional insights into why a\nprediction was made. This transparency can enhance clinician's trust thus,\nsupporting more informed decision-making in VCF diagnosis and treatment\nplanning.\n","authors":["Blanca Inigo","Yiqing Shen","Benjamin D. Killeen","Michelle Song","Axel Krieger","Christopher Bradley","Mathias Unberath"],"pdf_url":"https://arxiv.org/pdf/2412.17258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17254v1","updated":"2024-12-23T03:56:27Z","published":"2024-12-23T03:56:27Z","title":"Enhancing Multi-Text Long Video Generation Consistency without Tuning:\n  Time-Frequency Analysis, Prompt Alignment, and Theory","summary":"  Despite the considerable progress achieved in the long video generation\nproblem, there is still significant room to improve the consistency of the\nvideos, particularly in terms of smoothness and transitions between scenes. We\naddress these issues to enhance the consistency and coherence of videos\ngenerated with either single or multiple prompts. We propose the Time-frequency\nbased temporal Attention Reweighting Algorithm (TiARA), which meticulously\nedits the attention score matrix based on the Discrete Short-Time Fourier\nTransform. Our method is supported by a theoretical guarantee, the\nfirst-of-its-kind for frequency-based methods in diffusion models. For videos\ngenerated by multiple prompts, we further investigate key factors affecting\nprompt interpolation quality and propose PromptBlend, an advanced prompt\ninterpolation pipeline. The efficacy of our proposed method is validated via\nextensive experimental results, exhibiting consistent and impressive\nimprovements over baseline methods. The code will be released upon acceptance.\n","authors":["Xingyao Li","Fengzhuo Zhang","Jiachun Pan","Yunlong Hou","Vincent Y. F. Tan","Zhuoran Yang"],"pdf_url":"https://arxiv.org/pdf/2412.17254v1.pdf","comment":"34 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.17251v1","updated":"2024-12-23T03:49:29Z","published":"2024-12-23T03:49:29Z","title":"GCS-M3VLT: Guided Context Self-Attention based Multi-modal Medical\n  Vision Language Transformer for Retinal Image Captioning","summary":"  Retinal image analysis is crucial for diagnosing and treating eye diseases,\nyet generating accurate medical reports from images remains challenging due to\nvariability in image quality and pathology, especially with limited labeled\ndata. Previous Transformer-based models struggled to integrate visual and\ntextual information under limited supervision. In response, we propose a novel\nvision-language model for retinal image captioning that combines visual and\ntextual features through a guided context self-attention mechanism. This\napproach captures both intricate details and the global clinical context, even\nin data-scarce scenarios. Extensive experiments on the DeepEyeNet dataset\ndemonstrate a 0.023 BLEU@4 improvement, along with significant qualitative\nadvancements, highlighting the effectiveness of our model in generating\ncomprehensive medical captions.\n","authors":["Teja Krishna Cherukuri","Nagur Shareef Shaik","Jyostna Devi Bodapati","Dong Hye Ye"],"pdf_url":"https://arxiv.org/pdf/2412.17251v1.pdf","comment":"This paper has been accepted for presentation at the IEEE\n  International Conference on Acoustics, Speech, and Signal Processing (ICASSP\n  2025)"},{"id":"http://arxiv.org/abs/2412.17247v1","updated":"2024-12-23T03:40:04Z","published":"2024-12-23T03:40:04Z","title":"STeInFormer: Spatial-Temporal Interaction Transformer Architecture for\n  Remote Sensing Change Detection","summary":"  Convolutional neural networks and attention mechanisms have greatly benefited\nremote sensing change detection (RSCD) because of their outstanding\ndiscriminative ability. Existent RSCD methods often follow a paradigm of using\na non-interactive Siamese neural network for multi-temporal feature extraction\nand change detection heads for feature fusion and change representation.\nHowever, this paradigm lacks the contemplation of the characteristics of RSCD\nin temporal and spatial dimensions, and causes the drawback on spatial-temporal\ninteraction that hinders high-quality feature extraction. To address this\nproblem, we present STeInFormer, a spatial-temporal interaction Transformer\narchitecture for multi-temporal feature extraction, which is the first general\nbackbone network specifically designed for RSCD. In addition, we propose a\nparameter-free multi-frequency token mixer to integrate frequency-domain\nfeatures that provide spectral information for RSCD. Experimental results on\nthree datasets validate the effectiveness of the proposed method, which can\noutperform the state-of-the-art methods and achieve the most satisfactory\nefficiency-accuracy trade-off. Code is available at\nhttps://github.com/xwmaxwma/rschange.\n","authors":["Xiaowen Ma","Zhenkai Wu","Mengting Ma","Mengjiao Zhao","Fan Yang","Zhenhong Du","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.17247v1.pdf","comment":"JSTARS 2025"},{"id":"http://arxiv.org/abs/2412.17241v1","updated":"2024-12-23T03:22:44Z","published":"2024-12-23T03:22:44Z","title":"QTSeg: A Query Token-Based Architecture for Efficient 2D Medical Image\n  Segmentation","summary":"  Medical image segmentation is crucial in assisting medical doctors in making\ndiagnoses and enabling accurate automatic diagnosis. While advanced\nconvolutional neural networks (CNNs) excel in segmenting regions of interest\nwith pixel-level precision, they often struggle with long-range dependencies,\nwhich is crucial for enhancing model performance. Conversely, transformer\narchitectures leverage attention mechanisms to excel in handling long-range\ndependencies. However, the computational complexity of transformers grows\nquadratically, posing resource-intensive challenges, especially with\nhigh-resolution medical images. Recent research aims to combine CNN and\ntransformer architectures to mitigate their drawbacks and enhance performance\nwhile keeping resource demands low. Nevertheless, existing approaches have not\nfully leveraged the strengths of both architectures to achieve high accuracy\nwith low computational requirements. To address this gap, we propose a novel\narchitecture for 2D medical image segmentation (QTSeg) that leverages a feature\npyramid network (FPN) as the image encoder, a multi-level feature fusion (MLFF)\nas the adaptive module between encoder and decoder and a multi-query mask\ndecoder (MQM Decoder) as the mask decoder. In the first step, an FPN model\nextracts pyramid features from the input image. Next, MLFF is incorporated\nbetween the encoder and decoder to adapt features from different encoder stages\nto the decoder. Finally, an MQM Decoder is employed to improve mask generation\nby integrating query tokens with pyramid features at all stages of the mask\ndecoder. Our experimental results show that QTSeg outperforms state-of-the-art\nmethods across all metrics with lower computational demands than the baseline\nand the existing methods. Code is available at\nhttps://github.com/tpnam0901/QTSeg (v0.1.0)\n","authors":["Phuong-Nam Tran","Nhat Truong Pham","Duc Ngoc Minh Dang","Eui-Nam Huh","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2412.17241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17239v1","updated":"2024-12-23T03:19:19Z","published":"2024-12-23T03:19:19Z","title":"Unity is Strength: Unifying Convolutional and Transformeral Features for\n  Better Person Re-Identification","summary":"  Person Re-identification (ReID) aims to retrieve the specific person across\nnon-overlapping cameras, which greatly helps intelligent transportation\nsystems. As we all know, Convolutional Neural Networks (CNNs) and Transformers\nhave the unique strengths to extract local and global features, respectively.\nConsidering this fact, we focus on the mutual fusion between them to learn more\ncomprehensive representations for persons. In particular, we utilize the\ncomplementary integration of deep features from different model structures. We\npropose a novel fusion framework called FusionReID to unify the strengths of\nCNNs and Transformers for image-based person ReID. More specifically, we first\ndeploy a Dual-branch Feature Extraction (DFE) to extract features through CNNs\nand Transformers from a single image. Moreover, we design a novel\nDual-attention Mutual Fusion (DMF) to achieve sufficient feature fusions. The\nDMF comprises Local Refinement Units (LRU) and Heterogenous Transmission\nModules (HTM). LRU utilizes depth-separable convolutions to align deep features\nin channel dimensions and spatial sizes. HTM consists of a Shared Encoding Unit\n(SEU) and two Mutual Fusion Units (MFU). Through the continuous stacking of\nHTM, deep features after LRU are repeatedly utilized to generate more\ndiscriminative features. Extensive experiments on three public ReID benchmarks\ndemonstrate that our method can attain superior performances than most\nstate-of-the-arts. The source code is available at\nhttps://github.com/924973292/FusionReID.\n","authors":["Yuhao Wang","Pingping Zhang","Xuehu Liu","Zhengzheng Tu","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2412.17239v1.pdf","comment":"Accepted by Trans. on ITS"},{"id":"http://arxiv.org/abs/2412.17238v1","updated":"2024-12-23T03:17:21Z","published":"2024-12-23T03:17:21Z","title":"Modality-Aware Shot Relating and Comparing for Video Scene Detection","summary":"  Video scene detection involves assessing whether each shot and its\nsurroundings belong to the same scene. Achieving this requires meticulously\ncorrelating multi-modal cues, $\\it{e.g.}$ visual entity and place modalities,\namong shots and comparing semantic changes around each shot. However, most\nmethods treat multi-modal semantics equally and do not examine contextual\ndifferences between the two sides of a shot, leading to sub-optimal detection\nperformance. In this paper, we propose the $\\bf{M}$odality-$\\bf{A}$ware\n$\\bf{S}$hot $\\bf{R}$elating and $\\bf{C}$omparing approach (MASRC), which\nenables relating shots per their own characteristics of visual entity and place\nmodalities, as well as comparing multi-shots similarities to have scene changes\nexplicitly encoded. Specifically, to fully harness the potential of visual\nentity and place modalities in modeling shot relations, we mine long-term shot\ncorrelations from entity semantics while simultaneously revealing short-term\nshot correlations from place semantics. In this way, we can learn distinctive\nshot features that consolidate coherence within scenes and amplify\ndistinguishability across scenes. Once equipped with distinctive shot features,\nwe further encode the relations between preceding and succeeding shots of each\ntarget shot by similarity convolution, aiding in the identification of scene\nending shots. We validate the broad applicability of the proposed components in\nMASRC. Extensive experimental results on public benchmark datasets demonstrate\nthat the proposed MASRC significantly advances video scene detection.\n","authors":["Jiawei Tan","Hongxing Wang","Kang Dang","Jiaxin Li","Zhilong Ou"],"pdf_url":"https://arxiv.org/pdf/2412.17238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15509v2","updated":"2024-12-23T03:04:33Z","published":"2024-12-20T02:45:37Z","title":"PolySmart @ TRECVid 2024 Video-To-Text","summary":"  In this paper, we present our methods and results for the Video-To-Text (VTT)\ntask at TRECVid 2024, exploring the capabilities of Vision-Language Models\n(VLMs) like LLaVA and LLaVA-NeXT-Video in generating natural language\ndescriptions for video content. We investigate the impact of fine-tuning VLMs\non VTT datasets to enhance description accuracy, contextual relevance, and\nlinguistic consistency. Our analysis reveals that fine-tuning substantially\nimproves the model's ability to produce more detailed and domain-aligned text,\nbridging the gap between generic VLM tasks and the specialized needs of VTT.\nExperimental results demonstrate that our fine-tuned model outperforms baseline\nVLMs across various evaluation metrics, underscoring the importance of\ndomain-specific tuning for complex VTT tasks.\n","authors":["Jiaxin Wu","Wengyu Zhang","Xiao-Yong Wei","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2412.15509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08053v2","updated":"2024-12-23T02:53:13Z","published":"2024-12-11T03:00:15Z","title":"DynamicPAE: Generating Scene-Aware Physical Adversarial Examples in\n  Real-Time","summary":"  Physical adversarial examples (PAEs) are regarded as \"whistle-blowers\" of\nreal-world risks in deep-learning applications. However, current PAE generation\nstudies show limited adaptive attacking ability to diverse and varying scenes.\nThe key challenges in generating dynamic PAEs are exploring their patterns\nunder noisy gradient feedback and adapting the attack to agnostic scenario\nnatures. To address the problems, we present DynamicPAE, the first generative\nframework that enables scene-aware real-time physical attacks beyond static\nattacks. Specifically, to train the dynamic PAE generator under noisy gradient\nfeedback, we introduce the residual-driven sample trajectory guidance\ntechnique, which redefines the training task to break the limited feedback\ninformation restriction that leads to the degeneracy problem. Intuitively, it\nallows the gradient feedback to be passed to the generator through a low-noise\nauxiliary task, thereby guiding the optimization away from degenerate solutions\nand facilitating a more comprehensive and stable exploration of feasible PAEs.\nTo adapt the generator to agnostic scenario natures, we introduce the\ncontext-aligned scene expectation simulation process, consisting of the\nconditional-uncertainty-aligned data module and the skewness-aligned objective\nre-weighting module. The former enhances robustness in the context of\nincomplete observation by employing a conditional probabilistic model for\ndomain randomization, while the latter facilitates consistent stealth control\nacross different attack targets by automatically reweighting losses based on\nthe skewness indicator. Extensive digital and physical evaluations demonstrate\nthe superior attack performance of DynamicPAE, attaining a 1.95 $\\times$ boost\n(65.55% average AP drop under attack) on representative object detectors (e.g.,\nYolo-v8) over state-of-the-art static PAE generating methods.\n","authors":["Jin Hu","Xianglong Liu","Jiakai Wang","Junkai Zhang","Xianqi Yang","Haotong Qin","Yuqing Ma","Ke Xu"],"pdf_url":"https://arxiv.org/pdf/2412.08053v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2402.01269v2","updated":"2024-12-23T02:50:27Z","published":"2024-02-02T09:47:26Z","title":"Spectrum-guided Feature Enhancement Network for Event Person\n  Re-Identification","summary":"  As a cutting-edge biosensor, the event camera holds significant potential in\nthe field of computer vision, particularly regarding privacy preservation.\nHowever, compared to traditional cameras, event streams often contain noise and\npossess extremely sparse semantics, posing a formidable challenge for\nevent-based person re-identification (event Re-ID). To address this, we\nintroduce a novel event person re-identification network: the Spectrum-guided\nFeature Enhancement Network (SFE-Net). This network consists of two innovative\ncomponents: the Multi-grain Spectrum Attention Mechanism (MSAM) and the\nConsecutive Patch Dropout Module (CPDM). MSAM employs a fourier spectrum\ntransform strategy to filter event noise, while also utilizing an event-guided\nmulti-granularity attention strategy to enhance and capture discriminative\nperson semantics. CPDM employs a consecutive patch dropout strategy to generate\nmultiple incomplete feature maps, encouraging the deep Re-ID model to equally\nperceive each effective region of the person's body and capture robust person\ndescriptors. Extensive experiments on Event Re-ID datasets demonstrate that our\nSFE-Net achieves the best performance in this task.\n","authors":["Hongchen Tan","Yi Zhang","Xiuping Liu","Baocai Yin","Nan Ma","Xin Li","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2402.01269v2.pdf","comment":"Content needs to be revised"},{"id":"http://arxiv.org/abs/2412.17226v1","updated":"2024-12-23T02:43:29Z","published":"2024-12-23T02:43:29Z","title":"OLiDM: Object-aware LiDAR Diffusion Models for Autonomous Driving","summary":"  To enhance autonomous driving safety in complex scenarios, various methods\nhave been proposed to simulate LiDAR point cloud data. Nevertheless, these\nmethods often face challenges in producing high-quality, diverse, and\ncontrollable foreground objects. To address the needs of object-aware tasks in\n3D perception, we introduce OLiDM, a novel framework capable of generating\nhigh-fidelity LiDAR data at both the object and the scene levels. OLiDM\nconsists of two pivotal components: the Object-Scene Progressive Generation\n(OPG) module and the Object Semantic Alignment (OSA) module. OPG adapts to\nuser-specific prompts to generate desired foreground objects, which are\nsubsequently employed as conditions in scene generation, ensuring controllable\noutputs at both the object and scene levels. This also facilitates the\nassociation of user-defined object-level annotations with the generated LiDAR\nscenes. Moreover, OSA aims to rectify the misalignment between foreground\nobjects and background scenes, enhancing the overall quality of the generated\nobjects. The broad effectiveness of OLiDM is demonstrated across various LiDAR\ngeneration tasks, as well as in 3D perception tasks. Specifically, on the\nKITTI-360 dataset, OLiDM surpasses prior state-of-the-art methods such as\nUltraLiDAR by 17.5 in FPD. Additionally, in sparse-to-dense LiDAR completion,\nOLiDM achieves a significant improvement over LiDARGen, with a 57.47\\% increase\nin semantic IoU. Moreover, OLiDM enhances the performance of mainstream 3D\ndetectors by 2.4\\% in mAP and 1.9\\% in NDS, underscoring its potential in\nadvancing object-aware 3D tasks. Code is available at:\nhttps://yanty123.github.io/OLiDM.\n","authors":["Tianyi Yan","Junbo Yin","Xianpeng Lang","Ruigang Yang","Cheng-Zhong Xu","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2412.17226v1.pdf","comment":"AAAI 2025, https://yanty123.github.io/OLiDM"},{"id":"http://arxiv.org/abs/2412.17225v1","updated":"2024-12-23T02:40:07Z","published":"2024-12-23T02:40:07Z","title":"CharGen: High Accurate Character-Level Visual Text Generation Model with\n  MultiModal Encoder","summary":"  Recently, significant advancements have been made in diffusion-based visual\ntext generation models. Although the effectiveness of these methods in visual\ntext rendering is rapidly improving, they still encounter challenges such as\ninaccurate characters and strokes when rendering complex visual text. In this\npaper, we propose CharGen, a highly accurate character-level visual text\ngeneration and editing model. Specifically, CharGen employs a character-level\nmultimodal encoder that not only extracts character-level text embeddings but\nalso encodes glyph images character by character. This enables it to capture\nfine-grained cross-modality features more effectively. Additionally, we\nintroduce a new perceptual loss in CharGen to enhance character shape\nsupervision and address the issue of inaccurate strokes in generated text. It\nis worth mentioning that CharGen can be integrated into existing diffusion\nmodels to generate visual text with high accuracy. CharGen significantly\nimproves text rendering accuracy, outperforming recent methods in public\nbenchmarks such as AnyText-benchmark and MARIO-Eval, with improvements of more\nthan 8% and 6%, respectively. Notably, CharGen achieved a 5.5% increase in\naccuracy on Chinese test sets.\n","authors":["Lichen Ma","Tiezhu Yue","Pei Fu","Yujie Zhong","Kai Zhou","Xiaoming Wei","Jie Hu"],"pdf_url":"https://arxiv.org/pdf/2412.17225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16724v2","updated":"2024-12-23T02:19:20Z","published":"2024-11-23T03:40:05Z","title":"Devils in Middle Layers of Large Vision-Language Models: Interpreting,\n  Detecting and Mitigating Object Hallucinations via Attention Lens","summary":"  Hallucinations in Large Vision-Language Models (LVLMs) significantly\nundermine their reliability, motivating researchers to explore the causes of\nhallucination. However, most studies primarily focus on the language aspect\nrather than the visual. In this paper, we address how LVLMs process visual\ninformation and whether this process causes hallucination. Firstly, we use the\nattention lens to identify the stages at which LVLMs handle visual data,\ndiscovering that the middle layers are crucial. Moreover, we find that these\nlayers can be further divided into two stages: \"visual information enrichment\"\nand \"semantic refinement\" which respectively propagate visual data to object\ntokens and interpret it through text. By analyzing attention patterns during\nthe visual information enrichment stage, we find that real tokens consistently\nreceive higher attention weights than hallucinated ones, serving as a strong\nindicator of hallucination. Further examination of multi-head attention maps\nreveals that hallucination tokens often result from heads interacting with\ninconsistent objects. Based on these insights, we propose a simple\ninference-time method that adjusts visual attention by integrating information\nacross various heads. Extensive experiments demonstrate that this approach\neffectively mitigates hallucinations in mainstream LVLMs without additional\ntraining costs.\n","authors":["Zhangqi Jiang","Junkai Chen","Beier Zhu","Tingjin Luo","Yankun Shen","Xu Yang"],"pdf_url":"https://arxiv.org/pdf/2411.16724v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17219v1","updated":"2024-12-23T02:18:54Z","published":"2024-12-23T02:18:54Z","title":"Discriminative Image Generation with Diffusion Models for Zero-Shot\n  Learning","summary":"  Generative Zero-Shot Learning (ZSL) methods synthesize class-related features\nbased on predefined class semantic prototypes, showcasing superior performance.\nHowever, this feature generation paradigm falls short of providing\ninterpretable insights. In addition, existing approaches rely on semantic\nprototypes annotated by human experts, which exhibit a significant limitation\nin their scalability to generalized scenes. To overcome these deficiencies, a\nnatural solution is to generate images for unseen classes using text prompts.\nTo this end, We present DIG-ZSL, a novel Discriminative Image Generation\nframework for Zero-Shot Learning. Specifically, to ensure the generation of\ndiscriminative images for training an effective ZSL classifier, we learn a\ndiscriminative class token (DCT) for each unseen class under the guidance of a\npre-trained category discrimination model (CDM). Harnessing DCTs, we can\ngenerate diverse and high-quality images, which serve as informative unseen\nsamples for ZSL tasks. In this paper, the extensive experiments and\nvisualizations on four datasets show that our DIG-ZSL: (1) generates diverse\nand high-quality images, (2) outperforms previous state-of-the-art\nnonhuman-annotated semantic prototype-based methods by a large margin, and (3)\nachieves comparable or better performance than baselines that leverage\nhuman-annotated semantic prototypes. The codes will be made available upon\nacceptance of the paper.\n","authors":["Dingjie Fu","Wenjin Hou","Shiming Chen","Shuhuang Chen","Xinge You","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2412.17219v1.pdf","comment":"Tech report, 16 pages"},{"id":"http://arxiv.org/abs/2412.00378v2","updated":"2024-12-23T02:11:58Z","published":"2024-11-30T06:54:38Z","title":"Bi-Band ECoGNet for ECoG Decoding on Classification Task","summary":"  In the application of brain-computer interface (BCI), being able to\naccurately decode brain signals is a critical task. For the multi-class\nclassification task of brain signal ECoG, how to improve the classification\naccuracy is one of the current research hotspots. ECoG acquisition uses a\nhigh-density electrode array and a high sampling frequency, which makes ECoG\ndata have a certain high similarity and data redundancy in the temporal domain,\nand also unique spatial pattern in spatial domain. How to effectively extract\nfeatures is both exciting and challenging. Previous work found that\nvisual-related ECoG can carry visual information via frequency and spatial\ndomain. Based on this finding, we focused on using deep learning to design\nfrequency and spatial feature extraction modules, and proposed a Bi-Band\nECoGNet model based on deep learning. The main contributions of this paper are:\n1) The Bi-BCWT (Bi-Band Channel-Wise Transform) neural network module is\ndesigned to replace the time-consume method MST, this module greatly improves\nthe model calculation and data storage efficiency, and effectively increases\nthe training speed; 2) The Bi-BCWT module can effectively take into account the\ninformation both in low-frequency and high-frequency domain, which is more\nconducive to ECoG multi-classification tasks; 3) ECoG is acquired using 2D\nelectrode array, the newly designed 2D Spatial-Temporal feature encoder can\nextract the 2D spatial feature better. Experiments have shown that the unique\n2D spatial data structure can effectively improve classification accuracy; 3)\nCompared with previous work, the Bi-Band ECoGNet model is smaller and has\nhigher performance, with an accuracy increase of 1.24%, and the model training\nspeed is increased by 6 times, which is more suitable for BCI applications.\n","authors":["Changqing Ji","Keisuke Kawasaki","Isao Hasegwa","Takayuki Okatani"],"pdf_url":"https://arxiv.org/pdf/2412.00378v2.pdf","comment":"Method in this paper is too old, we need to merge with our later\n  method"},{"id":"http://arxiv.org/abs/2408.16227v3","updated":"2024-12-23T02:00:31Z","published":"2024-08-29T02:58:35Z","title":"Revisiting 360 Depth Estimation with PanoGabor: A New Fusion Perspective","summary":"  Depth estimation from a monocular 360 image is important to the perception of\nthe entire 3D environment. However, the inherent distortion and large field of\nview (FoV) in 360 images pose great challenges for this task. To this end,\nexisting mainstream solutions typically introduce additional perspective-based\n360 representations (\\textit{e.g.}, Cubemap) to achieve effective feature\nextraction. Nevertheless, regardless of the introduced representations, they\neventually need to be unified into the equirectangular projection (ERP) format\nfor the subsequent depth estimation, which inevitably reintroduces the\ntroublesome distortions. In this work, we propose an oriented distortion-aware\nGabor Fusion framework (PGFuse) to address the above challenges. First, we\nintroduce Gabor filters that analyze texture in the frequency domain, thereby\nextending the receptive fields and enhancing depth cues. To address the\nreintroduced distortions, we design a linear latitude-aware distortion\nrepresentation method to generate customized, distortion-aware Gabor filters\n(PanoGabor filters). Furthermore, we design a channel-wise and spatial-wise\nunidirectional fusion module (CS-UFM) that integrates the proposed PanoGabor\nfilters to unify other representations into the ERP format, delivering\neffective and distortion-free features. Considering the orientation sensitivity\nof the Gabor transform, we introduce a spherical gradient constraint to\nstabilize this sensitivity. Experimental results on three popular indoor 360\nbenchmarks demonstrate the superiority of the proposed PGFuse to existing\nstate-of-the-art solutions. Code can be available upon acceptance.\n","authors":["Zhijie Shen","Chunyu Lin","Lang Nie","Kang Liao"],"pdf_url":"https://arxiv.org/pdf/2408.16227v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14330v2","updated":"2024-12-23T01:44:52Z","published":"2024-09-22T06:29:54Z","title":"Thinking in Granularity: Dynamic Quantization for Image Super-Resolution\n  by Intriguing Multi-Granularity Clues","summary":"  Dynamic quantization has attracted rising attention in image super-resolution\n(SR) as it expands the potential of heavy SR models onto mobile devices while\npreserving competitive performance. Existing methods explore layer-to-bit\nconfiguration upon varying local regions, adaptively allocating the bit to each\nlayer and patch. Despite the benefits, they still fall short in the trade-off\nof SR accuracy and quantization efficiency. Apart from this, adapting the\nquantization level for each layer individually can disturb the original\ninter-layer relationships, thus diminishing the representation capability of\nquantized models. In this work, we propose Granular-DQ, which capitalizes on\nthe intrinsic characteristics of images while dispensing with the previous\nconsideration for layer sensitivity in quantization. Granular-DQ conducts a\nmulti-granularity analysis of local patches with further exploration of their\ninformation densities, achieving a distinctive patch-wise and layer-invariant\ndynamic quantization paradigm. Specifically, Granular-DQ initiates by\ndeveloping a granularity-bit controller (GBC) to apprehend the coarse-to-fine\ngranular representations of different patches, matching their proportional\ncontribution to the entire image to determine the proper bit-width allocation.\nOn this premise, we investigate the relation between bit-width and information\ndensity, devising an entropy-to-bit (E2B) mechanism that enables further\nfine-grained dynamic bit adaption of high-bit patches. Extensive experiments\nvalidate the superiority and generalization ability of Granular-DQ over recent\nstate-of-the-art methods on various SR models. Code and supplementary statement\ncan be found at \\url{https://github.com/MmmingS/Granular-DQ.git}.\n","authors":["Mingshen Wang","Zhao Zhang","Feng Li","Ke Xu","Kang Miao","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2409.14330v2.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.07072v2","updated":"2024-12-23T01:39:48Z","published":"2024-12-10T00:25:33Z","title":"Stable Mean Teacher for Semi-supervised Video Action Detection","summary":"  In this work, we focus on semi-supervised learning for video action\ndetection. Video action detection requires spatiotemporal localization in\naddition to classification, and a limited amount of labels makes the model\nprone to unreliable predictions. We present Stable Mean Teacher, a simple\nend-to-end teacher-based framework that benefits from improved and temporally\nconsistent pseudo labels. It relies on a novel Error Recovery (EoR) module,\nwhich learns from students' mistakes on labeled samples and transfers this\nknowledge to the teacher to improve pseudo labels for unlabeled samples.\nMoreover, existing spatiotemporal losses do not take temporal coherency into\naccount and are prone to temporal inconsistencies. To address this, we present\nDifference of Pixels (DoP), a simple and novel constraint focused on temporal\nconsistency, leading to coherent temporal detections. We evaluate our approach\non four different spatiotemporal detection benchmarks: UCF101-24, JHMDB21, AVA,\nand YouTube-VOS. Our approach outperforms the supervised baselines for action\ndetection by an average margin of 23.5% on UCF101-24, 16% on JHMDB21, and 3.3%\non AVA. Using merely 10% and 20% of data, it provides competitive performance\ncompared to the supervised baseline trained on 100% annotations on UCF101-24\nand JHMDB21, respectively. We further evaluate its effectiveness on AVA for\nscaling to large-scale datasets and YouTube-VOS for video object segmentation,\ndemonstrating its generalization capability to other tasks in the video domain.\nCode and models are publicly available.\n","authors":["Akash Kumar","Sirshapan Mitra","Yogesh Singh Rawat"],"pdf_url":"https://arxiv.org/pdf/2412.07072v2.pdf","comment":"AAAI Conference on Artificial Intelligence, Main Technical Track\n  (AAAI), 2025, Code: https://github.com/AKASH2907/stable_mean_teacher"},{"id":"http://arxiv.org/abs/2412.17210v1","updated":"2024-12-23T01:31:39Z","published":"2024-12-23T01:31:39Z","title":"Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection","summary":"  Video Anomaly Detection (VAD) is essential for computer vision research.\nExisting VAD methods utilize either reconstruction-based or prediction-based\nframeworks. The former excels at detecting irregular patterns or structures,\nwhereas the latter is capable of spotting abnormal deviations or trends. We\naddress pose-based video anomaly detection and introduce a novel framework\ncalled Dual Conditioned Motion Diffusion (DCMD), which enjoys the advantages of\nboth approaches. The DCMD integrates conditioned motion and conditioned\nembedding to comprehensively utilize the pose characteristics and latent\nsemantics of observed movements, respectively. In the reverse diffusion\nprocess, a motion transformer is proposed to capture potential correlations\nfrom multi-layered characteristics within the spectrum space of human motion.\nTo enhance the discriminability between normal and abnormal instances, we\ndesign a novel United Association Discrepancy (UAD) regularization that\nprimarily relies on a Gaussian kernel-based time association and a\nself-attention-based global association. Finally, a mask completion strategy is\nintroduced during the inference stage of the reverse diffusion process to\nenhance the utilization of conditioned motion for the prediction branch of\nanomaly detection. Extensive experiments on four datasets demonstrate that our\nmethod dramatically outperforms state-of-the-art methods and exhibits superior\ngeneralization performance.\n","authors":["Andi Xu","Hongsong Wang","Pinle Ding","Jie Gui"],"pdf_url":"https://arxiv.org/pdf/2412.17210v1.pdf","comment":"Code is on https://github.com/guijiejie/DCMD-main"},{"id":"http://arxiv.org/abs/2406.07966v5","updated":"2024-12-23T00:40:41Z","published":"2024-06-12T07:44:22Z","title":"Real-world Image Dehazing with Coherence-based Pseudo Labeling and\n  Cooperative Unfolding Network","summary":"  Real-world Image Dehazing (RID) aims to alleviate haze-induced degradation in\nreal-world settings. This task remains challenging due to the complexities in\naccurately modeling real haze distributions and the scarcity of paired\nreal-world data. To address these challenges, we first introduce a cooperative\nunfolding network that jointly models atmospheric scattering and image scenes,\neffectively integrating physical knowledge into deep networks to restore\nhaze-contaminated details. Additionally, we propose the first RID-oriented\niterative mean-teacher framework, termed the Coherence-based Label Generator,\nto generate high-quality pseudo labels for network training. Specifically, we\nprovide an optimal label pool to store the best pseudo-labels during network\ntraining, leveraging both global and local coherence to select high-quality\ncandidates and assign weights to prioritize haze-free regions. We verify the\neffectiveness of our method, with experiments demonstrating that it achieves\nstate-of-the-art performance on RID tasks. Code will be available at\n\\url{https://github.com/cnyvfang/CORUN-Colabator}.\n","authors":["Chengyu Fang","Chunming He","Fengyang Xiao","Yulun Zhang","Longxiang Tang","Yuelin Zhang","Kai Li","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2406.07966v5.pdf","comment":"Accepted at NeurIPS 2024 as a Spotlight Paper"}]},"2024-12-22T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.17170v1","updated":"2024-12-22T21:43:56Z","published":"2024-12-22T21:43:56Z","title":"Where Did Your Model Learn That? Label-free Influence for\n  Self-supervised Learning","summary":"  Self-supervised learning (SSL) has revolutionized learning from large-scale\nunlabeled datasets, yet the intrinsic relationship between pretraining data and\nthe learned representations remains poorly understood. Traditional supervised\nlearning benefits from gradient-based data attribution tools like influence\nfunctions that measure the contribution of an individual data point to model\npredictions. However, existing definitions of influence rely on labels, making\nthem unsuitable for SSL settings. We address this gap by introducing\nInfluence-SSL, a novel and label-free approach for defining influence functions\ntailored to SSL. Our method harnesses the stability of learned representations\nagainst data augmentations to identify training examples that help explain\nmodel predictions. We provide both theoretical foundations and empirical\nevidence to show the utility of Influence-SSL in analyzing pre-trained SSL\nmodels. Our analysis reveals notable differences in how SSL models respond to\ninfluential data compared to supervised models. Finally, we validate the\neffectiveness of Influence-SSL through applications in duplicate detection,\noutlier identification and fairness analysis. Code is available at:\n\\url{https://github.com/cryptonymous9/Influence-SSL}.\n","authors":["Nidhin Harilal","Amit Kiran Rege","Reza Akbarian Bafghi","Maziar Raissi","Claire Monteleoni"],"pdf_url":"https://arxiv.org/pdf/2412.17170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13401v2","updated":"2024-12-22T21:29:58Z","published":"2024-12-18T00:31:18Z","title":"Zero-Shot Low Light Image Enhancement with Diffusion Prior","summary":"  Balancing aesthetic quality with fidelity when enhancing images from\nchallenging, degraded sources is a core objective in computational photography.\nIn this paper, we address low light image enhancement (LLIE), a task in which\ndark images often contain limited visible information. Diffusion models, known\nfor their powerful image enhancement capacities, are a natural choice for this\nproblem. However, their deep generative priors can also lead to hallucinations,\nintroducing non-existent elements or substantially altering the visual\nsemantics of the original scene. In this work, we introduce a novel zero-shot\nmethod for controlling and refining the generative behavior of diffusion models\nfor dark-to-light image conversion tasks. Our method demonstrates superior\nperformance over existing state-of-the-art methods in the task of low-light\nimage enhancement, as evidenced by both quantitative metrics and qualitative\nanalysis.\n","authors":["Joshua Cho","Sara Aghajanzadeh","Zhen Zhu","D. A. Forsyth"],"pdf_url":"https://arxiv.org/pdf/2412.13401v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17162v1","updated":"2024-12-22T21:02:36Z","published":"2024-12-22T21:02:36Z","title":"Generative Diffusion Modeling: A Practical Handbook","summary":"  This handbook offers a unified perspective on diffusion models, encompassing\ndiffusion probabilistic models, score-based generative models, consistency\nmodels, rectified flow, and related methods. By standardizing notations and\naligning them with code implementations, it aims to bridge the \"paper-to-code\"\ngap and facilitate robust implementations and fair comparisons. The content\nencompasses the fundamentals of diffusion models, the pre-training process, and\nvarious post-training methods. Post-training techniques include model\ndistillation and reward-based fine-tuning. Designed as a practical guide, it\nemphasizes clarity and usability over theoretical depth, focusing on widely\nadopted approaches in generative modeling with diffusion models.\n","authors":["Zihan Ding","Chi Jin"],"pdf_url":"https://arxiv.org/pdf/2412.17162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17155v1","updated":"2024-12-22T20:33:59Z","published":"2024-12-22T20:33:59Z","title":"The Potential of Convolutional Neural Networks for Cancer Detection","summary":"  Early detection of cancer is critical in improving treatment outcomes and\nincreasing survival rates, particularly for common cancers such as lung,\nbreast, and prostate which collectively contribute to a significant global\nmortality burden. With advancements in imaging technologies and data\nprocessing, Convolutional Neural Networks (CNNs) have emerged as a powerful\ntool for analyzing and classifying medical images, enabling more precise cancer\ndetection. This paper provides a comprehensive review of recent studies\nleveraging CNN models for detecting ten different types of cancer. Each study\nemploys distinct CNN architectures to identify patterns associated with these\ncancers, utilizing diverse datasets. Key differences and strengths of these\narchitectures are meticulously compared and analyzed, highlighting their\nefficacy in improving early detection. Beyond reviewing the performance and\nlimitations of CNN-based cancer detection methods, this study explores the\nfeasibility of integrating CNNs into clinical settings as an early detection\ntool, potentially complementing or replacing traditional methods. Despite\nsignificant progress, challenges remain, including data diversity, result\ninterpretation, and ethical considerations. By identifying the best-performing\nCNN architectures and providing a comparative analysis, this study aims to\ncontribute a comprehensive perspective on the application of CNNs in cancer\ndetection and their role in advancing diagnostic capabilities in healthcare.\n","authors":["Hossein Molaeian","Kaveh Karamjani","Sina Teimouri","Saeed Roshani","Sobhan Roshani"],"pdf_url":"https://arxiv.org/pdf/2412.17155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17153v1","updated":"2024-12-22T20:21:54Z","published":"2024-12-22T20:21:54Z","title":"Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models\n  with Flow Matching","summary":"  Autoregressive (AR) models have achieved state-of-the-art performance in text\nand image generation but suffer from slow generation due to the token-by-token\nprocess. We ask an ambitious question: can a pre-trained AR model be adapted to\ngenerate outputs in just one or two steps? If successful, this would\nsignificantly advance the development and deployment of AR models. We notice\nthat existing works that try to speed up AR generation by generating multiple\ntokens at once fundamentally cannot capture the output distribution due to the\nconditional dependencies between tokens, limiting their effectiveness for\nfew-step generation. To address this, we propose Distilled Decoding (DD), which\nuses flow matching to create a deterministic mapping from Gaussian distribution\nto the output distribution of the pre-trained AR model. We then train a network\nto distill this mapping, enabling few-step generation. DD doesn't need the\ntraining data of the original AR model, making it more practical.We evaluate DD\non state-of-the-art image AR models and present promising results on\nImageNet-256. For VAR, which requires 10-step generation, DD enables one-step\ngeneration (6.3$\\times$ speed-up), with an acceptable increase in FID from 4.19\nto 9.96. For LlamaGen, DD reduces generation from 256 steps to 1, achieving an\n217.8$\\times$ speed-up with a comparable FID increase from 4.11 to 11.35. In\nboth cases, baseline methods completely fail with FID>100. DD also excels on\ntext-to-image generation, reducing the generation from 256 steps to 2 for\nLlamaGen with minimal FID increase from 25.70 to 28.95. As the first work to\ndemonstrate the possibility of one-step generation for image AR models, DD\nchallenges the prevailing notion that AR models are inherently slow, and opens\nup new opportunities for efficient AR generation. The project website is at\nhttps://imagination-research.github.io/distilled-decoding.\n","authors":["Enshu Liu","Xuefei Ning","Yu Wang","Zinan Lin"],"pdf_url":"https://arxiv.org/pdf/2412.17153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10397v2","updated":"2024-12-22T19:35:34Z","published":"2024-08-19T20:28:39Z","title":"Webcam-based Pupil Diameter Prediction Benefits from Upscaling","summary":"  Capturing pupil diameter is essential for assessing psychological and\nphysiological states such as stress levels and cognitive load. However, the low\nresolution of images in eye datasets often hampers precise measurement. This\nstudy evaluates the impact of various upscaling methods, ranging from bicubic\ninterpolation to advanced super-resolution, on pupil diameter predictions. We\ncompare several pre-trained methods, including CodeFormer, GFPGAN, Real-ESRGAN,\nHAT, and SRResNet. Our findings suggest that pupil diameter prediction models\ntrained on upscaled datasets are highly sensitive to the selected upscaling\nmethod and scale. Our results demonstrate that upscaling methods consistently\nenhance the accuracy of pupil diameter prediction models, highlighting the\nimportance of upscaling in pupilometry. Overall, our work provides valuable\ninsights for selecting upscaling techniques, paving the way for more accurate\nassessments in psychological and physiological research.\n","authors":["Vijul Shah","Brian B. Moser","Ko Watanabe","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2408.10397v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17139v1","updated":"2024-12-22T19:13:15Z","published":"2024-12-22T19:13:15Z","title":"Style Transfer Dataset: What Makes A Good Stylization?","summary":"  We present a new dataset with the goal of advancing image style transfer -\nthe task of rendering one image in the style of another image. The dataset\ncovers various content and style images of different size and contains 10.000\nstylizations manually rated by three annotators in 1-10 scale. Based on\nobtained ratings, we find which factors are mostly responsible for favourable\nand poor user evaluations and show quantitative measures having statistically\nsignificant impact on user grades. A methodology for creating style transfer\ndatasets is discussed. Presented dataset can be used in automating multiple\ntasks, related to style transfer configuration and evaluation.\n","authors":["Victor Kitov","Valentin Abramov","Mikhail Akhtyrchenko"],"pdf_url":"https://arxiv.org/pdf/2412.17139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17109v1","updated":"2024-12-22T17:52:29Z","published":"2024-12-22T17:52:29Z","title":"Similarity Trajectories: Linking Sampling Process to Artifacts in\n  Diffusion-Generated Images","summary":"  Artifact detection algorithms are crucial to correcting the output generated\nby diffusion models. However, because of the variety of artifact forms,\nexisting methods require substantial annotated data for training. This\nrequirement limits their scalability and efficiency, which restricts their wide\napplication. This paper shows that the similarity of denoised images between\nconsecutive time steps during the sampling process is related to the severity\nof artifacts in images generated by diffusion models. Building on this\nobservation, we introduce the concept of Similarity Trajectory to characterize\nthe sampling process and its correlation with the image artifacts presented.\nUsing an annotated data set of 680 images, which is only 0.1% of the amount of\ndata used in the prior work, we trained a classifier on these trajectories to\npredict the presence of artifacts in images. By performing 10-fold validation\ntesting on the balanced annotated data set, the classifier can achieve an\naccuracy of 72.35%, highlighting the connection between the Similarity\nTrajectory and the occurrence of artifacts. This approach enables\ndifferentiation between artifact-exhibiting and natural-looking images using\nlimited training data.\n","authors":["Dennis Menn","Feng Liang","Hung-Yueh Chiang","Diana Marculescu"],"pdf_url":"https://arxiv.org/pdf/2412.17109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17105v1","updated":"2024-12-22T17:34:01Z","published":"2024-12-22T17:34:01Z","title":"Refining CNN-based Heatmap Regression with Gradient-based Corner Points\n  for Electrode Localization","summary":"  We propose a method for detecting the electrode positions in lithium-ion\nbatteries. The process begins by identifying the region of interest (ROI) in\nthe battery's X-ray image through corner point detection. A convolutional\nneural network is then used to regress the pole positions within this ROI.\nFinally, the regressed positions are optimized and corrected using corner point\npriors, significantly mitigating the loss of localization accuracy caused by\noperations such as feature map down-sampling and padding during network\ntraining. Our findings show that combining traditional pixel gradient analysis\nwith CNN-based heatmap regression for keypoint extraction enhances both\naccuracy and efficiency, resulting in significant performance improvements.\n","authors":["Lin Wu"],"pdf_url":"https://arxiv.org/pdf/2412.17105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00401v2","updated":"2024-12-22T17:32:54Z","published":"2023-12-01T07:50:53Z","title":"VIoTGPT: Learning to Schedule Vision Tools in LLMs towards Intelligent\n  Video Internet of Things","summary":"  Video Internet of Things (VIoT) has shown full potential in collecting an\nunprecedented volume of video data. How to schedule the domain-specific\nperceiving models and analyze the collected videos uniformly, efficiently, and\nespecially intelligently to accomplish complicated tasks is challenging. To\naddress the challenge, we build VIoTGPT, the framework based on LLMs to\ncorrectly interact with humans, query knowledge videos, and invoke vision\nmodels to analyze multimedia data collaboratively. To support VIoTGPT and\nrelated future works, we meticulously crafted the VIoT-Tool dataset, including\nthe training dataset and the benchmark involving 11 representative vision\nmodels across three categories based on semi-automatic annotations. To guide\nLLM to act as the intelligent agent towards intelligent VIoT, we resort to the\nReAct instruction tuning method based on VIoT-Tool to learn the tool\ncapability. Quantitative and qualitative experiments and analyses demonstrate\nthe effectiveness of VIoTGPT. We believe VIoTGPT contributes to improving\nhuman-centered experiences in VIoT applications. The project website is\nhttps://github.com/zhongyy/VIoTGPT.\n","authors":["Yaoyao Zhong","Mengshi Qi","Rui Wang","Yuhan Qiu","Yang Zhang","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2312.00401v2.pdf","comment":"AAAI 2025, 12 pages"},{"id":"http://arxiv.org/abs/2412.17098v1","updated":"2024-12-22T17:17:28Z","published":"2024-12-22T17:17:28Z","title":"DreamOmni: Unified Image Generation and Editing","summary":"  Currently, the success of large language models (LLMs) illustrates that a\nunified multitasking approach can significantly enhance model usability,\nstreamline deployment, and foster synergistic benefits across different tasks.\nHowever, in computer vision, while text-to-image (T2I) models have\nsignificantly improved generation quality through scaling up, their framework\ndesign did not initially consider how to unify with downstream tasks, such as\nvarious types of editing. To address this, we introduce DreamOmni, a unified\nmodel for image generation and editing. We begin by analyzing existing\nframeworks and the requirements of downstream tasks, proposing a unified\nframework that integrates both T2I models and various editing tasks.\nFurthermore, another key challenge is the efficient creation of high-quality\nediting data, particularly for instruction-based and drag-based editing. To\nthis end, we develop a synthetic data pipeline using sticker-like elements to\nsynthesize accurate, high-quality datasets efficiently, which enables editing\ndata scaling up for unified model training. For training, DreamOmni jointly\ntrains T2I generation and downstream tasks. T2I training enhances the model's\nunderstanding of specific concepts and improves generation quality, while\nediting training helps the model grasp the nuances of the editing task. This\ncollaboration significantly boosts editing performance. Extensive experiments\nconfirm the effectiveness of DreamOmni. The code and model will be released.\n","authors":["Bin Xia","Yuechen Zhang","Jingyao Li","Chengyao Wang","Yitong Wang","Xinglong Wu","Bei Yu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2412.17098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19653v3","updated":"2024-12-22T16:55:37Z","published":"2023-10-30T15:38:39Z","title":"A Note on Generalization in Variational Autoencoders: How Effective Is\n  Synthetic Data & Overparameterization?","summary":"  Variational autoencoders (VAEs) are deep probabilistic models that are used\nin scientific applications. Many works try to mitigate this problem from the\nprobabilistic methods perspective by new inference techniques or training\nprocedures. In this paper, we approach the problem instead from the deep\nlearning perspective by investigating the effectiveness of using synthetic data\nand overparameterization for improving the generalization performance. Our\nmotivation comes from (1) the recent discussion on whether the increasing\namount of publicly accessible synthetic data will improve or hurt currently\ntrained generative models; and (2) the modern deep learning insights that\noverparameterization improves generalization. Our investigation shows how both\ntraining on samples from a pre-trained diffusion model, and using more\nparameters at certain layers are able to effectively mitigate overfitting in\nVAEs, therefore improving their generalization, amortized inference, and\nrobustness performance. Our study provides timely insights in the current era\nof synthetic data and scaling laws.\n","authors":["Tim Z. Xiao","Johannes Zenn","Robert Bamler"],"pdf_url":"https://arxiv.org/pdf/2310.19653v3.pdf","comment":"11 pages + appendix; Published in TMLR (12/2024) -\n  https://openreview.net/forum?id=bwyHf5eery"},{"id":"http://arxiv.org/abs/2412.17042v1","updated":"2024-12-22T14:49:55Z","published":"2024-12-22T14:49:55Z","title":"Adapting Image-to-Video Diffusion Models for Large-Motion Frame\n  Interpolation","summary":"  The development of video generation models has advanced significantly in\nrecent years. For video frame interpolation, we adopt a pre-trained large-scale\nimage-to-video diffusion model. To enable this adaptation, we propose a\nconditional encoder, which serves as a simple yet effective trainable module.\nBy leveraging the first and last frames, we extract spatial and temporal\nfeatures and input them into the conditional encoder. The computed features of\nthe conditional encoder guide the video diffusion model in generating\nkeyframe-guided video sequences. Our method demonstrates superior performance\non the Fr\\'echet Video Distance (FVD) metric compared to previous deterministic\napproaches in handling large-motion cases, highlighting advancements in\ngenerative-based methodologies.\n","authors":["Luoxu Jin","Hiroshi Watanabe"],"pdf_url":"https://arxiv.org/pdf/2412.17042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17041v1","updated":"2024-12-22T14:38:28Z","published":"2024-12-22T14:38:28Z","title":"An OpenMind for 3D medical vision self-supervised learning","summary":"  The field of 3D medical vision self-supervised learning lacks consistency and\nstandardization. While many methods have been developed it is impossible to\nidentify the current state-of-the-art, due to i) varying and small pre-training\ndatasets, ii) varying architectures, and iii) being evaluated on differing\ndownstream datasets. In this paper we bring clarity to this field and lay the\nfoundation for further method advancements: We a) publish the largest publicly\navailable pre-training dataset comprising 114k 3D brain MRI volumes and b)\nbenchmark existing SSL methods under common architectures and c) provide the\ncode of our framework publicly to facilitate rapid adoption and reproduction.\nThis pre-print \\textit{only describes} the dataset contribution (a); Data,\nbenchmark, and codebase will be made available shortly.\n","authors":["Tassilo Wald","Constantin Ulrich","Jonathan Suprijadi","Michal Nohel","Robin Peretzke","Klaus H. Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2412.17041v1.pdf","comment":"Pre-Print for Challenge proposal; Dataset, Benchmark and Codebase\n  will be made available shortly once Benchmarking concludes"},{"id":"http://arxiv.org/abs/2412.17038v1","updated":"2024-12-22T14:30:26Z","published":"2024-12-22T14:30:26Z","title":"ErasableMask: A Robust and Erasable Privacy Protection Scheme against\n  Black-box Face Recognition Models","summary":"  While face recognition (FR) models have brought remarkable convenience in\nface verification and identification, they also pose substantial privacy risks\nto the public. Existing facial privacy protection schemes usually adopt\nadversarial examples to disrupt face verification of FR models. However, these\nschemes often suffer from weak transferability against black-box FR models and\npermanently damage the identifiable information that cannot fulfill the\nrequirements of authorized operations such as forensics and authentication. To\naddress these limitations, we propose ErasableMask, a robust and erasable\nprivacy protection scheme against black-box FR models. Specifically, via\nrethinking the inherent relationship between surrogate FR models, ErasableMask\nintroduces a novel meta-auxiliary attack, which boosts black-box\ntransferability by learning more general features in a stable and balancing\noptimization strategy. It also offers a perturbation erasion mechanism that\nsupports the erasion of semantic perturbations in protected face without\ndegrading image quality. To further improve performance, ErasableMask employs a\ncurriculum learning strategy to mitigate optimization conflicts between\nadversarial attack and perturbation erasion. Extensive experiments on the\nCelebA-HQ and FFHQ datasets demonstrate that ErasableMask achieves the\nstate-of-the-art performance in transferability, achieving over 72% confidence\non average in commercial FR systems. Moreover, ErasableMask also exhibits\noutstanding perturbation erasion performance, achieving over 90% erasion\nsuccess rate.\n","authors":["Sipeng Shen","Yunming Zhang","Dengpan Ye","Xiuwen Shi","Long Tang","Haoran Duan","Ziyi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.17038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01816v2","updated":"2024-12-22T14:15:10Z","published":"2024-09-03T11:57:36Z","title":"GeoBEV: Learning Geometric BEV Representation for Multi-view 3D Object\n  Detection","summary":"  Bird's-Eye-View (BEV) representation has emerged as a mainstream paradigm for\nmulti-view 3D object detection, demonstrating impressive perceptual\ncapabilities. However, existing methods overlook the geometric quality of BEV\nrepresentation, leaving it in a low-resolution state and failing to restore the\nauthentic geometric information of the scene. In this paper, we identify the\ndrawbacks of previous approaches that limit the geometric quality of BEV\nrepresentation and propose Radial-Cartesian BEV Sampling (RC-Sampling), which\noutperforms other feature transformation methods in efficiently generating\nhigh-resolution dense BEV representation to restore fine-grained geometric\ninformation. Additionally, we design a novel In-Box Label to substitute the\ntraditional depth label generated from the LiDAR points. This label reflects\nthe actual geometric structure of objects rather than just their surfaces,\ninjecting real-world geometric information into the BEV representation. In\nconjunction with the In-Box Label, Centroid-Aware Inner Loss (CAI Loss) is\ndeveloped to capture the inner geometric structure of objects. Finally, we\nintegrate the aforementioned modules into a novel multi-view 3D object\ndetector, dubbed GeoBEV, which achieves a state-of-the-art result of 66.2\\% NDS\non the nuScenes test set. The code is available at\nhttps://github.com/mengtan00/GeoBEV.git.\n","authors":["Jinqing Zhang","Yanan Zhang","Yunlong Qi","Zehua Fu","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2409.01816v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2409.13976v2","updated":"2024-12-22T14:03:06Z","published":"2024-09-21T01:51:07Z","title":"Detecting Inpainted Video with Frequency Domain Insights","summary":"  Video inpainting enables seamless content removal and replacement within\nframes, posing ethical and legal risks when misused. To mitigate these risks,\ndetecting manipulated regions in inpainted videos is critical. Previous\ndetection methods often focus solely on the characteristics derived from\nspatial and temporal dimensions, which limits their effectiveness by\noverlooking the unique frequency characteristics of different inpainting\nalgorithms. In this paper, we propose the Frequency Domain Insights Network\n(FDIN), which significantly enhances detection accuracy by incorporating\ninsights from the frequency domain. Our network features an Adaptive Band\nSelective Response module to discern frequency characteristics specific to\nvarious inpainting techniques and a Fast Fourier Convolution-based Attention\nmodule for identifying periodic artifacts in inpainted regions. Utilizing 3D\nResBlocks for spatiotemporal analysis, FDIN progressively refines detection\nprecision from broad assessments to detailed localization. Experimental\nevaluations on public datasets demonstrate that FDIN achieves state-of-the-art\nperformance, setting a new benchmark in video inpainting detection.\n","authors":["Quanhui Tang","Jingtao Cao"],"pdf_url":"https://arxiv.org/pdf/2409.13976v2.pdf","comment":"Unsatisfied with this job"},{"id":"http://arxiv.org/abs/2412.17023v1","updated":"2024-12-22T13:58:12Z","published":"2024-12-22T13:58:12Z","title":"Parameter-Efficient Interventions for Enhanced Model Merging","summary":"  Model merging combines knowledge from task-specific models into a unified\nmulti-task model to avoid joint training on all task data. However, current\nmethods face challenges due to representation bias, which can interfere with\ntasks performance. As a remedy, we propose IntervMerge, a novel approach to\nmulti-task model merging that effectively mitigates representation bias across\nthe model using taskspecific interventions. To further enhance its efficiency,\nwe introduce mini-interventions, which modify only part of the representation,\nthereby reducing the additional parameters without compromising performance.\nExperimental results demonstrate that IntervMerge consistently outperforms the\nstate-of-the-art approaches using fewer parameters.\n","authors":["Marcin Osial","Daniel Marczak","Bartosz Zieliński"],"pdf_url":"https://arxiv.org/pdf/2412.17023v1.pdf","comment":"10 pages, 6 figures, SIAM International Conference on Data Mining\n  (SDM) 2025"},{"id":"http://arxiv.org/abs/2412.17022v1","updated":"2024-12-22T13:55:44Z","published":"2024-12-22T13:55:44Z","title":"FriendsQA: A New Large-Scale Deep Video Understanding Dataset with\n  Fine-grained Topic Categorization for Story Videos","summary":"  Video question answering (VideoQA) aims to answer natural language questions\naccording to the given videos. Although existing models perform well in the\nfactoid VideoQA task, they still face challenges in deep video understanding\n(DVU) task, which focuses on story videos. Compared to factoid videos, the most\nsignificant feature of story videos is storylines, which are composed of\ncomplex interactions and long-range evolvement of core story topics including\ncharacters, actions and locations. Understanding these topics requires models\nto possess DVU capability. However, existing DVU datasets rarely organize\nquestions according to these story topics, making them difficult to\ncomprehensively assess VideoQA models' DVU capability of complex storylines.\nAdditionally, the question quantity and video length of these dataset are\nlimited by high labor costs of handcrafted dataset building method. In this\npaper, we devise a large language model based multi-agent collaboration\nframework, StoryMind, to automatically generate a new large-scale DVU dataset.\nThe dataset, FriendsQA, derived from the renowned sitcom Friends with an\naverage episode length of 1,358 seconds, contains 44.6K questions evenly\ndistributed across 14 fine-grained topics. Finally, We conduct comprehensive\nexperiments on 10 state-of-the-art VideoQA models using the FriendsQA dataset.\n","authors":["Zhengqian Wu","Ruizhe Li","Zijun Xu","Zhongyuan Wang","Chunxia Xiao","Chao Liang"],"pdf_url":"https://arxiv.org/pdf/2412.17022v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08941v4","updated":"2024-12-22T13:47:27Z","published":"2024-12-12T05:08:05Z","title":"Optimized Gradient Clipping for Noisy Label Learning","summary":"  Previous research has shown that constraining the gradient of loss function\nwith respect to model-predicted probabilities can enhance the model robustness\nagainst noisy labels. These methods typically specify a fixed optimal threshold\nfor gradient clipping through validation data to obtain the desired robustness\nagainst noise. However, this common practice overlooks the dynamic distribution\nof gradients from both clean and noisy-labeled samples at different stages of\ntraining, significantly limiting the model capability to adapt to the variable\nnature of gradients throughout the training process. To address this issue, we\npropose a simple yet effective approach called Optimized Gradient Clipping\n(OGC), which dynamically adjusts the clipping threshold based on the ratio of\nnoise gradients to clean gradients after clipping, estimated by modeling the\ndistributions of clean and noisy samples. This approach allows us to modify the\nclipping threshold at each training step, effectively controlling the influence\nof noise gradients. Additionally, we provide statistical analysis to certify\nthe noise-tolerance ability of OGC. Our extensive experiments across various\ntypes of label noise, including symmetric, asymmetric, instance-dependent, and\nreal-world noise, demonstrate the effectiveness of our approach.\n","authors":["Xichen Ye","Yifan Wu","Weizhong Zhang","Xiaoqiang Li","Yifan Chen","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2412.08941v4.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2403.19306v2","updated":"2024-12-22T13:15:32Z","published":"2024-03-28T10:42:49Z","title":"Sparse Generation: Making Pseudo Labels Sparse for Point Weakly\n  Supervised Object Detection on Low Data Volume","summary":"  Existing pseudo label generation methods for point weakly supervised object\ndetection are inadequate in low data volume and dense object detection tasks.\nWe consider the generation of weakly supervised pseudo labels as the model's\nsparse output, and propose Sparse Generation as a solution to make pseudo\nlabels sparse. The method employs three processing stages (Mapping, Mask,\nRegression), constructs dense tensors through the relationship between data and\ndetector model, optimizes three of its parameters, and obtains a sparse tensor,\nthereby indirectly obtaining higher quality pseudo labels, and addresses the\nmodel's density problem on low data volume. Additionally, we propose\nperspective-based matching, which provides more rational pseudo boxes for\nprediction missed on instances. In comparison to the SOTA method, on four\ndatasets (MS COCO-val, RSOD, SIMD, Bullet-Hole), the experimental results\ndemonstrated a significant advantage.\n","authors":["Chuyang Shang","Tian Ma","Wanzhu Ren","Yuancheng Li","Jiiayi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.19306v2.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2412.17007v1","updated":"2024-12-22T13:13:10Z","published":"2024-12-22T13:13:10Z","title":"Where am I? Cross-View Geo-localization with Natural Language\n  Descriptions","summary":"  Cross-view geo-localization identifies the locations of street-view images by\nmatching them with geo-tagged satellite images or OSM. However, most studies\nfocus on image-to-image retrieval, with fewer addressing text-guided retrieval,\na task vital for applications like pedestrian navigation and emergency\nresponse. In this work, we introduce a novel task for cross-view\ngeo-localization with natural language descriptions, which aims to retrieve\ncorresponding satellite images or OSM database based on scene text. To support\nthis task, we construct the CVG-Text dataset by collecting cross-view data from\nmultiple cities and employing a scene text generation approach that leverages\nthe annotation capabilities of Large Multimodal Models to produce high-quality\nscene text descriptions with localization details.Additionally, we propose a\nnovel text-based retrieval localization method, CrossText2Loc, which improves\nrecall by 10% and demonstrates excellent long-text retrieval capabilities. In\nterms of explainability, it not only provides similarity scores but also offers\nretrieval reasons. More information can be found at\nhttps://yejy53.github.io/CVG-Text/.\n","authors":["Junyan Ye","Honglin Lin","Leyan Ou","Dairong Chen","Zihao Wang","Conghui He","Weijia Li"],"pdf_url":"https://arxiv.org/pdf/2412.17007v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.02843v2","updated":"2024-12-22T12:27:30Z","published":"2024-11-05T06:31:48Z","title":"Advances in Photoacoustic Imaging Reconstruction and Quantitative\n  Analysis for Biomedical Applications","summary":"  Photoacoustic imaging (PAI) represents an innovative biomedical imaging\nmodality that harnesses the advantages of optical resolution and acoustic\npenetration depth while ensuring enhanced safety. Despite its promising\npotential across a diverse array of preclinical and clinical applications, the\nclinical implementation of PAI faces significant challenges, including the\ntrade-off between penetration depth and spatial resolution, as well as the\ndemand for faster imaging speeds. This paper explores the fundamental\nprinciples underlying PAI, with a particular emphasis on three primary\nimplementations: photoacoustic computed tomography (PACT), photoacoustic\nmicroscopy (PAM), and photoacoustic endoscopy (PAE). We undertake a critical\nassessment of their respective strengths and practical limitations.\nFurthermore, recent developments in utilizing conventional or deep learning\n(DL) methodologies for image reconstruction and artefact mitigation across\nPACT, PAM, and PAE are outlined, demonstrating considerable potential to\nenhance image quality and accelerate imaging processes. Furthermore, this paper\nexamines the recent developments in quantitative analysis within PAI, including\nthe quantification of haemoglobin concentration, oxygen saturation, and other\nphysiological parameters within tissues. Finally, our discussion encompasses\ncurrent trends and future directions in PAI research while emphasizing the\ntransformative impact of deep learning on advancing PAI.\n","authors":["Lei Wang","Weiming Zeng","Kai Long","Hongyu Chen","Rongfeng Lan","Li Liu","Wai Ting Siok","Nizhuan Wang"],"pdf_url":"https://arxiv.org/pdf/2411.02843v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2412.16990v1","updated":"2024-12-22T12:09:27Z","published":"2024-12-22T12:09:27Z","title":"Multi-Scale Foreground-Background Confidence for Out-of-Distribution\n  Segmentation","summary":"  Deep neural networks have shown outstanding performance in computer vision\ntasks such as semantic segmentation and have defined the state-of-the-art.\nHowever, these segmentation models are trained on a closed and predefined set\nof semantic classes, which leads to significant prediction failures in\nopen-world scenarios on unknown objects. As this behavior prevents the\napplication in safety-critical applications such as automated driving, the\ndetection and segmentation of these objects from outside their predefined\nsemantic space (out-of-distribution (OOD) objects) is of the utmost importance.\nIn this work, we present a multi-scale OOD segmentation method that exploits\nthe confidence information of a foreground-background segmentation model. While\nsemantic segmentation models are trained on specific classes, this restriction\ndoes not apply to foreground-background methods making them suitable for OOD\nsegmentation. We consider the per pixel confidence score of the model\nprediction which is close to 1 for a pixel in a foreground object. By\naggregating these confidence values for different sized patches, objects of\nvarious sizes can be identified in a single image. Our experiments show\nimproved performance of our method in OOD segmentation compared to comparable\nbaselines in the SegmentMeIfYouCan benchmark.\n","authors":["Samuel Marschall","Kira Maag"],"pdf_url":"https://arxiv.org/pdf/2412.16990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16986v1","updated":"2024-12-22T12:04:02Z","published":"2024-12-22T12:04:02Z","title":"Pinwheel-shaped Convolution and Scale-based Dynamic Loss for Infrared\n  Small Target Detection","summary":"  These recent years have witnessed that convolutional neural network\n(CNN)-based methods for detecting infrared small targets have achieved\noutstanding performance. However, these methods typically employ standard\nconvolutions, neglecting to consider the spatial characteristics of the pixel\ndistribution of infrared small targets. Therefore, we propose a novel\npinwheel-shaped convolution (PConv) as a replacement for standard convolutions\nin the lower layers of the backbone network. PConv better aligns with the pixel\nGaussian spatial distribution of dim small targets, enhances feature\nextraction, significantly increases the receptive field, and introduces only a\nminimal increase in parameters. Additionally, while recent loss functions\ncombine scale and location losses, they do not adequately account for the\nvarying sensitivity of these losses across different target scales, limiting\ndetection performance on dim-small targets. To overcome this, we propose a\nscale-based dynamic (SD) Loss that dynamically adjusts the influence of scale\nand location losses based on target size, improving the network's ability to\ndetect targets of varying scales. We construct a new benchmark, SIRST-UAVB,\nwhich is the largest and most challenging dataset to date for real-shot\nsingle-frame infrared small target detection. Lastly, by integrating PConv and\nSD Loss into the latest small target detection algorithms, we achieved\nsignificant performance improvements on IRSTD-1K and our SIRST-UAVB dataset,\nvalidating the effectiveness and generalizability of our approach.\n  Code -- https://github.com/JN-Yang/PConv-SDloss-Data\n","authors":["Jiangnan Yang","Shuangli Liu","Jingjun Wu","Xinyu Su","Nan Hai","Xueli Huang"],"pdf_url":"https://arxiv.org/pdf/2412.16986v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.16982v1","updated":"2024-12-22T11:53:51Z","published":"2024-12-22T11:53:51Z","title":"InterDance:Reactive 3D Dance Generation with Realistic Duet Interactions","summary":"  Humans perform a variety of interactive motions, among which duet dance is\none of the most challenging interactions. However, in terms of human motion\ngenerative models, existing works are still unable to generate high-quality\ninteractive motions, especially in the field of duet dance. On the one hand, it\nis due to the lack of large-scale high-quality datasets. On the other hand, it\narises from the incomplete representation of interactive motion and the lack of\nfine-grained optimization of interactions. To address these challenges, we\npropose, InterDance, a large-scale duet dance dataset that significantly\nenhances motion quality, data scale, and the variety of dance genres. Built\nupon this dataset, we propose a new motion representation that can accurately\nand comprehensively describe interactive motion. We further introduce a\ndiffusion-based framework with an interaction refinement guidance strategy to\noptimize the realism of interactions progressively. Extensive experiments\ndemonstrate the effectiveness of our dataset and algorithm.\n","authors":["Ronghui Li","Youliang Zhang","Yachao Zhang","Yuxiang Zhang","Mingyang Su","Jie Guo","Ziwei Liu","Yebin Liu","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2412.16982v1.pdf","comment":"https://inter-dance.github.io/"},{"id":"http://arxiv.org/abs/2412.16979v1","updated":"2024-12-22T11:43:00Z","published":"2024-12-22T11:43:00Z","title":"A Conditional Diffusion Model for Electrical Impedance Tomography Image\n  Reconstruction","summary":"  Electrical impedance tomography (EIT) is a non-invasive imaging technique,\ncapable of reconstructing images of the electrical conductivity of tissues and\nmaterials. It is popular in diverse application areas, from medical imaging to\nindustrial process monitoring and tactile sensing, due to its low cost,\nreal-time capabilities and non-ionizing nature. EIT visualizes the conductivity\ndistribution within a body by measuring the boundary voltages, given a current\ninjection. However, EIT image reconstruction is ill-posed due to the mismatch\nbetween the under-sampled voltage data and the high-resolution conductivity\nimage. A variety of approaches, both conventional and deep learning-based, have\nbeen proposed, capitalizing on the use of spatial regularizers, and the\nparadigm of image regression. In this research, a novel method based on the\nconditional diffusion model for EIT reconstruction is proposed, termed CDEIT.\nSpecifically, CDEIT consists of the forward diffusion process, which first\ngradually adds Gaussian noise to the clean conductivity images, and a reverse\ndenoising process, which learns to predict the original conductivity image from\nits noisy version, conditioned on the boundary voltages. Following model\ntraining, CDEIT applies the conditional reverse process on test voltage data to\ngenerate the desired conductivities. Moreover, we provide the details of a\nnormalization procedure, which demonstrates how EIT image reconstruction models\ntrained on simulated datasets can be applied on real datasets with varying\nsizes, excitation currents and background conductivities. Experiments conducted\non a synthetic dataset and two real datasets demonstrate that the proposed\nmodel outperforms state-of-the-art methods. The CDEIT software is available as\nopen-source (https://github.com/shuaikaishi/CDEIT) for reproducibility\npurposes.\n","authors":["Shuaikai Shi","Ruiyuan Kang","Panos Liatsis"],"pdf_url":"https://arxiv.org/pdf/2412.16979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16540v2","updated":"2024-12-22T11:42:45Z","published":"2024-08-29T13:58:34Z","title":"GRPose: Learning Graph Relations for Human Image Generation with Pose\n  Priors","summary":"  Recent methods using diffusion models have made significant progress in human\nimage generation with various control signals such as pose priors. However,\nexisting efforts are still struggling to generate high-quality images with\nconsistent pose alignment, resulting in unsatisfactory output. In this paper,\nwe propose a framework that delves into the graph relations of pose priors to\nprovide control information for human image generation. The main idea is to\nestablish a graph topological structure between the pose priors and latent\nrepresentation of diffusion models to capture the intrinsic associations\nbetween different pose parts. A Progressive Graph Integrator (PGI) is designed\nto learn the spatial relationships of the pose priors with the graph structure,\nadopting a hierarchical strategy within an Adapter to gradually propagate\ninformation across different pose parts. Besides, a pose perception loss is\nintroduced based on a pretrained pose estimation network to minimize the pose\ndifferences. Extensive qualitative and quantitative experiments conducted on\nthe Human-Art and LAION-Human datasets clearly demonstrate that our model can\nachieve significant performance improvement over the latest benchmark models.\nThe code is available at \\url{https://xiangchenyin.github.io/GRPose/}.\n","authors":["Xiangchen Yin","Donglin Di","Lei Fan","Hao Li","Wei Chen","Xiaofei Gou","Yang Song","Xiao Sun","Xun Yang"],"pdf_url":"https://arxiv.org/pdf/2408.16540v2.pdf","comment":"Accepted at AAAI2025"},{"id":"http://arxiv.org/abs/2412.16978v1","updated":"2024-12-22T11:38:04Z","published":"2024-12-22T11:38:04Z","title":"PromptDresser: Improving the Quality and Controllability of Virtual\n  Try-On via Generative Textual Prompt and Prompt-aware Mask","summary":"  Recent virtual try-on approaches have advanced by fine-tuning the pre-trained\ntext-to-image diffusion models to leverage their powerful generative ability.\nHowever, the use of text prompts in virtual try-on is still underexplored. This\npaper tackles a text-editable virtual try-on task that changes the clothing\nitem based on the provided clothing image while editing the wearing style\n(e.g., tucking style, fit) according to the text descriptions. In the\ntext-editable virtual try-on, three key aspects exist: (i) designing rich text\ndescriptions for paired person-clothing data to train the model, (ii)\naddressing the conflicts where textual information of the existing person's\nclothing interferes the generation of the new clothing, and (iii) adaptively\nadjust the inpainting mask aligned with the text descriptions, ensuring proper\nediting areas while preserving the original person's appearance irrelevant to\nthe new clothing. To address these aspects, we propose PromptDresser, a\ntext-editable virtual try-on model that leverages large multimodal model (LMM)\nassistance to enable high-quality and versatile manipulation based on\ngenerative text prompts. Our approach utilizes LMMs via in-context learning to\ngenerate detailed text descriptions for person and clothing images\nindependently, including pose details and editing attributes using minimal\nhuman cost. Moreover, to ensure the editing areas, we adjust the inpainting\nmask depending on the text prompts adaptively. We found that our approach,\nutilizing detailed text prompts, not only enhances text editability but also\neffectively conveys clothing details that are difficult to capture through\nimages alone, thereby enhancing image quality. Our code is available at\nhttps://github.com/rlawjdghek/PromptDresser.\n","authors":["Jeongho Kim","Hoiyeong Jin","Sunghyun Park","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2412.16978v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2411.02449v2","updated":"2024-12-22T11:19:22Z","published":"2024-11-03T05:01:49Z","title":"Chronic Obstructive Pulmonary Disease Prediction Using Deep\n  Convolutional Network","summary":"  AI and deep learning are two recent innovations that have made a big\ndifference in helping to solve problems in the clinical space. Using clinical\nimaging and sound examination, they also work on improving their vision so that\nthey can spot diseases early and correctly. Because there aren't enough trained\nHR, clinical professionals are asking for help with innovation because it helps\nthem adapt to more patients. Aside from serious health problems like cancer and\ndiabetes, the effects of respiratory infections are also slowly getting worse\nand becoming dangerous for society. Respiratory diseases need to be found early\nand treated quickly, so listening to the sounds of the lungs is proving to be a\nvery helpful tool along with chest X-rays. The presented research hopes to use\ndeep learning ideas based on Convolutional Brain Organization to help clinical\nspecialists by giving a detailed and thorough analysis of clinical respiratory\nsound data for Ongoing Obstructive Pneumonic identification. We used MFCC,\nMel-Spectrogram, Chroma, Chroma (Steady Q), and Chroma CENS from the Librosa AI\nlibrary in the tests we ran. The new system could also figure out how serious\nthe infection was, whether it was mild, moderate, or severe. The test results\nagree with the outcome of the deep learning approach that was proposed. The\naccuracy of the framework arrangement has been raised to a score of 96% on the\nICBHI. Also, in the led tests, we used K-Crisp Cross-Approval with ten parts to\nmake the presentation of the new deep learning approach easier to understand.\nWith a 96 percent accuracy rate, the suggested network is better than the rest.\nIf you don't use cross-validation, the model is 90% accurate.\n","authors":["Shahran Rahman Alve","Muhammad Zawad Mahmud","Samiha Islam","Mohammad Monirujjaman Khan"],"pdf_url":"https://arxiv.org/pdf/2411.02449v2.pdf","comment":"16 Pages, 11 Figures"},{"id":"http://arxiv.org/abs/2406.13281v3","updated":"2024-12-22T11:06:33Z","published":"2024-06-19T07:21:31Z","title":"ECAFormer: Low-light Image Enhancement using Cross Attention","summary":"  Low-light image enhancement (LLIE) is critical in computer vision. Existing\nLLIE methods often fail to discover the underlying relationships between\ndifferent sub-components, causing the loss of complementary information between\nmultiple modules and network layers, ultimately resulting in the loss of image\ndetails. To beat this shortage, we design a hierarchical mutual Enhancement via\na Cross Attention transformer (ECAFormer), which introduces an architecture\nthat enables concurrent propagation and interaction of multiple features. The\nmodel preserves detailed information by introducing a Dual Multi-head\nself-attention (DMSA), which leverages visual and semantic features across\ndifferent scales, allowing them to guide and complement each other. Besides, a\nCross-Scale DMSA block is introduced to capture the residual connection,\nintegrating cross-layer information to further enhance image detail.\nExperimental results show that ECAFormer reaches competitive performance across\nmultiple benchmarks, yielding nearly a 3% improvement in PSNR over the\nsuboptimal method, demonstrating the effectiveness of information interaction\nin LLIE.\n","authors":["Yudi Ruan","Hao Ma","Weikai Li","Xiao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.13281v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.02402v2","updated":"2024-12-22T10:51:52Z","published":"2024-12-03T11:50:16Z","title":"RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D\n  Referring Expression Segmentation","summary":"  3D Referring Expression Segmentation (3D-RES) aims to segment 3D objects by\ncorrelating referring expressions with point clouds. However, traditional\napproaches frequently encounter issues like over-segmentation or\nmis-segmentation, due to insufficient emphasis on spatial information of\ninstances. In this paper, we introduce a Rule-Guided Spatial Awareness Network\n(RG-SAN) by utilizing solely the spatial information of the target instance for\nsupervision. This approach enables the network to accurately depict the spatial\nrelationships among all entities described in the text, thus enhancing the\nreasoning capabilities. The RG-SAN consists of the Text-driven Localization\nModule (TLM) and the Rule-guided Weak Supervision (RWS) strategy. The TLM\ninitially locates all mentioned instances and iteratively refines their\npositional information. The RWS strategy, acknowledging that only target\nobjects have supervised positional information, employs dependency tree rules\nto precisely guide the core instance's positioning. Extensive testing on the\nScanRefer benchmark has shown that RG-SAN not only establishes new performance\nbenchmarks, with an mIoU increase of 5.1 points, but also exhibits significant\nimprovements in robustness when processing descriptions with spatial ambiguity.\nAll codes are available at https://github.com/sosppxo/RG-SAN.\n","authors":["Changli Wu","Qi Chen","Jiayi Ji","Haowei Wang","Yiwei Ma","You Huang","Gen Luo","Hao Fei","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2412.02402v2.pdf","comment":"Accepted by NeurIPS 2024 (Oral), Code:\n  https://github.com/sosppxo/RG-SAN"},{"id":"http://arxiv.org/abs/2412.16958v1","updated":"2024-12-22T10:34:02Z","published":"2024-12-22T10:34:02Z","title":"Breaking Barriers in Physical-World Adversarial Examples: Improving\n  Robustness and Transferability via Robust Feature","summary":"  As deep neural networks (DNNs) are widely applied in the physical world, many\nresearches are focusing on physical-world adversarial examples (PAEs), which\nintroduce perturbations to inputs and cause the model's incorrect outputs.\nHowever, existing PAEs face two challenges: unsatisfactory attack performance\n(i.e., poor transferability and insufficient robustness to environment\nconditions), and difficulty in balancing attack effectiveness with\nstealthiness, where better attack effectiveness often makes PAEs more\nperceptible.\n  In this paper, we explore a novel perturbation-based method to overcome the\nchallenges. For the first challenge, we introduce a strategy Deceptive RF\ninjection based on robust features (RFs) that are predictive, robust to\nperturbations, and consistent across different models. Specifically, it\nimproves the transferability and robustness of PAEs by covering RFs of other\nclasses onto the predictive features in clean images. For the second challenge,\nwe introduce another strategy Adversarial Semantic Pattern Minimization, which\nremoves most perturbations and retains only essential adversarial patterns in\nAEsBased on the two strategies, we design our method Robust Feature Coverage\nAttack (RFCoA), comprising Robust Feature Disentanglement and Adversarial\nFeature Fusion. In the first stage, we extract target class RFs in feature\nspace. In the second stage, we use attention-based feature fusion to overlay\nthese RFs onto predictive features of clean images and remove unnecessary\nperturbations. Experiments show our method's superior transferability,\nrobustness, and stealthiness compared to existing state-of-the-art methods.\nAdditionally, our method's effectiveness can extend to Large Vision-Language\nModels (LVLMs), indicating its potential applicability to more complex tasks.\n","authors":["Yichen Wang","Yuxuan Chou","Ziqi Zhou","Hangtao Zhang","Wei Wan","Shengshan Hu","Minghui Li"],"pdf_url":"https://arxiv.org/pdf/2412.16958v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.16956v1","updated":"2024-12-22T10:28:52Z","published":"2024-12-22T10:28:52Z","title":"Semantic Hierarchical Prompt Tuning for Parameter-Efficient Fine-Tuning","summary":"  As the scale of vision models continues to grow, Visual Prompt Tuning (VPT)\nhas emerged as a parameter-efficient transfer learning technique, noted for its\nsuperior performance compared to full fine-tuning. However, indiscriminately\napplying prompts to every layer without considering their inherent\ncorrelations, can cause significant disturbances, leading to suboptimal\ntransferability. Additionally, VPT disrupts the original self-attention\nstructure, affecting the aggregation of visual features, and lacks a mechanism\nfor explicitly mining discriminative visual features, which are crucial for\nclassification.\n  To address these issues, we propose a Semantic Hierarchical Prompt (SHIP)\nfine-tuning strategy. We adaptively construct semantic hierarchies and use\nsemantic-independent and semantic-shared prompts to learn hierarchical\nrepresentations. We also integrate attribute prompts and a prompt matching loss\nto enhance feature discrimination and employ decoupled attention for robustness\nand reduced inference costs. SHIP significantly improves performance, achieving\na 4.8\\% gain in accuracy over VPT with a ViT-B/16 backbone on VTAB-1k tasks.\nOur code is available at https://github.com/haoweiz23/SHIP.\n","authors":["Haowei Zhu","Fangyuan Zhang","Rui Qin","Tianxiang Pan","Junhai Yong","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.16956v1.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2412.16955v1","updated":"2024-12-22T10:16:34Z","published":"2024-12-22T10:16:34Z","title":"NumbOD: A Spatial-Frequency Fusion Attack Against Object Detectors","summary":"  With the advancement of deep learning, object detectors (ODs) with various\narchitectures have achieved significant success in complex scenarios like\nautonomous driving. Previous adversarial attacks against ODs have been focused\non designing customized attacks targeting their specific structures (e.g., NMS\nand RPN), yielding some results but simultaneously constraining their\nscalability. Moreover, most efforts against ODs stem from image-level attacks\noriginally designed for classification tasks, resulting in redundant\ncomputations and disturbances in object-irrelevant areas (e.g., background).\nConsequently, how to design a model-agnostic efficient attack to\ncomprehensively evaluate the vulnerabilities of ODs remains challenging and\nunresolved. In this paper, we propose NumbOD, a brand-new spatial-frequency\nfusion attack against various ODs, aimed at disrupting object detection within\nimages. We directly leverage the features output by the OD without relying on\nits internal structures to craft adversarial examples. Specifically, we first\ndesign a dual-track attack target selection strategy to select high-quality\nbounding boxes from OD outputs for targeting. Subsequently, we employ\ndirectional perturbations to shift and compress predicted boxes and change\nclassification results to deceive ODs. Additionally, we focus on manipulating\nthe high-frequency components of images to confuse ODs' attention on critical\nobjects, thereby enhancing the attack efficiency. Our extensive experiments on\nnine ODs and two datasets show that NumbOD achieves powerful attack performance\nand high stealthiness.\n","authors":["Ziqi Zhou","Bowen Li","Yufei Song","Zhifei Yu","Shengshan Hu","Wei Wan","Leo Yu Zhang","Dezhong Yao","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2412.16955v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2407.15842v2","updated":"2024-12-22T10:03:12Z","published":"2024-07-22T17:58:05Z","title":"DiffArtist: Towards Aesthetic-Aligned Diffusion Model Control for\n  Training-free Text-Driven Stylization","summary":"  Diffusion models entangle content and style generation during the denoising\nprocess, leading to undesired content modification or insufficient style\nstrength when directly applied to stylization tasks. Existing methods struggle\nto effectively control the diffusion model to meet the aesthetic-level\nrequirements for stylization. In this paper, we introduce DiffArtist, the first\napproach that enables aesthetic-aligned control of content and style during the\nentire diffusion process, without additional training. Our key insight is to\ndesign disentangled representations for content and style in the noise space.\nBy sharing features between content and style representations, we enable\nfine-grained control of structural and appearance-level style strength without\ncompromising visual-appeal. We further propose Vision-Language Model\n(VLM)-based evaluation metrics for stylization, which align better with human\npreferences. Extensive experiments demonstrate that DiffArtist outperforms\nexisting methods in alignment with human preferences and offers enhanced\ncontrollability. Project homepage: https://DiffusionArtist.github.io\n","authors":["Ruixiang Jiang","Changwen Chen"],"pdf_url":"https://arxiv.org/pdf/2407.15842v2.pdf","comment":"Homepage: https://DiffusionArtist.github.io"},{"id":"http://arxiv.org/abs/2412.16948v1","updated":"2024-12-22T09:49:48Z","published":"2024-12-22T09:49:48Z","title":"DTSGAN: Learning Dynamic Textures via Spatiotemporal Generative\n  Adversarial Network","summary":"  Dynamic texture synthesis aims to generate sequences that are visually\nsimilar to a reference video texture and exhibit specific stationary properties\nin time. In this paper, we introduce a spatiotemporal generative adversarial\nnetwork (DTSGAN) that can learn from a single dynamic texture by capturing its\nmotion and content distribution. With the pipeline of DTSGAN, a new video\nsequence is generated from the coarsest scale to the finest one. To avoid mode\ncollapse, we propose a novel strategy for data updates that helps improve the\ndiversity of generated results. Qualitative and quantitative experiments show\nthat our model is able to generate high quality dynamic textures and natural\nmotion.\n","authors":["Xiangtian Li","Xiaobo Wang","Zhen Qi","Han Cao","Zhaoyang Zhang","Ao Xiang"],"pdf_url":"https://arxiv.org/pdf/2412.16948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16947v1","updated":"2024-12-22T09:44:43Z","published":"2024-12-22T09:44:43Z","title":"Separating Drone Point Clouds From Complex Backgrounds by Cluster Filter\n  -- Technical Report for CVPR 2024 UG2 Challenge","summary":"  The increasing deployment of small drones as tools of conflict and disruption\nhas amplified their threat, highlighting the urgent need for effective\nanti-drone measures. However, the compact size of most drones presents a\nsignificant challenge, as traditional supervised point cloud or image-based\nobject detection methods often fail to identify such small objects effectively.\nThis paper proposes a simple UAV detection method using an unsupervised\npipeline. It uses spatial-temporal sequence processing to fuse multiple lidar\ndatasets effectively, tracking and determining the position of UAVs, so as to\ndetect and track UAVs in challenging environments. Our method performs front\nand rear background segmentation of point clouds through a global-local\nsequence clusterer and parses point cloud data from both the spatial-temporal\ndensity and spatial-temporal voxels of the point cloud. Furthermore, a scoring\nmechanism for point cloud moving targets is proposed, using time series\ndetection to improve accuracy and efficiency. We used the MMAUD dataset, and\nour method achieved 4th place in the CVPR 2024 UG2+ Challenge, confirming the\neffectiveness of our method in practical applications.\n","authors":["Hanfang Liang","Jinming Hu","Xiaohuan Ling","Bing Wang"],"pdf_url":"https://arxiv.org/pdf/2412.16947v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.16946v1","updated":"2024-12-22T09:40:48Z","published":"2024-12-22T09:40:48Z","title":"Video Domain Incremental Learning for Human Action Recognition in Home\n  Environments","summary":"  It is significantly challenging to recognize daily human actions in homes due\nto the diversity and dynamic changes in unconstrained home environments. It\nspurs the need to continually adapt to various users and scenes. Fine-tuning\ncurrent video understanding models on newly encountered domains often leads to\ncatastrophic forgetting, where the models lose their ability to perform well on\npreviously learned scenarios. To address this issue, we formalize the problem\nof Video Domain Incremental Learning (VDIL), which enables models to learn\ncontinually from different domains while maintaining a fixed set of action\nclasses. Existing continual learning research primarily focuses on\nclass-incremental learning, while the domain incremental learning has been\nlargely overlooked in video understanding. In this work, we introduce a novel\nbenchmark of domain incremental human action recognition for unconstrained home\nenvironments. We design three domain split types (user, scene, hybrid) to\nsystematically assess the challenges posed by domain shifts in real-world home\nsettings. Furthermore, we propose a baseline learning strategy based on replay\nand reservoir sampling techniques without domain labels to handle scenarios\nwith limited memory and task agnosticism. Extensive experimental results\ndemonstrate that our simple sampling and replay strategy outperforms most\nexisting continual learning methods across the three proposed benchmarks.\n","authors":["Yuanda Hu","Xing Liu","Meiying Li","Yate Ge","Xiaohua Sun","Weiwei Guo"],"pdf_url":"https://arxiv.org/pdf/2412.16946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07541v2","updated":"2024-12-22T09:36:16Z","published":"2024-11-12T04:40:27Z","title":"HiCoM: Hierarchical Coherent Motion for Streamable Dynamic Scene with 3D\n  Gaussian Splatting","summary":"  The online reconstruction of dynamic scenes from multi-view streaming videos\nfaces significant challenges in training, rendering and storage efficiency.\nHarnessing superior learning speed and real-time rendering capabilities, 3D\nGaussian Splatting (3DGS) has recently demonstrated considerable potential in\nthis field. However, 3DGS can be inefficient in terms of storage and prone to\noverfitting by excessively growing Gaussians, particularly with limited views.\nThis paper proposes an efficient framework, dubbed HiCoM, with three key\ncomponents. First, we construct a compact and robust initial 3DGS\nrepresentation using a perturbation smoothing strategy. Next, we introduce a\nHierarchical Coherent Motion mechanism that leverages the inherent non-uniform\ndistribution and local consistency of 3D Gaussians to swiftly and accurately\nlearn motions across frames. Finally, we continually refine the 3DGS with\nadditional Gaussians, which are later merged into the initial 3DGS to maintain\nconsistency with the evolving scene. To preserve a compact representation, an\nequivalent number of low-opacity Gaussians that minimally impact the\nrepresentation are removed before processing subsequent frames. Extensive\nexperiments conducted on two widely used datasets show that our framework\nimproves learning efficiency of the state-of-the-art methods by about $20\\%$\nand reduces the data storage by $85\\%$, achieving competitive free-viewpoint\nvideo synthesis quality but with higher robustness and stability. Moreover, by\nparallel learning multiple frames simultaneously, our HiCoM decreases the\naverage training wall time to $<2$ seconds per frame with negligible\nperformance degradation, substantially boosting real-world applicability and\nresponsiveness.\n","authors":["Qiankun Gao","Jiarui Meng","Chengxiang Wen","Jie Chen","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.07541v2.pdf","comment":"Accepted to NeurIPS 2024; Code is avaliable at\n  https://github.com/gqk/HiCoM"},{"id":"http://arxiv.org/abs/2412.16944v1","updated":"2024-12-22T09:28:06Z","published":"2024-12-22T09:28:06Z","title":"Linguistics-Vision Monotonic Consistent Network for Sign Language\n  Production","summary":"  Sign Language Production (SLP) aims to generate sign videos corresponding to\nspoken language sentences, where the conversion of sign Glosses to Poses (G2P)\nis the key step. Due to the cross-modal semantic gap and the lack of\nword-action correspondence labels for strong supervision alignment, the SLP\nsuffers huge challenges in linguistics-vision consistency. In this work, we\npropose a Transformer-based Linguistics-Vision Monotonic Consistent Network\n(LVMCN) for SLP, which constrains fine-grained cross-modal monotonic alignment\nand coarse-grained multimodal semantic consistency in language-visual cues\nthrough Cross-modal Semantic Aligner (CSA) and Multimodal Semantic Comparator\n(MSC). In the CSA, we constrain the implicit alignment between corresponding\ngloss and pose sequences by computing the cosine similarity association matrix\nbetween cross-modal feature sequences (i.e., the order consistency of\nfine-grained sign glosses and actions). As for MSC, we construct multimodal\ntriplets based on paired and unpaired samples in batch data. By pulling closer\nthe corresponding text-visual pairs and pushing apart the non-corresponding\ntext-visual pairs, we constrain the semantic co-occurrence degree between\ncorresponding gloss and pose sequences (i.e., the semantic consistency of\ncoarse-grained textual sentences and sign videos). Extensive experiments on the\npopular PHOENIX14T benchmark show that the LVMCN outperforms the\nstate-of-the-art.\n","authors":["Xu Wang","Shengeng Tang","Peipei Song","Shuo Wang","Dan Guo","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2412.16944v1.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2412.16942v1","updated":"2024-12-22T09:18:58Z","published":"2024-12-22T09:18:58Z","title":"BloomCoreset: Fast Coreset Sampling using Bloom Filters for Fine-Grained\n  Self-Supervised Learning","summary":"  The success of deep learning in supervised fine-grained recognition for\ndomain-specific tasks relies heavily on expert annotations. The Open-Set for\nfine-grained Self-Supervised Learning (SSL) problem aims to enhance performance\non downstream tasks by strategically sampling a subset of images (the Core-Set)\nfrom a large pool of unlabeled data (the Open-Set). In this paper, we propose a\nnovel method, BloomCoreset, that significantly reduces sampling time from\nOpen-Set while preserving the quality of samples in the coreset. To achieve\nthis, we utilize Bloom filters as an innovative hashing mechanism to store both\nlow- and high-level features of the fine-grained dataset, as captured by\nOpen-CLIP, in a space-efficient manner that enables rapid retrieval of the\ncoreset from the Open-Set. To show the effectiveness of the sampled coreset, we\nintegrate the proposed method into the state-of-the-art fine-grained SSL\nframework, SimCore [1]. The proposed algorithm drastically outperforms the\nsampling strategy of the baseline in SimCore [1] with a $98.5\\%$ reduction in\nsampling time with a mere $0.83\\%$ average trade-off in accuracy calculated\nacross $11$ downstream datasets.\n","authors":["Prajwal Singh","Gautam Vashishtha","Indra Deep Mastan","Shanmuganathan Raman"],"pdf_url":"https://arxiv.org/pdf/2412.16942v1.pdf","comment":"Accepted at ICASSP 2025"},{"id":"http://arxiv.org/abs/2412.16939v1","updated":"2024-12-22T09:17:57Z","published":"2024-12-22T09:17:57Z","title":"Image Quality Assessment: Investigating Causal Perceptual Effects with\n  Abductive Counterfactual Inference","summary":"  Existing full-reference image quality assessment (FR-IQA) methods often fail\nto capture the complex causal mechanisms that underlie human perceptual\nresponses to image distortions, limiting their ability to generalize across\ndiverse scenarios. In this paper, we propose an FR-IQA method based on\nabductive counterfactual inference to investigate the causal relationships\nbetween deep network features and perceptual distortions. First, we explore the\ncausal effects of deep features on perception and integrate causal reasoning\nwith feature comparison, constructing a model that effectively handles complex\ndistortion types across different IQA scenarios. Second, the analysis of the\nperceptual causal correlations of our proposed method is independent of the\nbackbone architecture and thus can be applied to a variety of deep networks.\nThrough abductive counterfactual experiments, we validate the proposed causal\nrelationships, confirming the model's superior perceptual relevance and\ninterpretability of quality scores. The experimental results demonstrate the\nrobustness and effectiveness of the method, providing competitive quality\npredictions across multiple benchmarks. The source code is available at\nhttps://anonymous.4open.science/r/DeepCausalQuality-25BC.\n","authors":["Wenhao Shen","Mingliang Zhou","Yu Chen","Xuekai Wei","Jun Luo","Huayan Pu","Weijia Jia"],"pdf_url":"https://arxiv.org/pdf/2412.16939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16938v1","updated":"2024-12-22T09:17:08Z","published":"2024-12-22T09:17:08Z","title":"ImagineMap: Enhanced HD Map Construction with SD Maps","summary":"  Track Mapless demands models to process multi-view images and\nStandard-Definition (SD) maps, outputting lane and traffic element perceptions\nalong with their topological relationships. We propose a novel architecture\nthat integrates SD map priors to improve lane line and area detection\nperformance. Inspired by TopoMLP, our model employs a two-stage structure:\nperception and reasoning. The downstream topology head uses the output from the\nupstream detection head, meaning accuracy improvements in detection\nsignificantly boost downstream performance.\n","authors":["Yishen Ji","Zhiqi Li","Tong Lu"],"pdf_url":"https://arxiv.org/pdf/2412.16938v1.pdf","comment":"4 pages, 1 figures, technical report"},{"id":"http://arxiv.org/abs/2412.16937v1","updated":"2024-12-22T09:16:00Z","published":"2024-12-22T09:16:00Z","title":"PINN-EMFNet: PINN-based and Enhanced Multi-Scale Feature Fusion Network\n  for Breast Ultrasound Images Segmentation","summary":"  With the rapid development of deep learning and computer vision technologies,\nmedical image segmentation plays a crucial role in the early diagnosis of\nbreast cancer. However, due to the characteristics of breast ultrasound images,\nsuch as low contrast, speckle noise, and the highly diverse morphology of\ntumors, existing segmentation methods exhibit significant limitations in terms\nof accuracy and robustness. To address these challenges, this study proposes a\nPINN-based and Enhanced Multi-Scale Feature Fusion Network. The network\nintroduces a Hierarchical Aggregation Encoder in the backbone, which\nefficiently integrates and globally models multi-scale features through several\nstructural innovations and a novel PCAM module. In the decoder section, a\nMulti-Scale Feature Refinement Decoder is employed, which, combined with a\nMulti-Scale Supervision Mechanism and a correction module, significantly\nimproves segmentation accuracy and adaptability. Additionally, the loss\nfunction incorporating the PINN mechanism introduces physical constraints\nduring the segmentation process, enhancing the model's ability to accurately\ndelineate tumor boundaries. Comprehensive evaluations on two publicly available\nbreast ultrasound datasets, BUSIS and BUSI, demonstrate that the proposed\nmethod outperforms previous segmentation approaches in terms of segmentation\naccuracy and robustness, particularly under conditions of complex noise and low\ncontrast, effectively improving the accuracy and reliability of tumor\nsegmentation. This method provides a more precise and robust solution for\ncomputer-aided diagnosis of breast ultrasound images.\n","authors":["Jiajun Ding","Beiyao Zhu","Wenjie Wang","Shurong Zhang","Dian Zhua","Zhao Liua"],"pdf_url":"https://arxiv.org/pdf/2412.16937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16935v1","updated":"2024-12-22T09:14:01Z","published":"2024-12-22T09:14:01Z","title":"Detecting and Classifying Defective Products in Images Using YOLO","summary":"  With the continuous advancement of industrial automation, product quality\ninspection has become increasingly important in the manufacturing process.\nTraditional inspection methods, which often rely on manual checks or simple\nmachine vision techniques, suffer from low efficiency and insufficient\naccuracy. In recent years, deep learning technology, especially the YOLO (You\nOnly Look Once) algorithm, has emerged as a prominent solution in the field of\nproduct defect detection due to its efficient real-time detection capabilities\nand excellent classification performance. This study aims to use the YOLO\nalgorithm to detect and classify defects in product images. By constructing and\ntraining a YOLO model, we conducted experiments on multiple industrial product\ndatasets. The results demonstrate that this method can achieve real-time\ndetection while maintaining high detection accuracy, significantly improving\nthe efficiency and accuracy of product quality inspection. This paper further\nanalyzes the advantages and limitations of the YOLO algorithm in practical\napplications and explores future research directions.\n","authors":["Zhen Qi","Liwei Ding","Xiangtian Li","Jiacheng Hu","Bin Lyu","Ao Xiang"],"pdf_url":"https://arxiv.org/pdf/2412.16935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16932v1","updated":"2024-12-22T09:06:58Z","published":"2024-12-22T09:06:58Z","title":"GSemSplat: Generalizable Semantic 3D Gaussian Splatting from\n  Uncalibrated Image Pairs","summary":"  Modeling and understanding the 3D world is crucial for various applications,\nfrom augmented reality to robotic navigation. Recent advancements based on 3D\nGaussian Splatting have integrated semantic information from multi-view images\ninto Gaussian primitives. However, these methods typically require costly\nper-scene optimization from dense calibrated images, limiting their\npracticality. In this paper, we consider the new task of generalizable 3D\nsemantic field modeling from sparse, uncalibrated image pairs. Building upon\nthe Splatt3R architecture, we introduce GSemSplat, a framework that learns\nopen-vocabulary semantic representations linked to 3D Gaussians without the\nneed for per-scene optimization, dense image collections or calibration. To\nensure effective and reliable learning of semantic features in 3D space, we\nemploy a dual-feature approach that leverages both region-specific and\ncontext-aware semantic features as supervision in the 2D space. This allows us\nto capitalize on their complementary strengths. Experimental results on the\nScanNet++ dataset demonstrate the effectiveness and superiority of our approach\ncompared to the traditional scene-specific method. We hope our work will\ninspire more research into generalizable 3D understanding.\n","authors":["Xingrui Wang","Cuiling Lan","Hanxin Zhu","Zhibo Chen","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2412.16932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16928v1","updated":"2024-12-22T08:58:15Z","published":"2024-12-22T08:58:15Z","title":"AV-DTEC: Self-Supervised Audio-Visual Fusion for Drone Trajectory\n  Estimation and Classification","summary":"  The increasing use of compact UAVs has created significant threats to public\nsafety, while traditional drone detection systems are often bulky and costly.\nTo address these challenges, we propose AV-DTEC, a lightweight self-supervised\naudio-visual fusion-based anti-UAV system. AV-DTEC is trained using\nself-supervised learning with labels generated by LiDAR, and it simultaneously\nlearns audio and visual features through a parallel selective state-space\nmodel. With the learned features, a specially designed plug-and-play\nprimary-auxiliary feature enhancement module integrates visual features into\naudio features for better robustness in cross-lighting conditions. To reduce\nreliance on auxiliary features and align modalities, we propose a\nteacher-student model that adaptively adjusts the weighting of visual features.\nAV-DTEC demonstrates exceptional accuracy and effectiveness in real-world\nmulti-modality data. The code and trained models are publicly accessible on\nGitHub\n  \\url{https://github.com/AmazingDay1/AV-DETC}.\n","authors":["Zhenyuan Xiao","Yizhuo Yang","Guili Xu","Xianglong Zeng","Shenghai Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.16928v1.pdf","comment":"Submitted to ICRA 2025"},{"id":"http://arxiv.org/abs/2412.16923v1","updated":"2024-12-22T08:47:13Z","published":"2024-12-22T08:47:13Z","title":"Leveraging Consistent Spatio-Temporal Correspondence for Robust Visual\n  Odometry","summary":"  Recent approaches to VO have significantly improved performance by using deep\nnetworks to predict optical flow between video frames. However, existing\nmethods still suffer from noisy and inconsistent flow matching, making it\ndifficult to handle challenging scenarios and long-sequence estimation. To\novercome these challenges, we introduce Spatio-Temporal Visual Odometry (STVO),\na novel deep network architecture that effectively leverages inherent\nspatio-temporal cues to enhance the accuracy and consistency of multi-frame\nflow matching. With more accurate and consistent flow matching, STVO can\nachieve better pose estimation through the bundle adjustment (BA).\nSpecifically, STVO introduces two innovative components: 1) the Temporal\nPropagation Module that utilizes multi-frame information to extract and\npropagate temporal cues across adjacent frames, maintaining temporal\nconsistency; 2) the Spatial Activation Module that utilizes geometric priors\nfrom the depth maps to enhance spatial consistency while filtering out\nexcessive noise and incorrect matches. Our STVO achieves state-of-the-art\nperformance on TUM-RGBD, EuRoc MAV, ETH3D and KITTI Odometry benchmarks.\nNotably, it improves accuracy by 77.8% on ETH3D benchmark and 38.9% on KITTI\nOdometry benchmark over the previous best methods.\n","authors":["Zhaoxing Zhang","Junda Cheng","Gangwei Xu","Xiaoxiang Wang","Can Zhang","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2412.16923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00705v3","updated":"2024-12-22T08:46:39Z","published":"2024-12-01T07:02:36Z","title":"Photoacoustic Iterative Optimization Algorithm with Shape Prior\n  Regularization","summary":"  Photoacoustic imaging (PAI) suffers from inherent limitations that can\ndegrade the quality of reconstructed results, such as noise, artifacts and\nincomplete data acquisition caused by sparse sampling or partial array\ndetection. In this study, we proposed a new optimization method for both\ntwo-dimensional (2D) and three-dimensional (3D) PAI reconstruction results,\ncalled the regularized iteration method with shape prior. The shape prior is a\nprobability matrix derived from the reconstruction results of multiple sets of\nrandom partial array signals in a computational imaging system using any\nreconstruction algorithm, such as Delay-and-Sum (DAS) and Back-Projection (BP).\nIn the probability matrix, high-probability locations indicate high consistency\namong multiple reconstruction results at those positions, suggesting a high\nlikelihood of representing the true imaging results. In contrast,\nlow-probability locations indicate higher randomness, leaning more towards\nnoise or artifacts. As a shape prior, this probability matrix guides the\niteration and regularization of the entire array signal reconstruction results\nusing the original reconstruction algorithm (the same algorithm for processing\nrandom partial array signals). The method takes advantage of the property that\nthe similarity of the object to be imitated is higher than that of noise or\nartifact in the results reconstructed by multiple sets of random partial array\nsignals of the entire imaging system. The probability matrix is taken as a\nprerequisite for improving the original reconstruction results, and the\noptimizer is used to further iterate the imaging results to remove noise and\nartifacts and improve the imaging fidelity. Especially in the case involving\nsparse view which brings more artifacts, the effect is remarkable. Simulation\nand real experiments have both demonstrated the superiority of this method.\n","authors":["Yu Zhang","Shuang Li","Yibing Wang","Yu Sun","Wenyi Xiang"],"pdf_url":"https://arxiv.org/pdf/2412.00705v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12164v3","updated":"2024-12-22T08:42:13Z","published":"2024-07-16T20:40:25Z","title":"Subject-driven Text-to-Image Generation via Preference-based\n  Reinforcement Learning","summary":"  Text-to-image generative models have recently attracted considerable\ninterest, enabling the synthesis of high-quality images from textual prompts.\nHowever, these models often lack the capability to generate specific subjects\nfrom given reference images or to synthesize novel renditions under varying\nconditions. Methods like DreamBooth and Subject-driven Text-to-Image (SuTI)\nhave made significant progress in this area. Yet, both approaches primarily\nfocus on enhancing similarity to reference images and require expensive setups,\noften overlooking the need for efficient training and avoiding overfitting to\nthe reference images. In this work, we present the $\\lambda$-Harmonic reward\nfunction, which provides a reliable reward signal and enables early stopping\nfor faster training and effective regularization. By combining the\nBradley-Terry preference model, the $\\lambda$-Harmonic reward function also\nprovides preference labels for subject-driven generation tasks. We propose\nReward Preference Optimization (RPO), which offers a simpler setup (requiring\nonly $3\\%$ of the negative samples used by DreamBooth) and fewer gradient steps\nfor fine-tuning. Unlike most existing methods, our approach does not require\ntraining a text encoder or optimizing text embeddings and achieves text-image\nalignment by fine-tuning only the U-Net component. Empirically,\n$\\lambda$-Harmonic proves to be a reliable approach for model selection in\nsubject-driven generation tasks. Based on preference labels and early stopping\nvalidation from the $\\lambda$-Harmonic reward function, our algorithm achieves\na state-of-the-art CLIP-I score of 0.833 and a CLIP-T score of 0.314 on\nDreamBench.\n","authors":["Yanting Miao","William Loh","Suraj Kothawade","Pascal Poupart","Abdullah Rashwan","Yeqing Li"],"pdf_url":"https://arxiv.org/pdf/2407.12164v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.16919v1","updated":"2024-12-22T08:28:20Z","published":"2024-12-22T08:28:20Z","title":"TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction","summary":"  We present TAR3D, a novel framework that consists of a 3D-aware Vector\nQuantized-Variational AutoEncoder (VQ-VAE) and a Generative Pre-trained\nTransformer (GPT) to generate high-quality 3D assets. The core insight of this\nwork is to migrate the multimodal unification and promising learning\ncapabilities of the next-token prediction paradigm to conditional 3D object\ngeneration. To achieve this, the 3D VQ-VAE first encodes a wide range of 3D\nshapes into a compact triplane latent space and utilizes a set of discrete\nrepresentations from a trainable codebook to reconstruct fine-grained\ngeometries under the supervision of query point occupancy. Then, the 3D GPT,\nequipped with a custom triplane position embedding called TriPE, predicts the\ncodebook index sequence with prefilling prompt tokens in an autoregressive\nmanner so that the composition of 3D geometries can be modeled part by part.\nExtensive experiments on ShapeNet and Objaverse demonstrate that TAR3D can\nachieve superior generation quality over existing methods in text-to-3D and\nimage-to-3D tasks\n","authors":["Xuying Zhang","Yutong Liu","Yangguang Li","Renrui Zhang","Yufei Liu","Kai Wang","Wanli Ouyang","Zhiwei Xiong","Peng Gao","Qibin Hou","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2412.16919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16918v1","updated":"2024-12-22T08:27:15Z","published":"2024-12-22T08:27:15Z","title":"Detect Changes like Humans: Incorporating Semantic Priors for Improved\n  Change Detection","summary":"  When given two similar images, humans identify their differences by comparing\nthe appearance ({\\it e.g., color, texture}) with the help of semantics ({\\it\ne.g., objects, relations}). However, mainstream change detection models adopt a\nsupervised training paradigm, where the annotated binary change map is the main\nconstraint. Thus, these methods primarily emphasize the difference-aware\nfeatures between bi-temporal images and neglect the semantic understanding of\nthe changed landscapes, which undermines the accuracy in the presence of noise\nand illumination variations. To this end, this paper explores incorporating\nsemantic priors to improve the ability to detect changes. Firstly, we propose a\nSemantic-Aware Change Detection network, namely SA-CDNet, which transfers the\ncommon knowledge of the visual foundation models ({\\it i.e., FastSAM}) to\nchange detection. Inspired by the human visual paradigm, a novel dual-stream\nfeature decoder is derived to distinguish changes by combining semantic-aware\nfeatures and difference-aware features. Secondly, we design a single-temporal\nsemantic pre-training strategy to enhance the semantic understanding of\nlandscapes, which brings further increments. Specifically, we construct\npseudo-change detection data from public single-temporal remote sensing\nsegmentation datasets for large-scale pre-training, where an extra branch is\nalso introduced for the proxy semantic segmentation task. Experimental results\non five challenging benchmarks demonstrate the superiority of our method over\nthe existing state-of-the-art methods. The code is available at\n\\href{https://github.com/thislzm/SA-CD}{SA-CD}.\n","authors":["Yuhang Gan","Wenjie Xuan","Zhiming Luo","Lei Fang","Zengmao Wang","Juhua Liu","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2412.16918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16915v1","updated":"2024-12-22T08:19:22Z","published":"2024-12-22T08:19:22Z","title":"FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG\n  Distillation","summary":"  Diffusion-based audio-driven talking avatar methods have recently gained\nattention for their high-fidelity, vivid, and expressive results. However,\ntheir slow inference speed limits practical applications. Despite the\ndevelopment of various distillation techniques for diffusion models, we found\nthat naive diffusion distillation methods do not yield satisfactory results.\nDistilled models exhibit reduced robustness with open-set input images and a\ndecreased correlation between audio and video compared to teacher models,\nundermining the advantages of diffusion models. To address this, we propose\nFADA (Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG\nDistillation). We first designed a mixed-supervised loss to leverage data of\nvarying quality and enhance the overall model capability as well as robustness.\nAdditionally, we propose a multi-CFG distillation with learnable tokens to\nutilize the correlation between audio and reference image conditions, reducing\nthe threefold inference runs caused by multi-CFG with acceptable quality\ndegradation. Extensive experiments across multiple datasets show that FADA\ngenerates vivid videos comparable to recent diffusion model-based methods while\nachieving an NFE speedup of 4.17-12.5 times. Demos are available at our webpage\nhttp://fadavatar.github.io.\n","authors":["Tianyun Zhong","Chao Liang","Jianwen Jiang","Gaojie Lin","Jiaqi Yang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.16915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00320v4","updated":"2024-12-22T08:19:07Z","published":"2024-10-01T01:40:22Z","title":"PointAD: Comprehending 3D Anomalies from Points and Pixels for Zero-shot\n  3D Anomaly Detection","summary":"  Zero-shot (ZS) 3D anomaly detection is a crucial yet unexplored field that\naddresses scenarios where target 3D training samples are unavailable due to\npractical concerns like privacy protection. This paper introduces PointAD, a\nnovel approach that transfers the strong generalization capabilities of CLIP\nfor recognizing 3D anomalies on unseen objects. PointAD provides a unified\nframework to comprehend 3D anomalies from both points and pixels. In this\nframework, PointAD renders 3D anomalies into multiple 2D renderings and\nprojects them back into 3D space. To capture the generic anomaly semantics into\nPointAD, we propose hybrid representation learning that optimizes the learnable\ntext prompts from 3D and 2D through auxiliary point clouds. The collaboration\noptimization between point and pixel representations jointly facilitates our\nmodel to grasp underlying 3D anomaly patterns, contributing to detecting and\nsegmenting anomalies of unseen diverse 3D objects. Through the alignment of 3D\nand 2D space, our model can directly integrate RGB information, further\nenhancing the understanding of 3D anomalies in a plug-and-play manner.\nExtensive experiments show the superiority of PointAD in ZS 3D anomaly\ndetection across diverse unseen objects.\n","authors":["Qihang Zhou","Jiangtao Yan","Shibo He","Wenchao Meng","Jiming Chen"],"pdf_url":"https://arxiv.org/pdf/2410.00320v4.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2211.11188v3","updated":"2024-12-22T07:51:22Z","published":"2022-11-21T05:18:56Z","title":"Simultaneous Multiple Object Detection and Pose Estimation using 3D\n  Model Infusion with Monocular Vision","summary":"  Multiple object detection and pose estimation are vital computer vision\ntasks. The latter relates to the former as a downstream problem in applications\nsuch as robotics and autonomous driving. However, due to the high complexity of\nboth tasks, existing methods generally treat them independently, which is\nsub-optimal. We propose simultaneous neural modeling of both using monocular\nvision and 3D model infusion. Our Simultaneous Multiple Object detection and\nPose Estimation network (SMOPE-Net) is an end-to-end trainable multitasking\nnetwork with a composite loss that also provides the advantages of anchor-free\ndetections for efficient downstream pose estimation. To enable the annotation\nof training data for our learning objective, we develop a Twin-Space object\nlabeling method and demonstrate its correctness analytically and empirically.\nUsing the labeling method, we provide the KITTI-6DoF dataset with $\\sim7.5$K\nannotated frames. Extensive experiments on KITTI-6DoF and the popular LineMod\ndatasets show a consistent performance gain with SMOPE-Net over existing pose\nestimation methods. Here are links to our proposed SMOPE-Net, KITTI-6DoF\ndataset, and LabelImg3D labeling tool.\n","authors":["Congliang Li","Shijie Sun","Xiangyu Song","Huansheng Song","Naveed Akhtar","Ajmal Saeed Mian"],"pdf_url":"https://arxiv.org/pdf/2211.11188v3.pdf","comment":"Thesis needs further optimization"},{"id":"http://arxiv.org/abs/2412.16906v1","updated":"2024-12-22T07:48:49Z","published":"2024-12-22T07:48:49Z","title":"Self-Corrected Flow Distillation for Consistent One-Step and Few-Step\n  Text-to-Image Generation","summary":"  Flow matching has emerged as a promising framework for training generative\nmodels, demonstrating impressive empirical performance while offering relative\nease of training compared to diffusion-based models. However, this method still\nrequires numerous function evaluations in the sampling process. To address\nthese limitations, we introduce a self-corrected flow distillation method that\neffectively integrates consistency models and adversarial training within the\nflow-matching framework. This work is a pioneer in achieving consistent\ngeneration quality in both few-step and one-step sampling. Our extensive\nexperiments validate the effectiveness of our method, yielding superior results\nboth quantitatively and qualitatively on CelebA-HQ and zero-shot benchmarks on\nthe COCO dataset. Our implementation is released at\nhttps://github.com/VinAIResearch/SCFlow\n","authors":["Quan Dao","Hao Phung","Trung Dao","Dimitris Metaxas","Anh Tran"],"pdf_url":"https://arxiv.org/pdf/2412.16906v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2408.10178v2","updated":"2024-12-22T07:24:09Z","published":"2024-08-19T17:36:35Z","title":"NeuRodin: A Two-stage Framework for High-Fidelity Neural Surface\n  Reconstruction","summary":"  Signed Distance Function (SDF)-based volume rendering has demonstrated\nsignificant capabilities in surface reconstruction. Although promising,\nSDF-based methods often fail to capture detailed geometric structures,\nresulting in visible defects. By comparing SDF-based volume rendering to\ndensity-based volume rendering, we identify two main factors within the\nSDF-based approach that degrade surface quality: SDF-to-density representation\nand geometric regularization. These factors introduce challenges that hinder\nthe optimization of the SDF field. To address these issues, we introduce\nNeuRodin, a novel two-stage neural surface reconstruction framework that not\nonly achieves high-fidelity surface reconstruction but also retains the\nflexible optimization characteristics of density-based methods. NeuRodin\nincorporates innovative strategies that facilitate transformation of arbitrary\ntopologies and reduce artifacts associated with density bias. Extensive\nevaluations on the Tanks and Temples and ScanNet++ datasets demonstrate the\nsuperiority of NeuRodin, showing strong reconstruction capabilities for both\nindoor and outdoor environments using solely posed RGB captures. Project\nwebsite: https://open3dvlab.github.io/NeuRodin/\n","authors":["Yifan Wang","Di Huang","Weicai Ye","Guofeng Zhang","Wanli Ouyang","Tong He"],"pdf_url":"https://arxiv.org/pdf/2408.10178v2.pdf","comment":null}]}}